<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>首页 on LordFoxFairy的笔记本</title><link>https://LordFoxFairy.github.io/</link><description>Recent content in 首页 on LordFoxFairy的笔记本</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://LordFoxFairy.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>最佳提示词</title><link>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/1.-%E6%9C%80%E4%BD%B3%E6%8F%90%E7%A4%BA%E8%AF%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/1.-%E6%9C%80%E4%BD%B3%E6%8F%90%E7%A4%BA%E8%AF%8D/</guid><description/></item><item><title>第01章 机器学习概览</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/</guid><description>&lt;h1 id="第01章机器学习概览"&gt;第01章：机器学习概览&lt;a class="anchor" href="#%e7%ac%ac01%e7%ab%a0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%a7%88"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;All models are wrong, but some are useful.&amp;rdquo; —— George Box&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：本章不仅是概念的堆砌，更是&lt;strong&gt;世界观&lt;/strong&gt;的建立。&lt;/p&gt;
&lt;p&gt;我们将深入探讨&lt;strong&gt;频率派与贝叶斯派&lt;/strong&gt;的百年纠葛，这不仅仅是数学流派之争，更是我们认知世界的两种底层逻辑。此外，我们还将突破传统的教科书，带你领略现代深度学习中颠覆性的**&amp;ldquo;双下降&amp;rdquo; (Double Descent)** 现象，看看传统理论在过参数化时代是如何被挑战的。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e4%b8%96%e7%95%8c%e8%a7%82%e7%9a%84%e7%a2%b0%e6%92%9e%e9%a2%91%e7%8e%87%e6%b4%be-vs-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be"&gt;一、世界观的碰撞：频率派 vs 贝叶斯派&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e9%a2%91%e7%8e%87%e6%b4%be-the-frequentist-view"&gt;1.1 频率派 (The Frequentist View)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be-the-bayesian-view"&gt;1.2 贝叶斯派 (The Bayesian View)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e6%a0%b8%e5%bf%83%e6%a1%88%e4%be%8b%e6%8e%a8%e5%af%bc%e6%8a%9b%e7%a1%ac%e5%b8%81%e7%9a%84%e5%93%b2%e5%ad%a6"&gt;1.3 核心案例推导：抛硬币的哲学&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e4%b8%89%e8%a6%81%e7%b4%a0%e8%a7%a3%e6%9e%84%e7%ae%97%e6%b3%95%e7%9a%84%e4%b8%87%e8%83%bd%e5%85%ac%e5%bc%8f"&gt;二、统计学习三要素：解构算法的万能公式&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%a8%a1%e5%9e%8b-model"&gt;2.1 模型 (Model)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e7%ad%96%e7%95%a5-strategy"&gt;2.2 策略 (Strategy)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e6%95%b0%e5%ad%a6%e8%af%81%e6%98%8e%e4%b8%ba%e4%bb%80%e4%b9%88%e6%ad%a3%e5%88%99%e5%8c%96%e7%ad%89%e4%bb%b7%e4%ba%8e%e5%85%88%e9%aa%8c"&gt;2.3 数学证明：为什么正则化等价于先验？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#24-%e7%ae%97%e6%b3%95-algorithm"&gt;2.4 算法 (Algorithm)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e6%a0%b8%e5%bf%83%e9%9a%be%e9%a2%98%e5%81%8f%e5%b7%ae-%e6%96%b9%e5%b7%ae%e6%9d%83%e8%a1%a1-bias-variance-tradeoff"&gt;三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%ae%9e%e6%88%98%e6%bc%94%e7%bb%83%e4%ba%b2%e7%9c%bc%e7%9b%ae%e7%9d%b9%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e6%ad%a3%e5%88%99%e5%8c%96"&gt;四、实战演练：亲眼目睹过拟合与正则化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%8b%93%e5%b1%95%e6%b7%b1%e5%85%a5%e5%bd%93%e4%bc%a0%e7%bb%9f%e7%90%86%e8%ae%ba%e5%a4%b1%e6%95%88%e5%8f%8c%e4%b8%8b%e9%99%8d%e7%8e%b0%e8%b1%a1"&gt;五、拓展深入：当传统理论失效——双下降现象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;六、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一世界观的碰撞频率派-vs-贝叶斯派"&gt;一、世界观的碰撞：频率派 vs 贝叶斯派&lt;a class="anchor" href="#%e4%b8%80%e4%b8%96%e7%95%8c%e8%a7%82%e7%9a%84%e7%a2%b0%e6%92%9e%e9%a2%91%e7%8e%87%e6%b4%be-vs-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;统计机器学习领域长期存在着两个对立统一的流派。理解这个对立，对后续理解正则化（Regularization）和概率图模型（PGM）至关重要。&lt;/p&gt;
&lt;h3 id="11-频率派-the-frequentist-view"&gt;1.1 频率派 (The Frequentist View)&lt;a class="anchor" href="#11-%e9%a2%91%e7%8e%87%e6%b4%be-the-frequentist-view"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;核心信仰&lt;/strong&gt;：&lt;strong&gt;世界是确定的&lt;/strong&gt;。参数 $\theta$ 是一个&lt;strong&gt;未知但固定的常量&lt;/strong&gt; (Unknown Constant)。虽然我们不知道它具体是多少，但它真真切切地在那里，不增不减。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;方法论&lt;/strong&gt;：&lt;strong&gt;极大似然估计 (MLE)&lt;/strong&gt;。
$$ \hat{\theta}&lt;em&gt;{MLE} = \arg\max&lt;/em&gt;{\theta} P(X|\theta) $$&lt;/p&gt;</description></item><item><title>第1章 Hugging Face生态全景</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/</guid><description>&lt;h1 id="第1章hugging-face-生态全景-the-complete-guide"&gt;第1章：Hugging Face 生态全景 (The Complete Guide)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0hugging-face-%e7%94%9f%e6%80%81%e5%85%a8%e6%99%af-the-complete-guide"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：这是构建 LLM 应用的基石。我们将深入 Hugging Face 生态的五大核心组件：&lt;code&gt;Transformers&lt;/code&gt;, &lt;code&gt;Datasets&lt;/code&gt;, &lt;code&gt;Tokenizers&lt;/code&gt;, &lt;code&gt;Accelerate&lt;/code&gt;, &lt;code&gt;Hub&lt;/code&gt;。不仅覆盖基础 API，更包含&lt;strong&gt;量化加载&lt;/strong&gt;、&lt;strong&gt;词表扩充&lt;/strong&gt;、&lt;strong&gt;断点续训&lt;/strong&gt;、&lt;strong&gt;分布式配置&lt;/strong&gt;等工业级实战技巧。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-transformers%e6%a8%a1%e5%9e%8b%e5%8a%a0%e8%bd%bd%e4%b8%8e%e6%8e%a8%e7%90%86"&gt;1. Transformers：模型加载与推理&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-pipeline%e6%9e%81%e9%80%9f%e9%aa%8c%e8%af%81"&gt;1.1 Pipeline：极速验证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-autoclass%e5%ba%95%e5%b1%82%e6%8e%a7%e5%88%b6%e4%b8%8e-flash-attention"&gt;1.2 AutoClass：底层控制与 Flash Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-quantization4-bit8-bit-%e9%87%8f%e5%8c%96%e5%8a%a0%e8%bd%bd"&gt;1.3 Quantization：4-bit/8-bit 量化加载&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-datasets%e6%b5%b7%e9%87%8f%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b"&gt;2. Datasets：海量数据工程&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%b5%81%e5%bc%8f%e5%8a%a0%e8%bd%bd-streaming-%e4%b8%8e-%e6%b7%b7%e5%90%88-interleave"&gt;2.1 流式加载 (Streaming) 与 混合 (Interleave)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%b9%b6%e8%a1%8c%e5%a4%84%e7%90%86-map-%e4%b8%8e-%e6%95%b0%e6%8d%ae%e5%88%86%e7%89%87-sharding"&gt;2.2 并行处理 (Map) 与 数据分片 (Sharding)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e8%87%aa%e5%ae%9a%e4%b9%89%e6%95%b0%e6%8d%ae%e9%9b%86%e5%8a%a0%e8%bd%bd%e8%84%9a%e6%9c%ac"&gt;2.3 自定义数据集加载脚本&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-tokenizers%e5%88%86%e8%af%8d%e5%99%a8%e7%9a%84%e8%89%ba%e6%9c%af%e4%b8%8e%e9%99%b7%e9%98%b1"&gt;3. Tokenizers：分词器的艺术与陷阱&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-chat-template-%e5%8e%9f%e7%90%86%e5%a6%82%e4%bd%95%e9%81%bf%e5%85%8d%e7%ad%94%e9%9d%9e%e6%89%80%e9%97%ae"&gt;3.1 Chat Template 原理：如何避免&amp;quot;答非所问&amp;quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-padding-side%e5%b7%a6%e8%a1%a5%e9%bd%90-vs-%e5%8f%b3%e8%a1%a5%e9%bd%90"&gt;3.2 Padding Side：左补齐 vs 右补齐&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e5%ae%9e%e6%88%98%e6%89%a9%e5%85%85%e4%b8%ad%e6%96%87%e8%af%8d%e8%a1%a8-add-tokens"&gt;3.3 实战：扩充中文词表 (Add Tokens)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-training%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%88%86%e5%b8%83%e5%bc%8f"&gt;4. Training：训练与分布式&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-trainer-api-callbacks-%e4%b8%8e-%e6%96%ad%e7%82%b9%e7%bb%ad%e8%ae%ad"&gt;4.1 Trainer API：Callbacks 与 断点续训&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-accelerate--deepspeed%e5%88%86%e5%b8%83%e5%bc%8f%e9%85%8d%e7%bd%ae%e8%af%a6%e8%a7%a3"&gt;4.2 Accelerate + DeepSpeed：分布式配置详解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-hub%e6%a8%a1%e5%9e%8b%e7%ae%a1%e7%90%86%e4%b8%8e%e7%89%88%e6%9c%ac%e6%8e%a7%e5%88%b6"&gt;5. Hub：模型管理与版本控制&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e6%a8%a1%e5%9e%8b%e4%b8%8a%e4%bc%a0%e4%b8%8e-revision-%e9%94%81%e5%ae%9a"&gt;5.1 模型上传与 Revision 锁定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-model-card-%e7%bc%96%e5%86%99%e8%a7%84%e8%8c%83"&gt;5.2 Model Card 编写规范&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93%e5%bc%80%e5%8f%91%e6%b5%81-checklist"&gt;本章小结：开发流 CheckList&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-transformers模型加载与推理"&gt;1. Transformers：模型加载与推理&lt;a class="anchor" href="#1-transformers%e6%a8%a1%e5%9e%8b%e5%8a%a0%e8%bd%bd%e4%b8%8e%e6%8e%a8%e7%90%86"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-pipeline极速验证"&gt;1.1 Pipeline：极速验证&lt;a class="anchor" href="#11-pipeline%e6%9e%81%e9%80%9f%e9%aa%8c%e8%af%81"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;适合快速测试模型能力。&lt;/p&gt;</description></item><item><title>第1章 Transformer核心揭秘</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/</guid><description>&lt;h1 id="第1章transformer核心揭秘-the-transformer-architecture"&gt;第1章：Transformer核心揭秘 (The Transformer Architecture)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0transformer%e6%a0%b8%e5%bf%83%e6%8f%ad%e7%a7%98-the-transformer-architecture"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Attention is all you need.&amp;rdquo; - Vaswani et al., 2017&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：本章是全书中&lt;strong&gt;唯一详细讲解Transformer架构&lt;/strong&gt;的章节。后续章节将直接引用本章内容，不再重复讲解核心机制。&lt;/p&gt;
&lt;p&gt;本章将带你深入Transformer的每一个核心组件，从数学原理到代码实现，从直觉理解到工程优化。掌握了这些，你就掌握了现代大语言模型的基石。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%ae%8f%e8%a7%82%e8%93%9d%e5%9b%be%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84"&gt;一、宏观蓝图：编码器-解码器架构&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%8e%9f%e5%a7%8btransformer%e7%bf%bb%e8%af%91%e6%9c%ba%e5%99%a8%e7%9a%84%e8%ae%be%e8%ae%a1"&gt;原始Transformer：翻译机器的设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-%e7%bc%96%e7%a0%81%e5%99%a8encoder%e7%90%86%e8%a7%a3%e8%be%93%e5%85%a5"&gt;1. 编码器（Encoder）：理解输入&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e8%a7%a3%e7%a0%81%e5%99%a8decoder%e7%94%9f%e6%88%90%e8%be%93%e5%87%ba"&gt;2. 解码器（Decoder）：生成输出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e4%bf%a1%e6%81%af%e6%b5%81%e5%8a%a8%e7%bc%96%e7%a0%81%e5%99%a8%e5%88%b0%e8%a7%a3%e7%a0%81%e5%99%a8"&gt;3. 信息流动：编码器到解码器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%8e%b0%e4%bb%a3%e7%ae%80%e5%8c%96%e4%b8%ba%e4%bd%95%e5%8f%aa%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8%e6%88%96%e8%a7%a3%e7%a0%81%e5%99%a8"&gt;现代简化：为何只用编码器或解码器？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%80%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6self-attention"&gt;二、核心组件一：自注意力机制（Self-Attention）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%bb%8e%e4%b8%80%e4%b8%aa%e9%97%ae%e9%a2%98%e5%bc%80%e5%a7%8b"&gt;1. 为什么需要自注意力？从一个问题开始&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3querykeyvalue"&gt;2. 核心思想：Query、Key、Value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc%e7%bc%a9%e6%94%be%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b"&gt;3. 公式推导：缩放点积注意力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e6%a6%82%e7%8e%87%e8%ae%ba%e8%a7%a3%e9%87%8a"&gt;4. 注意力的概率论解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%bb%8e%e9%9b%b6%e5%ae%9e%e7%8e%b0%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b"&gt;动手实践：从零实现自注意力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%8e%a9%e7%a0%81attention-mask"&gt;深入理解：注意力掩码（Attention Mask）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81positional-encoding"&gt;三、核心组件二：位置编码（Positional Encoding）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81"&gt;1. 为什么需要位置编码？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%ad%a3%e5%bc%a6%e4%bd%99%e5%bc%a6%e6%96%b9%e6%a1%88"&gt;2. 绝对位置编码：正弦余弦方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81rope"&gt;3. 相对位置编码：RoPE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e5%85%b6%e4%bb%96%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%96%b9%e6%a1%88"&gt;4. 其他位置编码方案&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%89%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6multi-head-attention"&gt;四、核心组件三：多头注意力机制（Multi-Head Attention）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e4%b8%aa%e5%a4%b4"&gt;1. 为什么需要多个头？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89"&gt;2. 多头注意力的数学定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-mha%e7%9a%84%e5%8f%98%e4%bd%93gqa%e4%b8%8emqa"&gt;3. MHA的变体：GQA与MQA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b"&gt;动手实践：实现多头注意力&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e5%9b%9b%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9cfeed-forward-network"&gt;五、核心组件四：前馈网络（Feed-Forward Network）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c%e7%9a%84%e7%bb%93%e6%9e%84"&gt;1. 前馈网络的结构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e9%80%89%e6%8b%a9"&gt;2. 激活函数的选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e7%8e%b0%e4%bb%a3%e5%8f%98%e4%bd%93swiglu"&gt;3. 现代变体：SwiGLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c"&gt;动手实践：实现前馈网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e7%bb%84%e8%a3%85%e8%bd%a6%e9%97%b4%e6%9e%84%e5%bb%ba%e5%ae%8c%e6%95%b4%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8%e4%b8%8e%e8%a7%a3%e7%a0%81%e5%99%a8"&gt;六、组装车间：构建完整的编码器与解码器&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5residual-connection"&gt;1. 残差连接（Residual Connection）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96layer-normalization"&gt;2. 层归一化（Layer Normalization）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%ae%8c%e6%95%b4%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8%e5%b1%82"&gt;3. 完整的编码器层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e5%ae%8c%e6%95%b4%e7%9a%84%e8%a7%a3%e7%a0%81%e5%99%a8%e5%b1%82"&gt;4. 完整的解码器层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e7%bb%84%e8%a3%85%e5%ae%8c%e6%95%b4transformer"&gt;动手实践：组装完整Transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e6%b7%b1%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%86%85%e9%83%a8%e7%9c%8b%e6%89%a7%e8%a1%8c"&gt;七、动手实践：深入模型内部看执行&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%8a%a0%e8%bd%bd%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e5%b9%b6%e5%88%86%e6%9e%90%e7%bb%93%e6%9e%84"&gt;1. 加载预训练模型并分析结构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%8f%af%e8%a7%86%e5%8c%96%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d"&gt;2. 可视化注意力权重&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%8e%a2%e7%b4%a2kv%e7%bc%93%e5%ad%98%e6%9c%ba%e5%88%b6"&gt;3. 探索KV缓存机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ab%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%bb%8e%e7%90%86%e8%ae%ba%e5%88%b0%e5%ae%9e%e8%b7%b5%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98"&gt;八、深度问答：从理论到实践的关键问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;本章概览&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第1章 初识大语言模型</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第1章初识大语言模型"&gt;第1章：初识大语言模型&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e5%88%9d%e8%af%86%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The best way to predict the future is to invent it.&amp;rdquo;
— &lt;strong&gt;Alan Kay&lt;/strong&gt;, 计算机科学家&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;本章承诺&lt;/strong&gt;：带你穿越NLP发展史，理解为什么我们需要大语言模型，以及它们如何从&amp;quot;词袋&amp;quot;进化到&amp;quot;大脑&amp;quot;。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%bc%95%e8%a8%80%e7%a9%bf%e8%b6%8anlp%e5%8f%91%e5%b1%95%e5%8f%b2"&gt;引言：穿越NLP发展史&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e4%b8%80%e6%ae%b5%e7%ae%80%e5%8f%b2%e4%bb%8e%e8%af%8d%e8%a2%8b%e5%88%b0%e5%a4%a7%e8%84%91"&gt;一、一段简史：从&amp;quot;词袋&amp;quot;到&amp;quot;大脑&amp;quot;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b1%80%e9%99%90"&gt;词袋模型的局限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%9b%b4%e5%a5%bd%e7%9a%84%e8%a1%a8%e7%a4%ba"&gt;为什么需要更好的表示&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e8%af%8d%e5%b5%8c%e5%85%a5%e8%ae%a9%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%90%86%e8%a7%a3%e5%9b%bd%e7%8e%8b-%e7%94%b7%e4%ba%ba%e5%a5%b3%e7%8e%8b"&gt;二、词嵌入：让计算机理解&amp;quot;国王-男人=女王&amp;quot;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%88%86%e5%b8%83%e5%bc%8f%e5%81%87%e8%ae%be%e7%89%a9%e4%bb%a5%e7%b1%bb%e8%81%9a%e8%af%8d%e4%bb%a5%e7%be%a4%e5%88%86"&gt;分布式假设：物以类聚，词以群分&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#word2vec%e7%a5%9e%e5%a5%87%e7%9a%84%e8%af%8d%e5%90%91%e9%87%8f"&gt;Word2Vec：神奇的词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e8%af%8d%e5%b5%8c%e5%85%a5%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7"&gt;词嵌入的局限性&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89transformer%e9%9d%a9%e5%91%bd%e4%bb%8e%e8%af%bb%e6%ad%bb%e4%b9%a6%e5%88%b0%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89"&gt;三、Transformer革命：从&amp;quot;读死书&amp;quot;到&amp;quot;举一反三&amp;quot;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rnn%e7%9a%84%e5%9b%b0%e5%a2%83%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1"&gt;RNN的困境：梯度消失&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#self-attention%e7%90%86%e8%a7%a3%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e8%89%ba%e6%9c%af"&gt;Self-Attention：理解上下文的艺术&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#encoder-vs-decoder%e4%b8%a4%e7%a7%8d%e6%80%9d%e7%bb%b4%e6%96%b9%e5%bc%8f"&gt;Encoder vs Decoder：两种思维方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e8%ae%a4%e8%af%86%e4%b8%a4%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%ae%b6%e6%97%8f"&gt;四、认识两大模型家族&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bert%e5%8f%8c%e5%90%91%e7%90%86%e8%a7%a3%e7%9a%84%e5%a4%a7%e5%b8%88"&gt;BERT：双向理解的大师&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gpt%e7%94%9f%e6%88%90%e5%bc%8f%e7%9a%84%e9%ad%94%e6%b3%95%e5%b8%88"&gt;GPT：生成式的魔法师&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%a4%e8%80%85%e7%9a%84%e5%8c%ba%e5%88%ab%e4%b8%8e%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af"&gt;两者的区别与适用场景&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%8e%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%af%b9%e8%af%9d"&gt;五、动手实践：与大模型对话&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%ae%9e%e6%88%98%e4%b8%80%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%e6%b5%81%e5%bc%8f%e8%be%93%e5%87%ba"&gt;实战一：文本生成（流式输出）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%ae%9e%e6%88%98%e4%ba%8c%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90"&gt;实战二：文本分类（情感分析）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%ae%9e%e6%88%98%e4%b8%89token%e8%ae%a1%e6%95%b0%e4%b8%8e%e6%88%90%e6%9c%ac%e4%bc%b0%e7%ae%97"&gt;实战三：Token计数与成本估算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94"&gt;六、新手问答&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;七、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="引言穿越nlp发展史"&gt;引言：穿越NLP发展史&lt;a class="anchor" href="#%e5%bc%95%e8%a8%80%e7%a9%bf%e8%b6%8anlp%e5%8f%91%e5%b1%95%e5%8f%b2"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;想象一下，你是一个从未接触过语言的外星人，突然被投放到地球。你看到人类用奇怪的符号（文字）交流，发出各种声音（语言）。你的任务是：&lt;strong&gt;理解并使用这些符号和声音&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这就是自然语言处理（NLP）面临的核心挑战：&lt;strong&gt;如何让计算机理解人类语言&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;在过去的几十年里，人类尝试了各种方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1950-1990年代&lt;/strong&gt;：基于规则的方法（专家系统）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1990-2010年代&lt;/strong&gt;：统计方法（词袋模型、n-gram）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2013-2017年&lt;/strong&gt;：词嵌入时代（Word2Vec、GloVe）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-2020年&lt;/strong&gt;：Transformer革命（BERT、GPT）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2020年至今&lt;/strong&gt;：大语言模型时代（GPT-3/4、Claude、ChatGPT）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;让我们一起回到起点，看看这段激动人心的演化历程。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="一一段简史从词袋到大脑"&gt;一、一段简史：从&amp;quot;词袋&amp;quot;到&amp;quot;大脑&amp;quot;&lt;a class="anchor" href="#%e4%b8%80%e4%b8%80%e6%ae%b5%e7%ae%80%e5%8f%b2%e4%bb%8e%e8%af%8d%e8%a2%8b%e5%88%b0%e5%a4%a7%e8%84%91"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="词袋模型的局限"&gt;词袋模型的局限&lt;a class="anchor" href="#%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b1%80%e9%99%90"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在深度学习出现之前，NLP领域最常用的方法是&lt;strong&gt;词袋模型&lt;/strong&gt;（Bag of Words, BoW）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：把文本看作一个&amp;quot;词的袋子&amp;quot;，只关心词出现的频率，不关心词的顺序。&lt;/p&gt;
&lt;h4 id="举个例子"&gt;举个例子&lt;a class="anchor" href="#%e4%b8%be%e4%b8%aa%e4%be%8b%e5%ad%90"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;句子1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;我爱自然语言处理&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;句子2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;自然语言处理爱我&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 词袋表示（词频统计）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;句子1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;我&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;爱&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;自然&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;语言&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;处理&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;句子2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;自然&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;语言&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;处理&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;爱&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;我&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 结果：两个句子完全相同！❌&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;问题显而易见&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第1章 提示工程与上下文学习</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="第1章提示工程与上下文学习-prompt-engineering--icl"&gt;第1章：提示工程与上下文学习 (Prompt Engineering &amp;amp; ICL)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e4%b8%8e%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0-prompt-engineering--icl"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;In-Context Learning is meta-learning without gradient descent.&amp;rdquo; —— 上下文学习本质上是一种无需梯度更新的元学习。本章将深入探讨如何在不更新模型参数的情况下，通过提示工程（Prompt Engineering）和上下文学习（In-Context Learning）激发大模型的潜能，构建复杂的应用系统。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;第一节：提示工程最佳实践&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e7%bb%93%e6%9e%84%e5%8c%96%e6%8f%90%e7%a4%ba%e8%af%8d-structured-prompt"&gt;1.1 结构化提示词 (Structured Prompt)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e8%a7%92%e8%89%b2%e4%b8%8e%e7%ba%a6%e6%9d%9f-role--constraints"&gt;1.2 角色与约束 (Role &amp;amp; Constraints)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e8%be%93%e5%87%ba%e6%8e%a7%e5%88%b6-output-format"&gt;1.3 输出控制 (Output Format)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%ba%8c%e8%8a%82%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0-in-context-learning"&gt;第二节：上下文学习 (In-Context Learning)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-few-shot-learning-%e5%8e%9f%e7%90%86"&gt;2.1 Few-Shot Learning 原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%8a%a8%e6%80%81%e7%a4%ba%e4%be%8b%e9%80%89%e6%8b%a9-dynamic-few-shot"&gt;2.2 动态示例选择 (Dynamic Few-Shot)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba-few-shot-%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e5%99%a8"&gt;2.3 实战：构建 Few-Shot 文本分类器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%80%9d%e7%bb%b4%e9%93%be%e6%8e%a8%e7%90%86-chain-of-thought"&gt;第三节：思维链推理 (Chain-of-Thought)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-zero-shot-cot"&gt;3.1 Zero-Shot CoT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-manual-cot"&gt;3.2 Manual CoT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-least-to-most-prompting"&gt;3.3 Least-to-Most Prompting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e5%9b%9b%e8%8a%82rag-%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f%e9%a2%84%e8%a7%88"&gt;第四节：RAG 系统设计模式预览&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-rag"&gt;4.1 为什么需要 RAG？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e5%9f%ba%e7%a1%80-rag-%e6%b5%81%e7%a8%8b"&gt;4.2 基础 RAG 流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e6%a8%a1%e5%9d%97%e5%8c%96-rag-%e6%9e%b6%e6%9e%84"&gt;4.3 模块化 RAG 架构&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%ba%94%e8%8a%82%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e6%99%ba%e8%83%bd%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f"&gt;第五节：实战：从零构建智能对话系统&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e7%b3%bb%e7%bb%9f%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1"&gt;5.1 系统架构设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e6%a0%b8%e5%bf%83-prompt-%e7%bc%96%e6%8e%92"&gt;5.2 核心 Prompt 编排&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0"&gt;5.3 完整代码实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e5%85%ad%e8%8a%82%e8%bf%9b%e9%98%b6%e5%ba%94%e7%94%a8setfit-%e4%b8%8e-%e8%af%ad%e4%b9%89%e8%81%9a%e7%b1%bb"&gt;第六节：进阶应用：SetFit 与 语义聚类&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#61-setfit%e5%b0%91%e6%a0%b7%e6%9c%ac%e5%88%86%e7%b1%bb%e5%be%ae%e8%b0%83"&gt;6.1 SetFit：少样本分类微调&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#62-bertopic%e8%af%ad%e4%b9%89%e4%b8%bb%e9%a2%98%e5%bb%ba%e6%a8%a1"&gt;6.2 BERTopic：语义主题建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0"&gt;思考练习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99"&gt;参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="第一节提示工程最佳实践"&gt;第一节：提示工程最佳实践&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;提示工程（Prompt Engineering）并非玄学，而是与模型沟通的&lt;strong&gt;编程语言&lt;/strong&gt;。SOTA 的提示词设计通常遵循清晰的结构化原则。&lt;/p&gt;</description></item><item><title>第1章 数据工程基础</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="第1章数据炼金术---从垃圾到黄金的数据工程-data-alchemy-for-fine-tuning"&gt;第1章：数据炼金术 - 从垃圾到黄金的数据工程 (Data Alchemy for Fine-tuning)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%95%b0%e6%8d%ae%e7%82%bc%e9%87%91%e6%9c%af---%e4%bb%8e%e5%9e%83%e5%9c%be%e5%88%b0%e9%bb%84%e9%87%91%e7%9a%84%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b-data-alchemy-for-fine-tuning"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;Garbage In, Garbage Out (GIGO)&amp;rdquo; - 这是数据科学的铁律&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Data is the new oil, but if you don&amp;rsquo;t refine it, you&amp;rsquo;re just burning crude.&amp;rdquo; - Andrew Ng&lt;/p&gt;
&lt;p&gt;欢迎来到数据炼金术的世界!本章将带你从 &lt;strong&gt;Petabytes 的原始矿石&lt;/strong&gt; 中提炼出 &lt;strong&gt;Kilobytes 的精华黄金&lt;/strong&gt;。在微调阶段,数据质量比数量更重要 - 精心提纯的 10K 高质量数据集,往往比 100K 未经处理的&amp;quot;垃圾&amp;quot;更有效(如 Alpaca、Phi-3 的成功)。&lt;/p&gt;
&lt;p&gt;我们将学习如何成为一名合格的&amp;quot;数据炼金术师&amp;quot;,掌握 &lt;strong&gt;过滤、蒸馏、提纯&lt;/strong&gt; 的核心技术,构建属于你自己的高质量微调数据集。&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="数据炼金术-pipeline-全景图"&gt;数据炼金术 Pipeline 全景图&lt;a class="anchor" href="#%e6%95%b0%e6%8d%ae%e7%82%bc%e9%87%91%e6%9c%af-pipeline-%e5%85%a8%e6%99%af%e5%9b%be"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;让我们先看看从原始数据到精炼数据集的完整旅程:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;┌─────────────────────────────────────────────────────────────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ DATA ALCHEMY PIPELINE │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ (数据炼金术流水线) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└─────────────────────────────────────────────────────────────────────────┘
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; Petabytes Kilobytes
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (原始矿石) (精炼黄金)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├──&amp;gt; [1. 粗筛] ────────────────&amp;gt; Gigabytes │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · 去除明显垃圾 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · 基础格式化 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├──&amp;gt; [2. 质量过滤] ────────────&amp;gt; Megabytes │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · 长度/完整性检查 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · 毒性检测 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · PII 脱敏 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├──&amp;gt; [3. 去重提纯] ────────────&amp;gt; Hundreds of KB │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · MinHash 去重 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · 近似重复检测 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ ⚡ FLOPs 节省: 去重后训练成本 ↓ 3-5x! │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├──&amp;gt; [4. 蒸馏升华] ────────────&amp;gt; Tens of KB │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · Self-Instruct (知识蒸馏) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · Evol-Instruct (复杂度提升) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ · GPT-4 → 小模型的能力迁移 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └──&amp;gt; [5. 最终提纯] ────────────&amp;gt; ✨ Pure Gold ✨ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; · 人工抽检 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; · A/B 测试验证 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; · 数据分布平衡 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;输出: 10K-50K 条高纯度数据 → 足以训练一个强大的专属模型!&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;核心逻辑&lt;/strong&gt;:每一个阶段都在剔除&amp;quot;杂质&amp;quot;,提升&amp;quot;纯度&amp;quot;:&lt;/p&gt;</description></item><item><title>第1章 模型压缩与推理加速</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/</guid><description>&lt;h1 id="第1章模型压缩与推理加速"&gt;第1章：模型压缩与推理加速&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%a8%a1%e5%9e%8b%e5%8e%8b%e7%bc%a9%e4%b8%8e%e6%8e%a8%e7%90%86%e5%8a%a0%e9%80%9f"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;让大模型&amp;quot;瘦身&amp;quot;，从显存杀手变成生产力工具。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3"&gt;第一节：量化技术详解&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e6%a6%82%e8%a7%88"&gt;1.1 量化技术概览&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-gptq-%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ae%9e%e6%88%98"&gt;1.2 GPTQ 原理与实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-awq-%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ae%9e%e6%88%98"&gt;1.3 AWQ 原理与实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#14-%e9%87%8f%e5%8c%96%e5%ae%9e%e6%88%98%e5%af%b9%e6%af%94"&gt;1.4 量化实战对比&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%ba%8c%e8%8a%82%e5%89%aa%e6%9e%9d%e6%8a%80%e6%9c%af"&gt;第二节：剪枝技术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d-vs-%e9%9d%9e%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d"&gt;2.1 结构化剪枝 vs 非结构化剪枝&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%ae%9e%e6%88%98sparsegpt"&gt;2.2 实战：SparseGPT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%89%e8%8a%82%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f"&gt;第三节：知识蒸馏&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3"&gt;3.1 核心思想&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e5%ae%9e%e6%88%98%e8%92%b8%e9%a6%8f-bert"&gt;3.2 实战：蒸馏 BERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e5%9b%9b%e8%8a%82%e6%98%be%e5%ad%98%e4%bc%b0%e7%ae%97%e4%b8%8e%e4%bc%98%e5%8c%96"&gt;第四节：显存估算与优化&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e8%ae%a1%e7%ae%97%e5%85%ac%e5%bc%8f"&gt;4.1 显存占用计算公式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-kv-cache-%e4%bc%98%e5%8c%96"&gt;4.2 KV Cache 优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="第一节量化技术详解"&gt;第一节：量化技术详解&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-量化技术概览"&gt;1.1 量化技术概览&lt;a class="anchor" href="#11-%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e6%a6%82%e8%a7%88"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;量化 (Quantization)&lt;/strong&gt; 是将模型权重和激活值从高精度（如 FP16/BF16）转换为低精度（如 INT8, INT4）的过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心收益&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;显存占用剧减&lt;/strong&gt;：INT4 模型显存仅为 FP16 的 1/4。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;内存带宽压力减轻&lt;/strong&gt;：这是 LLM 推理的主要瓶颈。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算加速&lt;/strong&gt;：整数运算比浮点运算更快（取决于硬件支持）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;主流方案对比 (SOTA)&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th style="text-align: left"&gt;特性&lt;/th&gt;
 &lt;th style="text-align: left"&gt;GPTQ&lt;/th&gt;
 &lt;th style="text-align: left"&gt;AWQ&lt;/th&gt;
 &lt;th style="text-align: left"&gt;EXL2 (ExLlamaV2)&lt;/th&gt;
 &lt;th style="text-align: left"&gt;bitsandbytes (BnB)&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;全称&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;GPT-Quantization&lt;/td&gt;
 &lt;td style="text-align: left"&gt;Activation-aware Weight Quantization&lt;/td&gt;
 &lt;td style="text-align: left"&gt;ExLlamaV2 Quantization&lt;/td&gt;
 &lt;td style="text-align: left"&gt;-&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;核心原理&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;逐层量化，利用Hessian矩阵最小化误差&lt;/td&gt;
 &lt;td style="text-align: left"&gt;保护1%的关键&amp;quot;显著&amp;quot;权重通道&lt;/td&gt;
 &lt;td style="text-align: left"&gt;混合精度量化 (2-8 bit混合)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;运行时动态量化 (LLM.int8())&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;量化时间&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;慢 (需校准数据)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;较快 (需校准数据)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;慢 (极其精细的搜索)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;无 (加载时量化)&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;推理速度&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;快&lt;/td&gt;
 &lt;td style="text-align: left"&gt;快&lt;/td&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;极快&lt;/strong&gt; (针对性CUDA优化)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;较慢 (非计算密集型)&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;主要用途&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;早期主流，通用性好&lt;/td&gt;
 &lt;td style="text-align: left"&gt;边缘端、低比特高精度&lt;/td&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;生产环境高性能推理&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;训练微调 (QLoRA)&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;显存颗粒度&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;固定 (4-bit/8-bit)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;固定&lt;/td&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;灵活&lt;/strong&gt; (如 4.65 bpw)&lt;/td&gt;
 &lt;td style="text-align: left"&gt;固定&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id="12-gptq-vs-awq-vs-exl2-深度解析"&gt;1.2 GPTQ vs AWQ vs EXL2 深度解析&lt;a class="anchor" href="#12-gptq-vs-awq-vs-exl2-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="1-gptq-generative-pre-trained-transformer-quantization"&gt;1. GPTQ (Generative Pre-trained Transformer Quantization)&lt;a class="anchor" href="#1-gptq-generative-pre-trained-transformer-quantization"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;早期最流行的 Post-Training Quantization (PTQ) 方法。&lt;/p&gt;</description></item><item><title>第1章 深度学习基础</title><link>https://LordFoxFairy.github.io/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="第二篇深度学习基础快速回顾"&gt;第二篇:深度学习基础(快速回顾)&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e7%af%87%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e5%bf%ab%e9%80%9f%e5%9b%9e%e9%a1%be"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标读者&lt;/strong&gt;:有机器学习基础,需要快速掌握深度学习和PyTorch的读者&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习重点&lt;/strong&gt;:PyTorch实战、神经网络核心概念、CNN基础&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。&lt;/p&gt;
&lt;h3 id="为什么选择pytorch"&gt;为什么选择PyTorch?&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9pytorch"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动态计算图&lt;/strong&gt;:更符合Python编程习惯,易于调试&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学术界主流&lt;/strong&gt;:顶级会议论文大多使用PyTorch实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生态完善&lt;/strong&gt;:torchvision、torchaudio等丰富的扩展库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 2.x&lt;/strong&gt;:引入torch.compile,性能大幅提升&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="章节安排"&gt;章节安排&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第3章神经网络基础"&gt;&lt;a href="chapter03/README.md"&gt;第3章:神经网络基础&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3.1 从感知机到多层神经网络&lt;/li&gt;
&lt;li&gt;3.2 反向传播算法详解&lt;/li&gt;
&lt;li&gt;3.3 激活函数的选择与影响&lt;/li&gt;
&lt;li&gt;3.4 正则化技术:BatchNorm与Dropout&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实战&lt;/strong&gt;:使用PyTorch构建第一个神经网络(MNIST手写数字识别)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心技能&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;掌握PyTorch的基本操作(Tensor、autograd、nn.Module)&lt;/li&gt;
&lt;li&gt;理解神经网络的训练流程&lt;/li&gt;
&lt;li&gt;学会使用GPU加速训练&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第4章卷积神经网络cnn"&gt;&lt;a href="chapter04/README.md"&gt;第4章:卷积神经网络(CNN)&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9ccnn"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;4.1 卷积层的工作原理&lt;/li&gt;
&lt;li&gt;4.2 池化层与降维&lt;/li&gt;
&lt;li&gt;4.3 经典CNN架构:LeNet → AlexNet → VGG&lt;/li&gt;
&lt;li&gt;4.4 CNN的可视化与理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实战&lt;/strong&gt;:CIFAR-10图像分类(从零构建CNN)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心技能&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理解卷积操作的本质&lt;/li&gt;
&lt;li&gt;掌握CNN的设计原则&lt;/li&gt;
&lt;li&gt;学会使用torchvision进行图像处理&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="技术栈"&gt;技术栈&lt;a class="anchor" href="#%e6%8a%80%e6%9c%af%e6%a0%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境要求"&gt;环境要求&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e8%a6%81%e6%b1%82"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Python &amp;gt;= 3.10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;python --version
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 安装PyTorch (2025年推荐)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# CPU版本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision torchaudio
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# GPU版本(CUDA 12.1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 或使用uv(更快)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;uv pip install torch torchvision torchaudio&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="核心依赖"&gt;核心依赖&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e4%be%9d%e8%b5%96"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyTorch &amp;gt;= 2.0&lt;/strong&gt;:深度学习框架&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;torchvision&lt;/strong&gt;:计算机视觉工具库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;:可视化&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tqdm&lt;/strong&gt;:进度条&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="验证安装"&gt;验证安装&lt;a class="anchor" href="#%e9%aa%8c%e8%af%81%e5%ae%89%e8%a3%85"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;PyTorch版本: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__version__&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;CUDA可用: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;CUDA版本: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;GPU设备: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_device_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="学习建议"&gt;学习建议&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1-动手实践为主"&gt;1. 动手实践为主&lt;a class="anchor" href="#1-%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%ba%e4%b8%bb"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;每个代码示例都要运行&lt;/strong&gt;:不要只看代码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;修改超参数观察变化&lt;/strong&gt;:学习率、批次大小、网络层数等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;尝试不同的数据集&lt;/strong&gt;:Fashion-MNIST、SVHN等&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2-理解核心概念"&gt;2. 理解核心概念&lt;a class="anchor" href="#2-%e7%90%86%e8%a7%a3%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度下降&lt;/strong&gt;:深度学习的基石&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;:如何高效计算梯度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt;:防止过拟合的关键&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3-参考官方文档"&gt;3. 参考官方文档&lt;a class="anchor" href="#3-%e5%8f%82%e8%80%83%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/"&gt;PyTorch官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/index.html"&gt;PyTorch文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/vision/stable/index.html"&gt;torchvision文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="4-循序渐进"&gt;4. 循序渐进&lt;a class="anchor" href="#4-%e5%be%aa%e5%ba%8f%e6%b8%90%e8%bf%9b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第3章(1-2天) → 第4章(2-3天)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓ ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;理解基础 掌握CNN
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓ ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;为后续现代架构(ResNet、Transformer)打下坚实基础&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="与前后篇的关系"&gt;与前后篇的关系&lt;a class="anchor" href="#%e4%b8%8e%e5%89%8d%e5%90%8e%e7%af%87%e7%9a%84%e5%85%b3%e7%b3%bb"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第一篇:机器学习基础
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (线性模型、优化算法)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第二篇:深度学习基础 ← 当前篇
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (神经网络、CNN)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第三篇:现代CNN架构
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (ResNet、EfficientNet等)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="代码规范"&gt;代码规范&lt;a class="anchor" href="#%e4%bb%a3%e7%a0%81%e8%a7%84%e8%8c%83"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇所有代码遵循以下规范:&lt;/p&gt;</description></item><item><title>第1章 长上下文技术</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/</guid><description>&lt;h1 id="第1章长上下文技术-long-context"&gt;第1章：长上下文技术 (Long Context)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e6%8a%80%e6%9c%af-long-context"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;如何让模型拥有一目十行的&amp;quot;过目不忘&amp;quot;能力？从 RoPE 到 FlashAttention。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e6%8c%91%e6%88%98"&gt;一、长上下文的挑战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e7%9a%84%e8%bf%9b%e5%8c%96rope-rotary-positional-embeddings"&gt;二、位置编码的进化：RoPE (Rotary Positional Embeddings)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae-vs-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae"&gt;1. 绝对位置 vs 相对位置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-rope-%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86"&gt;2. RoPE 核心原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-pytorch-%e5%ae%9e%e7%8e%b0-rope"&gt;3. PyTorch 实现 RoPE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e5%a4%96%e6%8e%a8%e6%8a%80%e6%9c%af%e6%89%93%e7%a0%b4%e9%95%bf%e5%ba%a6%e9%99%90%e5%88%b6"&gt;三、外推技术：打破长度限制&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e7%ba%bf%e6%80%a7%e5%86%85%e6%8f%92-linear-interpolation"&gt;1. 线性内插 (Linear Interpolation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-ntk-aware-scaled-rope"&gt;2. NTK-Aware Scaled RoPE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-yarn-yet-another-rope-for-transformers"&gt;3. YaRN (Yet another RoPE for Transformers)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96flashattention"&gt;四、工程优化：FlashAttention&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%98%be%e5%ad%98%e5%b8%a6%e5%ae%bd%e7%93%b6%e9%a2%88-memory-bound"&gt;1. 显存带宽瓶颈 (Memory Bound)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-flashattention-v1-tiling--recomputation"&gt;2. FlashAttention V1: Tiling &amp;amp; Recomputation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-flashattention-v2-%e5%b9%b6%e8%a1%8c%e4%bc%98%e5%8c%96"&gt;3. FlashAttention V2: 并行优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e6%9c%af"&gt;五、显存优化技术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-pagedattention-vllm"&gt;1. PagedAttention (vLLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-kv-cache-quantization"&gt;2. KV Cache Quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-grouped-query-attention-gqa"&gt;3. Grouped-Query Attention (GQA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa%e6%94%af%e6%8c%81-32k-%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84-mini-llama"&gt;六、代码实战：手写一个支持 32k 上下文的 Mini-Llama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;七、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一长上下文的挑战"&gt;一、长上下文的挑战&lt;a class="anchor" href="#%e4%b8%80%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e6%8c%91%e6%88%98"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在 RAG 和 Agent 应用中，处理长文本（如 100k tokens 甚至 1M tokens）已成为刚需。但 Transformer 在处理长文本时面临三个核心物理瓶颈：&lt;/p&gt;</description></item><item><title>第一篇 基础认知</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/</guid><description>&lt;h1 id="第一篇基础认知"&gt;第一篇：基础认知&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e7%af%87%e5%9f%ba%e7%a1%80%e8%ae%a4%e7%9f%a5"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id="-前置准备"&gt;📋 前置准备&lt;a class="anchor" href="#-%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境配置"&gt;环境配置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在开始学习之前，请确保完成以下环境配置：&lt;/p&gt;
&lt;h4 id="1-python-版本"&gt;1. Python 版本&lt;a class="anchor" href="#1-python-%e7%89%88%e6%9c%ac"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;python --version &lt;span class="c1"&gt;# 需要 Python 3.10 或更高版本&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="2-安装依赖"&gt;2. 安装依赖&lt;a class="anchor" href="#2-%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 使用 pip 安装最新版本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain langchain-openai langgraph langchain-community
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 或使用 uv (推荐)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;uv pip install langchain langchain-openai langgraph langchain-community
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 如需指定版本（推荐使用1.0.7或更高版本）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.7 langchain-openai&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.3 langgraph&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.3&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="3-环境变量配置"&gt;3. 环境变量配置&lt;a class="anchor" href="#3-%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 创建 .env 文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sk&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;your&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;api&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;LANGSMITH_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;your&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;langsmith&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="c1"&gt;# 可选,用于监控&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;LANGSMITH_TRACING&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;true&lt;/span&gt; &lt;span class="c1"&gt;# 可选&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 在代码中加载&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 验证环境变量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;required_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;OPENAI_API_KEY&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;required_vars&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;EnvironmentError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;缺少必需的环境变量: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="4-依赖版本清单"&gt;4. 依赖版本清单&lt;a class="anchor" href="#4-%e4%be%9d%e8%b5%96%e7%89%88%e6%9c%ac%e6%b8%85%e5%8d%95"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-toml" data-lang="toml"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# pyproject.toml 推荐配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;tool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;poetry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dependencies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;python&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^3.10&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langchain&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^1.0.7&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langchain-openai&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^1.0.3&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langgraph&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^1.0.3&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langchain-community&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^0.3.0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langchain-core&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^1.0.7&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;langsmith&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^0.4.43&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nx"&gt;python-dotenv&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;^1.0.0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# requirements.txt 格式&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langchain&amp;gt;=1.0.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langchain-openai&amp;gt;=1.0.3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langgraph&amp;gt;=1.0.3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langchain-community&amp;gt;=0.3.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langchain-core&amp;gt;=1.0.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# langsmith&amp;gt;=0.4.43&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c"&gt;# python-dotenv&amp;gt;=1.0.0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="前置知识"&gt;前置知识&lt;a class="anchor" href="#%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;建议具备以下基础知识：&lt;/p&gt;</description></item><item><title>第一篇 机器学习基础</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="第一篇机器学习基础快速回顾"&gt;第一篇：机器学习基础（快速回顾）&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e7%af%87%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e5%bf%ab%e9%80%9f%e5%9b%9e%e9%a1%be"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇是计算机视觉学习的基础准备篇，快速回顾机器学习核心概念，为后续深度学习和计算机视觉内容打下基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习目标&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理解机器学习的基本概念和分类&lt;/li&gt;
&lt;li&gt;掌握损失函数、优化器等核心要素&lt;/li&gt;
&lt;li&gt;了解过拟合与正则化&lt;/li&gt;
&lt;li&gt;理解传统图像特征提取方法&lt;/li&gt;
&lt;li&gt;明确深度学习相比传统方法的优势&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;适合人群&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有Python基础，想快速了解机器学习概念&lt;/li&gt;
&lt;li&gt;准备学习深度学习和计算机视觉&lt;/li&gt;
&lt;li&gt;需要回顾机器学习基础知识&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="章节结构"&gt;章节结构&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e7%bb%93%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第1章机器学习核心概念"&gt;第1章：机器学习核心概念&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;涵盖机器学习的基本分类、损失函数、优化器等核心概念，并通过sklearn实现手写数字分类的实战案例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习 vs 无监督学习&lt;/li&gt;
&lt;li&gt;损失函数与优化器&lt;/li&gt;
&lt;li&gt;过拟合与正则化&lt;/li&gt;
&lt;li&gt;实战：手写数字分类（sklearn）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第2章从传统特征到深度学习"&gt;第2章：从传统特征到深度学习&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0%e4%bb%8e%e4%bc%a0%e7%bb%9f%e7%89%b9%e5%be%81%e5%88%b0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;介绍传统图像特征提取方法（SIFT、HOG等），解释为什么需要深度学习，并准备深度学习环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统图像特征（SIFT、HOG）&lt;/li&gt;
&lt;li&gt;传统方法的局限性&lt;/li&gt;
&lt;li&gt;为什么需要深度学习&lt;/li&gt;
&lt;li&gt;环境准备（PyTorch/TensorFlow）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="学习路径"&gt;学习路径&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e8%b7%af%e5%be%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第1章：机器学习核心概念
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;理解监督学习/无监督学习
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;掌握损失函数和优化器
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;实战：MNIST分类（sklearn）
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第2章：从传统特征到深度学习
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;了解SIFT、HOG等传统特征
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;理解深度学习的优势
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;准备深度学习环境
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;进入第二篇：深度学习基础&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="学习建议"&gt;学习建议&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;快速回顾&lt;/strong&gt;：本篇作为快速回顾，不需要深入每个细节&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动手实践&lt;/strong&gt;：运行所有代码示例，理解实际效果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概念理解&lt;/strong&gt;：重点理解核心概念，为后续学习打基础&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;环境准备&lt;/strong&gt;：确保环境配置正确，能够运行所有示例代码&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="环境要求"&gt;环境要求&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e8%a6%81%e6%b1%82"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Python版本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Python 3.10+
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 第1章所需库&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install scikit-learn numpy matplotlib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 第2章所需库（传统特征）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install opencv-python scikit-image
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 深度学习环境（第2章末尾准备）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision &lt;span class="c1"&gt;# PyTorch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 或&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install tensorflow &lt;span class="c1"&gt;# TensorFlow&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="预计学习时间"&gt;预计学习时间&lt;a class="anchor" href="#%e9%a2%84%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%97%b6%e9%97%b4"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;第1章：2-3小时&lt;/li&gt;
&lt;li&gt;第2章：2-3小时&lt;/li&gt;
&lt;li&gt;总计：4-6小时&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="后续安排"&gt;后续安排&lt;a class="anchor" href="#%e5%90%8e%e7%bb%ad%e5%ae%89%e6%8e%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;完成本篇后，将进入&lt;strong&gt;第二篇：深度学习基础&lt;/strong&gt;，学习神经网络、卷积神经网络等深度学习核心内容。&lt;/p&gt;</description></item><item><title>深入理解 FastAPI</title><link>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/</guid><description>&lt;h1 id="深入理解-fastapi"&gt;深入理解 FastAPI&lt;a class="anchor" href="#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3-fastapi"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;现代Python高性能API框架的完整指南&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-fastapi%e6%a6%82%e8%bf%b0%e4%b8%8e%e6%a0%b8%e5%bf%83%e7%89%b9%e6%80%a7"&gt;1. FastAPI概述与核心特性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%bc%82%e6%ad%a5%e7%bc%96%e7%a8%8b%e5%8e%9f%e7%90%86"&gt;2. 异步编程原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-pydantic%e6%95%b0%e6%8d%ae%e9%aa%8c%e8%af%81"&gt;3. Pydantic数据验证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e4%be%9d%e8%b5%96%e6%b3%a8%e5%85%a5%e7%b3%bb%e7%bb%9f"&gt;4. 依赖注入系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e4%b8%ad%e9%97%b4%e4%bb%b6%e4%b8%8e%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f"&gt;5. 中间件与生命周期&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-%e8%ae%a4%e8%af%81%e4%b8%8e%e5%ae%89%e5%85%a8"&gt;6. 认证与安全&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-%e6%95%b0%e6%8d%ae%e5%ba%93%e9%9b%86%e6%88%90"&gt;7. 数据库集成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-%e5%90%8e%e5%8f%b0%e4%bb%bb%e5%8a%a1%e4%b8%8ewebsocket"&gt;8. 后台任务与WebSocket&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-%e6%b5%8b%e8%af%95%e7%ad%96%e7%95%a5"&gt;9. 测试策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-%e7%94%9f%e4%ba%a7%e9%83%a8%e7%bd%b2%e4%b8%8e%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96"&gt;10. 生产部署与性能优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-fastapi概述与核心特性"&gt;1. FastAPI概述与核心特性&lt;a class="anchor" href="#1-fastapi%e6%a6%82%e8%bf%b0%e4%b8%8e%e6%a0%b8%e5%bf%83%e7%89%b9%e6%80%a7"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-什么是fastapi"&gt;1.1 什么是FastAPI&lt;a class="anchor" href="#11-%e4%bb%80%e4%b9%88%e6%98%affastapi"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;FastAPI是一个现代、高性能的Python Web框架，专门用于构建API。它诞生于2018年，由Sebastián Ramírez创建，目标是解决Python Web开发中长期存在的几个痛点：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;传统框架的问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Flask&lt;/strong&gt;：简单灵活，但缺乏数据验证、类型提示支持，需要大量第三方库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Django REST Framework&lt;/strong&gt;：功能强大但过于重量级，学习曲线陡峭&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;性能瓶颈&lt;/strong&gt;：传统同步框架在高并发场景下表现不佳&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;FastAPI的解决方案&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;FastAPI站在巨人的肩膀上，它不是从零开始，而是巧妙地组合了两个优秀的库：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Starlette&lt;/strong&gt;：提供Web框架的核心能力（路由、中间件、WebSocket等）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pydantic&lt;/strong&gt;：提供数据验证和序列化能力&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种设计哲学意味着FastAPI本身的代码量很小，但功能极其强大。当你使用FastAPI时，实际上是在使用这两个经过生产验证的成熟库。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心优势&lt;/strong&gt;：&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;特性&lt;/th&gt;
 &lt;th&gt;说明&lt;/th&gt;
 &lt;th&gt;对比传统框架&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;极高性能&lt;/td&gt;
 &lt;td&gt;与NodeJS、Go相当&lt;/td&gt;
 &lt;td&gt;比Flask快10-100倍&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;开发效率&lt;/td&gt;
 &lt;td&gt;开发速度提升200-300%&lt;/td&gt;
 &lt;td&gt;自动文档、自动验证&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;减少Bug&lt;/td&gt;
 &lt;td&gt;类型提示减少约40%的人为错误&lt;/td&gt;
 &lt;td&gt;编译时发现问题&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;标准化&lt;/td&gt;
 &lt;td&gt;基于OpenAPI和JSON Schema&lt;/td&gt;
 &lt;td&gt;无需手写API文档&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;学习曲线&lt;/td&gt;
 &lt;td&gt;只需了解Python类型注解&lt;/td&gt;
 &lt;td&gt;无需学习DSL&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="12-安装与环境配置"&gt;1.2 安装与环境配置&lt;a class="anchor" href="#12-%e5%ae%89%e8%a3%85%e4%b8%8e%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;FastAPI提供了多种安装方式，根据你的需求选择：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 方式1：标准安装（推荐，包含所有常用依赖）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 包含：uvicorn、httpx、jinja2、python-multipart等&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install &lt;span class="s2"&gt;&amp;#34;fastapi[standard]&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 方式2：最小安装（只有核心功能）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 适合：对依赖有严格控制的环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install fastapi
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 方式3：单独安装ASGI服务器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 如果你选择了最小安装，需要单独安装服务器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install uvicorn&lt;span class="o"&gt;[&lt;/span&gt;standard&lt;span class="o"&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;为什么需要ASGI服务器？&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第02章 矩阵运算与微积分</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/</guid><description>&lt;h1 id="第02章矩阵运算与微积分"&gt;第02章：矩阵运算与微积分&lt;a class="anchor" href="#%e7%ac%ac02%e7%ab%a0%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97%e4%b8%8e%e5%be%ae%e7%a7%af%e5%88%86"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于&lt;strong&gt;你真正需要的那部分&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;向量空间&lt;/strong&gt;：理解数据的结构和维度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;投影&lt;/strong&gt;：理解线性回归的几何本质&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;矩阵微积分&lt;/strong&gt;：理解梯度下降和反向传播&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们的目标是&lt;strong&gt;几何直觉&lt;/strong&gt; + &lt;strong&gt;计算技巧&lt;/strong&gt;。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e5%9f%ba%e7%a1%80%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84"&gt;2.1 基础数据结构&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e6%a0%87%e9%87%8f%e5%90%91%e9%87%8f%e7%9f%a9%e9%98%b5%e4%b8%8e%e5%bc%a0%e9%87%8f"&gt;标量、向量、矩阵与张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%a0%b8%e5%bf%83%e8%bf%90%e7%ae%97"&gt;核心运算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e8%bd%ac%e7%bd%ae%e9%80%86%e7%9f%a9%e9%98%b5%e4%b8%8e%e4%bc%aa%e9%80%86"&gt;转置、逆矩阵与伪逆&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e8%bf%b9%e4%b8%8e%e8%a1%8c%e5%88%97%e5%bc%8f"&gt;迹与行列式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%90%91%e9%87%8f%e7%a9%ba%e9%97%b4%e7%90%86%e8%a7%a3%e6%95%b0%e6%8d%ae%e7%9a%84%e7%bb%93%e6%9e%84"&gt;2.2 向量空间：理解数据的结构&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e7%ba%bf%e6%80%a7%e7%bb%84%e5%90%88%e4%b8%8e%e5%bc%a0%e6%88%90%e7%a9%ba%e9%97%b4"&gt;线性组合与张成空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ba%bf%e6%80%a7%e7%9b%b8%e5%85%b3%e4%b8%8e%e7%ba%bf%e6%80%a7%e6%97%a0%e5%85%b3"&gt;线性相关与线性无关&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9f%ba%e4%b8%8e%e7%bb%b4%e5%ba%a6"&gt;基与维度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%a7%a9%e7%9f%a9%e9%98%b5%e7%9a%84%e6%9c%ac%e8%b4%a8%e7%bb%b4%e5%ba%a6"&gt;秩：矩阵的本质维度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%a7%a9-%e9%9b%b6%e5%8c%96%e5%ba%a6%e5%ae%9a%e7%90%86"&gt;秩-零化度定理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4"&gt;四个基本子空间&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%ba%a6%e9%87%8f%e4%b8%8e%e6%ad%a3%e4%ba%a4"&gt;2.3 度量与正交&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e8%8c%83%e6%95%b0%e6%b5%8b%e9%87%8f%e5%a4%a7%e5%b0%8f"&gt;范数：测量大小&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%86%85%e7%a7%af%e4%b8%8e%e8%a7%92%e5%ba%a6"&gt;内积与角度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%ad%a3%e4%ba%a4%e4%b8%8e%e6%ad%a3%e4%ba%a4%e7%9f%a9%e9%98%b5"&gt;正交与正交矩阵&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#24-%e6%8a%95%e5%bd%b1%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e5%87%a0%e4%bd%95%e6%9c%ac%e8%b4%a8"&gt;2.4 投影：线性回归的几何本质&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e6%8a%95%e5%bd%b1"&gt;为什么我们需要投影&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%bb%8e%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%8e%a8%e5%af%bc%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5"&gt;从最小二乘推导投影矩阵&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5%e7%9a%84%e6%80%a7%e8%b4%a8"&gt;投影矩阵的性质&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89"&gt;几何直觉&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#25-%e7%9f%a9%e9%98%b5%e5%be%ae%e7%a7%af%e5%88%86%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80"&gt;2.5 矩阵微积分：反向传播的数学基础&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc"&gt;为什么需要矩阵求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%b8%83%e5%b1%80%e7%ba%a6%e5%ae%9a%e5%88%86%e6%af%8d%e5%b8%83%e5%b1%80"&gt;布局约定：分母布局&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%a0%87%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc"&gt;标量对向量求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%a0%87%e9%87%8f%e5%af%b9%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc"&gt;标量对矩阵求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%90%91%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5"&gt;向量对向量求导：雅可比矩阵&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99"&gt;链式法则&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e9%87%8d%e8%a6%81%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc"&gt;重要公式推导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%ae%9e%e6%88%98%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e6%a2%af%e5%ba%a6"&gt;实战：线性回归的梯度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="21-基础数据结构"&gt;2.1 基础数据结构&lt;a class="anchor" href="#21-%e5%9f%ba%e7%a1%80%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="标量向量矩阵与张量"&gt;标量、向量、矩阵与张量&lt;a class="anchor" href="#%e6%a0%87%e9%87%8f%e5%90%91%e9%87%8f%e7%9f%a9%e9%98%b5%e4%b8%8e%e5%bc%a0%e9%87%8f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;数学对象的定义由其维度决定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;标量 (Scalar)&lt;/strong&gt;：$x \in \mathbb{R}$。单个数值，如温度、距离。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;向量 (Vector)&lt;/strong&gt;：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。&lt;strong&gt;本书默认向量为列向量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;例如，$\mathbf{x} = \begin{bmatrix} 2 \ 3 \end{bmatrix}$ 表示2D平面上的一个点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;矩阵 (Matrix)&lt;/strong&gt;：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的&lt;strong&gt;线性变换&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>第2章 LLaMA-Factory微调工厂</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/</guid><description>&lt;h1 id="第2章llama-factory-微调工厂"&gt;第2章：LLaMA-Factory 微调工厂&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0llama-factory-%e5%be%ae%e8%b0%83%e5%b7%a5%e5%8e%82"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;项目地址&lt;/strong&gt;：&lt;a href="https://github.com/hiyouga/LLaMA-Factory"&gt;https://github.com/hiyouga/LLaMA-Factory&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：从手写 PyTorch 进阶到“流水线工厂”。学会利用 LLaMA-Factory 进行零代码（WebUI）和低代码（CLI）的高效微调，涵盖从 SFT 到模型导出（Merge）的全流程。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9-llama-factory"&gt;1. 为什么选择 LLaMA-Factory？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e7%8e%af%e5%a2%83%e6%90%ad%e5%bb%ba%e4%b8%8e-unsloth-%e5%8a%a0%e9%80%9f"&gt;2. 环境搭建与 Unsloth 加速&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%a0%87%e5%87%86%e5%ae%89%e8%a3%85"&gt;2.1 标准安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%bc%80%e5%90%af-unsloth-%e6%9e%81%e9%80%9f%e6%a8%a1%e5%bc%8f%e6%8e%a8%e8%8d%90"&gt;2.2 开启 Unsloth 极速模式（推荐）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8bdataset-registration"&gt;3. 数据工程：Dataset Registration&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f%e6%a0%87%e5%87%86-alpaca-vs-sharegpt"&gt;3.1 数据格式标准 (Alpaca vs ShareGPT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e6%b3%a8%e5%86%8c%e8%87%aa%e5%ae%9a%e4%b9%89%e6%95%b0%e6%8d%ae%e9%9b%86-dataset_infojson"&gt;3.2 注册自定义数据集 (&lt;code&gt;dataset_info.json&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e5%8f%af%e8%a7%86%e5%8c%96%e5%be%ae%e8%b0%83webui-%e5%85%a8%e6%b5%81%e7%a8%8b"&gt;4. 可视化微调：WebUI 全流程&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e5%90%af%e5%8a%a8%e4%b8%8e%e7%95%8c%e9%9d%a2%e6%a6%82%e8%a7%88"&gt;4.1 启动与界面概览&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e8%ae%ad%e7%bb%83%e5%8f%82%e6%95%b0%e9%85%8d%e7%bd%ae%e8%af%a6%e8%a7%a3"&gt;4.2 训练参数配置详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e8%ae%ad%e7%bb%83%e7%9b%91%e6%8e%a7%e4%b8%8e%e8%af%84%e4%bc%b0"&gt;4.3 训练监控与评估&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e7%94%9f%e4%ba%a7%e5%8c%96%e4%bb%8e-webui-%e5%88%b0-cli-%e8%87%aa%e5%8a%a8%e5%8c%96"&gt;5. 生产化：从 WebUI 到 CLI 自动化&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e5%af%bc%e5%87%ba-yaml-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6"&gt;5.1 导出 YAML 配置文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e5%91%bd%e4%bb%a4%e8%a1%8c%e5%90%af%e5%8a%a8%e8%ae%ad%e7%bb%83"&gt;5.2 命令行启动训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e5%a4%9a%e6%9c%ba%e5%a4%9a%e5%8d%a1%e5%88%86%e5%b8%83%e5%bc%8f%e9%85%8d%e7%bd%ae"&gt;5.3 多机多卡分布式配置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-%e6%a8%a1%e5%9e%8b%e5%af%bc%e5%87%ba%e4%b8%8e%e5%90%88%e5%b9%b6"&gt;6. 模型导出与合并&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-为什么选择-llama-factory"&gt;1. 为什么选择 LLaMA-Factory？&lt;a class="anchor" href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9-llama-factory"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在 LLaMA-Factory 出现之前，微调一个模型需要自己手写 PEFT 代码、处理复杂的 Padding、适配 Flash Attention。LLaMA-Factory 解决了以下&lt;strong&gt;核心痛点&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第2章 vLLM高性能推理</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/</guid><description>&lt;h1 id="第2章vllm-高性能推理引擎实战"&gt;第2章：vLLM 高性能推理引擎实战&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0vllm-%e9%ab%98%e6%80%a7%e8%83%bd%e6%8e%a8%e7%90%86%e5%bc%95%e6%93%8e%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;项目地址&lt;/strong&gt;：&lt;a href="https://github.com/vllm-project/vllm"&gt;https://github.com/vllm-project/vllm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：vLLM 是目前 LLM 推理生态的&lt;strong&gt;事实标准&lt;/strong&gt;。本章将从 PagedAttention 原理出发，带你掌握 &lt;strong&gt;20倍吞吐量提升&lt;/strong&gt; 的秘诀，并解锁 &lt;strong&gt;多 LoRA 并发&lt;/strong&gt; 和 &lt;strong&gt;Prefix Caching&lt;/strong&gt; 等生产级特性。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88-vllm-%e8%83%bd%e5%bf%ab%e8%bf%99%e4%b9%88%e5%a4%9a"&gt;1. 为什么 vLLM 能快这么多？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e6%98%be%e5%ad%98%e7%a2%8e%e7%89%87%e7%9a%84%e5%99%a9%e6%a2%a6"&gt;1.1 显存碎片的噩梦&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-pagedattention-%e5%8e%9f%e7%90%86%e5%9b%be%e8%a7%a3"&gt;1.2 PagedAttention 原理图解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-continuous-batching-%e6%8c%81%e7%bb%ad%e6%89%b9%e5%a4%84%e7%90%86"&gt;1.3 Continuous Batching (持续批处理)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-vllm-%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b"&gt;2. vLLM 快速上手&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e7%a6%bb%e7%ba%bf%e6%89%b9%e9%87%8f%e6%8e%a8%e7%90%86-offline-inference"&gt;2.1 离线批量推理 (Offline Inference)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e5%90%af%e5%8a%a8-openai-%e5%85%bc%e5%ae%b9%e6%9c%8d%e5%8a%a1-api-server"&gt;2.2 启动 OpenAI 兼容服务 (API Server)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e8%bf%9b%e9%98%b6%e7%89%b9%e6%80%a7%e5%ae%9e%e6%88%98"&gt;3. 进阶特性实战&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-prefix-cachingrag-%e5%9c%ba%e6%99%af%e6%8f%90%e9%80%9f-10-%e5%80%8d"&gt;3.1 Prefix Caching：RAG 场景提速 10 倍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-multi-lora%e5%8d%95%e5%8d%a1%e6%9c%8d%e5%8a%a1%e5%a4%9a%e4%b8%aa%e5%be%ae%e8%b0%83%e6%a8%a1%e5%9e%8b"&gt;3.2 Multi-LoRA：单卡服务多个微调模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e5%88%86%e5%b8%83%e5%bc%8f%e6%8e%a8%e7%90%86-tensor-parallelism"&gt;3.3 分布式推理 (Tensor Parallelism)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e8%b0%83%e4%bc%98%e6%8c%87%e5%8d%97"&gt;4. 生产环境调优指南&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e6%98%be%e5%ad%98%e5%88%a9%e7%94%a8%e7%8e%87-gpu-memory-utilization"&gt;4.1 显存利用率 (&lt;code&gt;gpu-memory-utilization&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e6%9c%80%e5%a4%a7%e5%b9%b6%e5%8f%91%e6%95%b0-max-num-seqs"&gt;4.2 最大并发数 (&lt;code&gt;max-num-seqs&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-docker-%e9%83%a8%e7%bd%b2%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;4.3 Docker 部署最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-为什么-vllm-能快这么多"&gt;1. 为什么 vLLM 能快这么多？&lt;a class="anchor" href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88-vllm-%e8%83%bd%e5%bf%ab%e8%bf%99%e4%b9%88%e5%a4%9a"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在 vLLM 出现之前，Hugging Face 的原生推理（Naive Generation）存在严重的显存浪费问题。&lt;/p&gt;</description></item><item><title>第2章 与模型对话：提示工程基础</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="第2章与模型对话从提示工程到上下文工程"&gt;第2章：与模型对话—从提示工程到上下文工程&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0%e4%b8%8e%e6%a8%a1%e5%9e%8b%e5%af%b9%e8%af%9d%e4%bb%8e%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e5%88%b0%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b7%a5%e7%a8%8b"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Prompt Engineering is dead. Long live Context Engineering.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;当模型的上下文窗口从 4K 跃升至 128K、200K 甚至 1M tokens 时，游戏规则已经改变。我们不再受限于精心雕琢的&amp;quot;魔法咒语&amp;quot;，而是进入了一个可以直接塞入 100 个示例、缓存整本手册、用数据替代微调的新时代。这不是提示工程的终结，而是上下文工程的开端。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e6%8f%90%e7%a4%ba%e7%9a%84%e6%9e%84%e6%88%90%e6%8b%86%e8%a7%a3%e4%b8%80%e6%9d%a1%e5%ae%8c%e7%be%8e%e6%8c%87%e4%bb%a4"&gt;一、提示的构成：拆解一条完美指令&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e8%a7%92%e8%89%b2role%e8%ae%be%e5%ae%9a%e8%ba%ab%e4%bb%bd"&gt;1. 角色（Role）：设定身份&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%8c%87%e4%bb%a4instruction%e6%98%8e%e7%a1%ae%e4%bb%bb%e5%8a%a1"&gt;2. 指令（Instruction）：明确任务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e4%b8%8a%e4%b8%8b%e6%96%87context%e6%8f%90%e4%be%9b%e8%83%8c%e6%99%af"&gt;3. 上下文（Context）：提供背景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e8%be%93%e5%87%ba%e6%a0%bc%e5%bc%8foutput-format%e8%a7%84%e8%8c%83%e8%be%93%e5%87%ba"&gt;4. 输出格式（Output Format）：规范输出&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e6%a0%b8%e5%bf%83%e6%8a%80%e5%b7%a7zero-shot%e4%b8%8efew-shot"&gt;二、核心技巧：Zero-shot与Few-shot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-zero-shot%e7%9b%b4%e6%8e%a5%e6%8f%90%e9%97%ae"&gt;1. Zero-shot：直接提问&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-few-shot%e9%80%9a%e8%bf%87%e7%a4%ba%e4%be%8b%e5%bc%95%e5%af%bc"&gt;2. Few-shot：通过示例引导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-few-shot-%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;3. Few-shot 最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89context-engineering%e9%95%bf%e7%aa%97%e5%8f%a3%e6%97%b6%e4%bb%a3%e7%9a%84%e6%96%b0%e8%8c%83%e5%bc%8f"&gt;三、Context Engineering：长窗口时代的新范式&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-many-shot-icl%e7%94%a8%e6%95%b0%e6%8d%ae%e6%9b%bf%e4%bb%a3%e5%be%ae%e8%b0%83"&gt;1. Many-Shot ICL：用数据替代微调&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-prompt-caching%e9%99%8d%e4%bd%8e%e6%88%90%e6%9c%ac%e4%b8%8e%e5%bb%b6%e8%bf%9f"&gt;2. Prompt Caching：降低成本与延迟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-lost-in-the-middle%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e9%99%b7%e9%98%b1"&gt;3. Lost in the Middle：长上下文的陷阱&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e8%ae%a9%e6%a8%a1%e5%9e%8b%e6%80%9d%e8%80%83chain-of-thought-cot"&gt;四、让模型思考：Chain-of-Thought (CoT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-cot"&gt;1. 为什么需要 CoT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-zero-shot-cot%e9%ad%94%e6%b3%95%e5%92%92%e8%af%ad"&gt;2. Zero-shot CoT：魔法咒语&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-few-shot-cot%e6%8f%90%e4%be%9b%e6%8e%a8%e7%90%86%e7%a4%ba%e4%be%8b"&gt;3. Few-shot CoT：提供推理示例&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-self-consistency%e6%8a%95%e7%a5%a8%e6%8f%90%e5%8d%87%e5%87%86%e7%a1%ae%e7%8e%87"&gt;4. Self-Consistency：投票提升准确率&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94react-%e6%a8%a1%e5%bc%8f%e6%8e%a8%e7%90%86%e8%a1%8c%e5%8a%a8"&gt;五、ReAct 模式：推理+行动&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-react-%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3"&gt;1. ReAct 的核心思想&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-react-prompt-%e6%a8%a1%e6%9d%bf"&gt;2. ReAct Prompt 模板&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-react-%e5%ae%9e%e6%88%98%e7%a4%ba%e4%be%8b"&gt;3. ReAct 实战示例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%adprompt-automation%e7%bc%96%e7%a8%8b%e8%80%8c%e9%9d%9e%e6%8f%90%e7%a4%ba"&gt;六、Prompt Automation：编程而非提示&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-dspy%e5%a3%b0%e6%98%8e%e5%bc%8f%e6%8f%90%e7%a4%ba%e7%bc%96%e7%a8%8b"&gt;1. DSPy：声明式提示编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%bc%a0%e7%bb%9f-prompt-vs-dspy-%e5%af%b9%e6%af%94"&gt;2. 传统 Prompt vs DSPy 对比&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e5%ae%9e%e7%94%a8-prompt-%e6%a8%a1%e6%9d%bf%e5%ba%93"&gt;七、实用 Prompt 模板库&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%96%87%e6%9c%ac%e6%80%bb%e7%bb%93%e6%a8%a1%e6%9d%bf"&gt;1. 文本总结模板&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e6%a8%a1%e6%9d%bf"&gt;2. 分类任务模板&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e4%bf%a1%e6%81%af%e6%8f%90%e5%8f%96%e6%a8%a1%e6%9d%bf"&gt;3. 信息提取模板&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e5%86%85%e5%ae%b9%e6%94%b9%e5%86%99%e6%a8%a1%e6%9d%bf"&gt;4. 内容改写模板&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ab%e6%8e%a7%e5%88%b6%e9%9a%8f%e6%9c%ba%e6%80%a7%e9%87%87%e6%a0%b7%e5%8f%82%e6%95%b0%e8%af%a6%e8%a7%a3"&gt;八、控制随机性：采样参数详解&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-temperature%e6%8e%a7%e5%88%b6%e5%88%9b%e9%80%a0%e5%8a%9b"&gt;1. Temperature：控制创造力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-top-p%e5%8a%a8%e6%80%81%e6%88%aa%e6%96%ad"&gt;2. Top-p：动态截断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e9%87%87%e6%a0%b7%e7%ad%96%e7%95%a5%e5%ae%9e%e6%88%98%e6%8c%87%e5%8d%97"&gt;3. 采样策略实战指南&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b9%9d%e7%bb%93%e6%9e%84%e5%8c%96%e8%be%93%e5%87%ba%e5%ae%9e%e6%88%98"&gt;九、结构化输出实战&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-json-mode-%e4%bd%bf%e7%94%a8"&gt;1. JSON Mode 使用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%bd%bf%e7%94%a8-pydantic-%e5%92%8c-instructor"&gt;2. 使用 Pydantic 和 Instructor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8d%81%e5%ae%89%e5%85%a8%e9%98%b2%e6%8a%a4%e6%8f%90%e7%a4%ba%e8%af%8d%e6%b3%a8%e5%85%a5%e5%9f%ba%e7%a1%80"&gt;十、安全防护：提示词注入基础&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%bb%80%e4%b9%88%e6%98%af%e6%8f%90%e7%a4%ba%e8%af%8d%e6%b3%a8%e5%85%a5"&gt;1. 什么是提示词注入&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%9f%ba%e7%a1%80%e9%98%b2%e5%be%a1%e7%ad%96%e7%95%a5"&gt;2. 基础防御策略&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8d%81%e4%b8%80%e5%ae%9e%e6%88%98%e9%97%ae%e7%ad%94"&gt;十一、实战问答&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8d%81%e4%ba%8c%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;十二、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一提示的构成拆解一条完美指令"&gt;一、提示的构成：拆解一条完美指令&lt;a class="anchor" href="#%e4%b8%80%e6%8f%90%e7%a4%ba%e7%9a%84%e6%9e%84%e6%88%90%e6%8b%86%e8%a7%a3%e4%b8%80%e6%9d%a1%e5%ae%8c%e7%be%8e%e6%8c%87%e4%bb%a4"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一个高质量的提示词（Prompt）通常包含四个核心要素。让我们通过对比来理解它们的重要性。&lt;/p&gt;</description></item><item><title>第2章 微调你的专属模型</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第2章-微调你的专属模型从原理到实战的完全指南"&gt;第2章 微调你的专属模型：从原理到实战的完全指南&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0-%e5%be%ae%e8%b0%83%e4%bd%a0%e7%9a%84%e4%b8%93%e5%b1%9e%e6%a8%a1%e5%9e%8b%e4%bb%8e%e5%8e%9f%e7%90%86%e5%88%b0%e5%ae%9e%e6%88%98%e7%9a%84%e5%ae%8c%e5%85%a8%e6%8c%87%e5%8d%97"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;微调不是魔法，而是精准的外科手术 —— 在冻结的知识海洋中，只激活你需要的那几个神经元。”&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%be%ae%e8%b0%83"&gt;引言：为什么需要微调？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%be%ae%e8%b0%83%e7%9a%84%e6%9c%ac%e8%b4%a8loss%e5%87%bd%e6%95%b0%e8%a7%86%e8%a7%92"&gt;一、微调的本质：Loss函数视角&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e9%a2%84%e8%ae%ad%e7%bb%83-vs-%e5%be%ae%e8%b0%83%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e7%9a%84%e5%b7%ae%e5%bc%82"&gt;1.1 预训练 vs 微调：目标函数的差异&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-sft-loss-%e5%9b%be%e8%a7%a3token%e7%ba%a7%e6%8e%a9%e7%a0%81%e8%a1%a8"&gt;1.2 SFT Loss 图解：Token级掩码表&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e6%98%be%e5%ad%98%e8%b4%a6%e5%8d%95%e4%b8%ba%e4%bb%80%e4%b9%88%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83%e8%bf%99%e4%b9%88%e8%b4%b5"&gt;二、显存账单：为什么全量微调这么贵？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e7%9a%84%e5%9b%9b%e5%a4%a7%e5%bc%80%e9%94%80"&gt;2.1 显存占用的四大开销&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-adamw%e7%9a%8412%e5%ad%97%e8%8a%82%e7%a7%98%e5%af%86"&gt;2.2 AdamW的12字节秘密&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83-vs-peft-%e6%98%be%e5%ad%98%e5%af%b9%e6%af%94"&gt;2.3 全量微调 vs PEFT 显存对比&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89lora%e6%a0%b8%e5%bf%83%e4%bd%8e%e7%a7%a9%e9%80%82%e9%85%8d%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8"&gt;三、LoRA核心：低秩适配的数学本质&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e7%a7%a9%e5%88%86%e8%a7%a3"&gt;3.1 核心公式：秩分解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-lora%e6%9e%b6%e6%9e%84%e5%9b%be"&gt;3.2 LoRA架构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e5%85%a80%e5%88%9d%e5%a7%8b%e5%8c%96"&gt;3.3 深度问答：为什么不能全0初始化？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-pytorch%e5%8e%9f%e7%94%9f%e5%ae%9e%e7%8e%b0loralinear"&gt;3.4 PyTorch原生实现：LoRALinear&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9blora%e5%ae%b6%e6%97%8f%e6%bc%94%e8%bf%9b%e4%bb%8eqlora%e5%88%b0galore"&gt;四、LoRA家族演进：从QLoRA到GaLore&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-qlora%e9%87%8f%e5%8c%96%e7%9a%84%e8%89%ba%e6%9c%af"&gt;4.1 QLoRA：量化的艺术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#411-nf4%e9%87%8f%e5%8c%96%e5%8e%9f%e7%90%86"&gt;4.1.1 NF4量化原理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-dora%e6%96%b9%e5%90%91%e4%b8%8e%e5%b9%85%e5%ba%a6%e7%9a%84%e8%a7%a3%e8%80%a6"&gt;4.2 DoRA：方向与幅度的解耦&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#421-dora%e5%ae%8c%e6%95%b4%e5%ae%9e%e7%8e%b0"&gt;4.2.1 DoRA完整实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-galore%e6%a2%af%e5%ba%a6%e4%bd%8e%e7%a7%a9%e6%8a%95%e5%bd%b1"&gt;4.3 GaLore：梯度低秩投影&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#431-%e5%8e%9f%e7%90%86%e6%8e%a8%e5%af%bc"&gt;4.3.1 原理推导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#432-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b"&gt;4.3.2 算法流程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-lora%e9%9d%9e%e5%af%b9%e7%a7%b0%e5%ad%a6%e4%b9%a0%e7%8e%87"&gt;4.4 LoRA+：非对称学习率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#45-%e5%85%b6%e4%bb%96%e5%8f%98%e7%a7%8d%e7%ae%80%e4%bb%8b"&gt;4.5 其他变种简介&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#adalora%e8%87%aa%e9%80%82%e5%ba%94%e7%a7%a9%e5%88%86%e9%85%8d"&gt;AdaLoRA：自适应秩分配&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vera%e6%9e%81%e8%87%b4%e5%8f%82%e6%95%b0%e6%95%88%e7%8e%87"&gt;VeRA：极致参数效率&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e5%be%ae%e8%b0%83%e6%b7%b1%e5%ba%a6%e7%90%86%e8%a7%a3"&gt;五、微调深度理解&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e6%9e%84%e5%bb%ba%e7%9a%84%e8%89%ba%e6%9c%af"&gt;5.1 指令数据构建的艺术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e7%9a%84%e9%bb%84%e9%87%91%e6%a0%87%e5%87%86"&gt;(1) 指令数据的黄金标准&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-self-instruct-%e7%94%a8gpt-4%e7%94%9f%e6%88%90%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae"&gt;(2) Self-Instruct: 用GPT-4生成训练数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%e6%8a%80%e6%9c%af"&gt;(3) 数据增强技术&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e7%81%be%e9%9a%be%e6%80%a7%e9%81%97%e5%bf%98-catastrophic-forgetting"&gt;5.2 灾难性遗忘 (Catastrophic Forgetting)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a51-%e6%b7%b7%e5%90%88%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae"&gt;缓解策略1: 混合训练数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a52-elastic-weight-consolidation-ewc"&gt;缓解策略2: Elastic Weight Consolidation (EWC)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%be%ae%e8%b0%83-multi-task-fine-tuning"&gt;5.3 多任务微调 (Multi-task Fine-tuning)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%bb%bb%e5%8a%a1%e6%a0%87%e8%af%86%e7%ac%a6-task-prefix"&gt;(1) 任务标识符 (Task Prefix)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%bb%bb%e5%8a%a1%e7%89%b9%e5%ae%9a%e9%80%82%e9%85%8d%e5%99%a8"&gt;(2) 任务特定适配器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#54-%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0-continual-learning"&gt;5.4 持续学习 (Continual Learning)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%b8%90%e8%bf%9b%e5%bc%8flora-progressive-lora"&gt;(1) 渐进式LoRA (Progressive LoRA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f-knowledge-distillation"&gt;(2) 知识蒸馏 (Knowledge Distillation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%ae%8c%e6%95%b4%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0%e6%b5%81%e7%a8%8b"&gt;(3) 完整持续学习流程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e5%b7%a5%e7%a8%8b%e5%ae%9e%e6%88%98%e7%94%a8trl%e5%ba%93%e5%be%ae%e8%b0%83%e6%a8%a1%e5%9e%8b"&gt;六、工程实战：用TRL库微调模型&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#61-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b"&gt;6.1 完整训练流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#62-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3"&gt;6.2 关键技术详解&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#621-chat-template%e7%9a%84%e6%ad%a3%e7%a1%ae%e4%bd%bf%e7%94%a8"&gt;6.2.1 Chat Template的正确使用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#622-padding%e9%81%bf%e5%9d%91%e6%8c%87%e5%8d%97"&gt;6.2.2 Padding避坑指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#623-neftune%e5%b5%8c%e5%85%a5%e5%b1%82%e5%8a%a0%e5%99%aa%e6%8a%80%e5%b7%a7"&gt;6.2.3 NEFTune：嵌入层加噪技巧&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#63-%e6%8e%a8%e7%90%86%e4%b8%8e%e9%83%a8%e7%bd%b2"&gt;6.3 推理与部署&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e6%a8%a1%e5%9e%8b%e5%90%88%e5%b9%b6%e6%8a%80%e6%9c%af"&gt;七、模型合并技术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#71-%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc%e5%90%88%e5%b9%b6-weight-averaging"&gt;7.1 线性插值合并 (Weight Averaging)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#72-slerp-%e7%90%83%e9%9d%a2%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc"&gt;7.2 SLERP: 球面线性插值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#73-ties-%e4%bf%ae%e5%89%aa%e9%80%89%e4%b8%be%e4%b8%8e%e5%90%88%e5%b9%b6"&gt;7.3 TIES: 修剪、选举与合并&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#74-dare-%e4%b8%a2%e5%bc%83%e4%b8%8e%e9%87%8d%e7%bc%a9%e6%94%be"&gt;7.4 DARE: 丢弃与重缩放&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#75-task-arithmetic-%e4%bb%bb%e5%8a%a1%e7%ae%97%e6%9c%af"&gt;7.5 Task Arithmetic: 任务算术&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#76-%e5%90%88%e5%b9%b6%e6%96%b9%e6%b3%95%e5%af%b9%e6%af%94%e4%b8%8e%e9%80%89%e6%8b%a9"&gt;7.6 合并方法对比与选择&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ab%e6%80%bb%e7%bb%93%e5%be%ae%e8%b0%83%e7%9f%a5%e8%af%86%e5%9c%b0%e5%9b%be"&gt;八、总结：微调知识地图&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#81-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5%e8%a1%a8"&gt;8.1 核心公式速查表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#82-%e6%96%b9%e6%b3%95%e9%80%89%e6%8b%a9%e5%86%b3%e7%ad%96%e6%a0%91"&gt;8.2 方法选择决策树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#83-%e5%ae%9e%e6%88%98%e5%bb%ba%e8%ae%ae%e6%b8%85%e5%8d%95"&gt;8.3 实战建议清单&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#831-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87"&gt;8.3.1 数据准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#832-%e8%b6%85%e5%8f%82%e6%95%b0%e8%b0%83%e4%bc%98"&gt;8.3.2 超参数调优&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#833-%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96"&gt;8.3.3 显存优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#834-%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7"&gt;8.3.4 训练技巧&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#835-%e6%8e%a8%e7%90%86%e4%bc%98%e5%8c%96"&gt;8.3.5 推理优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#84-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98%e6%8e%92%e6%9f%a5"&gt;8.4 常见问题排查&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#85-%e8%bf%9b%e9%98%b6%e8%b5%84%e6%ba%90"&gt;8.5 进阶资源&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#-%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94%e4%bb%8e%e5%9b%b0%e6%83%91%e5%88%b0%e7%90%86%e8%a7%a3"&gt;💡 新手问答：从困惑到理解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%bb%93%e8%af%ad%e5%be%ae%e8%b0%83%e7%9a%84%e8%89%ba%e6%9c%af%e4%b8%8e%e7%a7%91%e5%ad%a6"&gt;结语：微调的艺术与科学&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="引言为什么需要微调"&gt;引言：为什么需要微调？&lt;a class="anchor" href="#%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%be%ae%e8%b0%83"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;想象一下，你拥有一位博学的教授（预训练模型），他知晓天文地理，但对你公司的业务一无所知。微调（Fine-tuning）就像是给他补习专业课程，让他在保留通用知识的同时，掌握你的领域专长。&lt;/p&gt;</description></item><item><title>第2章 新型架构探索</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/</guid><description>&lt;h1 id="第2章新型架构探索-new-architectures"&gt;第2章：新型架构探索 (New Architectures)&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0%e6%96%b0%e5%9e%8b%e6%9e%b6%e6%9e%84%e6%8e%a2%e7%b4%a2-new-architectures"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：打破 Transformer 的垄断。我们将深入 DeepSeek 和 Mixtral 及其背后的 &lt;strong&gt;MoE (混合专家)&lt;/strong&gt; 技术，并探索挑战 Attention 机制的 &lt;strong&gt;SSM (Mamba)&lt;/strong&gt; 架构。这也是 DeepSeek-V3 能在极低成本下训练出来的核心秘密。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e6%b7%b7%e5%90%88%e4%b8%93%e5%ae%b6%e6%a8%a1%e5%9e%8b-moe-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90"&gt;1. 混合专家模型 (MoE) 深度解析&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e7%a8%80%e7%96%8f%e6%bf%80%e6%b4%bb%e4%bb%8e-dense-%e5%88%b0-sparse"&gt;1.1 稀疏激活：从 Dense 到 Sparse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6router-gate-%e5%8e%9f%e7%90%86"&gt;1.2 核心组件：Router (Gate) 原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e4%b8%8e%e8%be%85%e5%8a%a9%e6%8d%9f%e5%a4%b1-aux-loss"&gt;1.3 负载均衡与辅助损失 (Aux Loss)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#14-%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa-moe-layer"&gt;1.4 实战：手写一个 MoE Layer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-deepseek-v3-%e6%a0%b8%e5%bf%83mla-multi-head-latent-attention"&gt;2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-kv-cache-%e7%9a%84%e6%98%be%e5%ad%98%e7%93%b6%e9%a2%88"&gt;2.1 KV Cache 的显存瓶颈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-mla-%e5%8e%9f%e7%90%86%e4%bd%8e%e7%a7%a9%e5%8e%8b%e7%bc%a9"&gt;2.2 MLA 原理：低秩压缩&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e6%98%be%e5%ad%98%e8%8a%82%e7%9c%81%e8%ae%a1%e7%ae%97%e6%a1%88%e4%be%8b"&gt;2.3 显存节省计算案例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e7%8a%b6%e6%80%81%e7%a9%ba%e9%97%b4%e6%a8%a1%e5%9e%8b-ssm-%e4%b8%8e-mamba"&gt;3. 状态空间模型 (SSM) 与 Mamba&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e7%ba%bf%e6%80%a7%e5%a4%8d%e6%9d%82%e5%ba%a6on-vs-on2"&gt;3.1 线性复杂度：O(N) vs O(N^2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e9%80%89%e6%8b%a9%e6%80%a7%e6%9c%ba%e5%88%b6-selection-mechanism"&gt;3.2 选择性机制 (Selection Mechanism)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-mamba-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0"&gt;3.3 Mamba 代码实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-混合专家模型-moe-深度解析"&gt;1. 混合专家模型 (MoE) 深度解析&lt;a class="anchor" href="#1-%e6%b7%b7%e5%90%88%e4%b8%93%e5%ae%b6%e6%a8%a1%e5%9e%8b-moe-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-稀疏激活从-dense-到-sparse"&gt;1.1 稀疏激活：从 Dense 到 Sparse&lt;a class="anchor" href="#11-%e7%a8%80%e7%96%8f%e6%bf%80%e6%b4%bb%e4%bb%8e-dense-%e5%88%b0-sparse"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;传统 Transformer 是 &lt;strong&gt;Dense (稠密)&lt;/strong&gt; 的：每个 Token 都要经过模型的所有参数计算。
MoE 是 &lt;strong&gt;Sparse (稀疏)&lt;/strong&gt; 的：&lt;/p&gt;</description></item><item><title>第2章 检索增强生成（RAG）原理</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/</guid><description>&lt;h1 id="第2章-检索增强生成rag原理"&gt;第2章 检索增强生成（RAG）原理&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0-%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag%e5%8e%9f%e7%90%86"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;: 构建基于外部知识库的增强生成系统&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;: RAG标准架构 → Chunking策略 → 检索技术 → 重排序 → 高级RAG变体&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前置知识&lt;/strong&gt;: Part 1 第3章（Embedding）、Part 3 第4章（Embedding模型训练）&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80rag%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%96%e9%83%a8%e7%9f%a5%e8%af%86"&gt;一、RAG：为什么需要外部知识？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8crag-%e6%a0%87%e5%87%86%e6%9e%b6%e6%9e%84%e8%af%a6%e8%a7%a3"&gt;二、RAG 标准架构详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%afchunking-%e4%b8%8e-indexing"&gt;三、核心技术：Chunking 与 Indexing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e6%a3%80%e7%b4%a2-retrieval"&gt;四、核心技术：检索 (Retrieval)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e9%87%8d%e6%8e%92%e5%ba%8f-reranking"&gt;五、核心技术：重排序 (Reranking)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e9%ab%98%e7%ba%a7-rag-%e5%8f%98%e4%bd%93"&gt;六、高级 RAG 变体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;七、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一rag为什么需要外部知识"&gt;一、RAG：为什么需要外部知识？&lt;a class="anchor" href="#%e4%b8%80rag%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%96%e9%83%a8%e7%9f%a5%e8%af%86"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-大模型的知识困境"&gt;1.1 大模型的知识困境&lt;a class="anchor" href="#11-%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e7%9f%a5%e8%af%86%e5%9b%b0%e5%a2%83"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;问题1：知识过时&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 向GPT-4提问（假设训练数据截止2023年10月）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;最新的诺贝尔物理学奖获得者是谁？&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;llm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 输出: &amp;#34;我的知识截止到2023年，无法回答...&amp;#34;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;问题2：私域知识缺失&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;企业内部文档、财报、技术规范&lt;/li&gt;
&lt;li&gt;实时更新的法律法规、医疗指南&lt;/li&gt;
&lt;li&gt;个人笔记、代码库&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;问题3：幻觉（Hallucination）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型&amp;quot;编造&amp;quot;看似合理但实际错误的信息&lt;/li&gt;
&lt;li&gt;在知识密集型任务中尤为严重&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="12-rag的核心思想"&gt;1.2 RAG的核心思想&lt;a class="anchor" href="#12-rag%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Generation（检索增强生成）&lt;/strong&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;传统LLM: 问题 ──&amp;gt; LLM ──&amp;gt; 答案（基于参数化知识）
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; 可能过时/缺失/幻觉
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;RAG流程: 问题 ──&amp;gt; 检索器 ──&amp;gt; 相关文档
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; LLM + 文档上下文 ──&amp;gt; 答案（有依据）&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;核心优势&lt;/strong&gt;:&lt;/p&gt;</description></item><item><title>第2章 模型家族谱系：从编码器到解码器</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/</guid><description>&lt;h1 id="第2章模型家族谱系从编码器到解码器-model-architectures"&gt;第2章：模型家族谱系：从编码器到解码器 (Model Architectures)&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0%e6%a8%a1%e5%9e%8b%e5%ae%b6%e6%97%8f%e8%b0%b1%e7%b3%bb%e4%bb%8e%e7%bc%96%e7%a0%81%e5%99%a8%e5%88%b0%e8%a7%a3%e7%a0%81%e5%99%a8-model-architectures"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The best way to predict the future is to invent it.&amp;rdquo; - Alan Kay&lt;/p&gt;
&lt;p&gt;本章将带你理解Transformer的三大架构分支，掌握每种架构的设计哲学、技术细节和当前的主流选择，助你在实际应用中做出明智的架构选型。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%85%b1%e5%90%8c%e7%9a%84%e7%a5%96%e5%85%88%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84"&gt;一、共同的祖先：编码器-解码器架构&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e5%8e%9f%e5%a7%8btransformer%e7%9a%84%e5%8f%8c%e5%a1%94%e8%ae%be%e8%ae%a1"&gt;1.1 原始Transformer的双塔设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b"&gt;1.2 编码器-解码器的工作流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-t5%e7%8e%b0%e4%bb%a3%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e4%bb%a3%e8%a1%a8"&gt;1.3 T5：现代编码器-解码器的代表&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e5%88%86%e8%a3%82%e4%b8%8e%e6%bc%94%e5%8c%96%e4%b8%ba%e4%bd%95%e4%b8%8d%e9%83%bd%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8"&gt;二、分裂与演化：为何不都用编码器-解码器？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e8%80%83%e9%87%8f"&gt;2.1 计算效率考量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e4%bb%bb%e5%8a%a1%e7%89%b9%e6%80%a7%e9%80%82%e9%85%8d"&gt;2.2 任务特性适配&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e4%bb%85%e7%bc%96%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e5%8f%8c%e5%90%91%e7%9a%84%e7%90%86%e8%a7%a3%e4%b8%93%e5%ae%b6"&gt;三、仅编码器架构：双向的理解专家&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-bert%e7%9a%84%e9%9d%a9%e5%91%bd%e6%80%a7%e8%ae%be%e8%ae%a1"&gt;3.1 BERT的革命性设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-mlm"&gt;3.2 掩码语言模型 (MLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e4%b8%ba%e4%bb%80%e4%b9%88bert%e4%b8%8d%e8%83%bd%e7%94%9f%e6%88%90%e6%96%87%e6%9c%ac"&gt;3.3 为什么BERT不能生成文本？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-%e5%ae%9e%e6%88%98bert%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb"&gt;3.4 实战：BERT文本分类&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e4%bb%85%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e7%94%9f%e6%88%90%e7%9a%84%e7%8e%8b%e8%80%85"&gt;四、仅解码器架构：生成的王者&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-gpt%e7%9a%84%e5%8d%95%e5%90%91%e8%ae%be%e8%ae%a1%e5%93%b2%e5%ad%a6"&gt;4.1 GPT的单向设计哲学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-causal-mask"&gt;4.2 因果注意力机制 (Causal Mask)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92decoder-only%e7%9a%84%e5%85%a8%e9%9d%a2%e8%83%9c%e5%88%a9"&gt;4.3 当前视角：Decoder-only的全面胜利&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-%e5%ae%9e%e6%88%98gpt%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90"&gt;4.4 实战：GPT文本生成&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%9e%b6%e6%9e%84%e9%80%89%e5%9e%8b%e6%8c%87%e5%8d%97"&gt;五、架构选型指南&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e4%b8%89%e5%a4%a7%e6%9e%b6%e6%9e%84%e5%af%b9%e6%af%94%e8%a1%a8"&gt;5.1 三大架构对比表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e5%86%b3%e7%ad%96%e6%a0%91%e6%88%91%e8%af%a5%e7%94%a8%e5%93%aa%e4%b8%aa"&gt;5.2 决策树：我该用哪个？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e9%bb%84%e9%87%91%e7%bb%84%e5%90%88encoder%e5%81%9a%e7%b4%a2%e5%bc%95decoder%e5%81%9a%e7%94%9f%e6%88%90"&gt;5.3 黄金组合：Encoder做索引，Decoder做生成&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94"&gt;六、深度问答&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;本章概览&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在第1章中，我们深入学习了Transformer的核心机制。但你是否好奇：&lt;strong&gt;为什么BERT擅长理解文本，而GPT擅长生成文本？为什么现在的新模型（如DeepSeek-V3, LLaMA-3）几乎全都是Decoder-only架构？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这一切的答案，藏在Transformer的&lt;strong&gt;三大架构分支&lt;/strong&gt;中：&lt;/p&gt;
&lt;pre class="mermaid"&gt;graph TD
 A[Transformer 原始架构&amp;lt;br&amp;gt;Encoder-Decoder 2017] --&amp;gt; B[仅编码器&amp;lt;br&amp;gt;Encoder-only]
 A --&amp;gt; C[仅解码器&amp;lt;br&amp;gt;Decoder-only]
 A --&amp;gt; D[编码器-解码器&amp;lt;br&amp;gt;Encoder-Decoder]

 B --&amp;gt; B1[BERT 2018&amp;lt;br&amp;gt;RoBERTa 2019&amp;lt;br&amp;gt;Embedding Models 2025]
 C --&amp;gt; C1[GPT-3/4 2020-2023&amp;lt;br&amp;gt;LLaMA-3 2024&amp;lt;br&amp;gt;DeepSeek-V3 2024]
 D --&amp;gt; D1[T5 2020&amp;lt;br&amp;gt;BART 2020&amp;lt;br&amp;gt;GLM-130B 2022]

 style A fill:#FFE4E1,stroke:#E87461
 style B fill:#E8F5E9,stroke:#81C784
 style C fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px
 style D fill:#FFF9C4,stroke:#FDD835
 style C1 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;难度级别&lt;/strong&gt;：⭐⭐（进阶）- 需要理解第1章的Transformer基础&lt;/p&gt;</description></item><item><title>第二篇 快速上手实战</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/</guid><description>&lt;h1 id="第二篇-快速上手实战"&gt;第二篇 快速上手实战&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e7%af%87-%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id="-前置知识说明"&gt;📌 前置知识说明&lt;a class="anchor" href="#-%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86%e8%af%b4%e6%98%8e"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇将使用以下核心概念，如需深入理解请参考相关章节：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;StateGraph&lt;/strong&gt;: LangGraph的状态图，用于编排复杂流程 → 本篇仅使用基础功能，高级用法详见&lt;strong&gt;第三篇第7章&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Runnable Protocol&lt;/strong&gt;: 统一执行接口（invoke/stream/batch） → 已在&lt;strong&gt;第一篇第2章&lt;/strong&gt;讲解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LCEL语法&lt;/strong&gt;: 管道操作符&lt;code&gt;|&lt;/code&gt;和并行&lt;code&gt;{}&lt;/code&gt; → 已在&lt;strong&gt;第一篇第2.2节&lt;/strong&gt;讲解&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;💡 &lt;strong&gt;学习建议&lt;/strong&gt;: 初学者可以先跟着本篇代码实践，遇到不理解的概念再回看相关章节。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="第1章message-与-tools-基础"&gt;第1章：Message 与 Tools 基础&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0message-%e4%b8%8e-tools-%e5%9f%ba%e7%a1%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-message-消息系统"&gt;1.1 Message 消息系统&lt;a class="anchor" href="#11-message-%e6%b6%88%e6%81%af%e7%b3%bb%e7%bb%9f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="111-消息类型humanmessageaimessagesystemmessagetoolmessage"&gt;1.1.1 消息类型：HumanMessage、AIMessage、SystemMessage、ToolMessage&lt;a class="anchor" href="#111-%e6%b6%88%e6%81%af%e7%b1%bb%e5%9e%8bhumanmessageaimessagesystemmessagetoolmessage"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;LangChain 1.0 引入了统一的消息类型系统，用于表示人机对话中的不同角色和内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心消息类型&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="mermaid"&gt;graph TD
 A[BaseMessage] --&amp;gt; B[HumanMessage]
 A --&amp;gt; C[AIMessage]
 A --&amp;gt; D[SystemMessage]
 A --&amp;gt; E[ToolMessage]
 A --&amp;gt; F[FunctionMessage]

 style A fill:#E3F2FD
 style B fill:#C8E6C9
 style C fill:#FFF9C4
 style D fill:#FFCCBC
 style E fill:#E1BEE7
 style F fill:#B2DFDB&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;1. HumanMessage - 用户消息&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第二篇 深度学习基础</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="第二篇深度学习基础快速回顾"&gt;第二篇:深度学习基础(快速回顾)&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e7%af%87%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e5%bf%ab%e9%80%9f%e5%9b%9e%e9%a1%be"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标读者&lt;/strong&gt;:有机器学习基础,需要快速掌握深度学习和PyTorch的读者&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习重点&lt;/strong&gt;:PyTorch实战、神经网络核心概念、CNN基础&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。&lt;/p&gt;
&lt;h3 id="为什么选择pytorch"&gt;为什么选择PyTorch?&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9pytorch"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动态计算图&lt;/strong&gt;:更符合Python编程习惯,易于调试&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学术界主流&lt;/strong&gt;:顶级会议论文大多使用PyTorch实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生态完善&lt;/strong&gt;:torchvision、torchaudio等丰富的扩展库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 2.x&lt;/strong&gt;:引入torch.compile,性能大幅提升&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="章节安排"&gt;章节安排&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第3章神经网络基础"&gt;&lt;a href="chapter03/README.md"&gt;第3章:神经网络基础&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3.1 从感知机到多层神经网络&lt;/li&gt;
&lt;li&gt;3.2 反向传播算法详解&lt;/li&gt;
&lt;li&gt;3.3 激活函数的选择与影响&lt;/li&gt;
&lt;li&gt;3.4 正则化技术:BatchNorm与Dropout&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实战&lt;/strong&gt;:使用PyTorch构建第一个神经网络(MNIST手写数字识别)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心技能&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;掌握PyTorch的基本操作(Tensor、autograd、nn.Module)&lt;/li&gt;
&lt;li&gt;理解神经网络的训练流程&lt;/li&gt;
&lt;li&gt;学会使用GPU加速训练&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第4章卷积神经网络cnn"&gt;&lt;a href="chapter04/README.md"&gt;第4章:卷积神经网络(CNN)&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9ccnn"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;4.1 卷积层的工作原理&lt;/li&gt;
&lt;li&gt;4.2 池化层与降维&lt;/li&gt;
&lt;li&gt;4.3 经典CNN架构:LeNet → AlexNet → VGG&lt;/li&gt;
&lt;li&gt;4.4 CNN的可视化与理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实战&lt;/strong&gt;:CIFAR-10图像分类(从零构建CNN)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;核心技能&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理解卷积操作的本质&lt;/li&gt;
&lt;li&gt;掌握CNN的设计原则&lt;/li&gt;
&lt;li&gt;学会使用torchvision进行图像处理&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="技术栈"&gt;技术栈&lt;a class="anchor" href="#%e6%8a%80%e6%9c%af%e6%a0%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境要求"&gt;环境要求&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e8%a6%81%e6%b1%82"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Python &amp;gt;= 3.10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;python --version
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 安装PyTorch (2025年推荐)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# CPU版本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision torchaudio
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# GPU版本(CUDA 12.1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 或使用uv(更快)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;uv pip install torch torchvision torchaudio&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="核心依赖"&gt;核心依赖&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e4%be%9d%e8%b5%96"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PyTorch &amp;gt;= 2.0&lt;/strong&gt;:深度学习框架&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;torchvision&lt;/strong&gt;:计算机视觉工具库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;:可视化&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tqdm&lt;/strong&gt;:进度条&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="验证安装"&gt;验证安装&lt;a class="anchor" href="#%e9%aa%8c%e8%af%81%e5%ae%89%e8%a3%85"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;PyTorch版本: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__version__&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;CUDA可用: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;CUDA版本: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;GPU设备: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_device_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="学习建议"&gt;学习建议&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1-动手实践为主"&gt;1. 动手实践为主&lt;a class="anchor" href="#1-%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%ba%e4%b8%bb"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;每个代码示例都要运行&lt;/strong&gt;:不要只看代码&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;修改超参数观察变化&lt;/strong&gt;:学习率、批次大小、网络层数等&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;尝试不同的数据集&lt;/strong&gt;:Fashion-MNIST、SVHN等&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="2-理解核心概念"&gt;2. 理解核心概念&lt;a class="anchor" href="#2-%e7%90%86%e8%a7%a3%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度下降&lt;/strong&gt;:深度学习的基石&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;:如何高效计算梯度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt;:防止过拟合的关键&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3-参考官方文档"&gt;3. 参考官方文档&lt;a class="anchor" href="#3-%e5%8f%82%e8%80%83%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/"&gt;PyTorch官方教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/index.html"&gt;PyTorch文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/vision/stable/index.html"&gt;torchvision文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="4-循序渐进"&gt;4. 循序渐进&lt;a class="anchor" href="#4-%e5%be%aa%e5%ba%8f%e6%b8%90%e8%bf%9b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第3章(1-2天) → 第4章(2-3天)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓ ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;理解基础 掌握CNN
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓ ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;为后续现代架构(ResNet、Transformer)打下坚实基础&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="与前后篇的关系"&gt;与前后篇的关系&lt;a class="anchor" href="#%e4%b8%8e%e5%89%8d%e5%90%8e%e7%af%87%e7%9a%84%e5%85%b3%e7%b3%bb"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第一篇:机器学习基础
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (线性模型、优化算法)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第二篇:深度学习基础 ← 当前篇
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (神经网络、CNN)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第三篇:现代CNN架构
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; (ResNet、EfficientNet等)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="代码规范"&gt;代码规范&lt;a class="anchor" href="#%e4%bb%a3%e7%a0%81%e8%a7%84%e8%8c%83"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇所有代码遵循以下规范:&lt;/p&gt;</description></item><item><title>大模型设计思想</title><link>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/3.-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/3.-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/</guid><description/></item><item><title>第03章 SVD与矩阵分解</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/</guid><description>&lt;h1 id="第03章svd与矩阵分解"&gt;第03章：SVD与矩阵分解&lt;a class="anchor" href="#%e7%ac%ac03%e7%ab%a0svd%e4%b8%8e%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：任何矩阵都可以看作&amp;quot;旋转-拉伸-旋转&amp;quot;的组合。SVD 是线性代数的终极武器。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="前言"&gt;前言&lt;a class="anchor" href="#%e5%89%8d%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;如果说线性代数有皇冠,那么&lt;strong&gt;奇异值分解 (SVD)&lt;/strong&gt; 就是皇冠上的明珠。Gilbert Strang 教授称其为&amp;quot;线性代数的顶峰&amp;quot;。&lt;/p&gt;
&lt;p&gt;在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的&lt;strong&gt;万能钥匙&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本章我们将从&lt;strong&gt;几何变换&lt;/strong&gt;的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为&lt;strong&gt;旋转、拉伸、再旋转&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#1-%e5%bc%95%e8%a8%80%e4%bb%8e%e5%9c%86%e5%88%b0%e6%a4%ad%e5%9c%86"&gt;引言:从圆到椭圆&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.1 &lt;a href="#11-%e7%9f%a9%e9%98%b5%e5%8f%98%e6%8d%a2%e7%9a%84%e6%9c%ac%e8%b4%a8"&gt;矩阵变换的本质&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1.2 &lt;a href="#12-%e7%89%b9%e5%be%81%e5%80%bc%e5%88%86%e8%a7%a3%e7%9a%84%e5%b1%80%e9%99%90"&gt;特征值分解的局限&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#2-%e7%89%b9%e5%be%81%e5%88%86%e8%a7%a3evd%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%e7%9a%84%e7%be%8e%e5%ad%a6"&gt;特征分解(EVD):对称矩阵的美学&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.1 &lt;a href="#21-%e8%b0%b1%e5%ae%9a%e7%90%86spectral-theorem"&gt;谱定理(Spectral Theorem)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.2 &lt;a href="#22-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89"&gt;几何直觉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.3 &lt;a href="#23-%e6%ad%a3%e5%ae%9a%e6%80%a7%e7%a2%97%e7%9a%84%e5%bd%a2%e7%8a%b6"&gt;正定性:碗的形状&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#3-%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3svd%e4%b8%87%e8%83%bd%e9%92%a5%e5%8c%99"&gt;奇异值分解(SVD):万能钥匙&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3.1 &lt;a href="#31-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%ae%a9%e9%9d%9e%e6%96%b9%e9%98%b5%e4%b9%9f%e8%83%bd%e5%af%b9%e8%a7%92%e5%8c%96"&gt;核心思想:让非方阵也能对角化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2 &lt;a href="#32-%e6%8e%a8%e5%af%bc-svd"&gt;推导 SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3 &lt;a href="#33-svd-%e7%9a%84%e5%87%a0%e4%bd%95%e5%9b%be%e6%99%af%e6%97%8b%e8%bd%ac-%e6%8b%89%e4%bc%b8-%e6%97%8b%e8%bd%ac"&gt;SVD 的几何图景:旋转-拉伸-旋转&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.4 &lt;a href="#34-%e8%96%84-svdreduced-svd"&gt;薄 SVD(Reduced SVD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.5 &lt;a href="#35-%e5%a4%96%e7%a7%af%e5%bd%a2%e5%bc%8fdyadic-expansion"&gt;外积形式(Dyadic Expansion)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#4-%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4%e7%9a%84-svd-%e8%a7%86%e8%a7%92"&gt;四个基本子空间的 SVD 视角&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4.1 &lt;a href="#41-%e5%9b%9e%e9%a1%be%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4"&gt;回顾:四个基本子空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.2 &lt;a href="#42-svd-%e7%9a%84%e5%ae%8c%e7%be%8e%e5%88%87%e5%88%86"&gt;SVD 的完美切分&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.3 &lt;a href="#43-%e6%ad%a3%e4%ba%a4%e5%85%b3%e7%b3%bb%e5%9b%be"&gt;正交关系图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.4 &lt;a href="#44-%e4%bc%aa%e9%80%86%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89"&gt;伪逆的几何意义&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#5-%e4%bd%8e%e7%a7%a9%e8%bf%91%e4%bc%bcsvd-%e7%9a%84%e6%9d%80%e6%89%8b%e7%ba%a7%e5%ba%94%e7%94%a8"&gt;低秩近似:SVD 的杀手级应用&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5.1 &lt;a href="#51-%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a"&gt;问题设定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 &lt;a href="#52-eckart-young-mirsky-%e5%ae%9a%e7%90%86"&gt;Eckart-Young-Mirsky 定理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.3 &lt;a href="#53-%e7%9b%b4%e8%a7%89%e4%b8%a2%e5%bc%83%e5%b0%8f%e5%a5%87%e5%bc%82%e5%80%bc--%e5%8e%bb%e5%99%aa"&gt;直觉:丢弃小奇异值 = 去噪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.4 &lt;a href="#54-%e5%ba%94%e7%94%a8-1%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9"&gt;应用 1:图像压缩&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.5 &lt;a href="#55-%e5%ba%94%e7%94%a8-2%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f%e4%b8%8e%e7%9f%a9%e9%98%b5%e8%a1%a5%e5%85%a8"&gt;应用 2:推荐系统与矩阵补全&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.6 &lt;a href="#56-%e5%ba%94%e7%94%a8-3%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90pca"&gt;应用 3:主成分分析(PCA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#6-svd-%e4%b8%8e-evd-%e7%9a%84%e8%81%94%e7%b3%bb"&gt;SVD 与 EVD 的联系&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6.1 &lt;a href="#61-%e6%a0%b8%e5%bf%83%e5%85%b3%e7%b3%bb"&gt;核心关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 &lt;a href="#62-%e7%89%b9%e6%ae%8a%e6%83%85%e5%86%b5%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5"&gt;特殊情况:对称矩阵&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#7-%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e7%ae%80%e8%bf%b0"&gt;计算方法简述&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;7.1 &lt;a href="#71-%e7%9b%b4%e6%8e%a5%e6%96%b9%e6%b3%95%e4%b8%8d%e6%8e%a8%e8%8d%90"&gt;直接方法(不推荐)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;7.2 &lt;a href="#72-%e5%ae%9e%e9%99%85%e7%ae%97%e6%b3%95"&gt;实际算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#8-%e6%80%bb%e7%bb%93"&gt;总结&lt;/a&gt;&lt;/p&gt;</description></item><item><title>第3章 TRL与强化学习实战</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</guid><description>&lt;h1 id="第3章trl-与强化学习实战-sft--dpo--ppo"&gt;第3章：TRL 与强化学习实战 (SFT / DPO / PPO)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0trl-%e4%b8%8e%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98-sft--dpo--ppo"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：从微调（SFT）到对齐（Alignment）。我们将复现 Hugging Face 官方 &lt;strong&gt;Alignment Handbook&lt;/strong&gt; 的核心流程，但为了让每位读者都能跑通，我们将基座模型替换为轻量级的 &lt;strong&gt;Qwen2-0.5B&lt;/strong&gt;。无论你是在 Colab 还是单卡 3090，都能完整体验 RLHF 的全过程。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%ae%8c%e6%95%b4%e7%9a%84%e5%af%b9%e9%bd%90%e6%b5%81%e6%b0%b4%e7%ba%bf-the-alignment-pipeline"&gt;1. 完整的对齐流水线 (The Alignment Pipeline)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-sft%e8%ae%a9-qwen-05b-%e5%ad%a6%e4%bc%9a%e6%8c%87%e4%bb%a4"&gt;2. SFT：让 Qwen-0.5B 学会指令&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f%e4%b8%8e-chat-template"&gt;2.1 数据格式与 Chat Template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e6%a0%b8%e5%bf%83%e6%8a%80%e5%b7%a7packing-%e5%8a%a0%e9%80%9f"&gt;2.2 核心技巧：Packing 加速&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%ae%9e%e6%88%98%e4%bb%a3%e7%a0%81"&gt;2.3 实战代码&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-dpo%e5%b7%a5%e4%b8%9a%e7%95%8c%e5%af%b9%e9%bd%90%e9%a6%96%e9%80%89"&gt;3. DPO：工业界对齐首选&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%95%b0%e6%8d%ae%e9%9b%86%e5%81%8f%e5%a5%bd%e5%af%b9%e6%98%af%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e7%9a%84"&gt;3.1 数据集：偏好对是如何构建的？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82beta-%e7%9a%84%e9%ad%94%e6%b3%95"&gt;3.2 关键超参：Beta 的魔法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-qwen-05b-%e8%b7%91%e9%80%9a-dpo"&gt;3.3 实战：使用 Qwen-0.5B 跑通 DPO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-ppo%e7%bb%8f%e5%85%b8-rlhf-%e4%b8%89%e9%98%b6%e6%ae%b5-%e8%bf%9b%e9%98%b6"&gt;4. PPO：经典 RLHF 三阶段 (进阶)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e8%ae%ad%e7%bb%83-reward-model-rm"&gt;4.1 训练 Reward Model (RM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-ppo-%e6%b5%81%e7%a8%8b%e8%af%a6%e8%a7%a3-actor-critic"&gt;4.2 PPO 流程详解 (Actor-Critic)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-2024-%e6%96%b0%e8%b6%8b%e5%8a%bforpo-%e4%b8%8e-kto"&gt;5.新兴趋势：ORPO 与 KTO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-完整的对齐流水线-the-alignment-pipeline"&gt;1. 完整的对齐流水线 (The Alignment Pipeline)&lt;a class="anchor" href="#1-%e5%ae%8c%e6%95%b4%e7%9a%84%e5%af%b9%e9%bd%90%e6%b5%81%e6%b0%b4%e7%ba%bf-the-alignment-pipeline"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一个标准的工业级 LLM 训练流程包含三个阶段：&lt;/p&gt;</description></item><item><title>第3章 与人类对齐：偏好优化</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/</guid><description>&lt;h1 id="第3章与人类对齐偏好优化-preference-alignment"&gt;第3章：与人类对齐：偏好优化 (Preference Alignment)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e4%b8%8e%e4%ba%ba%e7%b1%bb%e5%af%b9%e9%bd%90%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96-preference-alignment"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Alignment is the art of getting what you want, not just what you asked for.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;即使是最强的预训练模型，也只是学会了&amp;quot;续写&amp;quot;。是偏好优化让它学会了&amp;quot;对话&amp;quot;、&amp;ldquo;拒绝&amp;quot;和&amp;quot;价值观&amp;rdquo;。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%af%b9%e9%bd%90%e4%b8%89%e5%8e%9f%e5%88%99%e4%b8%8e-sft-%e7%9a%84%e5%b1%80%e9%99%90"&gt;一、对齐三原则与 SFT 的局限&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-hhh-%e5%8e%9f%e5%88%99%e6%9c%89%e7%94%a8%e8%af%9a%e5%ae%9e%e6%97%a0%e5%ae%b3"&gt;1. HHH 原则：有用、诚实、无害&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%b8%ba%e4%bb%80%e4%b9%88-sft-%e8%bf%98%e4%b8%8d%e5%a4%9f"&gt;2. 为什么 SFT 还不够？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e7%bb%8f%e5%85%b8%e8%b7%af%e7%ba%bfrlhf-ppo"&gt;二、经典路线：RLHF (PPO)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e8%ae%ad%e7%bb%83-reward-model-%e5%a5%96%e5%8a%b1%e6%a8%a1%e5%9e%8b"&gt;1. 训练 Reward Model (奖励模型)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-ppo-%e7%ae%97%e6%b3%95%e6%a0%b8%e5%bf%83kl-%e6%95%a3%e5%ba%a6%e4%b8%8e-policy-%e6%9b%b4%e6%96%b0"&gt;2. PPO 算法核心：KL 散度与 Policy 更新&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%ae%9e%e6%88%98%e6%89%8b%e5%8a%a8%e5%ae%9e%e7%8e%b0-ppo-step"&gt;3. 实战：手动实现 PPO Step&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e7%8e%b0%e4%bb%a3%e8%b7%af%e7%ba%bfdpo-direct-preference-optimization"&gt;三、现代路线：DPO (Direct Preference Optimization)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-dpo-%e7%9a%84%e6%95%b0%e5%ad%a6%e9%ad%94%e6%9c%af%e6%97%a0%e9%9c%80-reward-model"&gt;1. DPO 的数学魔术：无需 Reward Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-dpo-vs-ppo%e8%b0%81%e8%b5%a2%e4%ba%86"&gt;2. DPO vs PPO：谁赢了？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-trl-%e8%ae%ad%e7%bb%83-dpo"&gt;3. 实战：使用 TRL 训练 DPO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%89%8d%e6%b2%bf%e5%8f%98%e4%bd%93kto--ipo--orpo"&gt;四、前沿变体：KTO / IPO / ORPO&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-kto-%e5%a6%82%e6%9e%9c%e5%8f%aa%e6%9c%89%e8%b5%9e%e5%92%8c%e8%b8%a9%e6%b2%a1%e6%9c%89%e6%af%94%e8%be%83%e5%af%b9"&gt;1. KTO: 如果只有赞和踩，没有比较对&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-ipo-%e4%bf%ae%e5%a4%8d-dpo-%e7%9a%84%e9%95%bf%e5%ba%a6%e5%81%8f%e5%a5%bd%e9%97%ae%e9%a2%98"&gt;2. IPO: 修复 DPO 的长度偏好问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-orpo-%e8%bf%9e-sft-%e9%83%bd%e4%b8%8d%e9%9c%80%e8%a6%81%e4%ba%86"&gt;3. ORPO: 连 SFT 都不需要了？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-spin-%e8%87%aa%e6%88%91%e5%af%b9%e5%bc%88%e6%97%a0%e9%9c%80%e4%ba%ba%e5%b7%a5%e6%95%b0%e6%8d%ae"&gt;4. SPIN: 自我对弈，无需人工数据&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%9c%80%e6%96%b0%e8%bf%9b%e5%b1%95%e4%b8%8e%e8%b6%8b%e5%8a%bf"&gt;五、最新进展与趋势&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%bb%8e-rlhf-%e5%88%b0-rlaif-ai-feedback"&gt;1. 从 RLHF 到 RLAIF (AI Feedback)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-online-dpo-%e6%91%86%e8%84%b1%e9%9d%99%e6%80%81%e6%95%b0%e6%8d%ae%e9%9b%86"&gt;2. Online DPO: 摆脱静态数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%a4%9a%e7%9b%ae%e6%a0%87%e5%af%b9%e9%bd%90%e4%b8%8d%e5%8f%aa%e6%98%af-hhh"&gt;3. 多目标对齐：不只是 HHH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e5%af%b9%e9%bd%90%e7%a8%8e-alignment-tax"&gt;4. 对齐税 (Alignment Tax)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e4%b8%bb%e6%b5%81%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%af%b9%e9%bd%90%e7%ad%96%e7%95%a5"&gt;5. 主流模型的对齐策略 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;六、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一对齐三原则与-sft-的局限"&gt;一、对齐三原则与 SFT 的局限&lt;a class="anchor" href="#%e4%b8%80%e5%af%b9%e9%bd%90%e4%b8%89%e5%8e%9f%e5%88%99%e4%b8%8e-sft-%e7%9a%84%e5%b1%80%e9%99%90"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1-hhh-原则有用诚实无害"&gt;1. HHH 原则：有用、诚实、无害&lt;a class="anchor" href="#1-hhh-%e5%8e%9f%e5%88%99%e6%9c%89%e7%94%a8%e8%af%9a%e5%ae%9e%e6%97%a0%e5%ae%b3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;OpenAI 定义了对齐的三大支柱：&lt;/p&gt;</description></item><item><title>第3章 推理加速黑科技</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/</guid><description>&lt;h1 id="第3章推理加速黑科技-inference-acceleration"&gt;第3章：推理加速黑科技 (Inference Acceleration)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e6%8e%a8%e7%90%86%e5%8a%a0%e9%80%9f%e9%bb%91%e7%a7%91%e6%8a%80-inference-acceleration"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：在不改变模型权重的前提下,让推理速度提升 2-3 倍。核心技术:&lt;strong&gt;投机解码&lt;/strong&gt;(Speculative Decoding)、&lt;strong&gt;Medusa 多头预测&lt;/strong&gt;、&lt;strong&gt;Lookahead 前瞻解码&lt;/strong&gt;。这些技术已被集成到 vLLM/TGI/SGLang 等生产系统中。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e8%87%aa%e5%9b%9e%e5%bd%92%e8%a7%a3%e7%a0%81%e7%9a%84%e6%80%a7%e8%83%bd%e7%93%b6%e9%a2%88"&gt;1. 自回归解码的性能瓶颈&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e6%8e%a8%e7%90%86%e8%bf%99%e4%b9%88%e6%85%a2"&gt;1.1 为什么 Transformer 推理这么慢?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-batch-size1-%e7%9a%84gpu%e5%88%a9%e7%94%a8%e7%8e%87%e7%81%be%e9%9a%be"&gt;1.2 Batch Size=1 的GPU利用率灾难&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%8a%95%e6%9c%ba%e8%a7%a3%e7%a0%81-speculative-decoding"&gt;2. 投机解码 (Speculative Decoding)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%8d%89%e7%a8%bf%e6%a8%a1%e5%9e%8b--%e5%b9%b6%e8%a1%8c%e9%aa%8c%e8%af%81"&gt;2.1 核心思想：草稿模型 + 并行验证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e6%97%a0%e6%8d%9f%e5%8a%a0%e9%80%9f%e7%9a%84%e4%bf%9d%e8%af%81"&gt;2.2 数学原理：无损加速的保证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%ae%9e%e6%88%98%e7%94%a8-qwen2-05b-%e5%8a%a0%e9%80%9f-qwen2-7b"&gt;2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-medusa%e5%a4%9a%e5%a4%b4%e5%b9%b6%e8%a1%8c%e9%a2%84%e6%b5%8b"&gt;3. Medusa：多头并行预测&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%9e%b6%e6%9e%84%e5%9c%a8-lm-head-%e4%b9%8b%e4%b8%8a%e5%a2%9e%e5%8a%a0%e5%a4%9a%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%a4%b4"&gt;3.1 架构：在 LM Head 之上增加多个预测头&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e8%ae%ad%e7%bb%83%e8%87%aa%e7%9b%91%e7%9d%a3%e8%92%b8%e9%a6%8f"&gt;3.2 训练：自监督蒸馏&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-tree-attention-%e4%bc%98%e5%8c%96"&gt;3.3 Tree Attention 优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-lookahead-decoding%e5%89%8d%e7%9e%bb%e8%a7%a3%e7%a0%81"&gt;4. Lookahead Decoding：前瞻解码&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-n-gram-%e7%bc%93%e5%ad%98%e5%8e%9f%e7%90%86"&gt;4.1 N-gram 缓存原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-jacobi-%e8%bf%ad%e4%bb%a3%e5%b9%b6%e8%a1%8c%e5%8c%96"&gt;4.2 Jacobi 迭代并行化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90"&gt;4.3 适用场景分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e5%85%b6%e4%bb%96%e5%89%8d%e6%b2%bf%e6%8a%80%e6%9c%af"&gt;5. 其他前沿技术&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-eagle%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e6%8e%a8%e6%b5%8b"&gt;5.1 Eagle：基于特征的推测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-cascade-speculation%e5%b1%82%e7%ba%a7%e6%8e%a8%e6%b5%8b"&gt;5.2 Cascade Speculation：层级推测&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-自回归解码的性能瓶颈"&gt;1. 自回归解码的性能瓶颈&lt;a class="anchor" href="#1-%e8%87%aa%e5%9b%9e%e5%bd%92%e8%a7%a3%e7%a0%81%e7%9a%84%e6%80%a7%e8%83%bd%e7%93%b6%e9%a2%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-为什么-transformer-推理这么慢"&gt;1.1 为什么 Transformer 推理这么慢?&lt;a class="anchor" href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e6%8e%a8%e7%90%86%e8%bf%99%e4%b9%88%e6%85%a2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;LLM 的生成是&lt;strong&gt;自回归 (Autoregressive)&lt;/strong&gt; 的:每次只能生成一个 Token,必须等上一个 Token 出来才能生成下一个。&lt;/p&gt;</description></item><item><title>第3章 智能体（Agent）核心机制</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/</guid><description>&lt;h1 id="第3章智能体agent核心机制"&gt;第3章:智能体(Agent)核心机制&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e6%99%ba%e8%83%bd%e4%bd%93agent%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The future of AI is not just about better models, but about better systems.&amp;rdquo; - Andrew Ng&lt;/p&gt;
&lt;p&gt;智能体(Agent)将 LLM 从&amp;quot;大脑&amp;quot;变成了&amp;quot;双手&amp;quot;,让 AI 具备了与世界交互的能力。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="本章导读"&gt;本章导读&lt;a class="anchor" href="#%e6%9c%ac%e7%ab%a0%e5%af%bc%e8%af%bb"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本章专注于 &lt;strong&gt;Agent 设计模式与工程实现&lt;/strong&gt;,是构建自主智能系统的核心技术。我们将深入探讨:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ReAct/Plan-and-Solve&lt;/strong&gt; 等规划模式的代码实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tool Use / Function Calling&lt;/strong&gt; 的 JSON Schema 定义与解析&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MCP (Model Context Protocol)&lt;/strong&gt; 协议标准与实战&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt; 的 StateGraph 编程范式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory 系统&lt;/strong&gt;的短期/长期记忆设计&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Agent&lt;/strong&gt; 协作模式 (Supervisor/Hierarchical)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;边界说明&lt;/strong&gt; (参考 chapter-boundaries.md):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ &lt;strong&gt;本章包含&lt;/strong&gt;: Agent 架构设计、工具调用、MCP 协议、多智能体协作、Memory 机制&lt;/li&gt;
&lt;li&gt;❌ &lt;strong&gt;不包含&lt;/strong&gt;: CoT 数学原理 (→ Part 7 Ch3)、推理时搜索/MCTS (→ Part 7 Ch4)、强化学习训练 Agent (→ Part 7 Ch4)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e4%bb%8e-prompt-engineering-%e5%88%b0-agentic-workflow"&gt;一、从 Prompt Engineering 到 Agentic Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e8%a7%84%e5%88%92-planningreact-%e4%b8%8e-plan-and-solve"&gt;二、规划 (Planning):ReAct 与 Plan-and-Solve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8-tool-use-%e4%b8%8e-function-calling"&gt;三、工具使用 (Tool Use) 与 Function Calling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9bmcp-model-context-protocol-%e9%9d%a9%e5%91%bd"&gt;四、MCP (Model Context Protocol) 革命&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e8%ae%b0%e5%bf%86%e7%b3%bb%e7%bb%9f-memory-%e8%ae%be%e8%ae%a1"&gt;五、记忆系统 (Memory) 设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%adlanggraph%e7%8a%b6%e6%80%81%e6%9c%ba%e7%bc%96%e7%a8%8b%e8%8c%83%e5%bc%8f"&gt;六、LangGraph:状态机编程范式&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;6.1 StateGraph 核心概念&lt;/li&gt;
&lt;li&gt;6.2 实战:基于 LangGraph 的 ReAct Agent&lt;/li&gt;
&lt;li&gt;6.3 条件边与循环控制&lt;/li&gt;
&lt;li&gt;6.4 持久化 (Persistence): Multi-turn 对话的基础&lt;/li&gt;
&lt;li&gt;6.5 Human-in-the-loop: 敏感操作的审批机制&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e5%a4%9a%e6%99%ba%e8%83%bd%e4%bd%93%e5%8d%8f%e4%bd%9c-multi-agent"&gt;七、多智能体协作 (Multi-Agent)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%aboutput-parser%e7%bb%93%e6%9e%84%e5%8c%96%e8%be%93%e5%87%ba%e8%a7%a3%e6%9e%90"&gt;八、Output Parser:结构化输出解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b9%9d%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;九、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一从-prompt-engineering-到-agentic-workflow"&gt;一、从 Prompt Engineering 到 Agentic Workflow&lt;a class="anchor" href="#%e4%b8%80%e4%bb%8e-prompt-engineering-%e5%88%b0-agentic-workflow"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Andrew Ng 最近提出一个重要观点:&lt;strong&gt;与其追求更强的模型 (GPT-5),不如优化 Agent 工作流 (Agentic Workflow)&lt;/strong&gt;。
GPT-3.5 + 良好的工作流,往往能超越零样本的 GPT-4。&lt;/p&gt;</description></item><item><title>第3章 模型评估体系</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/</guid><description>&lt;h1 id="第3章模型评估体系-evaluation"&gt;第3章：模型评估体系 (Evaluation)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e4%bd%93%e7%b3%bb-evaluation"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：没有评估及格，模型绝不上线。本章将从传统的 BLEU 分数讲起，深入到 RAG 专属的 RAGAS 框架，并最终掌握目前业界最主流的 &lt;strong&gt;LLM-as-a-Judge&lt;/strong&gt; 模式。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%af%84%e4%bc%b0%e8%bf%99%e4%b9%88%e9%9a%be"&gt;1. 为什么评估这么难？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e8%af%84%e4%bc%b0%e7%bb%b4%e5%ba%a6%e7%9a%84%e5%b1%82%e7%ba%a7"&gt;2. 评估维度的层级&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e4%bc%a0%e7%bb%9f%e7%9a%84-n-gram-%e6%8c%87%e6%a0%87-bleurouge"&gt;2.1 传统的 N-gram 指标 (BLEU/ROUGE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e4%bb%bb%e5%8a%a1%e5%9e%8b%e6%8c%87%e6%a0%87-accf1"&gt;2.2 任务型指标 (Acc/F1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e8%af%ad%e4%b9%89%e5%9e%8b%e6%8c%87%e6%a0%87-bertscore"&gt;2.3 语义型指标 (BERTScore)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-rag-%e4%b8%93%e9%a1%b9%e8%af%84%e4%bc%b0ragas-%e6%a1%86%e6%9e%b6"&gt;3. RAG 专项评估：RAGAS 框架&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%a0%b8%e5%bf%83%e4%b8%89%e5%85%83%e7%bb%84query-context-answer"&gt;3.1 核心三元组：Query, Context, Answer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e5%bf%a0%e5%ae%9e%e5%ba%a6-faithfulness"&gt;3.2 忠实度 (Faithfulness)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e7%ad%94%e6%a1%88%e7%9b%b8%e5%85%b3%e6%80%a7-answer-relevancy"&gt;3.3 答案相关性 (Answer Relevancy)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-%e5%ae%9e%e6%88%98%e4%bb%a3%e7%a0%81%e4%bd%bf%e7%94%a8-ragas-%e8%af%84%e4%bc%b0%e6%9c%ac%e5%9c%b0-rag-%e7%b3%bb%e7%bb%9f"&gt;3.4 实战代码：使用 RAGAS 评估本地 RAG 系统&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e9%80%9a%e7%94%a8%e8%83%bd%e5%8a%9b%e8%af%84%e4%bc%b0opencompass-%e5%ae%9e%e6%88%98"&gt;4. 通用能力评估：OpenCompass 实战&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e6%a0%b8%e5%bf%83%e6%a6%9c%e5%8d%95%e8%a7%a3%e8%af%bb-c-eval--cmmlu--mmlu"&gt;4.1 核心榜单解读 (C-Eval / CMMLU / MMLU)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e4%b8%80%e9%94%ae%e8%b7%91%e5%88%86%e8%84%9a%e6%9c%ac"&gt;4.2 一键跑分脚本&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e7%bb%88%e6%9e%81%e6%96%b9%e6%a1%88llm-as-a-judge"&gt;5. 终极方案：LLM-as-a-Judge&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e5%8e%9f%e7%90%86%e7%94%a8%e9%ad%94%e6%b3%95%e6%89%93%e8%b4%a5%e9%ad%94%e6%b3%95"&gt;5.1 原理：用魔法打败魔法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e7%bc%96%e5%86%99-judge-prompt"&gt;5.2 编写 Judge Prompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-prometheus-eval%e5%bc%80%e6%ba%90%e8%a3%81%e5%88%a4%e6%a8%a1%e5%9e%8b"&gt;5.3 实战：使用 Prometheus-Eval（开源裁判模型）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-为什么评估这么难"&gt;1. 为什么评估这么难？&lt;a class="anchor" href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%af%84%e4%bc%b0%e8%bf%99%e4%b9%88%e9%9a%be"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在判别式 AI 时代（如分类、推荐），评估很简单：&lt;code&gt;Precision&lt;/code&gt;、&lt;code&gt;Recall&lt;/code&gt;、&lt;code&gt;F1&lt;/code&gt;，答案是确定的。&lt;/p&gt;</description></item><item><title>第3章 语言的基石：分词与嵌入</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/</guid><description>&lt;h1 id="第3章语言的基石分词与嵌入-tokenization--embedding"&gt;第3章：语言的基石：分词与嵌入 (Tokenization &amp;amp; Embedding)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e8%af%ad%e8%a8%80%e7%9a%84%e5%9f%ba%e7%9f%b3%e5%88%86%e8%af%8d%e4%b8%8e%e5%b5%8c%e5%85%a5-tokenization--embedding"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Words are, in my not-so-humble opinion, our most inexhaustible source of magic.&amp;rdquo; — Albus Dumbledore&lt;/p&gt;
&lt;p&gt;揭开 LLM 的第一个黑盒：理解机器如何将人类的语言转化为数学的语言。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%88%86%e8%af%8d%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5"&gt;一、分词：机器阅读的第一步&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%88%86%e8%af%8d"&gt;1. 为什么要分词？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%b8%bb%e6%b5%81%e5%88%86%e8%af%8d%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3"&gt;2. 主流分词算法详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%89%8d%e6%b2%bf%e5%88%86%e8%af%8d%e6%8a%80%e6%9c%af%e8%b6%8b%e5%8a%bf"&gt;3. 前沿分词技术趋势&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e5%b5%8c%e5%85%a5%e8%b5%8b%e4%ba%88%e8%af%8d%e8%af%ad%e6%95%b0%e5%ad%a6%e7%81%b5%e9%ad%82"&gt;二、嵌入：赋予词语数学灵魂&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%bb%8e-token-id-%e5%88%b0%e9%ab%98%e7%bb%b4%e5%90%91%e9%87%8f"&gt;1. 从 Token ID 到高维向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e5%87%a0%e4%bd%95%e5%a5%a5%e7%a7%98"&gt;2. 嵌入空间的几何奥秘&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b5%8c%e5%85%a5-vs-%e9%9d%99%e6%80%81%e5%b5%8c%e5%85%a5"&gt;3. 上下文嵌入 vs 静态嵌入&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e4%b8%8e%e4%bd%bf%e7%94%a8"&gt;三、代码实战：从零构建与使用&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-tiktoken-%e9%ab%98%e6%95%88%e5%88%86%e8%af%8d"&gt;1. 实战：使用 TikToken 高效分词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%ae%9e%e6%88%98%e5%8f%af%e8%a7%86%e5%8c%96%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4"&gt;2. 实战：可视化嵌入空间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba%e8%af%ad%e4%b9%89%e6%90%9c%e7%b4%a2%e5%bc%95%e6%93%8e"&gt;3. 实战：构建语义搜索引擎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;四、工程最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;五、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一分词机器阅读的第一步"&gt;一、分词：机器阅读的第一步&lt;a class="anchor" href="#%e4%b8%80%e5%88%86%e8%af%8d%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在模型眼中，&amp;ldquo;我爱你&amp;rdquo; 不是情感的表达，而是一串数字。将文本转换为这串数字的过程，就是&lt;strong&gt;分词 (Tokenization)&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="1-为什么要分词"&gt;1. 为什么要分词？&lt;a class="anchor" href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%88%86%e8%af%8d"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;你可能会问：为什么不直接用字符（Character）或者单词（Word）作为最小单位？&lt;/p&gt;
&lt;h4 id="-方案-a按字符切分-character-level"&gt;❌ 方案 A：按字符切分 (Character-level)&lt;a class="anchor" href="#-%e6%96%b9%e6%a1%88-a%e6%8c%89%e5%ad%97%e7%ac%a6%e5%88%87%e5%88%86-character-level"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;做法&lt;/strong&gt;：&lt;code&gt;&amp;quot;apple&amp;quot;&lt;/code&gt; → &lt;code&gt;['a', 'p', 'p', 'l', 'e']&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：词表极小（26个字母+符号），不会有未知词（OOV）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：序列太长。一句话变成几百个字符，模型注意力机制的计算量是序列长度的平方 ($O(N^2)$)，成本太高。而且单个字符缺乏语义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="-方案-b按单词切分-word-level"&gt;❌ 方案 B：按单词切分 (Word-level)&lt;a class="anchor" href="#-%e6%96%b9%e6%a1%88-b%e6%8c%89%e5%8d%95%e8%af%8d%e5%88%87%e5%88%86-word-level"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;做法&lt;/strong&gt;：&lt;code&gt;&amp;quot;I love apples&amp;quot;&lt;/code&gt; → &lt;code&gt;['I', 'love', 'apples']&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：语义完整，序列短。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：&lt;strong&gt;词表爆炸&lt;/strong&gt;。英语有几十万词，而且还要处理变形（run, running, ran）和新词（Covid-19, ChatGPT）。如果遇到词表中没有的词，只能由 &lt;code&gt;&amp;lt;UNK&amp;gt;&lt;/code&gt; 代替，丢失信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="-方案-c子词切分-subword-level--现代-llm-的选择"&gt;✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择&lt;a class="anchor" href="#-%e6%96%b9%e6%a1%88-c%e5%ad%90%e8%af%8d%e5%88%87%e5%88%86-subword-level--%e7%8e%b0%e4%bb%a3-llm-%e7%9a%84%e9%80%89%e6%8b%a9"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：&lt;strong&gt;常用词保持完整，生僻词拆解为字根&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;例子&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apple&lt;/code&gt; (常用) → &lt;code&gt;['apple']&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;unbelievable&lt;/code&gt; (较长) → &lt;code&gt;['un', 'believ', 'able']&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;平衡性&lt;/strong&gt;：词表大小适中（通常 30k-150k）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;处理未知词&lt;/strong&gt;：任何新词都可以拆成见过的子词。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多语言能力&lt;/strong&gt;：不同语言共享子词结构。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="2-主流分词算法详解"&gt;2. 主流分词算法详解&lt;a class="anchor" href="#2-%e4%b8%bb%e6%b5%81%e5%88%86%e8%af%8d%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;目前主流的大模型主要使用以下三种算法的变体：&lt;/p&gt;</description></item><item><title>第3章 预训练的奥秘：从数据到智能</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/</guid><description>&lt;h1 id="第3章预训练的奥秘从数据到智能-pretraining-from-data-to-intelligence"&gt;第3章：预训练的奥秘：从数据到智能 (Pretraining: From Data to Intelligence)&lt;a class="anchor" href="#%e7%ac%ac3%e7%ab%a0%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e5%a5%a5%e7%a7%98%e4%bb%8e%e6%95%b0%e6%8d%ae%e5%88%b0%e6%99%ba%e8%83%bd-pretraining-from-data-to-intelligence"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;We are drowning in information but starved for knowledge.&amp;rdquo; - John Naisbitt&lt;/p&gt;
&lt;p&gt;本章揭示预训练的核心秘密：如何将海量原始数据转化为模型的智能，理解Scaling Law背后的数学原理，掌握工业级预训练的工程技巧。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e4%b8%87%e7%89%a9%e7%9a%86%e5%8f%af%e5%ad%a6"&gt;一、预训练数据：万物皆可学&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e6%95%b0%e6%8d%ae%e8%a7%84%e6%a8%a1%e4%bb%8egb%e5%88%b0tb%e7%9a%84%e6%bc%94%e8%bf%9b"&gt;1.1 数据规模：从GB到TB的演进&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e6%95%b0%e6%8d%ae%e6%9d%a5%e6%ba%90%e4%b8%8e%e6%9e%84%e6%88%90"&gt;1.2 数据来源与构成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e8%b4%a8%e9%87%8f%e8%83%9c%e4%ba%8e%e6%95%b0%e9%87%8f"&gt;1.3 数据清洗：质量胜于数量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#14-%e6%95%b0%e6%8d%ae%e9%85%8d%e6%af%94%e4%b8%8e%e8%af%be%e7%a8%8b%e5%ad%a6%e4%b9%a0"&gt;1.4 数据配比与课程学习&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%80%83%e8%af%95%e9%a2%98"&gt;二、预训练目标：语言模型的&amp;quot;考试题&amp;quot;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bcausal-language-modeling-clm"&gt;2.1 因果语言模型（Causal Language Modeling, CLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bmasked-language-modeling-mlm"&gt;2.2 掩码语言模型（Masked Language Modeling, MLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%89%8d%e7%bc%80%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%85%b6%e4%bb%96%e5%8f%98%e4%bd%93"&gt;2.3 前缀语言模型与其他变体&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89scaling-law%e8%a7%84%e6%a8%a1%e7%9a%84%e5%8a%9b%e9%87%8f"&gt;三、Scaling Law：规模的力量&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e6%97%a9%e6%9c%9f%e5%8f%91%e7%8e%b0kaplan-scaling-law-2020"&gt;3.1 早期发现：Kaplan Scaling Law (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e8%8c%83%e5%bc%8f%e8%bd%ac%e5%8f%98chinchilla-scaling-law-2022"&gt;3.2 范式转变：Chinchilla Scaling Law (2022)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#4-%e5%ae%9e%e6%88%98%e7%ae%97%e8%b4%a6%e8%ae%ad%e7%bb%83%e9%a2%84%e7%ae%97%e4%bc%b0%e7%ae%97"&gt;实战算账：训练预算估算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%e8%b4%a8%e5%8f%98%e7%9a%84%e4%b8%b4%e7%95%8c%e7%82%b9"&gt;3.3 涌现能力：质变的临界点&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#331-the-grokking-phenomenon%e9%a1%bf%e6%82%9f%e7%8e%b0%e8%b1%a1"&gt;3.3.1 The Grokking Phenomenon：顿悟现象&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92scaling-law%e7%9a%84%e6%96%b0%e5%8f%91%e7%8e%b0"&gt;3.4 当前视角：Scaling Law的新发现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e5%b7%a5%e7%a8%8b%e6%8c%91%e6%88%98"&gt;四、预训练的工程挑战&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e8%ae%ad%e7%bb%83%e7%a8%b3%e5%ae%9a%e6%80%a7%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8%e4%b8%8e%e6%b6%88%e5%a4%b1"&gt;4.1 训练稳定性：梯度爆炸与消失&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83fp16-vs-bf16"&gt;4.2 混合精度训练：FP16 vs BF16&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5"&gt;4.3 分布式训练策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-%e5%86%85%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e5%b7%a7"&gt;4.4 内存优化技巧&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#45-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92%e6%96%b0%e4%b8%80%e4%bb%a3%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e6%8a%80%e6%9c%af"&gt;4.5 当前视角：新一代高效训练技术&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a0%b8%e5%bf%83%e5%9b%b0%e6%83%91"&gt;💡 深度问答：预训练核心困惑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e6%b7%b1%e5%b1%82%e5%8e%9f%e7%90%86%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e6%95%88"&gt;五、预训练的深层原理：为什么有效？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%a2%84%e8%ae%ad%e7%bb%83-%e5%be%ae%e8%b0%83%e8%8c%83%e5%bc%8f%e6%9c%89%e6%95%88"&gt;5.1 为什么预训练-微调范式有效？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%80%9d%e8%80%83%e4%b8%8e%e7%bb%83%e4%b9%a0"&gt;思考与练习&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;本章概览&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第三篇 LangGraph 深入</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/</guid><description>&lt;h1 id="第三篇-langgraph-深入从-chain-到-graph-的思维跃迁"&gt;第三篇 LangGraph 深入：从 Chain 到 Graph 的思维跃迁&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%89%e7%af%87-langgraph-%e6%b7%b1%e5%85%a5%e4%bb%8e-chain-%e5%88%b0-graph-%e7%9a%84%e6%80%9d%e7%bb%b4%e8%b7%83%e8%bf%81"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id="-本篇概要"&gt;📌 本篇概要&lt;a class="anchor" href="#-%e6%9c%ac%e7%af%87%e6%a6%82%e8%a6%81"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇将深入 LangGraph 的核心架构，从生产级 State 设计模式到原子化的控制流。&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th style="text-align: left"&gt;章节&lt;/th&gt;
 &lt;th style="text-align: left"&gt;核心内容&lt;/th&gt;
 &lt;th style="text-align: left"&gt;学习目标&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;第1章&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;架构哲学&lt;/td&gt;
 &lt;td style="text-align: left"&gt;BSP 模型、Pregel 运行时机制&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;第2章&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;状态工程&lt;/td&gt;
 &lt;td style="text-align: left"&gt;&lt;code&gt;MessagesState&lt;/code&gt; 标准范式、Input/Output Schema 分离&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;第3章&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;路由控制&lt;/td&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;Command API&lt;/strong&gt; 原子化路由&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;第4章&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;持久化与记忆&lt;/td&gt;
 &lt;td style="text-align: left"&gt;Checkpoint 快照机制、Time Travel 状态回滚&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td style="text-align: left"&gt;&lt;strong&gt;第5章&lt;/strong&gt;&lt;/td&gt;
 &lt;td style="text-align: left"&gt;生产级模式&lt;/td&gt;
 &lt;td style="text-align: left"&gt;Streaming 流式输出、运行时配置 Config&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;💡 &lt;strong&gt;前置知识&lt;/strong&gt;: 需掌握第二篇的 Agent 基础。本篇代码基于 LangChain 1.0+ 和 LangGraph 最新标准。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="第1章langgraph-架构哲学-architecture"&gt;第1章：LangGraph 架构哲学 (Architecture)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0langgraph-%e6%9e%b6%e6%9e%84%e5%93%b2%e5%ad%a6-architecture"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-从无状态-dag-到有状态-actor"&gt;1.1 从无状态 DAG 到有状态 Actor&lt;a class="anchor" href="#11-%e4%bb%8e%e6%97%a0%e7%8a%b6%e6%80%81-dag-%e5%88%b0%e6%9c%89%e7%8a%b6%e6%80%81-actor"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在 LangChain 时代，我们构建的是 &lt;strong&gt;DAG (有向无环图)&lt;/strong&gt;，数据像流水一样经过 &lt;code&gt;Prompt -&amp;gt; Model -&amp;gt; Parser&lt;/code&gt;。&lt;/p&gt;</description></item><item><title>第三篇 计算机视觉核心技术</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/</guid><description>&lt;h1 id="第三篇计算机视觉核心技术"&gt;第三篇:计算机视觉核心技术&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%89%e7%af%87%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇深入讲解现代计算机视觉的核心技术,包括经典CNN架构、注意力机制、Transformer以及先进的训练技巧。这些技术是当今计算机视觉领域的基石。&lt;/p&gt;
&lt;h2 id="本篇目标"&gt;本篇目标&lt;a class="anchor" href="#%e6%9c%ac%e7%af%87%e7%9b%ae%e6%a0%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;掌握现代CNN架构&lt;/strong&gt;:ResNet、MobileNet、EfficientNet的设计思想和实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;理解注意力机制&lt;/strong&gt;:从Self-Attention到Vision Transformer的演进&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掌握训练技巧&lt;/strong&gt;:数据增强、学习率调度、正则化等高级技术&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实战能力&lt;/strong&gt;:能够使用预训练模型进行迁移学习和fine-tuning&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="技术栈"&gt;技术栈&lt;a class="anchor" href="#%e6%8a%80%e6%9c%af%e6%a0%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;框架&lt;/strong&gt;: PyTorch 2.x&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型库&lt;/strong&gt;: torchvision.models, timm, transformers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据增强&lt;/strong&gt;: albumentations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工具&lt;/strong&gt;: tensorboard, wandb(可选)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="章节安排"&gt;章节安排&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第5章现代cnn架构"&gt;第5章:现代CNN架构&lt;a class="anchor" href="#%e7%ac%ac5%e7%ab%a0%e7%8e%b0%e4%bb%a3cnn%e6%9e%b6%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;深入讲解ResNet、MobileNet、EfficientNet等经典架构,理解残差连接、深度可分离卷积、复合缩放等核心概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ResNet残差连接解决梯度消失&lt;/li&gt;
&lt;li&gt;MobileNet轻量化设计思想&lt;/li&gt;
&lt;li&gt;EfficientNet复合缩放策略&lt;/li&gt;
&lt;li&gt;迁移学习与fine-tuning实战&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战项目&lt;/strong&gt;: 使用ResNet50在自定义数据集上进行迁移学习&lt;/p&gt;
&lt;h3 id="第6章attention与transformer"&gt;第6章:Attention与Transformer&lt;a class="anchor" href="#%e7%ac%ac6%e7%ab%a0attention%e4%b8%8etransformer"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;从注意力机制的基本原理出发,深入理解Transformer架构,并学习Vision Transformer(ViT)在图像领域的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self-Attention机制原理&lt;/li&gt;
&lt;li&gt;Multi-Head Attention设计&lt;/li&gt;
&lt;li&gt;Transformer架构详解&lt;/li&gt;
&lt;li&gt;Vision Transformer(ViT)实现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战项目&lt;/strong&gt;: 使用ViT进行图像分类&lt;/p&gt;
&lt;h3 id="第7章数据增强与训练技巧"&gt;第7章:数据增强与训练技巧&lt;a class="anchor" href="#%e7%ac%ac7%e7%ab%a0%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%e4%b8%8e%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;掌握现代深度学习训练的各种技巧,包括数据增强、学习率调度、正则化等,构建高性能训练流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统数据增强:翻转、裁剪、色彩变换&lt;/li&gt;
&lt;li&gt;现代数据增强:Mixup、CutMix、AutoAugment&lt;/li&gt;
&lt;li&gt;学习率调度:Cosine Annealing、Warmup&lt;/li&gt;
&lt;li&gt;完整训练流程设计&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实战项目&lt;/strong&gt;: 构建生产级训练流程&lt;/p&gt;
&lt;h2 id="学习路径"&gt;学习路径&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e8%b7%af%e5%be%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第5章:现代CNN架构
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;理解残差连接、轻量化设计
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第6章:Attention与Transformer
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;掌握注意力机制、ViT架构
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;第7章:数据增强与训练技巧
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ↓
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;完整训练流程实战&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="环境准备"&gt;环境准备&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 安装核心依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install timm &lt;span class="c1"&gt;# PyTorch Image Models&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install transformers &lt;span class="c1"&gt;# Hugging Face Transformers&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install albumentations &lt;span class="c1"&gt;# 数据增强&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install tensorboard &lt;span class="c1"&gt;# 可视化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install opencv-python pillow matplotlib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 可选:实验追踪&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install wandb&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="性能基准"&gt;性能基准&lt;a class="anchor" href="#%e6%80%a7%e8%83%bd%e5%9f%ba%e5%87%86"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;不同架构在ImageNet-1K上的性能对比(Top-1准确率):&lt;/p&gt;</description></item><item><title>Agent最佳设计模式</title><link>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid><description>&lt;h1 id="4-agent最佳设计模式与生产实践"&gt;4. Agent最佳设计模式与生产实践&lt;a class="anchor" href="#4-agent%e6%9c%80%e4%bd%b3%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f%e4%b8%8e%e7%94%9f%e4%ba%a7%e5%ae%9e%e8%b7%b5"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;版本&lt;/strong&gt;: LangChain 1.0.7+ | LangGraph 1.0.3+
&lt;strong&gt;定位&lt;/strong&gt;: Agent系统从设计到生产的完整实践指南
&lt;strong&gt;更新&lt;/strong&gt;: 2025-11-20&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="概述"&gt;概述&lt;a class="anchor" href="#%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本笔记系统总结了大模型Agent开发与部署的核心实践,涵盖架构设计、性能优化、可靠性保障、监控运维等生产环节。所有内容基于真实项目经验,提供可运行的完整代码示例。&lt;/p&gt;
&lt;h3 id="与langchain笔记的关系"&gt;与《LangChain笔记》的关系&lt;a class="anchor" href="#%e4%b8%8elangchain%e7%ac%94%e8%ae%b0%e7%9a%84%e5%85%b3%e7%b3%bb"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;建议学习路径:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;《LangChain笔记》第一~三篇 (基础) → 本实践笔记 (生产)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本笔记深化《LangChain笔记》第七篇(高级应用)和第八篇(生产实践)的内容,补充实战细节。&lt;/p&gt;
&lt;h3 id="学习目标"&gt;学习目标&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e7%9b%ae%e6%a0%87"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;掌握核心技能&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设计可扩展的Agent架构&lt;/li&gt;
&lt;li&gt;优化响应速度和成本&lt;/li&gt;
&lt;li&gt;构建可靠的错误处理机制&lt;/li&gt;
&lt;li&gt;建立完善的监控体系&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;达成生产标准&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;响应时间 &amp;lt; 1s (P95)&lt;/li&gt;
&lt;li&gt;系统可用性 &amp;gt; 99.9%&lt;/li&gt;
&lt;li&gt;错误恢复自动化&lt;/li&gt;
&lt;li&gt;全链路可观测&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第一部分-架构设计模式"&gt;&lt;a href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f"&gt;第一部分: 架构设计模式&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86-%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e5%8d%95agent%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1"&gt;1.1 单Agent架构设计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e7%8a%b6%e6%80%81%e7%ae%a1%e7%90%86%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5"&gt;1.2 状态管理最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e5%b7%a5%e5%85%b7%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1"&gt;1.3 工具系统设计&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第二部分-性能优化实战"&gt;&lt;a href="#%e7%ac%ac%e4%ba%8c%e9%83%a8%e5%88%86%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e5%ae%9e%e6%88%98"&gt;第二部分: 性能优化实战&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e9%83%a8%e5%88%86-%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e5%93%8d%e5%ba%94%e9%80%9f%e5%ba%a6%e4%bc%98%e5%8c%96"&gt;2.1 响应速度优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e6%88%90%e6%9c%ac%e4%bc%98%e5%8c%96%e7%ad%96%e7%95%a5"&gt;2.2 成本优化策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e5%b9%b6%e5%8f%91%e4%b8%8e%e5%bc%82%e6%ad%a5%e5%a4%84%e7%90%86"&gt;2.3 并发与异步处理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第三部分-可靠性保障"&gt;&lt;a href="#%e7%ac%ac%e4%b8%89%e9%83%a8%e5%88%86%e5%8f%af%e9%9d%a0%e6%80%a7%e4%bf%9d%e9%9a%9c"&gt;第三部分: 可靠性保障&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%89%e9%83%a8%e5%88%86-%e5%8f%af%e9%9d%a0%e6%80%a7%e4%bf%9d%e9%9a%9c"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e9%94%99%e8%af%af%e5%a4%84%e7%90%86%e4%b8%8e%e9%87%8d%e8%af%95"&gt;3.1 错误处理与重试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e9%99%8d%e7%ba%a7%e4%b8%8e%e7%86%94%e6%96%ad"&gt;3.2 降级与熔断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e6%b5%8b%e8%af%95%e7%ad%96%e7%95%a5"&gt;3.3 测试策略&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第四部分-监控运维"&gt;&lt;a href="#%e7%ac%ac%e5%9b%9b%e9%83%a8%e5%88%86%e7%9b%91%e6%8e%a7%e8%bf%90%e7%bb%b4"&gt;第四部分: 监控运维&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac%e5%9b%9b%e9%83%a8%e5%88%86-%e7%9b%91%e6%8e%a7%e8%bf%90%e7%bb%b4"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e5%8f%af%e8%a7%82%e6%b5%8b%e6%80%a7%e5%bb%ba%e8%ae%be"&gt;4.1 可观测性建设&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e5%85%b3%e9%94%ae%e6%8c%87%e6%a0%87%e7%9b%91%e6%8e%a7"&gt;4.2 关键指标监控&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第五部分-案例研究"&gt;&lt;a href="#%e7%ac%ac%e4%ba%94%e9%83%a8%e5%88%86%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6"&gt;第五部分: 案例研究&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%94%e9%83%a8%e5%88%86-%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e6%99%ba%e8%83%bd%e5%ae%a2%e6%9c%8dagent%e4%bc%98%e5%8c%96%e5%ae%9e%e6%88%98"&gt;5.1 智能客服Agent优化实战&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id="第一部分架构设计模式"&gt;第一部分:架构设计模式&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="11-单agent架构设计"&gt;1.1 单Agent架构设计&lt;a class="anchor" href="#11-%e5%8d%95agent%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="核心原则"&gt;核心原则&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;单Agent架构遵循以下设计原则:&lt;/p&gt;</description></item><item><title>第04章 概率分布 指数族与共轭先验</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/</guid><description>&lt;h1 id="第04章-指数族分布"&gt;第04章 指数族分布&lt;a class="anchor" href="#%e7%ac%ac04%e7%ab%a0-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="前言"&gt;前言&lt;a class="anchor" href="#%e5%89%8d%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在概率论的浩瀚海洋中,&lt;strong&gt;指数族分布 (Exponential Family)&lt;/strong&gt; 是一座灯塔。它不仅仅是高斯分布、伯努利分布等常见分布的集合,更是它们背后的&lt;strong&gt;通用模版&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;为什么线性回归、逻辑回归的梯度公式长得一模一样?为什么最大熵原理最终指向了它?为什么贝叶斯推断需要共轭先验?&lt;/p&gt;
&lt;p&gt;本章将带你深入这个&amp;quot;上帝的指纹&amp;quot;,揭示看似无关的算法背后统一的数学本质。学完本章,你将不再是一个个地记忆公式,而是掌握了生成公式的&lt;strong&gt;元规则&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e5%bc%95%e8%a8%80"&gt;引言&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83%e7%9a%84%e5%ae%9a%e4%b9%89"&gt;1. 指数族分布的定义&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e4%bb%8e%e4%bc%af%e5%8a%aa%e5%88%a9%e5%88%86%e5%b8%83%e5%bc%80%e5%a7%8b"&gt;1.1 从伯努利分布开始&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83%e7%9a%84%e6%94%b9%e5%86%99"&gt;1.2 高斯分布的改写&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e6%8c%87%e6%95%b0%e6%97%8f%e7%9a%84%e6%a0%87%e5%87%86%e5%bd%a2%e5%bc%8f"&gt;1.3 指数族的标准形式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#14-%e5%af%b9%e6%95%b0%e9%85%8d%e5%88%86%e5%87%bd%e6%95%b0%e7%9a%84%e5%ae%9a%e4%b9%89"&gt;1.4 对数配分函数的定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#15-%e5%b8%b8%e8%a7%81%e5%88%86%e5%b8%83%e7%9a%84%e6%8c%87%e6%95%b0%e6%97%8f%e5%bd%a2%e5%bc%8f"&gt;1.5 常见分布的指数族形式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83%e7%9a%84%e6%80%a7%e8%b4%a8"&gt;2. 指数族分布的性质&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e4%b8%80%e9%98%b6%e5%af%bc%e6%95%b0%e6%9c%9f%e6%9c%9b"&gt;2.1 一阶导数:期望&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e4%ba%8c%e9%98%b6%e5%af%bc%e6%95%b0%e6%96%b9%e5%b7%ae"&gt;2.2 二阶导数:方差&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-fisher-%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5"&gt;2.3 Fisher 信息矩阵&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#24-%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1%e7%9a%84%e7%9f%a9%e5%8c%b9%e9%85%8d"&gt;2.4 最大似然估计的矩匹配&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5"&gt;3. 指数族分布与最大熵&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e9%97%ae%e9%a2%98%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e6%a6%82%e7%8e%87%e5%88%86%e5%b8%83"&gt;3.1 问题:如何选择概率分布?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e7%86%b5%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86"&gt;3.2 熵与最大熵原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e6%8e%a8%e5%af%bc%e6%9c%80%e5%a4%a7%e7%86%b5%e5%88%86%e5%b8%83%e6%98%af%e6%8c%87%e6%95%b0%e6%97%8f"&gt;3.3 推导:最大熵分布是指数族&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-%e4%be%8b%e5%ad%90-1%e9%ab%98%e6%96%af%e5%88%86%e5%b8%83"&gt;3.4 例子 1:高斯分布&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#35-%e4%be%8b%e5%ad%90-2%e6%8c%87%e6%95%b0%e5%88%86%e5%b8%83"&gt;3.5 例子 2:指数分布&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#36-%e4%be%8b%e5%ad%90-3%e7%a6%bb%e6%95%a3%e5%9d%87%e5%8c%80%e5%88%86%e5%b8%83"&gt;3.6 例子 3:离散均匀分布&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83%e4%b8%8e%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b-glm"&gt;4. 指数族分布与广义线性模型 (GLM)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e9%97%ae%e9%a2%98%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e4%b8%8e%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e7%bb%9f%e4%b8%80"&gt;4.1 问题:线性回归与逻辑回归的统一&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-glm-%e7%9a%84%e5%ae%9a%e4%b9%89"&gt;4.2 GLM 的定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e6%a0%b8%e5%bf%83%e6%8e%a8%e5%af%bcglm-%e7%9a%84%e7%bb%9f%e4%b8%80%e6%a2%af%e5%ba%a6%e5%85%ac%e5%bc%8f"&gt;4.3 核心推导:GLM 的统一梯度公式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-hessian-%e7%9f%a9%e9%98%b5%e5%87%b8%e6%80%a7%e4%bf%9d%e8%af%81"&gt;4.4 Hessian 矩阵:凸性保证&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#45-%e4%be%8b%e5%ad%90-1%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92-%e9%ab%98%e6%96%af-glm"&gt;4.5 例子 1:线性回归 (高斯 GLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#46-%e4%be%8b%e5%ad%90-2%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92-%e4%bc%af%e5%8a%aa%e5%88%a9-glm"&gt;4.6 例子 2:逻辑回归 (伯努利 GLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#47-%e4%be%8b%e5%ad%90-3%e6%b3%8a%e6%9d%be%e5%9b%9e%e5%bd%92-%e6%b3%8a%e6%9d%be-glm"&gt;4.7 例子 3:泊松回归 (泊松 GLM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#48-glm-%e7%9a%84%e5%87%a0%e4%bd%95%e7%90%86%e8%a7%a3"&gt;4.8 GLM 的几何理解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e6%80%bb%e7%bb%93"&gt;5. 总结&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e4%b8%bb%e8%a6%81%e7%bb%93%e8%ae%ba"&gt;5.1 主要结论&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8c%87%e6%95%b0%e6%97%8f%e5%a6%82%e6%ad%a4%e9%87%8d%e8%a6%81"&gt;5.2 为什么指数族如此重要?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e5%85%b3%e9%94%ae%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5%e8%a1%a8"&gt;5.3 关键公式速查表&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae"&gt;参考文献&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="引言"&gt;引言&lt;a class="anchor" href="#%e5%bc%95%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在机器学习中,我们会遇到各种各样的概率分布:&lt;/p&gt;</description></item><item><title>第4章 DeepSpeed分布式训练</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</guid><description>&lt;h1 id="第4章deepspeed分布式训练"&gt;第4章：DeepSpeed分布式训练&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0deepspeed%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：突破单卡显存瓶颈。学习编写 &lt;code&gt;ds_config.json&lt;/code&gt;，掌握 ZeRO 系列优化器，并对比 PyTorch 原生 FSDP。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-deepspeed"&gt;1. 为什么需要 DeepSpeed？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%a0%b8%e5%bf%83ds_configjson-%e9%85%8d%e7%bd%ae%e5%ae%9e%e6%88%98"&gt;2. 核心：ds_config.json 配置实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-zero-3%e4%b8%8eoffload%e5%ae%9e%e6%88%98"&gt;3. ZeRO-3与Offload实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83"&gt;4. 混合精度训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e5%a4%9a%e8%8a%82%e7%82%b9%e8%ae%ad%e7%bb%83-multi-node"&gt;5. 多节点训练 (Multi-Node)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-为什么需要-deepspeed"&gt;1. 为什么需要 DeepSpeed？&lt;a class="anchor" href="#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-deepspeed"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;当模型参数量超过显存限制（例如在 24G 显存上训练 13B 模型）时，普通的 DDP (Distributed Data Parallel) 就无能为力了。DeepSpeed 的核心武器是 &lt;strong&gt;ZeRO (Zero Redundancy Optimizer)&lt;/strong&gt;，它将模型状态切分到不同的 GPU 上。&lt;/p&gt;
&lt;h3 id="zero-三阶段简单记忆版"&gt;ZeRO 三阶段（简单记忆版）&lt;a class="anchor" href="#zero-%e4%b8%89%e9%98%b6%e6%ae%b5%e7%ae%80%e5%8d%95%e8%ae%b0%e5%bf%86%e7%89%88"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ZeRO-1&lt;/strong&gt;: 切分&lt;strong&gt;优化器状态&lt;/strong&gt; (Optimizer States)。显存节省 4 倍。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZeRO-2&lt;/strong&gt;: 切分&lt;strong&gt;优化器状态 + 梯度&lt;/strong&gt; (Gradients)。显存节省 8 倍。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ZeRO-3&lt;/strong&gt;: 切分&lt;strong&gt;优化器状态 + 梯度 + 模型参数&lt;/strong&gt; (Parameters)。显存节省与 GPU 数量成正比 (线性扩展)。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="model-states-详解显存的三大占用来源"&gt;Model States 详解：显存的三大占用来源&lt;a class="anchor" href="#model-states-%e8%af%a6%e8%a7%a3%e6%98%be%e5%ad%98%e7%9a%84%e4%b8%89%e5%a4%a7%e5%8d%a0%e7%94%a8%e6%9d%a5%e6%ba%90"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在训练过程中，GPU 显存主要被以下三类数据占用（称为 &lt;strong&gt;Model States&lt;/strong&gt;）：&lt;/p&gt;</description></item><item><title>第4章 创建更优的嵌入模型</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第4章高性能嵌入模型实战-embedding-models"&gt;第4章：高性能嵌入模型实战 (Embedding Models)&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0%e9%ab%98%e6%80%a7%e8%83%bd%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%ae%9e%e6%88%98-embedding-models"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Good representations are the foundation of AI.&amp;rdquo; —— 优秀的表示层是人工智能的基石。本章将从零开始，深入探讨如何构建用于语义检索（Semantic Search）和 RAG 的高性能嵌入模型。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%b8%8e%e6%9e%b6%e6%9e%84"&gt;第一节：嵌入模型的本质与架构&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e8%af%ad%e4%b9%89%e9%b8%bf%e6%b2%9f"&gt;1.1 为什么需要嵌入模型？（语义鸿沟）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#12-%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e6%a0%b8%e5%bf%83%e6%9e%b6%e6%9e%84-bi-vs-cross"&gt;1.2 嵌入模型核心架构 (Bi vs Cross)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#13-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8"&gt;1.3 嵌入空间的数学本质&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#14-sota%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%af%b9%e6%af%94"&gt;1.4 SOTA嵌入模型对比&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#15-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93"&gt;1.5 本节小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%ba%8c%e8%8a%82%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e4%b8%8einfonce%e6%8d%9f%e5%a4%b1"&gt;第二节：对比学习与InfoNCE损失&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3"&gt;2.1 对比学习的核心思想&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-infonce%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e8%af%a6%e8%a7%a3"&gt;2.2 InfoNCE损失函数详解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-in-batch-negatives-%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5"&gt;2.3 In-Batch Negatives 高效训练策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#24-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82%e6%b8%a9%e5%ba%a6%e7%b3%bb%e6%95%b0-tau-%e7%9a%84%e5%bd%b1%e5%93%8d"&gt;2.4 关键超参：温度系数 ($\tau$) 的影响&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#25-%e5%ae%9e%e6%88%98%e5%ae%9e%e7%8e%b0%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e8%ae%ad%e7%bb%83%e5%99%a8"&gt;2.5 实战：实现对比学习训练器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#26-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93"&gt;2.6 本节小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac%e6%8c%96%e6%8e%98"&gt;第三节：数据工程：难负样本挖掘&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac"&gt;3.1 为什么需要难负样本？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-%e9%9d%99%e6%80%81%e6%8c%96%e6%8e%98bm25%e7%ad%96%e7%95%a5"&gt;3.2 静态挖掘：BM25策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e5%8a%a8%e6%80%81%e6%8c%96%e6%8e%98ance%e7%ae%97%e6%b3%95"&gt;3.3 动态挖掘：ANCE算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-%e5%90%88%e6%88%90%e6%95%b0%e6%8d%aellm%e8%92%b8%e9%a6%8f-data-distillation"&gt;3.4 合成数据：LLM蒸馏 (Data Distillation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#35-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93"&gt;3.5 本节小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e5%9b%9b%e8%8a%82%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%81%94%e5%90%88%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%b5%8c%e5%a5%97%e8%a1%a8%e7%a4%ba"&gt;第四节：多任务联合训练与嵌套表示&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83"&gt;4.1 为什么需要多任务训练？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6"&gt;4.2 多任务训练框架&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-matryoshka%e5%b5%8c%e5%a5%97%e5%b5%8c%e5%85%a5-mrl"&gt;4.3 Matryoshka嵌套嵌入 (MRL)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93"&gt;4.4 本节小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac%e4%ba%94%e8%8a%82%e4%bb%8e%e9%9b%b6%e5%ae%9e%e6%88%98%e8%ae%ad%e7%bb%83%e4%b8%8e%e9%83%a8%e7%bd%b2"&gt;第五节：从零实战：训练与部署&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#51-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b%e4%bb%a3%e7%a0%81"&gt;5.1 完整训练流程代码&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#52-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0mteb%e5%9f%ba%e5%87%86"&gt;5.2 模型评估：MTEB基准&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#53-%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2%e5%bb%ba%e8%ae%ae"&gt;5.3 生产环境部署建议&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e7%ac%ac4%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;第4章小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0"&gt;思考练习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99"&gt;参考资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="第一节嵌入模型的本质与架构"&gt;第一节：嵌入模型的本质与架构&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e8%8a%82%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%b8%8e%e6%9e%b6%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-为什么需要嵌入模型语义鸿沟"&gt;1.1 为什么需要嵌入模型？（语义鸿沟）&lt;a class="anchor" href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e8%af%ad%e4%b9%89%e9%b8%bf%e6%b2%9f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在深入技术细节前，我们先回答一个根本问题：为什么传统的关键词搜索（如 ElasticSearch 的默认设置）在 AI 时代不够用了？&lt;/p&gt;</description></item><item><title>第4章 多模态大模型原理</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/</guid><description>&lt;h1 id="第4章多模态大模型原理"&gt;第4章：多模态大模型原理&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%8e%9f%e7%90%86"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;核心定位&lt;/strong&gt;：理解文本-图像等多模态交互的核心技术（CLIP、ViT、LLaVA）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边界约束&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ 包含：CLIP 对比学习、ViT 架构、LLaVA 连接器、多模态推理实战&lt;/li&gt;
&lt;li&gt;❌ 不包含：Transformer 基础机制（已在 Part 2 第1章）、对比学习基础理论（已在 Part 3 第4章）&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%a4%9a%e6%a8%a1%e6%80%81%e7%9a%84%e7%9b%b4%e8%a7%89%e7%90%86%e8%a7%a3%e5%9b%be%e5%83%8f%e4%bd%9c%e4%b8%ba%e5%a4%96%e8%af%ad"&gt;多模态的直觉理解：图像作为&amp;quot;外语&amp;quot;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e7%bb%9f%e4%b8%80-token-%e5%8c%96omni-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e7%9f%b3"&gt;统一 Token 化：Omni 模型的基石&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e8%a7%86%e8%a7%89%e7%bc%96%e7%a0%81%e5%99%a8vision-transformer-vit"&gt;视觉编码器：Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%9b%be%e6%96%87%e5%af%b9%e9%bd%90clip"&gt;图文对齐：CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e5%a4%9a%e6%a8%a1%e6%80%81%e5%a4%a7%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84llava"&gt;多模态大模型架构：LLaVA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e8%a7%86%e9%a2%91%e7%90%86%e8%a7%a3video-as-frames"&gt;视频理解：Video as Frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e5%ae%9e%e6%88%98%e5%a4%9a%e6%a8%a1%e6%80%81%e7%90%86%e8%a7%a3%e5%ba%94%e7%94%a8"&gt;实战：多模态理解应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ab2025%e8%a7%86%e8%a7%92connector-vs-native-multimodal"&gt;2025视角：Connector vs Native Multimodal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b9%9d%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b"&gt;总结与展望&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="一多模态的直觉理解图像作为外语"&gt;一、多模态的直觉理解：图像作为&amp;quot;外语&amp;quot;&lt;a class="anchor" href="#%e4%b8%80%e5%a4%9a%e6%a8%a1%e6%80%81%e7%9a%84%e7%9b%b4%e8%a7%89%e7%90%86%e8%a7%a3%e5%9b%be%e5%83%8f%e4%bd%9c%e4%b8%ba%e5%a4%96%e8%af%ad"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-token-space-alignment为什么图像可以被视为外语"&gt;1.1 Token Space Alignment：为什么图像可以被视为&amp;quot;外语&amp;quot;&lt;a class="anchor" href="#11-token-space-alignment%e4%b8%ba%e4%bb%80%e4%b9%88%e5%9b%be%e5%83%8f%e5%8f%af%e4%bb%a5%e8%a2%ab%e8%a7%86%e4%b8%ba%e5%a4%96%e8%af%ad"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;想象你是一个只懂中文的语言模型（LLM）。现在，有人拿着一张图片，用一种你从未见过的语言（&amp;ldquo;图像语&amp;rdquo;）向你描述。你该怎么办？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心挑战&lt;/strong&gt;：LLM 只理解文本 Token，而图像是像素矩阵。就像中文和英文一样，它们是&lt;strong&gt;两个完全不同的&amp;quot;语言空间&amp;quot;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案：跨模态对齐（Cross-Modal Alignment）&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;┌─────────────┐ ┌─────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ 图像空间 │ │ 文本空间 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ (像素矩阵) │ │ (Token 序列) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ │ │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ [255, 0] │ │ &amp;#34;一只猫&amp;#34; │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ [128, 64] │ │ &amp;#34;在草地上&amp;#34; │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ [...] │ │ &amp;#34;躺着&amp;#34; │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└──────┬──────┘ └──────┬──────┘
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ 通过对齐训练 │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │ (CLIP、LLaVA 等) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ▼ ▼
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;┌────────────────────────────────────────────┐
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ 共享语义空间 (Shared Latent Space) │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ &amp;#34;猫&amp;#34; ≈ [0.8, -0.3, 0.5, ...] │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ 🐱 ≈ [0.82, -0.28, 0.51, ...] │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;│ 距离很近 → 语义相似！ │
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└────────────────────────────────────────────┘&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第4章 推理模型专题</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/</guid><description>&lt;h1 id="第4章推理模型专题-reasoning-models-o1--deepseek-r1"&gt;第4章：推理模型专题 (Reasoning Models: O1 &amp;amp; DeepSeek-R1)&lt;a class="anchor" href="#%e7%ac%ac4%e7%ab%a0%e6%8e%a8%e7%90%86%e6%a8%a1%e5%9e%8b%e4%b8%93%e9%a2%98-reasoning-models-o1--deepseek-r1"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;当前最前沿的赛道。从 OpenAI 的闭源 o1 到 DeepSeek 的开源 R1，大模型终于学会了&amp;quot;慢思考&amp;quot;。本章深入探讨推理模型的理论基础、核心技术与实战应用。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e6%8e%a8%e7%90%86%e6%a8%a1%e5%9e%8b%e7%9a%84%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80"&gt;一、推理模型的理论基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8copenai-o1-%e7%9a%84%e6%8a%80%e6%9c%af%e7%8c%9c%e6%83%b3"&gt;二、OpenAI o1 的技术猜想&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89deepseek-r1-%e7%9a%84%e6%8a%80%e6%9c%af%e8%a7%a3%e5%af%86"&gt;三、DeepSeek-R1 的技术解密&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e6%8e%a8%e7%90%86%e6%97%b6%e8%ae%a1%e7%ae%97%e5%a2%9e%e5%bc%ba%e5%ae%9e%e6%88%98"&gt;四、推理时计算增强实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94%e9%aa%8c%e8%af%81%e5%99%a8-verifier-%e8%ae%ad%e7%bb%83"&gt;五、验证器 (Verifier) 训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一推理模型的理论基础"&gt;一、推理模型的理论基础&lt;a class="anchor" href="#%e4%b8%80%e6%8e%a8%e7%90%86%e6%a8%a1%e5%9e%8b%e7%9a%84%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1-什么是推理模型"&gt;1. 什么是推理模型?&lt;a class="anchor" href="#1-%e4%bb%80%e4%b9%88%e6%98%af%e6%8e%a8%e7%90%86%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;推理模型(Reasoning Model)是指能够进行&lt;strong&gt;多步骤逻辑推理&lt;/strong&gt;的大语言模型。与传统的&amp;quot;快速响应&amp;quot;模式不同,推理模型会：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;展开思维链&lt;/strong&gt; - 将复杂问题分解为多个子问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自我验证&lt;/strong&gt; - 检查中间步骤的正确性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回溯修正&lt;/strong&gt; - 发现错误时重新推理&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种能力在数学、代码、逻辑推理等任务上至关重要。&lt;/p&gt;
&lt;h4 id="快速推理-vs-深度推理"&gt;快速推理 vs 深度推理&lt;a class="anchor" href="#%e5%bf%ab%e9%80%9f%e6%8e%a8%e7%90%86-vs-%e6%b7%b1%e5%ba%a6%e6%8e%a8%e7%90%86"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;特性&lt;/th&gt;
 &lt;th&gt;快速推理 (Fast Thinking)&lt;/th&gt;
 &lt;th&gt;深度推理 (Slow Thinking)&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;响应时间&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;秒级&lt;/td&gt;
 &lt;td&gt;分钟级&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;思考步骤&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;1-3步&lt;/td&gt;
 &lt;td&gt;10-100+步&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;闲聊、翻译、摘要&lt;/td&gt;
 &lt;td&gt;数学、编程、逻辑推理&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;代表模型&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;GPT-4、Claude-3&lt;/td&gt;
 &lt;td&gt;O1、R1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;成本&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;低&lt;/td&gt;
 &lt;td&gt;高(10-50倍)&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class="mermaid"&gt;graph LR
 A[用户问题] --&amp;gt; B{是否需要深度推理?}
 B --&amp;gt;|否| C[快速推理模型&amp;lt;br/&amp;gt;直接生成答案]
 B --&amp;gt;|是| D[推理模型&amp;lt;br/&amp;gt;展开思维链]
 D --&amp;gt; E[步骤1: 理解问题]
 E --&amp;gt; F[步骤2: 制定方案]
 F --&amp;gt; G[步骤3: 执行计算]
 G --&amp;gt; H[步骤4: 验证答案]
 H --&amp;gt; I{正确?}
 I --&amp;gt;|否| F
 I --&amp;gt;|是| J[输出最终答案]&lt;/pre&gt;&lt;h3 id="2-chain-of-thought-cot-的数学原理"&gt;2. Chain-of-Thought (CoT) 的数学原理&lt;a class="anchor" href="#2-chain-of-thought-cot-%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;CoT 的核心思想是:&lt;strong&gt;让模型输出中间推理步骤,而不是直接给出答案&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>第四篇 RAG基础篇(LangChain篇)</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/</guid><description>&lt;h1 id="第四篇rag基础篇langchain生产实战"&gt;第四篇：RAG基础篇（LangChain生产实战）&lt;a class="anchor" href="#%e7%ac%ac%e5%9b%9b%e7%af%87rag%e5%9f%ba%e7%a1%80%e7%af%87langchain%e7%94%9f%e4%ba%a7%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="-前置准备"&gt;📋 前置准备&lt;a class="anchor" href="#-%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境配置"&gt;环境配置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 核心依赖（LangChain 1.0+）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.7
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain-openai&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.3
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain-core&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;1.0.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain-community&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.4.1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain-text-splitters&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.4.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 向量数据库&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install langchain-chroma&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.2.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install chromadb&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.5.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 可选依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install pypdf &lt;span class="c1"&gt;# PDF文档支持&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install python-dotenv &lt;span class="c1"&gt;# 环境变量管理&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="环境变量设置"&gt;环境变量设置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%ae%be%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# .env&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sk&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;your&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;api&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 可选：启用LangSmith追踪&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;LANGSMITH_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;your&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;langsmith&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;LANGSMITH_TRACING&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;LANGSMITH_PROJECT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rag&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="第-1-章rag架构与核心概念"&gt;第 1 章：RAG架构与核心概念&lt;a class="anchor" href="#%e7%ac%ac-1-%e7%ab%a0rag%e6%9e%b6%e6%9e%84%e4%b8%8e%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h4 id="11-什么是rag"&gt;1.1 什么是RAG？&lt;a class="anchor" href="#11-%e4%bb%80%e4%b9%88%e6%98%afrag"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;**RAG（Retrieval-Augmented Generation）**是一种结合检索和生成的技术，通过从外部知识库检索相关信息来增强LLM的回答能力。&lt;/p&gt;
&lt;h5 id="111-为什么需要rag"&gt;1.1.1 为什么需要RAG？&lt;a class="anchor" href="#111-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81rag"&gt;#&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;LLM的两大限制&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;有限的上下文窗口&lt;/strong&gt; - 无法一次性处理整个文档库&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;静态知识&lt;/strong&gt; - 训练数据固化在某个时间点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;RAG的解决方案&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在查询时动态检索相关外部知识&lt;/li&gt;
&lt;li&gt;将检索到的上下文注入到LLM提示中&lt;/li&gt;
&lt;li&gt;生成基于实时数据的准确回答&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="112-rag完整架构"&gt;1.1.2 RAG完整架构&lt;a class="anchor" href="#112-rag%e5%ae%8c%e6%95%b4%e6%9e%b6%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h5&gt;
&lt;pre class="mermaid"&gt;graph TB
 subgraph &amp;#34;离线索引阶段 Indexing&amp;#34;
 A[📄 原始文档&amp;lt;br/&amp;gt;Documents] --&amp;gt; B[📥 文档加载&amp;lt;br/&amp;gt;Document Loaders]
 B --&amp;gt; C[✂️ 文本分割&amp;lt;br/&amp;gt;Text Splitters]
 C --&amp;gt; D[🔢 向量化&amp;lt;br/&amp;gt;Embeddings]
 D --&amp;gt; E[(🗄️ 向量存储&amp;lt;br/&amp;gt;Vector Store)]
 end

 subgraph &amp;#34;在线检索阶段 Retrieval&amp;#34;
 F[❓ 用户查询&amp;lt;br/&amp;gt;User Query] --&amp;gt; G[🔢 查询向量化&amp;lt;br/&amp;gt;Query Embedding]
 G --&amp;gt; H[🔍 相似度检索&amp;lt;br/&amp;gt;Similarity Search]
 H --&amp;gt; E
 E --&amp;gt; I[📑 Top-K文档&amp;lt;br/&amp;gt;Retrieved Docs]
 end

 subgraph &amp;#34;生成阶段 Generation&amp;#34;
 F --&amp;gt; J[💬 提示模板&amp;lt;br/&amp;gt;Prompt Template]
 I --&amp;gt; J
 J --&amp;gt; K[🤖 LLM生成&amp;lt;br/&amp;gt;Chat Model]
 K --&amp;gt; L[✅ 最终答案&amp;lt;br/&amp;gt;Response]
 end

 style A fill:#FFE4E1
 style E fill:#E3F2FD
 style L fill:#C8E6C9&lt;/pre&gt;&lt;h5 id="113-rag工作流程"&gt;1.1.3 RAG工作流程&lt;a class="anchor" href="#113-rag%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b"&gt;#&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;阶段一：离线索引（Indexing）&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第四篇 RAG基础篇(LlamaIndex篇)</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/</guid><description>&lt;h1 id="第四篇-rag基础篇-llamaindex"&gt;第四篇 RAG基础篇 (LlamaIndex)&lt;a class="anchor" href="#%e7%ac%ac%e5%9b%9b%e7%af%87-rag%e5%9f%ba%e7%a1%80%e7%af%87-llamaindex"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="前置准备"&gt;前置准备&lt;a class="anchor" href="#%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境配置"&gt;环境配置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 核心依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.11.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-core&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.11.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-llms-openai&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.2.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-embeddings-openai&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.2.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 向量数据库集成（可选）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-vector-stores-chroma
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install chromadb&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;0.5.0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 其他依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install pypdf &lt;span class="c1"&gt;# PDF支持&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install python-dotenv &lt;span class="c1"&gt;# 环境变量管理&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="环境变量设置"&gt;环境变量设置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%ae%be%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# .env 文件&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nv"&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;sk-your-api-key-here&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="准备测试数据"&gt;准备测试数据&lt;a class="anchor" href="#%e5%87%86%e5%a4%87%e6%b5%8b%e8%af%95%e6%95%b0%e6%8d%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 创建数据目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir -p ./data
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 创建示例文档&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;LlamaIndex 是一个数据框架，专为 RAG（检索增强生成）应用设计。它提供了简单的接口来加载、索引和查询数据。&amp;#34;&lt;/span&gt; &amp;gt; ./data/intro.txt&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="第-1-章为什么选择-llamaindex"&gt;第 1 章：为什么选择 LlamaIndex？&lt;a class="anchor" href="#%e7%ac%ac-1-%e7%ab%a0%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9-llamaindex"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-llamaindex-vs-langchain设计哲学对比"&gt;1.1 LlamaIndex vs LangChain：设计哲学对比&lt;a class="anchor" href="#11-llamaindex-vs-langchain%e8%ae%be%e8%ae%a1%e5%93%b2%e5%ad%a6%e5%af%b9%e6%af%94"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="核心定位差异"&gt;核心定位差异&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e5%ae%9a%e4%bd%8d%e5%b7%ae%e5%bc%82"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;维度&lt;/th&gt;
 &lt;th&gt;LlamaIndex&lt;/th&gt;
 &lt;th&gt;LangChain&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;核心定位&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;数据优先框架（Data Framework）&lt;/td&gt;
 &lt;td&gt;编排优先框架（Orchestration Framework）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;主要用途&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;RAG、文档问答、知识库&lt;/td&gt;
 &lt;td&gt;Agent、复杂链式调用、工作流&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;抽象层级&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;高层抽象（开箱即用）&lt;/td&gt;
 &lt;td&gt;低层抽象（灵活组合）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;学习曲线&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;平缓（5行代码启动）&lt;/td&gt;
 &lt;td&gt;陡峭（需理解LCEL、Runnable）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;索引能力&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;强（多种索引类型）&lt;/td&gt;
 &lt;td&gt;弱（需自行实现）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;数据连接&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;丰富（100+ Loaders）&lt;/td&gt;
 &lt;td&gt;基础（需集成）&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;最佳场景&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;RAG、搜索、文档分析&lt;/td&gt;
 &lt;td&gt;Agent、复杂工作流、多步推理&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id="设计哲学"&gt;设计哲学&lt;a class="anchor" href="#%e8%ae%be%e8%ae%a1%e5%93%b2%e5%ad%a6"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;LlamaIndex 的核心理念&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第四篇 目标检测与YOLO系列</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/</guid><description>&lt;h1 id="第四篇目标检测深入yolo系列重点"&gt;第四篇：目标检测深入(YOLO系列重点)&lt;a class="anchor" href="#%e7%ac%ac%e5%9b%9b%e7%af%87%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e6%b7%b1%e5%85%a5yolo%e7%b3%bb%e5%88%97%e9%87%8d%e7%82%b9"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;核心篇章&lt;/strong&gt; - 深入讲解YOLO系列从v1到YOLO11的完整演进，理论与实战并重&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="篇章定位"&gt;篇章定位&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e5%ae%9a%e4%bd%8d"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇是整个计算机视觉笔记的&lt;strong&gt;重点篇章&lt;/strong&gt;，专注于目标检测领域最重要的YOLO系列算法。从2016年YOLOv1的横空出世，到2024年YOLO11的最新进展，我们将系统学习YOLO如何改变目标检测领域。&lt;/p&gt;
&lt;h2 id="为什么yolo如此重要"&gt;为什么YOLO如此重要？&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88yolo%e5%a6%82%e6%ad%a4%e9%87%8d%e8%a6%81"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;单阶段检测的开创者&lt;/strong&gt; - 将检测问题转换为回归问题，实现真正的实时检测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工业界首选方案&lt;/strong&gt; - 在速度和精度间达到最佳平衡，广泛应用于生产环境&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续快速迭代&lt;/strong&gt; - 从v1到v11，每一代都带来显著的性能提升和创新&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;易用性强&lt;/strong&gt; - Ultralytics提供的API简洁高效，降低了应用门槛&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="内容结构"&gt;内容结构&lt;a class="anchor" href="#%e5%86%85%e5%ae%b9%e7%bb%93%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第9章yolo系列演进理论核心"&gt;第9章：YOLO系列演进(理论核心)&lt;a class="anchor" href="#%e7%ac%ac9%e7%ab%a0yolo%e7%b3%bb%e5%88%97%e6%bc%94%e8%bf%9b%e7%90%86%e8%ae%ba%e6%a0%b8%e5%bf%83"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;深入讲解YOLO各版本的架构演进和核心创新：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.1 YOLOv1-v3：单阶段检测的崛起&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv1：开创性的单阶段检测&lt;/li&gt;
&lt;li&gt;YOLOv2(YOLO9000)：Anchor机制引入&lt;/li&gt;
&lt;li&gt;YOLOv3：多尺度特征金字塔&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.2 YOLOv4-v5：工程优化与实用化&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv4：Bag of Freebies和Bag of Specials&lt;/li&gt;
&lt;li&gt;YOLOv5：Ultralytics的工程实现&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.3 YOLOv6-v7：架构创新&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv6：工业应用优化&lt;/li&gt;
&lt;li&gt;YOLOv7：可训练Bag-of-Freebies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.4 YOLOv8：Ultralytics新一代&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anchor-free设计&lt;/li&gt;
&lt;li&gt;多任务统一框架&lt;/li&gt;
&lt;li&gt;性能基准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.5 YOLOv9、YOLOv10、YOLO11：最新进展&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv9：PGI和GELAN&lt;/li&gt;
&lt;li&gt;YOLOv10：NMS-free设计&lt;/li&gt;
&lt;li&gt;YOLO11：当前最优方案&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;9.6 YOLO-World：开放词汇检测&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;零样本检测能力&lt;/li&gt;
&lt;li&gt;与视觉-语言模型的结合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第10章yolo实战项目代码实战"&gt;第10章：YOLO实战项目(代码实战)&lt;a class="anchor" href="#%e7%ac%ac10%e7%ab%a0yolo%e5%ae%9e%e6%88%98%e9%a1%b9%e7%9b%ae%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;基于最新的YOLOv8和YOLO11的完整实战：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;10.1 YOLOv8快速上手&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;环境配置&lt;/li&gt;
&lt;li&gt;预训练模型使用&lt;/li&gt;
&lt;li&gt;多种推理模式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;10.2 自定义数据集训练&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据集准备和标注&lt;/li&gt;
&lt;li&gt;训练配置详解&lt;/li&gt;
&lt;li&gt;训练监控和调优&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;10.3 模型导出与部署&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ONNX导出&lt;/li&gt;
&lt;li&gt;TensorRT加速&lt;/li&gt;
&lt;li&gt;边缘设备部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;10.4 实战：构建实时检测系统&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第05章 线性回归</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid><description>&lt;h1 id="第05章-线性回归"&gt;第05章 线性回归&lt;a class="anchor" href="#%e7%ac%ac05%e7%ab%a0-%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Simplicity is the ultimate sophistication.&amp;rdquo; —— Leonardo da Vinci&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：线性回归是机器学习的 &amp;ldquo;Hello World&amp;rdquo;，但请不要轻视它。&lt;/p&gt;
&lt;p&gt;它是寻找真理的第一步。当我们试图用一条直线去拟合混沌的世界时，我们在坚持一种古老的信仰：&lt;strong&gt;世界在本质上是简单的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本章将带你经历一次认知的跃迁：从&lt;strong&gt;几何的投影&lt;/strong&gt;(最小二乘)，到&lt;strong&gt;概率的似然&lt;/strong&gt;(高斯噪声)，再到&lt;strong&gt;信念的约束&lt;/strong&gt;(贝叶斯正则化)。当你发现这三种截然不同的视角最终指向同一个公式时，你将领悟到数学那令人战栗的统一之美。这不仅仅是推导公式，这是在触摸统计学习的灵魂。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-%e5%bc%95%e8%a8%80"&gt;引言&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95least-squares-estimation-lse"&gt;最小二乘法(Least Squares Estimation, LSE)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;2.1 &lt;a href="#21-%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0"&gt;目标函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.2 &lt;a href="#22-%e5%87%a0%e4%bd%95%e8%a7%86%e8%a7%92%e6%8a%95%e5%bd%b1"&gt;几何视角:投影&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.3 &lt;a href="#23-%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89"&gt;正规方程的物理意义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.4 &lt;a href="#24-%e8%a7%a3%e6%9e%90%e8%a7%a3"&gt;解析解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%a6%82%e7%8e%87%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1mle"&gt;概率视角:最大似然估计(MLE)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;3.1 &lt;a href="#31-%e6%a6%82%e7%8e%87%e6%a8%a1%e5%9e%8b"&gt;概率模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2 &lt;a href="#32-%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0"&gt;似然函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3 &lt;a href="#33-mle--lse"&gt;MLE ⟺ LSE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e6%ad%a3%e5%88%99%e5%8c%96regularization"&gt;正则化(Regularization)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;4.1 &lt;a href="#41-%e9%97%ae%e9%a2%98%e7%9a%84%e6%8f%90%e5%87%ba"&gt;问题的提出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.2 &lt;a href="#42-ridge-%e5%9b%9e%e5%bd%92l2-%e6%ad%a3%e5%88%99%e5%8c%96"&gt;Ridge 回归(L2 正则化)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.3 &lt;a href="#43-lasso-%e5%9b%9e%e5%bd%92l1-%e6%ad%a3%e5%88%99%e5%8c%96"&gt;Lasso 回归(L1 正则化)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e5%90%8e%e9%aa%8c%e4%bc%b0%e8%ae%a1map"&gt;贝叶斯视角:最大后验估计(MAP)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;5.1 &lt;a href="#51-%e5%85%88%e9%aa%8c%e5%88%86%e5%b8%83"&gt;先验分布&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 &lt;a href="#52-ridge--%e9%ab%98%e6%96%af%e5%85%88%e9%aa%8c"&gt;Ridge = 高斯先验&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.3 &lt;a href="#53-lasso--%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c"&gt;Lasso = 拉普拉斯先验&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.4 &lt;a href="#54-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c%e5%af%bc%e8%87%b4%e7%a8%80%e7%96%8f%e6%80%a7"&gt;为什么拉普拉斯先验导致稀疏性?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-%e6%80%bb%e7%bb%93"&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-引言"&gt;1. 引言&lt;a class="anchor" href="#1-%e5%bc%95%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;回归问题&lt;/strong&gt;的目标是预测&lt;strong&gt;连续值&lt;/strong&gt;。给定训练数据 ${(\mathbf{x}&lt;em&gt;i, y_i)}&lt;/em&gt;{i=1}^N$，其中 $\mathbf{x}_i \in \mathbb{R}^p$ 是特征向量，$y_i \in \mathbb{R}$ 是标签，我们希望学习一个函数 $f: \mathbb{R}^p \to \mathbb{R}$，使得对新的输入 $\mathbf{x}$，能够准确预测 $y = f(\mathbf{x})$。&lt;/p&gt;</description></item><item><title>第5章 模型安全与可解释性</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/</guid><description>&lt;h1 id="第5章模型安全与可解释性"&gt;第5章：模型安全与可解释性&lt;a class="anchor" href="#%e7%ac%ac5%e7%ab%a0%e6%a8%a1%e5%9e%8b%e5%ae%89%e5%85%a8%e4%b8%8e%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;即使是最强大的模型，如果不可控，也是危险的。本章探讨如何给AI装上&amp;quot;刹车&amp;quot;（Safety）和&amp;quot;显微镜&amp;quot;（Interpretability）。&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;聚焦&lt;strong&gt;机械可解释性&lt;/strong&gt;（Mechanistic Interpretability）与&lt;strong&gt;稀疏自编码器&lt;/strong&gt;（SAE）&lt;/li&gt;
&lt;li&gt;区分安全攻击类型：&lt;strong&gt;Prompt Injection&lt;/strong&gt;（提示词注入）vs &lt;strong&gt;Jailbreak&lt;/strong&gt;（越狱）&lt;/li&gt;
&lt;li&gt;理论（Superposition、Induction Heads）+ 实战（SAE训练、TransformerLens）&lt;/li&gt;
&lt;li&gt;面向研究与工程的安全与可解释性完整方案&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;学习目标&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;掌握Prompt Injection与Jailbreak的本质区别与防御策略&lt;/li&gt;
&lt;li&gt;理解机械可解释性的核心原理（归纳头、特征叠加）&lt;/li&gt;
&lt;li&gt;实践稀疏自编码器（SAE）训练与特征提取&lt;/li&gt;
&lt;li&gt;使用TransformerLens进行模型内部机制探索&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%ae%89%e5%85%a8%e7%bb%b4%e5%ba%a6prompt-injection-vs-jailbreak"&gt;一、安全维度：Prompt Injection vs Jailbreak&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-prompt-injection%e6%8c%87%e4%bb%a4%e5%8a%ab%e6%8c%81"&gt;1. Prompt Injection：指令劫持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-jailbreak%e5%af%b9%e9%bd%90%e7%aa%81%e7%a0%b4"&gt;2. Jailbreak：对齐突破&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e8%87%aa%e5%8a%a8%e5%8c%96%e8%b6%8a%e7%8b%b1gcg%e6%94%bb%e5%87%bb"&gt;3. 自动化越狱：GCG攻击&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-many-shot-jailbreaking%e9%95%bf%e6%96%87%e6%9c%ac%e6%b4%97%e8%84%91"&gt;4. Many-Shot Jailbreaking：长文本洗脑&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e9%98%b2%e5%be%a1%e4%bd%93%e7%b3%bb%e6%9e%84%e5%bb%ba%e4%bc%81%e4%b8%9a%e7%ba%a7%e6%8a%a4%e6%a0%8f"&gt;二、防御体系：构建企业级护栏&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba%e8%bf%87%e6%bb%a4guardrails"&gt;1. 输入输出过滤（Guardrails）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e9%98%b2%e5%be%a1%e5%ae%9e%e6%88%98nvidia-nemo-guardrails%e9%85%8d%e7%bd%ae"&gt;2. 防御实战：NVIDIA NeMo Guardrails配置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e9%b2%81%e6%a3%92%e6%80%a7%e5%af%b9%e9%bd%90robust-alignment"&gt;3. 鲁棒性对齐（Robust Alignment）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e6%9c%ba%e6%a2%b0%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7%e6%89%93%e5%bc%80%e9%bb%91%e7%9b%92"&gt;三、机械可解释性：打开黑盒&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%b9%b6%e4%b8%8d%e6%98%afshaplime"&gt;1. 并不是SHAP/LIME&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%bd%92%e7%ba%b3%e5%a4%b4induction-headsicl%e7%9a%84%e7%89%a9%e7%90%86%e6%9c%ba%e5%88%b6"&gt;2. 归纳头（Induction Heads）：ICL的物理机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e7%89%b9%e5%be%81%e5%8f%a0%e5%8a%a0superposition%e4%b8%8e%e5%b9%b2%e6%89%b0"&gt;3. 特征叠加（Superposition）与干扰&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e5%89%8d%e6%b2%bf%e7%a0%94%e7%a9%b6%e7%a8%80%e7%96%8f%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8sae"&gt;四、前沿研究：稀疏自编码器（SAE）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-%e5%8d%95%e8%af%ad%e4%b9%89%e6%80%a7monosemanticity%e9%9a%be%e9%a2%98"&gt;1. 单语义性（Monosemanticity）难题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-sae%e6%9e%b6%e6%9e%84%e4%b8%8e%e5%8e%9f%e7%90%86"&gt;2. SAE架构与原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e8%ae%ad%e7%bb%83%e4%b8%80%e4%b8%aatoy-sae"&gt;3. 代码实战：训练一个Toy SAE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e7%89%b9%e5%be%81%e5%8f%af%e8%a7%86%e5%8c%96%e4%b8%8e%e8%a7%a3%e9%87%8a"&gt;4. 特征可视化与解释&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94transformerlens%e6%89%8b%e6%9c%af%e5%88%80%e5%ae%9e%e6%88%98"&gt;五、TransformerLens手术刀实战&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-activation-patching"&gt;1. Activation Patching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e6%bc%94%e7%a4%ba%e4%bb%a3%e7%a0%81%e5%b9%b2%e9%a2%84%e6%a8%a1%e5%9e%8b%e8%be%93%e5%87%ba"&gt;2. 演示代码：干预模型输出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%a4%b4%e5%88%86%e6%9e%90"&gt;3. 注意力头分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;六、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一安全维度prompt-injection-vs-jailbreak"&gt;一、安全维度：Prompt Injection vs Jailbreak&lt;a class="anchor" href="#%e4%b8%80%e5%ae%89%e5%85%a8%e7%bb%b4%e5%ba%a6prompt-injection-vs-jailbreak"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;安全不仅仅是&amp;quot;不要说脏话&amp;quot;。在对抗环境下，攻击者会利用模型的概率特性进行数学攻击。&lt;/p&gt;</description></item><item><title>第5章 端到端LLM项目实战</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/</guid><description>&lt;h1 id="第5章端到端项目lawglm-法律咨询助手"&gt;第5章：端到端项目：LawGLM 法律咨询助手&lt;a class="anchor" href="#%e7%ac%ac5%e7%ab%a0%e7%ab%af%e5%88%b0%e7%ab%af%e9%a1%b9%e7%9b%aelawglm-%e6%b3%95%e5%be%8b%e5%92%a8%e8%af%a2%e5%8a%a9%e6%89%8b"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章定位&lt;/strong&gt;：综合大作业。串联前4章知识，从零构建一个垂直领域的法律问答助手。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e9%a1%b9%e7%9b%ae%e7%9b%ae%e6%a0%87"&gt;项目目标&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%8a%80%e6%9c%af%e6%a0%88"&gt;技术栈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-step-1-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87-data-engineering"&gt;1. Step 1: 数据准备 (Data Engineering)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-step-2-%e5%be%ae%e8%b0%83%e8%ae%ad%e7%bb%83-fine-tuning"&gt;2. Step 2: 微调训练 (Fine-tuning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-step-3-%e6%a8%a1%e5%9e%8b%e5%90%88%e5%b9%b6%e4%b8%8e%e9%87%8f%e5%8c%96"&gt;3. Step 3: 模型合并与量化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-step-4-%e6%9c%8d%e5%8a%a1api%e5%bc%80%e5%8f%91"&gt;4. Step 4: 服务API开发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-step-5-%e5%89%8d%e7%ab%af%e4%ba%a4%e4%ba%92%e4%b8%8e%e8%af%84%e4%bc%b0"&gt;5. Step 5: 前端交互与评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;本章小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="项目目标"&gt;项目目标&lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e7%9b%ae%e6%a0%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;构建一个能够回答中国法律问题、辅助撰写法律文书的 LLM。&lt;/p&gt;
&lt;h2 id="技术栈"&gt;技术栈&lt;a class="anchor" href="#%e6%8a%80%e6%9c%af%e6%a0%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据&lt;/strong&gt;：Pandas, Datasets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微调&lt;/strong&gt;：LLaMA-Factory (LoRA + ZeRO-2)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估&lt;/strong&gt;：LLM-as-a-Judge (GPT-4 打分)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;部署&lt;/strong&gt;：vLLM&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="1-step-1-数据准备-data-engineering"&gt;1. Step 1: 数据准备 (Data Engineering)&lt;a class="anchor" href="#1-step-1-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87-data-engineering"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我们需要构建三类数据：&lt;strong&gt;法律条文知识注入&lt;/strong&gt;、&lt;strong&gt;判例问答对&lt;/strong&gt; 和 &lt;strong&gt;法律咨询对话&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="11-数据源规划"&gt;1.1 数据源规划&lt;a class="anchor" href="#11-%e6%95%b0%e6%8d%ae%e6%ba%90%e8%a7%84%e5%88%92"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;数据来源：
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;1. 法律条文：中国裁判文书网、法律法规数据库
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2. 判例分析：最高人民法院公报案例
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;3. 咨询问答：Legal Advice Reddit、知乎法律话题（经人工清洗）
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;目标数据量：
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;- 训练集：10,000+ 条高质量问答对
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;- 验证集：500 条
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;- 测试集：500 条（用于 GPT-4 评估）&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-数据清洗脚本"&gt;1.2 数据清洗脚本&lt;a class="anchor" href="#12-%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e8%84%9a%e6%9c%ac"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="121-法律条文处理"&gt;1.2.1 法律条文处理&lt;a class="anchor" href="#121-%e6%b3%95%e5%be%8b%e6%9d%a1%e6%96%87%e5%a4%84%e7%90%86"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_law_articles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;law_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; 从法律条文中提取结构化数据
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; text: 原始法律条文
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; law_name: 法律名称（如&amp;#34;民法典&amp;#34;）
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; Returns:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; list: 结构化的问答对
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 正则匹配 &amp;#34;第X条&amp;#34; 格式&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;第([零一二三四五六七八九十百千万\d]+)条\s+(.*?)(?=第[零一二三四五六七八九十百千万\d]+条|$)&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findall&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DOTALL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;article_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;matches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# 过滤过短的条文&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 生成多种问法（数据增强）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;请解释《&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;law_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;》第&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;article_num&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;条的内容。&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;input&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;《&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;law_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;》第&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;article_num&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;条规定了什么？&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;input&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;法律问题咨询&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;input&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;请帮我查询《&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;law_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;》第&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;article_num&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;条&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;《&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;law_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;》第&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;article_num&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;条规定：&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 示例：处理民法典&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;civil_code_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;第一条 为了保护民事主体的合法权益，调整民事关系，维护社会和经济秩序，适应中国特色社会主义发展要求，弘扬社会主义核心价值观，根据宪法，制定本法。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;第二条 民法调整平等主体的自然人、法人和非法人组织之间的人身关系和财产关系。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;第三条 民事主体的人身权利、财产权利以及其他合法权益受法律保护，任何组织或者个人不得侵犯。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;law_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_law_articles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;civil_code_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;民法典&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;提取了 &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;law_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; 条法律知识&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="122-判例问答对构造"&gt;1.2.2 判例问答对构造&lt;a class="anchor" href="#122-%e5%88%a4%e4%be%8b%e9%97%ae%e7%ad%94%e5%af%b9%e6%9e%84%e9%80%a0"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_case_qa&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;case_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; 将判例转换为问答格式
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; case_dict: 包含 case_title, facts, judgment 等字段的判例
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; Returns:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; dict: Alpaca 格式的问答对
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;请分析以下案件，并给出法律意见。&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;input&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;案件：&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;case_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;case_title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;事实：&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;case_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;facts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;法律分析：&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;case_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;legal_analysis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;判决结果：&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;case_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;judgment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 示例数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;sample_case&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;case_title&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;张某诉李某房屋租赁合同纠纷案&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;facts&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;原告张某与被告李某签订房屋租赁合同，约定租期一年，租金每月3000元。租期届满后，被告拒不退还押金5000元，理由是房屋内设施损坏。&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;legal_analysis&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;根据《民法典》第704条，租赁期限届满，承租人应当返还租赁物。因承租人原因导致租赁物毁损的，出租人可以扣除相应押金。但本案中，被告未能提供充分证据证明设施损坏系原告造成，且损坏价值未经评估。&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;judgment&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;判决被告李某于判决生效之日起十日内返还原告张某押金5000元。&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;case_qa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_case_qa&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_case&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;case_qa&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ensure_ascii&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="123-数据质量控制"&gt;1.2.3 数据质量控制&lt;a class="anchor" href="#123-%e6%95%b0%e6%8d%ae%e8%b4%a8%e9%87%8f%e6%8e%a7%e5%88%b6"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;validate_data_quality&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s2"&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; 过滤低质量数据
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s2"&gt; &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;filtered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 1. 长度检查&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 2. 关键词检查（避免包含敏感内容）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;sensitive_keywords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;暴力&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;色情&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;赌博&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;kw&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sensitive_keywords&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 3. 格式规范检查&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;instruction&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;filtered&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filtered&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 合并所有数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;all_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;law_data&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;case_qa&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 实际项目中添加更多数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;validate_data_quality&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 保存为 Alpaca 格式&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;output_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;data/law_glm_train.json&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mkdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exist_ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;w&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;utf-8&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ensure_ascii&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;✓ 数据清洗完成，保存了 &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; 条数据到 &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="12-数据注册"&gt;1.2 数据注册&lt;a class="anchor" href="#12-%e6%95%b0%e6%8d%ae%e6%b3%a8%e5%86%8c"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在 LLaMA-Factory 的 &lt;code&gt;data/dataset_info.json&lt;/code&gt; 中注册：&lt;/p&gt;</description></item><item><title>第五篇 RAG高级篇(LangChain篇)</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/</guid><description>&lt;h1 id="第五篇rag高级篇---高级检索与优化"&gt;第五篇：RAG高级篇 - 高级检索与优化&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%94%e7%af%87rag%e9%ab%98%e7%ba%a7%e7%af%87---%e9%ab%98%e7%ba%a7%e6%a3%80%e7%b4%a2%e4%b8%8e%e4%bc%98%e5%8c%96"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="前言"&gt;前言&lt;a class="anchor" href="#%e5%89%8d%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在第四篇中,我们学习了RAG的基础概念,实现了基本的RAG系统。但在生产环境中,基础的向量检索往往无法满足复杂的业务需求:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;基础RAG的局限性&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;召回不全面&lt;/strong&gt;:单一向量检索可能遗漏关键信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;排序不精确&lt;/strong&gt;:top-k结果中可能包含不相关内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文冗余&lt;/strong&gt;:检索到的文本可能包含大量无关信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;复杂查询支持弱&lt;/strong&gt;:难以处理多跳推理、实体关系查询&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本篇将深入探讨&lt;strong&gt;LangChain高级检索技术&lt;/strong&gt;和&lt;strong&gt;优化方案&lt;/strong&gt;,帮助你构建生产级的RAG系统。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="核心概念对比"&gt;核心概念对比&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5%e5%af%b9%e6%af%94"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;技术&lt;/th&gt;
 &lt;th&gt;解决的问题&lt;/th&gt;
 &lt;th&gt;性能提升&lt;/th&gt;
 &lt;th&gt;复杂度&lt;/th&gt;
 &lt;th&gt;适用场景&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;混合检索&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;单一检索召回不全&lt;/td&gt;
 &lt;td&gt;+20-30%&lt;/td&gt;
 &lt;td&gt;低&lt;/td&gt;
 &lt;td&gt;通用RAG&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;重排序&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;top-k结果不精确&lt;/td&gt;
 &lt;td&gt;+15-25%&lt;/td&gt;
 &lt;td&gt;中&lt;/td&gt;
 &lt;td&gt;精度要求高&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;查询改写&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;查询表达不匹配&lt;/td&gt;
 &lt;td&gt;+10-20%&lt;/td&gt;
 &lt;td&gt;低&lt;/td&gt;
 &lt;td&gt;口语化查询&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;上下文压缩&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;token成本过高&lt;/td&gt;
 &lt;td&gt;成本-50%&lt;/td&gt;
 &lt;td&gt;中&lt;/td&gt;
 &lt;td&gt;长上下文&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;知识图谱RAG&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;实体关系查询弱&lt;/td&gt;
 &lt;td&gt;+30-40%&lt;/td&gt;
 &lt;td&gt;高&lt;/td&gt;
 &lt;td&gt;结构化知识&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;&lt;strong&gt;Self-RAG&lt;/strong&gt;&lt;/td&gt;
 &lt;td&gt;检索结果不可靠&lt;/td&gt;
 &lt;td&gt;+20-30%&lt;/td&gt;
 &lt;td&gt;高&lt;/td&gt;
 &lt;td&gt;高质量要求&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id="第1章混合检索技术hybrid-search"&gt;第1章:混合检索技术(Hybrid Search)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2%e6%8a%80%e6%9c%afhybrid-search"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-为什么需要混合检索"&gt;1.1 为什么需要混合检索&lt;a class="anchor" href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="111-向量检索的局限性"&gt;1.1.1 向量检索的局限性&lt;a class="anchor" href="#111-%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;问题示例&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 用户查询:&amp;#34;Python 3.11的新特性&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 向量检索可能返回:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# ❌ &amp;#34;Python 3.10的新特性&amp;#34;(语义相似,但版本不对)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# ❌ &amp;#34;Python的发展历史&amp;#34;(相关,但不精确)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# ✅ &amp;#34;Python 3.11 release notes&amp;#34;(精确匹配)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;向量检索的问题&lt;/strong&gt;:&lt;/p&gt;</description></item><item><title>第五篇 RAG高级篇(LlamaIndex篇)</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/</guid><description>&lt;h1 id="第五篇-rag高级篇-llamaindex"&gt;第五篇 RAG高级篇 (LlamaIndex)&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%94%e7%af%87-rag%e9%ab%98%e7%ba%a7%e7%af%87-llamaindex"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;: 在掌握 LlamaIndex 基础组件（Index, Retriever, QueryEngine）的基础上，深入学习其&amp;quot;杀手锏&amp;quot;级的高级检索策略与 Agent 集成能力。本篇将带你从&amp;quot;能用&amp;quot;进化到&amp;quot;好用&amp;quot;。&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="-前置准备"&gt;📋 前置准备&lt;a class="anchor" href="#-%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇基于 LlamaIndex v0.10+ 版本，建议先完成第四篇的环境配置。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 安装高级组件依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-retrievers-bm25
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-postprocessor-cohere-rerank
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-index-graph-stores-neo4j
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install llama-parse&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="第1章混合检索-hybrid-retrieval"&gt;第1章：混合检索 (Hybrid Retrieval)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2-hybrid-retrieval"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;单一的向量检索（Semantic Search）在处理精确关键词匹配（如产品型号、专有名词）时往往表现不佳。混合检索通过结合 &lt;strong&gt;BM25（关键词）&lt;/strong&gt; 和 &lt;strong&gt;Vector（语义）&lt;/strong&gt;，互补长短。&lt;/p&gt;
&lt;h3 id="11-为什么需要混合检索"&gt;1.1 为什么需要混合检索？&lt;a class="anchor" href="#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;向量检索&lt;/strong&gt;：擅长理解&amp;quot;意图&amp;quot;和&amp;quot;概念&amp;quot;。例如搜&amp;quot;苹果手机&amp;quot;，能匹配到&amp;quot;iPhone&amp;quot;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键词检索&lt;/strong&gt;：擅长精确匹配。例如搜&amp;quot;错误码 502&amp;quot;，向量可能会匹配到&amp;quot;网络错误&amp;quot;，但 BM25 能精确命中包含&amp;quot;502&amp;quot;的文档。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="12-实战构建混合检索器"&gt;1.2 实战：构建混合检索器&lt;a class="anchor" href="#12-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2%e5%99%a8"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;LlamaIndex 提供了 &lt;code&gt;QueryFusionRetriever&lt;/code&gt; 来优雅地融合多种检索结果。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;llama_index.core&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;VectorStoreIndex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SimpleDirectoryReader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StorageContext&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;llama_index.retrievers.bm25&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BM25Retriever&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;llama_index.core.retrievers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;QueryFusionRetriever&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;llama_index.core&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Settings&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 1. 准备数据与向量索引&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleDirectoryReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;./data&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vector_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VectorStoreIndex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_documents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 2. 创建 BM25 检索器 (基于关键词)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 注意：BM25 需要 docstore 来构建倒排索引&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;bm25_retriever&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BM25Retriever&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_defaults&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vector_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;similarity_top_k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 3. 创建 Vector 检索器 (基于语义)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vector_retriever&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vector_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_retriever&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similarity_top_k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 4. 创建融合检索器 (Hybrid)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;hybrid_retriever&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;QueryFusionRetriever&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;retrievers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vector_retriever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bm25_retriever&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;num_queries&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 不生成扩展查询，仅融合当前结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;reciprocal_rerank&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# 使用 RRF (倒数排名融合) 算法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;similarity_top_k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;use_async&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 5. 测试检索&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hybrid_retriever&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;retrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;LlamaIndex 的自动合并检索原理是什么？&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;得分: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.4f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; | 内容: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;...&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="第2章查询优化与路由-routing--transformation"&gt;第2章：查询优化与路由 (Routing &amp;amp; Transformation)&lt;a class="anchor" href="#%e7%ac%ac2%e7%ab%a0%e6%9f%a5%e8%af%a2%e4%bc%98%e5%8c%96%e4%b8%8e%e8%b7%af%e7%94%b1-routing--transformation"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;用户的 Query 往往是不完美的（模糊、复杂、缺失上下文）。LlamaIndex 提供了一系列工具来&amp;quot;修复&amp;quot;或&amp;quot;分发&amp;quot;用户的查询。&lt;/p&gt;</description></item><item><title>第五篇 图像分割</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</guid><description>&lt;h1 id="第五篇图像分割与实例分割"&gt;第五篇：图像分割与实例分割&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%94%e7%af%87%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2%e4%b8%8e%e5%ae%9e%e4%be%8b%e5%88%86%e5%89%b2"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;从像素级理解到万物分割，掌握图像分割的完整技术栈&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="篇章概览"&gt;篇章概览&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%a7%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;图像分割是计算机视觉的核心任务之一，它不仅要识别&amp;quot;哪里有物体&amp;quot;（目标检测），还要精确描绘&amp;quot;物体的每一个像素&amp;quot;。本篇将系统学习：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义分割&lt;/strong&gt;：为每个像素分配类别标签&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例分割&lt;/strong&gt;：区分同一类别的不同个体&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segment Anything&lt;/strong&gt;：零样本分割的革命性突破&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="为什么要学习图像分割"&gt;为什么要学习图像分割？&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%ad%a6%e4%b9%a0%e5%9b%be%e5%83%8f%e5%88%86%e5%89%b2"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1-更精细的视觉理解"&gt;1. 更精细的视觉理解&lt;a class="anchor" href="#1-%e6%9b%b4%e7%b2%be%e7%bb%86%e7%9a%84%e8%a7%86%e8%a7%89%e7%90%86%e8%a7%a3"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;目标检测：这里有一辆车 [矩形框]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;语义分割：这些像素是车 [像素级mask]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;实例分割：这是第1辆车，那是第2辆车 [区分个体]&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="2-广泛的应用场景"&gt;2. 广泛的应用场景&lt;a class="anchor" href="#2-%e5%b9%bf%e6%b3%9b%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;医学影像&lt;/strong&gt;：肿瘤分割、器官分割&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动驾驶&lt;/strong&gt;：道路分割、车道线检测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;遥感分析&lt;/strong&gt;：土地利用分类&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像编辑&lt;/strong&gt;：抠图、背景替换&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工业检测&lt;/strong&gt;：缺陷分割&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="3-技术发展迅速"&gt;3. 技术发展迅速&lt;a class="anchor" href="#3-%e6%8a%80%e6%9c%af%e5%8f%91%e5%b1%95%e8%bf%85%e9%80%9f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从FCN到U-Net：编码器-解码器架构&lt;/li&gt;
&lt;li&gt;从Mask R-CNN到YOLACT：实时实例分割&lt;/li&gt;
&lt;li&gt;从SAM到SAM 2：零样本视频分割&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="篇章结构"&gt;篇章结构&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e7%bb%93%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第11章语义分割"&gt;&lt;a href="./chapter11/README.md"&gt;第11章：语义分割&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac11%e7%ab%a0%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;语义分割为每个像素分配类别标签，不区分同类物体的个体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;11.1 FCN：全卷积网络的开创性工作&lt;/li&gt;
&lt;li&gt;11.2 U-Net：医学图像分割的经典架构&lt;/li&gt;
&lt;li&gt;11.3 DeepLab系列：空洞卷积与ASPP&lt;/li&gt;
&lt;li&gt;11.4 实战：医学图像分割项目&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;代码实践&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;U-Net完整实现与训练&lt;/li&gt;
&lt;li&gt;医学影像数据处理&lt;/li&gt;
&lt;li&gt;分割评估指标（IoU、Dice）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第12章实例分割"&gt;&lt;a href="./chapter12/README.md"&gt;第12章：实例分割&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac12%e7%ab%a0%e5%ae%9e%e4%be%8b%e5%88%86%e5%89%b2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;实例分割不仅识别像素类别，还要区分同类物体的不同个体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;12.1 Mask R-CNN：两阶段实例分割&lt;/li&gt;
&lt;li&gt;12.2 YOLACT：实时实例分割&lt;/li&gt;
&lt;li&gt;12.3 YOLOv8-Seg：YOLO的分割版本&lt;/li&gt;
&lt;li&gt;12.4 实战：COCO实例分割&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;代码实践&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv8-Seg训练与推理&lt;/li&gt;
&lt;li&gt;实例分割后处理&lt;/li&gt;
&lt;li&gt;可视化mask输出&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第13章segment-anything-sam"&gt;&lt;a href="./chapter13/README.md"&gt;第13章：Segment Anything (SAM)&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac13%e7%ab%a0segment-anything-sam"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Meta的SAM模型开启了&amp;quot;万物分割&amp;quot;的新时代，支持零样本分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心内容&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;13.1 SAM模型架构详解&lt;/li&gt;
&lt;li&gt;13.2 Prompt Engineering for SAM&lt;/li&gt;
&lt;li&gt;13.3 SAM 2：视频分割能力&lt;/li&gt;
&lt;li&gt;13.4 实战：零样本分割应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;代码实践&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第06章 感知机</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>&lt;h1 id="第06章-感知机-perceptron"&gt;第06章 感知机 (Perceptron)&lt;a class="anchor" href="#%e7%ac%ac06%e7%ab%a0-%e6%84%9f%e7%9f%a5%e6%9c%ba-perceptron"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The perceptron has probably given rise to more hope, and more disappointment, than any other idea in AI.&amp;rdquo; —— Marvin Minsky&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;:这不仅仅是一章关于分类器的笔记,这是人类试图用数学模拟大脑的第一次&lt;strong&gt;史诗般的尝试&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;感知机是现代深度学习的&lt;strong&gt;线粒体&lt;/strong&gt;。虽然它结构简单,但它蕴含了神经网络最核心的灵魂——&lt;strong&gt;通过误差修正自我&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本章我们将见证两个极端:一个是数学上的&lt;strong&gt;奇迹&lt;/strong&gt;——Novikoff 定理证明了只要真理(线性可分)存在,感知机就一定能找到它;另一个是历史的&lt;strong&gt;悲剧&lt;/strong&gt;——Minsky 如何用一个简单的 XOR 问题,将 AI 推入了长达二十年的寒冬。这是一个关于希望、幻灭与重生的故事。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-%e5%ae%9a%e4%b9%89"&gt;定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82"&gt;几何直观&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;超平面的性质&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc"&gt;损失函数推导&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;3.1 &lt;a href="#31-%e4%bb%8e-0-1-loss-%e5%bc%80%e5%a7%8b"&gt;从 0-1 Loss 开始&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2 &lt;a href="#32-%e8%bf%87%e6%b8%a1%e5%88%b0%e8%b7%9d%e7%a6%bb%e6%8d%9f%e5%a4%b1"&gt;过渡到距离损失&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3 &lt;a href="#33-%e6%84%9f%e7%9f%a5%e6%9c%ba%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0"&gt;感知机损失函数&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d"&gt;随机梯度下降&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;4.1 &lt;a href="#41-%e6%a2%af%e5%ba%a6%e8%ae%a1%e7%ae%97"&gt;梯度计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.2 &lt;a href="#42-%e6%9b%b4%e6%96%b0%e8%a7%84%e5%88%99"&gt;更新规则&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.3 &lt;a href="#43-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89"&gt;几何直觉&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%ae%97%e6%b3%95"&gt;感知机算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-novikoff-%e6%94%b6%e6%95%9b%e5%ae%9a%e7%90%86"&gt;Novikoff 收敛定理&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;6.1 &lt;a href="#61-%e5%ae%9a%e7%90%86%e9%99%88%e8%bf%b0"&gt;定理陈述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 &lt;a href="#62-%e5%ae%9a%e7%90%86%e7%9a%84%e6%84%8f%e4%b9%89"&gt;定理的意义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.3 &lt;a href="#63-%e8%af%81%e6%98%8e%e6%80%9d%e8%b7%af-%e5%8f%af%e9%80%89"&gt;证明思路 (可选)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-%e6%84%9f%e7%9f%a5%e6%9c%ba-vs-svm"&gt;感知机 vs SVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-xor-%e9%97%ae%e9%a2%98%e4%b8%8e-ai-%e7%9a%84%e5%af%92%e5%86%ac"&gt;XOR 问题与 AI 的寒冬&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;8.1 &lt;a href="#81-xor-%e7%9a%84%e5%8f%8d%e4%be%8b"&gt;XOR 的反例&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;a href="#82-%e5%8e%86%e5%8f%b2%e7%9a%84%e6%95%99%e8%ae%ad"&gt;历史的教训&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-%e6%80%bb%e7%bb%93"&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e9%99%84%e5%bd%95-%e5%af%b9%e5%81%b6%e5%bd%a2%e5%bc%8f-dual-form"&gt;附录: 对偶形式 (Dual Form)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-定义"&gt;1. 定义&lt;a class="anchor" href="#1-%e5%ae%9a%e4%b9%89"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;感知机是二分类的线性判别模型:&lt;/p&gt;</description></item><item><title>第六篇 文档处理与数据清洗</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/</guid><description>&lt;h1 id="第六篇-文档处理与数据清洗从非结构化到结构化"&gt;第六篇 文档处理与数据清洗：从非结构化到结构化&lt;a class="anchor" href="#%e7%ac%ac%e5%85%ad%e7%af%87-%e6%96%87%e6%a1%a3%e5%a4%84%e7%90%86%e4%b8%8e%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e4%bb%8e%e9%9d%9e%e7%bb%93%e6%9e%84%e5%8c%96%e5%88%b0%e7%bb%93%e6%9e%84%e5%8c%96"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;在RAG（检索增强生成）系统中，&lt;strong&gt;文档处理质量（ETL）直接决定了最终效果的上限&lt;/strong&gt;。&amp;ldquo;Garbage In, Garbage Out&amp;rdquo; 是绝对真理。无论你的模型多么强大，如果喂给它的数据是破碎、混乱或含有噪声的，检索效果一定很差。&lt;/p&gt;
&lt;p&gt;本篇不仅介绍工具的使用，更侧重于&lt;strong&gt;生产级文档处理方法论&lt;/strong&gt;，对比 LangChain 和 LlamaIndex 的最佳实践，并涵盖最新的 PDF 解析技术（如 MinerU, LlamaParse）。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="学习路径"&gt;学习路径&lt;a class="anchor" href="#%e5%ad%a6%e4%b9%a0%e8%b7%af%e5%be%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;pre class="mermaid"&gt;graph LR
 A[ETL核心方法论] --&amp;gt; B[Loading&amp;lt;br/&amp;gt;多模态加载]
 B --&amp;gt; C[Chunking&amp;lt;br/&amp;gt;智能切分]
 C --&amp;gt; D[Metadata&amp;lt;br/&amp;gt;元数据增强]
 D --&amp;gt; E[实战&amp;lt;br/&amp;gt;复杂PDF处理]

 style A fill:#e1f5e1
 style B fill:#fff4e1
 style D fill:#ffe1e1
 style E fill:#e1f5fe&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id="part-1-etl-核心方法论"&gt;Part 1: ETL 核心方法论&lt;a class="anchor" href="#part-1-etl-%e6%a0%b8%e5%bf%83%e6%96%b9%e6%b3%95%e8%ae%ba"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在构建 LLM 应用时，我们遵循标准的 &lt;strong&gt;ETL (Extract, Transform, Load)&lt;/strong&gt; 流程，但在向量数据库语境下，通常描述为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Load (加载)&lt;/strong&gt;: 将各种非结构化数据（PDF, HTML, MarkDown）统一为标准 &lt;code&gt;Document&lt;/code&gt; 对象。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Split (切分)&lt;/strong&gt;: 将长文档切分为适合 Embedding 模型窗口（如 512/1024 tokens）的 &lt;code&gt;Chunks&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed (向量化)&lt;/strong&gt;: 将文本块转化为向量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Store (存储)&lt;/strong&gt;: 存入向量数据库。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关键数据结构对比&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第六篇 生成模型</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第六篇生成模型gandiffusion"&gt;第六篇:生成模型(GAN/Diffusion)&lt;a class="anchor" href="#%e7%ac%ac%e5%85%ad%e7%af%87%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8bgandiffusion"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标读者&lt;/strong&gt;:掌握CNN和Transformer基础,希望深入理解生成式AI的读者&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习重点&lt;/strong&gt;:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。&lt;/p&gt;
&lt;h3 id="为什么学习生成模型"&gt;为什么学习生成模型?&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%ad%a6%e4%b9%a0%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AIGC时代核心技术&lt;/strong&gt;:Midjourney、Stable Diffusion、DALL-E等产品的底层技术&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模态理解基础&lt;/strong&gt;:理解文生图是学习VLM的前置知识&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实用价值高&lt;/strong&gt;:图像生成、编辑、超分辨率等多种应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术快速迭代&lt;/strong&gt;:从DDPM到FLUX,扩散模型仍在快速发展&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="技术演进时间线"&gt;技术演进时间线&lt;a class="anchor" href="#%e6%8a%80%e6%9c%af%e6%bc%94%e8%bf%9b%e6%97%b6%e9%97%b4%e7%ba%bf"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2014-2019: GAN时代
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2014: GAN提出 (Goodfellow)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2015: DCGAN - 稳定训练的GAN
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2018: StyleGAN - 高质量人脸生成
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└── 2019: StyleGAN2 - 生成质量巅峰
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2020-至今: Diffusion崛起
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2020: DDPM提出 (Ho et al.)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2021: DALL-E (OpenAI)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2022: Stable Diffusion开源
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2023: ControlNet、SDXL
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── 2024: Stable Diffusion 3、FLUX.1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└── 2025: 扩散模型持续迭代&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="章节安排"&gt;章节安排&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第14章生成对抗网络gan"&gt;&lt;a href="chapter14/README.md"&gt;第14章:生成对抗网络(GAN)&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac14%e7%ab%a0%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9cgan"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;快速回顾,不作为重点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;14.1 GAN基础原理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成器与判别器的对抗训练&lt;/li&gt;
&lt;li&gt;GAN的损失函数&lt;/li&gt;
&lt;li&gt;训练稳定性问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.2 DCGAN:深度卷积GAN&lt;/p&gt;</description></item><item><title>第07章 支持向量机(SVM)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/</guid><description>&lt;h1 id="第07章支持向量机-svm"&gt;第07章：支持向量机 (SVM)&lt;a class="anchor" href="#%e7%ac%ac07%e7%ab%a0%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba-svm"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Nothing is more practical than a good theory.&amp;rdquo; —— Vladimir Vapnik&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：如果说感知机是神经网络的始祖，那么支持向量机 (SVM) 就是统计学习理论的&lt;strong&gt;皇冠&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在深度学习爆发之前的二十年里，SVM 统治着机器学习的世界。它的强大不仅仅在于分类效果，更在于其背后坚如磐石的数学理论——&lt;strong&gt;VC 维理论&lt;/strong&gt;和&lt;strong&gt;结构风险最小化&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;本章我们将见证一个算法如何将几何直觉（最大间隔）转化为一个凸优化问题，并通过对偶性（Duality）巧妙地通过&amp;quot;支持向量&amp;quot;来稀疏化模型。这不仅是一个算法，这是数学美学的典范。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%e4%b8%80%e5%bc%95%e8%a8%80%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e9%81%97%e6%86%be"&gt;一、引言：感知机的遗憾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%8c%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94-margin%e6%9c%80%e5%ae%bd%e7%9a%84%e5%88%86%e7%95%8c%e7%ba%bf"&gt;二、几何间隔 (Margin)：最宽的分界线&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#21-%e5%87%bd%e6%95%b0%e9%97%b4%e9%9a%94-vs-%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94"&gt;2.1 函数间隔 vs 几何间隔&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#22-%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%9a%84%e6%95%b0%e5%ad%a6%e8%a1%a8%e8%be%be"&gt;2.2 最大化间隔的数学表达&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#23-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%ad%89%e4%bb%b7%e4%ba%8e%e6%9c%80%e5%b0%8f%e5%8c%96-frac12%5cmathbf%7bw%7d2"&gt;2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%89%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98-duality%e4%bc%98%e9%9b%85%e7%9a%84%e8%bd%ac%e6%8d%a2"&gt;三、对偶问题 (Duality)：优雅的转换&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#31-%e4%b8%ba%e4%bd%95%e8%a6%81%e5%af%b9%e5%81%b6"&gt;3.1 为何要对偶？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#32-lagrange-%e5%87%bd%e6%95%b0%e6%9e%84%e5%bb%ba"&gt;3.2 Lagrange 函数构建&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#33-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e6%8e%a8%e5%af%bc"&gt;3.3 对偶问题推导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#34-kkt-%e6%9d%a1%e4%bb%b6%e4%b8%8e%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f"&gt;3.4 KKT 条件与支持向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#35-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89"&gt;3.5 对偶问题的物理意义&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%9b%9b%e8%bd%af%e9%97%b4%e9%9a%94-soft-margin%e6%8b%a5%e6%8a%b1%e4%b8%8d%e5%ae%8c%e7%be%8e"&gt;四、软间隔 (Soft Margin)：拥抱不完美&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#41-%e7%8e%b0%e5%ae%9e%e4%b8%96%e7%95%8c%e5%b9%b6%e4%b8%8d%e5%ae%8c%e7%be%8e"&gt;4.1 现实世界并不完美&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#42-%e5%bc%95%e5%85%a5%e6%9d%be%e5%bc%9b%e5%8f%98%e9%87%8f-xi"&gt;4.2 引入松弛变量 $\xi$&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#43-%e8%bd%af%e9%97%b4%e9%9a%94%e7%9a%84%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98"&gt;4.3 软间隔的对偶问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#44-hinge-loss%e6%89%93%e9%80%9a%e4%bc%98%e5%8c%96%e8%a7%86%e8%a7%92"&gt;4.4 Hinge Loss：打通优化视角&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#45-svm--hinge-loss--l2-%e6%ad%a3%e5%88%99%e5%8c%96"&gt;4.5 SVM = Hinge Loss + L2 正则化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%ba%94smo-%e7%ae%97%e6%b3%95%e9%ab%98%e6%95%88%e6%b1%82%e8%a7%a3%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98"&gt;五、SMO 算法：高效求解对偶问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93"&gt;六、本章小结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%e4%b8%83%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb"&gt;七、推荐阅读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="一引言感知机的遗憾"&gt;一、引言：感知机的遗憾&lt;a class="anchor" href="#%e4%b8%80%e5%bc%95%e8%a8%80%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e9%81%97%e6%86%be"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在第6章中，我们学习了感知机算法。Novikoff 定理保证了只要数据线性可分，感知机就一定能找到一个分离超平面。但这个定理也暴露了一个致命的问题：&lt;/p&gt;</description></item><item><title>第七篇 Deep Agents</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/</guid><description>&lt;h1 id="第七篇-deep-agents构建具备规划与子智能体能力的深度-agent"&gt;第七篇 Deep Agents：构建具备规划与子智能体能力的深度 Agent&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%83%e7%af%87-deep-agents%e6%9e%84%e5%bb%ba%e5%85%b7%e5%a4%87%e8%a7%84%e5%88%92%e4%b8%8e%e5%ad%90%e6%99%ba%e8%83%bd%e4%bd%93%e8%83%bd%e5%8a%9b%e7%9a%84%e6%b7%b1%e5%ba%a6-agent"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：掌握 deepagents 库，使用 &lt;strong&gt;Agent Harness&lt;/strong&gt; 模式构建能够处理复杂、多步骤、长上下文任务的生产级智能体。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="-前置准备"&gt;📋 前置准备&lt;a class="anchor" href="#-%e5%89%8d%e7%bd%ae%e5%87%86%e5%a4%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="环境配置"&gt;环境配置&lt;a class="anchor" href="#%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在开始学习之前，请确保完成以下环境配置：&lt;/p&gt;
&lt;h4 id="1-安装依赖"&gt;1. 安装依赖&lt;a class="anchor" href="#1-%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 核心库&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install deepagents
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 推荐工具（用于本篇实战）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;pip install tavily-python langchain-community&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="2-环境变量配置"&gt;2. 环境变量配置&lt;a class="anchor" href="#2-%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 必须配置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;ANTHROPIC_API_KEY&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;sk-...&amp;#34;&lt;/span&gt; &lt;span class="c1"&gt;# 默认基座模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;TAVILY_API_KEY&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;tvly-...&amp;#34;&lt;/span&gt; &lt;span class="c1"&gt;# 用于搜索能力&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# 可选配置（但强烈推荐用于追踪）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;LANGSMITH_API_KEY&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;lsv2-...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;LANGSMITH_TRACING&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;true&amp;#34;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="前置知识"&gt;前置知识&lt;a class="anchor" href="#%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;建议具备以下基础知识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ &lt;strong&gt;LangGraph 基础&lt;/strong&gt; (State, Node, Edge 的概念)&lt;/li&gt;
&lt;li&gt;✅ &lt;strong&gt;Tool Calling&lt;/strong&gt; (如何定义和使用工具)&lt;/li&gt;
&lt;li&gt;✅ &lt;strong&gt;异步编程&lt;/strong&gt; (async/await)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="第1章deep-agents-核心架构"&gt;第1章：Deep Agents 核心架构&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0deep-agents-%e6%a0%b8%e5%bf%83%e6%9e%b6%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-什么是-agent-harness"&gt;1.1 什么是 Agent Harness？&lt;a class="anchor" href="#11-%e4%bb%80%e4%b9%88%e6%98%af-agent-harness"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在软件工程中，&lt;strong&gt;Harness&lt;/strong&gt;（挽具/测试套件）通常指用来控制和测试组件的外部框架。&lt;strong&gt;Deep Agents&lt;/strong&gt; 引入了 &lt;strong&gt;Agent Harness&lt;/strong&gt; 的核心设计理念：它不改变底层的 LLM 或 LangGraph 图，而是像给赛马套上挽具一样，在 Agent 循环之外包裹了一层&lt;strong&gt;强制性的行为规范&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>第七篇 视觉大模型</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第七篇视觉大模型时代"&gt;第七篇:视觉大模型时代&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%83%e7%af%87%e8%a7%86%e8%a7%89%e5%a4%a7%e6%a8%a1%e5%9e%8b%e6%97%b6%e4%bb%a3"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;从多模态基础模型到视觉AGI的演进之路&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="篇章概述"&gt;篇章概述&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;视觉大模型(Vision-Language Model, VLM)是2023-2024年计算机视觉领域最重要的技术突破。本篇深入讲解:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多模态基础模型(CLIP、BLIP、LLaVA)&lt;/li&gt;
&lt;li&gt;前沿视觉大模型(Florence-2、GPT-4V、Gemini)&lt;/li&gt;
&lt;li&gt;3D视觉与视频理解新进展&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="为什么学习视觉大模型"&gt;为什么学习视觉大模型?&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%ad%a6%e4%b9%a0%e8%a7%86%e8%a7%89%e5%a4%a7%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;范式转变&lt;/strong&gt;: 从单一任务模型到统一多模态模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;零样本能力&lt;/strong&gt;: 无需训练即可完成新任务&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;产业应用&lt;/strong&gt;: 正在重塑计算机视觉应用格局&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术前沿&lt;/strong&gt;: 是通向AGI的重要路径&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="章节组织"&gt;章节组织&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e7%bb%84%e7%bb%87"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第16章多模态基础模型"&gt;&lt;a href="chapter16/README.md"&gt;第16章:多模态基础模型&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac16%e7%ab%a0%e5%a4%9a%e6%a8%a1%e6%80%81%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;核心主题&lt;/strong&gt;: CLIP、BLIP、LLaVA三大基础模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;16.1 CLIP:视觉-语言对比学习&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对比学习原理与双编码器架构&lt;/li&gt;
&lt;li&gt;零样本分类、图像检索&lt;/li&gt;
&lt;li&gt;transformers库实战&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;16.2 BLIP系列:视觉问答&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BLIP-2架构:Q-Former设计&lt;/li&gt;
&lt;li&gt;图像描述、VQA任务&lt;/li&gt;
&lt;li&gt;量化优化与部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;16.3 LLaVA:大语言模型+视觉&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉指令微调方法&lt;/li&gt;
&lt;li&gt;多模态对话系统&lt;/li&gt;
&lt;li&gt;LLaVA 1.5/1.6新特性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;16.4 实战:多模态理解应用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;商品图像搜索&lt;/li&gt;
&lt;li&gt;智能客服机器人&lt;/li&gt;
&lt;li&gt;图像内容审核&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;技术栈&lt;/strong&gt;: &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;PIL&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码文件&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;code/clip_zero_shot.py&lt;/code&gt; - CLIP零样本分类&lt;/li&gt;
&lt;li&gt;&lt;code&gt;code/blip2_vqa.py&lt;/code&gt; - BLIP-2视觉问答&lt;/li&gt;
&lt;li&gt;&lt;code&gt;code/llava_chat.py&lt;/code&gt; - LLaVA多模态对话&lt;/li&gt;
&lt;li&gt;&lt;code&gt;code/multimodal_app.py&lt;/code&gt; - 综合应用示例&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第17章视觉大模型前沿"&gt;&lt;a href="chapter17/README.md"&gt;第17章:视觉大模型前沿&lt;/a&gt;&lt;a class="anchor" href="#%e7%ac%ac17%e7%ab%a0%e8%a7%86%e8%a7%89%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%89%8d%e6%b2%bf"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;核心主题&lt;/strong&gt;: 工业级VLM与商业API&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;17.1 Florence-2:微软视觉基础模型&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一提示词范式&lt;/li&gt;
&lt;li&gt;支持10+视觉任务&lt;/li&gt;
&lt;li&gt;开源可商用(MIT协议)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;17.2 GPT-4V/GPT-4o:多模态GPT&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第08章 核方法</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/</guid><description>&lt;h1 id="第-08-章-核方法-kernel-methods"&gt;第 08 章 核方法 (Kernel Methods)&lt;a class="anchor" href="#%e7%ac%ac-08-%e7%ab%a0-%e6%a0%b8%e6%96%b9%e6%b3%95-kernel-methods"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The curse of dimensionality is the blessing of kernel methods.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;很多时候，我们在低维空间撞得头破血流（比如 XOR 问题），却不知道只要退后一步，升到一个更高的维度，一切都会因为稀疏而变得线性可分。&lt;/p&gt;
&lt;p&gt;核方法是机器学习中**&amp;ldquo;升维打击&amp;rdquo;**的数学实现。它的魔力在于：我们可以在无限维的空间中挥舞利剑，却只需要支付有限维的计算代价。本章将揭示这个&amp;quot;免费午餐&amp;quot;背后的数学秘密——核技巧 (Kernel Trick)。&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;与前序章节的联系&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;第 5 章（线性回归）&lt;/strong&gt; 为我们建立了岭回归的原始形式：$w^* = (X^T X + \lambda I)^{-1} X^T y$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本章&lt;/strong&gt;将展示如何通过核技巧将其推广到无穷维特征空间：$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$&lt;/li&gt;
&lt;li&gt;两者通过&lt;strong&gt;对偶性&lt;/strong&gt;完美呼应：线性岭回归在特征空间优化，核岭回归在样本空间优化&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-%e7%9b%b4%e8%a7%89%e7%bb%b4%e5%ba%a6%e6%89%93%e5%87%bb"&gt;直觉：维度打击&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e4%bb%a3%e4%bb%b7%e7%bb%b4%e5%ba%a6%e7%9a%84%e8%af%85%e5%92%92"&gt;代价：维度的诅咒&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%95%91%e8%b5%8e%e6%a0%b8%e6%8a%80%e5%b7%a7-kernel-trick"&gt;救赎：核技巧 (Kernel Trick)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-mercer-%e5%ae%9a%e7%90%86%e4%bb%80%e4%b9%88%e6%a0%b7%e7%9a%84%e5%87%bd%e6%95%b0%e8%83%bd%e5%bd%93%e6%a0%b8"&gt;Mercer 定理：什么样的函数能当核？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-rbf-%e6%a0%b8%e9%80%9a%e5%be%80%e6%97%a0%e7%a9%b7%e7%bb%b4"&gt;RBF 核：通往无穷维&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-%e5%ba%94%e7%94%a8%e6%a0%b8%e5%8c%96%e4%b8%80%e5%88%87"&gt;应用：核化一切&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;6.1 &lt;a href="#61-%e8%a1%a8%e7%a4%ba%e5%ae%9a%e7%90%86-representer-theorem"&gt;表示定理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 &lt;a href="#62-%e6%a0%b8%e5%8c%96%e7%9a%84%e4%b8%80%e8%88%ac%e6%ad%a5%e9%aa%a4"&gt;核化的一般步骤&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.3 &lt;a href="#63-%e6%a1%88%e4%be%8b%e6%a0%b8%e5%b2%ad%e5%9b%9e%e5%bd%92-kernel-ridge-regression"&gt;案例：核岭回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.4 &lt;a href="#64-%e5%85%b6%e4%bb%96%e5%8f%af%e6%a0%b8%e5%8c%96%e7%ae%97%e6%b3%95"&gt;其他可核化算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b"&gt;总结与展望&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-直觉维度打击"&gt;1. 直觉：维度打击&lt;a class="anchor" href="#1-%e7%9b%b4%e8%a7%89%e7%bb%b4%e5%ba%a6%e6%89%93%e5%87%bb"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-xor-问题二维空间的绝望"&gt;1.1 XOR 问题：二维空间的绝望&lt;a class="anchor" href="#11-xor-%e9%97%ae%e9%a2%98%e4%ba%8c%e7%bb%b4%e7%a9%ba%e9%97%b4%e7%9a%84%e7%bb%9d%e6%9c%9b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;考虑经典的 XOR (异或) 问题：&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;$x_1$&lt;/th&gt;
 &lt;th&gt;$x_2$&lt;/th&gt;
 &lt;th&gt;类别&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;td&gt;1&lt;/td&gt;
 &lt;td&gt;0&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：在二维平面上，&lt;strong&gt;不存在一条直线&lt;/strong&gt;能够将两个类别分开。&lt;/p&gt;</description></item><item><title>第八篇 Middleware 工程化</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/</guid><description>&lt;h1 id="第八篇-middleware-工程化"&gt;第八篇 Middleware 工程化&lt;a class="anchor" href="#%e7%ac%ac%e5%85%ab%e7%af%87-middleware-%e5%b7%a5%e7%a8%8b%e5%8c%96"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;: 掌握 LangChain Middleware 机制,实现对 Agent 行为的精准控制&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;在前面的篇章中,我们学会了如何创建Agent(&lt;code&gt;create_agent&lt;/code&gt;)、构建复杂工作流(&lt;code&gt;LangGraph&lt;/code&gt;)、处理复杂任务(&lt;code&gt;Deep Agents&lt;/code&gt;)。但这些都是&amp;quot;功能实现&amp;quot;层面,本篇进入&lt;strong&gt;工程化阶段&lt;/strong&gt;:如何让Agent在生产环境中&lt;strong&gt;安全、可靠、可控&lt;/strong&gt;地运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心问题&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何防止Agent泄露敏感信息?&lt;/li&gt;
&lt;li&gt;如何限制Agent的调用成本?&lt;/li&gt;
&lt;li&gt;如何在关键操作前要求人工审批?&lt;/li&gt;
&lt;li&gt;如何在对话过长时自动摘要?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;: &lt;strong&gt;Middleware&lt;/strong&gt; - LangChain 1.0的核心机制,允许你在Agent执行的各个阶段精准干预。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="第1章middleware-核心机制"&gt;第1章：Middleware 核心机制&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0middleware-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章目标&lt;/strong&gt;: 理解Middleware的本质、运行原理和基本用法&lt;/p&gt;
&lt;/blockquote&gt;&lt;h3 id="11-什么是-middleware"&gt;1.1 什么是 Middleware&lt;a class="anchor" href="#11-%e4%bb%80%e4%b9%88%e6%98%af-middleware"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="111-agent执行流程回顾"&gt;1.1.1 Agent执行流程回顾&lt;a class="anchor" href="#111-agent%e6%89%a7%e8%a1%8c%e6%b5%81%e7%a8%8b%e5%9b%9e%e9%a1%be"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;首先回顾&lt;code&gt;create_agent&lt;/code&gt;创建的Agent是如何工作的:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;langchain.agents&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_agent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;langchain_openai&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ChatOpenAI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ChatOpenAI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;gpt-4o&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;search_tool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;calculator_tool&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invoke&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;messages&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;搜索最新新闻&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)]})&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;内部执行流程&lt;/strong&gt;：&lt;/p&gt;
&lt;img src="./assets/image-20260122210222416.png" alt="image-20260122210222416" style="zoom:50%;" /&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;: 这个流程是&amp;quot;黑盒&amp;quot;,我们无法干预中间步骤。而 Middleware 正是解决这个问题的关键机制。&lt;/p&gt;
&lt;h4 id="112-middleware的切入点与生命周期"&gt;1.1.2 Middleware的切入点与生命周期&lt;a class="anchor" href="#112-middleware%e7%9a%84%e5%88%87%e5%85%a5%e7%82%b9%e4%b8%8e%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Middleware&lt;/strong&gt;在Agent执行的关键节点提供&lt;strong&gt;Hook(钩子)&lt;/strong&gt;,允许你精准干预。下图展示了 Agent Loop 与 Middleware Hooks 的交互流程,清晰呈现每个 Hook 的触发时机:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Middleware Lifecycle (生命周期)&lt;/strong&gt;&lt;/p&gt;
&lt;img src="./assets/image-20260122205654033.png" alt="image-20260122205654033" style="zoom:50%;" /&gt;
&lt;p&gt;&lt;strong&gt;核心流程说明&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;用户请求&lt;/strong&gt; → &lt;code&gt;before_agent&lt;/code&gt; Hook: 权限检查、输入验证、初始化&lt;/li&gt;
&lt;li&gt;&lt;code&gt;before_model&lt;/code&gt; Hook → &lt;strong&gt;模型推理前&lt;/strong&gt;: 修改提示词、检查 Token、条件跳转&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wrap_model_call&lt;/code&gt; Hook → &lt;strong&gt;包装模型调用&lt;/strong&gt;: 缓存、重试、降级、成本控制&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型响应&lt;/strong&gt; → &lt;code&gt;after_model&lt;/code&gt; Hook: 审核输出、记录日志、质量评分&lt;/li&gt;
&lt;li&gt;如需工具 → &lt;code&gt;wrap_tool_call&lt;/code&gt; Hook → &lt;strong&gt;工具执行&lt;/strong&gt;: 重试、限流、审批、模拟执行 → 返回循环&lt;/li&gt;
&lt;li&gt;无需工具 → &lt;code&gt;after_agent&lt;/code&gt; Hook → &lt;strong&gt;Agent 结束&lt;/strong&gt;: 保存结果、计费、清理资源&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Hook 快速参考&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第八篇 生产实践</title><link>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/</guid><description>&lt;h1 id="第八篇生产实践与工程化"&gt;第八篇：生产实践与工程化&lt;a class="anchor" href="#%e7%ac%ac%e5%85%ab%e7%af%87%e7%94%9f%e4%ba%a7%e5%ae%9e%e8%b7%b5%e4%b8%8e%e5%b7%a5%e7%a8%8b%e5%8c%96"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;工程实战篇章&lt;/strong&gt; - 将计算机视觉模型从实验室带到生产环境的完整指南&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="篇章定位"&gt;篇章定位&lt;a class="anchor" href="#%e7%af%87%e7%ab%a0%e5%ae%9a%e4%bd%8d"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本篇是整个计算机视觉笔记的&lt;strong&gt;工程实战篇章&lt;/strong&gt;，专注于将训练好的模型真正部署到生产环境。从模型优化到服务化部署，从性能监控到最佳实践，系统讲解工程化的全流程。&lt;/p&gt;
&lt;h2 id="为什么需要生产实践"&gt;为什么需要生产实践？&lt;a class="anchor" href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e7%94%9f%e4%ba%a7%e5%ae%9e%e8%b7%b5"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;性能要求&lt;/strong&gt; - 生产环境对延迟、吞吐量有严格要求&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源限制&lt;/strong&gt; - 边缘设备内存、算力有限，需要模型压缩&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;稳定性&lt;/strong&gt; - 7x24小时运行，需要完善的监控和容错&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可维护性&lt;/strong&gt; - 便于更新、回滚、A/B测试&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="内容结构"&gt;内容结构&lt;a class="anchor" href="#%e5%86%85%e5%ae%b9%e7%bb%93%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="第19章模型优化与加速"&gt;第19章：模型优化与加速&lt;a class="anchor" href="#%e7%ac%ac19%e7%ab%a0%e6%a8%a1%e5%9e%8b%e4%bc%98%e5%8c%96%e4%b8%8e%e5%8a%a0%e9%80%9f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;深入讲解模型压缩和加速技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;19.1 量化：INT8/FP16推理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;量化原理与类型&lt;/li&gt;
&lt;li&gt;PyTorch原生量化&lt;/li&gt;
&lt;li&gt;ONNX Runtime量化&lt;/li&gt;
&lt;li&gt;精度损失分析&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;19.2 剪枝与蒸馏&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结构化剪枝&lt;/li&gt;
&lt;li&gt;非结构化剪枝&lt;/li&gt;
&lt;li&gt;知识蒸馏方法&lt;/li&gt;
&lt;li&gt;实战：压缩ResNet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;19.3 TensorRT加速&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorRT工作原理&lt;/li&gt;
&lt;li&gt;ONNX转TensorRT&lt;/li&gt;
&lt;li&gt;性能优化技巧&lt;/li&gt;
&lt;li&gt;INT8校准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;19.4 实战：模型压缩与部署&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完整优化流程&lt;/li&gt;
&lt;li&gt;性能基准测试&lt;/li&gt;
&lt;li&gt;精度-速度权衡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="第20章生产部署"&gt;第20章：生产部署&lt;a class="anchor" href="#%e7%ac%ac20%e7%ab%a0%e7%94%9f%e4%ba%a7%e9%83%a8%e7%bd%b2"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;系统讲解部署方案和最佳实践：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;20.1 ONNX模型转换&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyTorch转ONNX&lt;/li&gt;
&lt;li&gt;模型验证与简化&lt;/li&gt;
&lt;li&gt;跨框架部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;20.2 服务化部署（FastAPI/Triton）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FastAPI构建推理服务&lt;/li&gt;
&lt;li&gt;Triton Inference Server&lt;/li&gt;
&lt;li&gt;负载均衡与扩展&lt;/li&gt;
&lt;li&gt;Docker容器化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;20.3 边缘设备部署&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;移动端部署（TFLite/CoreML）&lt;/li&gt;
&lt;li&gt;Jetson嵌入式设备&lt;/li&gt;
&lt;li&gt;ONNX Runtime Mobile&lt;/li&gt;
&lt;li&gt;性能优化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;20.4 实战：构建生产级服务&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>第09章 决策树与集成学习</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="第09章-决策树与集成学习"&gt;第09章 决策树与集成学习&lt;a class="anchor" href="#%e7%ac%ac09%e7%ab%a0-%e5%86%b3%e7%ad%96%e6%a0%91%e4%b8%8e%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;三个臭皮匠，顶个诸葛亮。&amp;rdquo; —— 中国谚语&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The whole is greater than the sum of its parts.&amp;rdquo; —— Aristotle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：集成学习是机器学习中最优雅的哲学之一。&lt;/p&gt;
&lt;p&gt;当你意识到&lt;strong&gt;一群弱小的模型，通过恰当的组织方式，能够超越任何单一的强大模型&lt;/strong&gt;时，你触碰到了群体智慧的数学本质。这不仅仅是工程技巧，更是对**&amp;ldquo;涌现&amp;rdquo; (emergence)** 这一概念的深刻诠释。&lt;/p&gt;
&lt;p&gt;本章将带你经历一次从个体到集体的认知跃迁：从&lt;strong&gt;单棵树的分裂策略&lt;/strong&gt; (信息增益)，到&lt;strong&gt;多棵树的协同方式&lt;/strong&gt; (Bagging vs Boosting)，再到&lt;strong&gt;损失函数的梯度优化&lt;/strong&gt; (GBDT &amp;amp; XGBoost)。当你发现 AdaBoost 不是拍脑袋发明的，而是在最小化指数损失；当你理解 GBDT 拟合残差的本质是负梯度下降时，你将领悟到集成学习那令人战栗的统一之美。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-%e5%bc%95%e8%a8%80"&gt;引言&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e5%86%b3%e7%ad%96%e6%a0%91decision-tree"&gt;决策树(Decision Tree)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;2.1 &lt;a href="#21-%e7%9b%b4%e8%a7%8920%e4%b8%aa%e9%97%ae%e9%a2%98%e6%b8%b8%e6%88%8f"&gt;直觉:20个问题游戏&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.2 &lt;a href="#22-%e7%ba%af%e5%ba%a6%e7%9a%84%e5%ba%a6%e9%87%8f%e7%86%b5%e4%b8%8e%e5%9f%ba%e5%b0%bc"&gt;纯度的度量:熵与基尼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.3 &lt;a href="#23-%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9%e6%9c%80%e5%a4%a7%e5%8c%96%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a"&gt;特征选择:最大化信息增益&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.4 &lt;a href="#24-%e5%86%b3%e7%ad%96%e6%a0%91%e7%9a%84%e6%9e%84%e5%bb%ba%e7%ae%97%e6%b3%95"&gt;决策树的构建算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.5 &lt;a href="#25-%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e5%89%aa%e6%9e%9d"&gt;过拟合与剪枝&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%80%bb%e7%ba%b2"&gt;集成学习的总纲&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;3.1 &lt;a href="#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9b%86%e6%88%90"&gt;为什么需要集成?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2 &lt;a href="#32-bias-variance-tradeoff"&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3 &lt;a href="#33-%e9%9b%86%e6%88%90%e7%9a%84%e4%b8%a4%e5%a4%a7%e6%b5%81%e6%b4%bebagging-vs-boosting"&gt;集成的两大流派:Bagging vs Boosting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-bagging%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97"&gt;Bagging与随机森林&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;4.1 &lt;a href="#41-bootstrap%e9%87%87%e6%a0%b7"&gt;Bootstrap采样&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.2 &lt;a href="#42-bagging%e7%9a%84%e9%99%8d%e6%96%b9%e5%b7%ae%e6%9c%ba%e5%88%b6"&gt;Bagging的降方差机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.3 &lt;a href="#43-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%9a%84%e5%8f%8c%e9%87%8d%e9%9a%8f%e6%9c%ba%e6%80%a7"&gt;随机森林的双重随机性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.4 &lt;a href="#44-out-of-bag%e4%bc%b0%e8%ae%a1"&gt;Out-of-Bag估计&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-boosting%e4%b9%8b%e9%ad%82adaboost"&gt;Boosting之魂:AdaBoost&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;5.1 &lt;a href="#51-adaboost%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b"&gt;AdaBoost算法流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 &lt;a href="#52-%e6%a0%b8%e5%bf%83%e6%8e%a8%e5%af%bc%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af%e6%8c%87%e6%95%b0%e6%8d%9f%e5%a4%b1"&gt;核心推导:为什么是指数损失?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.3 &lt;a href="#53-%e6%a0%b7%e6%9c%ac%e6%9d%83%e9%87%8d%e6%9b%b4%e6%96%b0%e5%85%ac%e5%bc%8f%e7%9a%84%e6%8e%a8%e5%af%bc"&gt;样本权重更新公式的推导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.4 &lt;a href="#54-adaboost%e7%9a%84%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89"&gt;AdaBoost的几何直觉&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-boosting%e4%b9%8b%e7%a5%9egbdt%e4%b8%8exgboost"&gt;Boosting之神:GBDT与XGBoost&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;6.1 &lt;a href="#61-gbdt%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e5%86%b3%e7%ad%96%e6%a0%91"&gt;GBDT:梯度提升决策树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.2 &lt;a href="#62-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%9f%e5%90%88%e6%ae%8b%e5%b7%ae"&gt;为什么拟合残差?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.3 &lt;a href="#63-xgboost%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80"&gt;XGBoost:二阶泰勒展开&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.4 &lt;a href="#64-xgboost%e7%9a%84%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc"&gt;XGBoost的目标函数推导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6.5 &lt;a href="#65-xgboost%e7%9a%84%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96"&gt;XGBoost的工程优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-%e6%80%bb%e7%bb%93"&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-%e9%99%84%e5%bd%95xgboost%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f"&gt;附录:XGBoost核心公式&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-引言"&gt;1. 引言&lt;a class="anchor" href="#1-%e5%bc%95%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;集成学习 (Ensemble Learning)&lt;/strong&gt; 的核心思想非常简单：&lt;strong&gt;如果你不能信任单个专家，那就组织一个委员会&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>第九篇 Agent 架构设计</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/</guid><description>&lt;h1 id="第九篇-agent-架构设计"&gt;第九篇 Agent 架构设计&lt;a class="anchor" href="#%e7%ac%ac%e4%b9%9d%e7%af%87-agent-%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;本章摘要&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;引用吴恩达教授观点：Agentic Workflow 的未来在于 &lt;strong&gt;协作 (Collaboration)&lt;/strong&gt;。本章将深入探讨 &lt;strong&gt;Multi-Agent Systems (MAS)&lt;/strong&gt;，学习如何构建 &lt;strong&gt;去中心化 (Swarm)&lt;/strong&gt;、&lt;strong&gt;分布式 (Distributed)&lt;/strong&gt; 和 &lt;strong&gt;开放连接 (MCP)&lt;/strong&gt; 的智能系统。&lt;/p&gt;
&lt;p&gt;我们将采用 &lt;strong&gt;LangGraph 官方标准&lt;/strong&gt;，废弃手写的轮子，聚焦于工业级的架构模式和最佳实践。&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="目录导航"&gt;目录导航&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95%e5%af%bc%e8%88%aa"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;协作模式演进 (Patterns)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Swarm 模式详解 (Official Way)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分布式协作 (Distributed State)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务化标准 (LangServe)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标准化工具协议 (MCP)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;架构总结与选型指南&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="第1章协作模式演进-patterns"&gt;第1章：协作模式演进 (Patterns)&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0%e5%8d%8f%e4%bd%9c%e6%a8%a1%e5%bc%8f%e6%bc%94%e8%bf%9b-patterns"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="11-吴恩达的四种-agentic-模式"&gt;1.1 吴恩达的四种 Agentic 模式&lt;a class="anchor" href="#11-%e5%90%b4%e6%81%a9%e8%be%be%e7%9a%84%e5%9b%9b%e7%a7%8d-agentic-%e6%a8%a1%e5%bc%8f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;在 2024 年的演讲中，Andrew Ng 总结了四种核心的 Agentic Workflow 模式：&lt;/p&gt;
&lt;pre class="mermaid"&gt;graph TB
 subgraph &amp;#34;Pattern 1: Reflection&amp;#34;
 A1[Generate] --&amp;gt; A2[Self-Critique] --&amp;gt; A3[Revise]
 end

 subgraph &amp;#34;Pattern 2: Tool Use&amp;#34;
 B1[Reasoning] --&amp;gt; B2[Tool Call] --&amp;gt; B3[Integrate Results]
 end

 subgraph &amp;#34;Pattern 3: Planning&amp;#34;
 C1[Decompose] --&amp;gt; C2[Execute Steps] --&amp;gt; C3[Synthesize]
 end

 subgraph &amp;#34;Pattern 4: Multi-Agent&amp;#34;
 D1[Agent A] &amp;lt;--&amp;gt; D2[Agent B]
 D2 &amp;lt;--&amp;gt; D3[Agent C]
 D1 &amp;lt;--&amp;gt; D3
 end

 style D1 fill:#e1f5ff
 style D2 fill:#e1f5ff
 style D3 fill:#e1f5ff&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;本章聚焦&lt;/strong&gt; Pattern 4：&lt;strong&gt;Multi-Agent Collaboration（多智能体协作）&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>第10章 逻辑回归与最大熵模型</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="第10章逻辑回归与最大熵模型"&gt;第10章：逻辑回归与最大熵模型&lt;a class="anchor" href="#%e7%ac%ac10%e7%ab%a0%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The entropy of the universe tends to a maximum.&amp;rdquo; —— Rudolf Clausius&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;重要提示&lt;/strong&gt;：本章将揭示一个惊人的数学事实：&lt;strong&gt;逻辑回归 (Logistic Regression)&lt;/strong&gt; 只是 &lt;strong&gt;最大熵模型 (Maximum Entropy Model)&lt;/strong&gt; 的一个特例。&lt;/p&gt;
&lt;p&gt;当我们承认由于信息不足而必须保留&amp;quot;最大的不确定性&amp;quot;时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。&lt;/p&gt;
&lt;p&gt;我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="目录"&gt;目录&lt;a class="anchor" href="#%e7%9b%ae%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-%e5%bc%95%e8%a8%80%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e7%9a%84%e4%b8%a4%e7%a7%8d%e8%a7%86%e8%a7%92"&gt;引言：分类问题的两种视角&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92-logistic-regression"&gt;逻辑回归 (Logistic Regression)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;2.1 &lt;a href="#21-sigmoid-%e5%87%bd%e6%95%b0%e7%9a%84%e7%94%b1%e6%9d%a5"&gt;Sigmoid 函数的由来&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.2 &lt;a href="#22-%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1-mle"&gt;极大似然估计 (MLE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2.3 &lt;a href="#23-%e4%bf%a1%e6%81%af%e8%ae%ba%e8%a7%86%e8%a7%92%e6%9c%80%e5%b0%8f%e5%8c%96%e4%ba%a4%e5%8f%89%e7%86%b5"&gt;信息论视角：最小化交叉熵&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b-maximum-entropy-model"&gt;最大熵模型 (Maximum Entropy Model)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;3.1 &lt;a href="#31-%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86%e6%97%a0%e7%9f%a5%e6%98%af%e7%9a%84%e6%99%ba%e6%85%a7"&gt;最大熵原理：无知是的智慧&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2 &lt;a href="#32-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9a%e4%b9%89"&gt;最大熵模型的定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3 &lt;a href="#33-lagrange-%e5%af%b9%e5%81%b6%e6%8e%a8%e5%af%bc"&gt;Lagrange 对偶推导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7"&gt;殊途同归：逻辑回归与最大熵的等价性&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;4.1 &lt;a href="#41-%e4%bb%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e6%8e%a8%e5%af%bc%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92"&gt;从最大熵推导逻辑回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;4.2 &lt;a href="#42-%e5%a4%9a%e5%88%86%e7%b1%bb%e6%8e%a8%e5%b9%bfsoftmax-%e5%9b%9e%e5%bd%92"&gt;多分类推广：Softmax 回归&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95"&gt;模型学习算法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;5.1 &lt;a href="#51-%e6%94%b9%e8%bf%9b%e7%9a%84%e8%bf%ad%e4%bb%a3%e5%b0%ba%e5%ba%a6%e6%b3%95-iis"&gt;改进的迭代尺度法 (IIS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5.2 &lt;a href="#52-%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95-bfgsl-bfgs"&gt;拟牛顿法 (BFGS/L-BFGS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-%e6%80%bb%e7%bb%93"&gt;总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb"&gt;推荐阅读&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="1-引言分类问题的两种视角"&gt;1. 引言：分类问题的两种视角&lt;a class="anchor" href="#1-%e5%bc%95%e8%a8%80%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e7%9a%84%e4%b8%a4%e7%a7%8d%e8%a7%86%e8%a7%92"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带&amp;quot;回归&amp;quot;，但它是一个纯粹的&lt;strong&gt;分类模型&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;理解逻辑回归有两种路径：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;统计学视角&lt;/strong&gt;：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息论视角&lt;/strong&gt;：在满足数据约束的前提下，选择熵最大的分布。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本章将证明，这两种视角最终指向了同一个数学形式。&lt;/p&gt;</description></item><item><title>第十篇 生产实践与监控评估</title><link>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/</guid><description>&lt;h1 id="第十篇-生产实践与监控评估"&gt;第十篇 生产实践与监控评估&lt;a class="anchor" href="#%e7%ac%ac%e5%8d%81%e7%af%87-%e7%94%9f%e4%ba%a7%e5%ae%9e%e8%b7%b5%e4%b8%8e%e7%9b%91%e6%8e%a7%e8%af%84%e4%bc%b0"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;: 构建生产级LLM应用的完整体系&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;从监控追踪到架构设计,从性能优化到安全防护,从部署运维到故障排查,全面掌握生产环境的关键要素。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="第1章langsmith-tracing-与-evaluation"&gt;第1章：LangSmith Tracing 与 Evaluation&lt;a class="anchor" href="#%e7%ac%ac1%e7%ab%a0langsmith-tracing-%e4%b8%8e-evaluation"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;关注点&lt;/strong&gt;：掌握 Agent 执行的全链路可观测性，建立科学的评估框架。&lt;/p&gt;
&lt;/blockquote&gt;&lt;h3 id="11-追踪体系"&gt;1.1 追踪体系&lt;a class="anchor" href="#11-%e8%bf%bd%e8%b8%aa%e4%bd%93%e7%b3%bb"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="111-追踪原理与数据模型"&gt;1.1.1 追踪原理与数据模型&lt;a class="anchor" href="#111-%e8%bf%bd%e8%b8%aa%e5%8e%9f%e7%90%86%e4%b8%8e%e6%95%b0%e6%8d%ae%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;什么是追踪（Tracing）？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;追踪是记录和分析 Agent 执行过程的完整链路，从用户输入开始，记录每一个中间步骤（模型调用、工具执行、状态变化），最终得到输出。LangSmith 追踪形成一棵执行树：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;root_run&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Agent&lt;/span&gt; &lt;span class="err"&gt;执行&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;before_model_hook&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Middleware&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;model_call&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;模型调用&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;│&lt;/span&gt; &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;system_prompt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;│&lt;/span&gt; &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;messages&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;│&lt;/span&gt; &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;tool_run&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;工具执行&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;│&lt;/span&gt; &lt;span class="err"&gt;├──&lt;/span&gt; &lt;span class="n"&gt;search_tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;│&lt;/span&gt; &lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;get_weather_tool&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;└──&lt;/span&gt; &lt;span class="n"&gt;after_model_hook&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;后处理&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;追踪的核心作用&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;调试&lt;/strong&gt;：看到完整的执行链，快速定位问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;监控&lt;/strong&gt;：追踪延迟、Token 成本、错误率等指标&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化&lt;/strong&gt;：识别瓶颈，比较不同版本的性能差异&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;审计&lt;/strong&gt;：记录谁做了什么，满足合规要求&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;数据模型&lt;/strong&gt;：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="c1"&gt;# 唯一 ID&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="c1"&gt;# 运行名称&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;run_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="c1"&gt;# &amp;#34;agent&amp;#34;, &amp;#34;model&amp;#34;, &amp;#34;tool&amp;#34;, &amp;#34;chain&amp;#34; 等&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;parent_run_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 父 Run ID（形成树关系）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 输入输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Any&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 输入参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Any&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 输出结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 时间和成本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt; &lt;span class="c1"&gt;# 开始时间&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end_time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt; &lt;span class="c1"&gt;# 结束时间&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;duration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="c1"&gt;# 执行耗时（秒）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# Token 和成本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;token_usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TokenUsage&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 美元成本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 状态和错误&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="c1"&gt;# &amp;#34;success&amp;#34;, &amp;#34;error&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 错误信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 元数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Any&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 自定义元数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;tags&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 标签（用于筛选）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;# 反馈&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;feedback_records&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Feedback&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 用户反馈&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="112-自动追踪环境变量配置"&gt;1.1.2 自动追踪：环境变量配置&lt;a class="anchor" href="#112-%e8%87%aa%e5%8a%a8%e8%bf%bd%e8%b8%aa%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e9%85%8d%e7%bd%ae"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;最简单的开启方式&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第11章 广义线性模型(GLM)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/</guid><description>&lt;h1 id="第11章-广义线性模型-generalized-linear-models"&gt;第11章 广义线性模型 (Generalized Linear Models)&lt;a class="anchor" href="#%e7%ac%ac11%e7%ab%a0-%e5%b9%bf%e4%b9%89%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b-generalized-linear-models"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The purpose of models is not to fit the data but to sharpen the questions.&amp;rdquo;
— Samuel Karlin&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="111-引言从线性回归到glm"&gt;11.1 引言：从线性回归到GLM&lt;a class="anchor" href="#111-%e5%bc%95%e8%a8%80%e4%bb%8e%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e5%88%b0glm"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在前面的章节中,我们已经学习了&lt;strong&gt;线性回归&lt;/strong&gt;和&lt;strong&gt;逻辑回归&lt;/strong&gt;两个重要模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;线性回归&lt;/strong&gt;：假设 $y \sim \mathcal{N}(\boldsymbol{w}^T\boldsymbol{x}, \sigma^2)$，用于预测连续值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;逻辑回归&lt;/strong&gt;：假设 $y \sim \text{Bernoulli}(\sigma(\boldsymbol{w}^T\boldsymbol{x}))$，用于二分类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两个看似不同的模型，实际上可以统一在&lt;strong&gt;广义线性模型 (Generalized Linear Model, GLM)&lt;/strong&gt; 的框架下。GLM 通过引入&lt;strong&gt;指数族分布&lt;/strong&gt;和&lt;strong&gt;链接函数&lt;/strong&gt;，为处理各种类型的响应变量（连续、离散、计数等）提供了统一的理论框架。&lt;/p&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：GLM 不直接建模 $E[y|\boldsymbol{x}]$，而是对其进行某种变换后再与线性预测器 $\boldsymbol{w}^T\boldsymbol{x}$ 建立关系。&lt;/p&gt;
&lt;/blockquote&gt;&lt;h3 id="1111-为什么需要glm"&gt;11.1.1 为什么需要GLM？&lt;a class="anchor" href="#1111-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81glm"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;传统线性回归的局限性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;响应变量类型受限&lt;/strong&gt;：只能处理服从正态分布的连续变量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;异方差问题&lt;/strong&gt;：方差与均值相关时，模型假设被违背&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;取值范围限制&lt;/strong&gt;：无法保证预测值在合理范围内（如概率 $\in [0,1]$，计数 $\in \mathbb{N}$）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;GLM 通过以下方式解决这些问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;允许响应变量服从&lt;strong&gt;指数族分布&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;通过&lt;strong&gt;链接函数&lt;/strong&gt;将均值映射到实数域&lt;/li&gt;
&lt;li&gt;方差可以是均值的函数&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="112-指数族分布"&gt;11.2 指数族分布&lt;a class="anchor" href="#112-%e6%8c%87%e6%95%b0%e6%97%8f%e5%88%86%e5%b8%83"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1121-指数族的通用形式"&gt;11.2.1 指数族的通用形式&lt;a class="anchor" href="#1121-%e6%8c%87%e6%95%b0%e6%97%8f%e7%9a%84%e9%80%9a%e7%94%a8%e5%bd%a2%e5%bc%8f"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;如果随机变量 $y$ 的概率密度（或质量）函数可以写成以下形式，则称 $y$ 服从&lt;strong&gt;指数族分布&lt;/strong&gt;：&lt;/p&gt;</description></item><item><title>第12章 朴素贝叶斯与高斯判别分析</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC12%E7%AB%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC12%E7%AB%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</guid><description/></item><item><title>第13章 概率图模型 表示</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/</guid><description>&lt;h1 id="第13章-概率图模型表示"&gt;第13章 概率图模型：表示&lt;a class="anchor" href="#%e7%ac%ac13%e7%ab%a0-%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%e8%a1%a8%e7%a4%ba"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;The purpose of computing is insight, not numbers.&amp;rdquo;
— Richard Hamming&lt;/p&gt;
&lt;/blockquote&gt;&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Graphical models are a marriage between probability theory and graph theory.&amp;rdquo;
— Michael I. Jordan&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="131-引言为什么需要概率图模型"&gt;13.1 引言：为什么需要概率图模型？&lt;a class="anchor" href="#131-%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1311-高维联合概率分布的困境"&gt;13.1.1 高维联合概率分布的困境&lt;a class="anchor" href="#1311-%e9%ab%98%e7%bb%b4%e8%81%94%e5%90%88%e6%a6%82%e7%8e%87%e5%88%86%e5%b8%83%e7%9a%84%e5%9b%b0%e5%a2%83"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;考虑 $n$ 个二值随机变量 $X_1, X_2, \ldots, X_n$。完整的联合概率分布 $P(X_1, X_2, \ldots, X_n)$ 需要存储 $2^n - 1$ 个参数（减1是因为概率和为1的约束）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;存储复杂度&lt;/strong&gt;：随着变量数量指数增长，参数空间爆炸。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;估计复杂度&lt;/strong&gt;：从数据中估计如此多的参数需要海量样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推断复杂度&lt;/strong&gt;：在高维空间中进行边缘化或条件化计算不可行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：利用变量间的&lt;strong&gt;条件独立性&lt;/strong&gt;来分解联合概率分布。&lt;/p&gt;
&lt;h3 id="1312-条件独立性的威力"&gt;13.1.2 条件独立性的威力&lt;a class="anchor" href="#1312-%e6%9d%a1%e4%bb%b6%e7%8b%ac%e7%ab%8b%e6%80%a7%e7%9a%84%e5%a8%81%e5%8a%9b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;如果变量 $X$ 和 $Y$ 在给定 $Z$ 的条件下独立，记作 $X \perp Y \mid Z$，则：&lt;/p&gt;</description></item><item><title>第14章 概率图模型 推断</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/</guid><description>&lt;h1 id="第14章-概率图模型推断"&gt;第14章 概率图模型：推断&lt;a class="anchor" href="#%e7%ac%ac14%e7%ab%a0-%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%e6%8e%a8%e6%96%ad"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&amp;ldquo;Probabilistic inference is nothing but counting, in appropriate ways.&amp;rdquo; — Judea Pearl&lt;/p&gt;
&lt;/blockquote&gt;&lt;h2 id="引言"&gt;引言&lt;a class="anchor" href="#%e5%bc%95%e8%a8%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在概率图模型中，&lt;strong&gt;推断（Inference）&lt;/strong&gt; 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。&lt;/p&gt;
&lt;p&gt;本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="141-推断问题的分类"&gt;14.1 推断问题的分类&lt;a class="anchor" href="#141-%e6%8e%a8%e6%96%ad%e9%97%ae%e9%a2%98%e7%9a%84%e5%88%86%e7%b1%bb"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="1411-推断任务的类型"&gt;14.1.1 推断任务的类型&lt;a class="anchor" href="#1411-%e6%8e%a8%e6%96%ad%e4%bb%bb%e5%8a%a1%e7%9a%84%e7%b1%bb%e5%9e%8b"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;设概率图模型定义在变量集合 $\mathcal{V} = {X_1, X_2, \ldots, X_n}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;查询变量（Query Variables）&lt;/strong&gt;：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;证据变量（Evidence Variables）&lt;/strong&gt;：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐变量（Hidden Variables）&lt;/strong&gt;：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的推断任务包括：&lt;/p&gt;
&lt;h4 id="1-边缘推断marginal-inference"&gt;(1) 边缘推断（Marginal Inference）&lt;a class="anchor" href="#1-%e8%be%b9%e7%bc%98%e6%8e%a8%e6%96%admarginal-inference"&gt;#&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;计算查询变量的&lt;strong&gt;边缘概率分布&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$
P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}
$$&lt;/p&gt;</description></item><item><title>第15章 EM算法</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC15%E7%AB%A0_em%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC15%E7%AB%A0_em%E7%AE%97%E6%B3%95/</guid><description/></item><item><title>第16章 高斯混合模型(GMM)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC16%E7%AB%A0_%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8Bgmm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC16%E7%AB%A0_%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8Bgmm/</guid><description/></item><item><title>第17章 隐马尔可夫模型(HMM)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC17%E7%AB%A0_%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8Bhmm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC17%E7%AB%A0_%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8Bhmm/</guid><description/></item><item><title>第18章 线性动态系统(LDS)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC18%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%8A%A8%E6%80%81%E7%B3%BB%E7%BB%9Flds/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC18%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%8A%A8%E6%80%81%E7%B3%BB%E7%BB%9Flds/</guid><description/></item><item><title>第19章 粒子滤波</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC19%E7%AB%A0_%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC19%E7%AB%A0_%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2/</guid><description/></item><item><title>第20章 变分推断(VI)</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC20%E7%AB%A0_%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%ADvi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC20%E7%AB%A0_%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%ADvi/</guid><description/></item><item><title>第21章 MCMC采样</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC21%E7%AB%A0_mcmc%E9%87%87%E6%A0%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC21%E7%AB%A0_mcmc%E9%87%87%E6%A0%B7/</guid><description/></item><item><title>第22章 狄利克雷过程与非参数贝叶斯</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC22%E7%AB%A0_%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E8%BF%87%E7%A8%8B%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E8%B4%9D%E5%8F%B6%E6%96%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC22%E7%AB%A0_%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E8%BF%87%E7%A8%8B%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E8%B4%9D%E5%8F%B6%E6%96%AF/</guid><description/></item><item><title>第23章 受限玻尔兹曼机(RBM)与深度信念网络</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC23%E7%AB%A0_%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BArbm%E4%B8%8E%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC23%E7%AB%A0_%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BArbm%E4%B8%8E%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C/</guid><description/></item><item><title>第24章 PCA与PPCA</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC24%E7%AB%A0_pca%E4%B8%8Eppca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC24%E7%AB%A0_pca%E4%B8%8Eppca/</guid><description/></item><item><title>第25章 流形学习</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC25%E7%AB%A0_%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC25%E7%AB%A0_%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/</guid><description/></item><item><title>GLOSSARY</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/</guid><description>&lt;h1 id="glossary-术语表"&gt;GLOSSARY 术语表&lt;a class="anchor" href="#glossary-%e6%9c%af%e8%af%ad%e8%a1%a8"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;大语言模型技术索引 (2025年版)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本术语表包含大语言模型领域的核心概念、前沿技术与工程实践术语。每个术语提供精炼定义及章节交叉引用。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="a"&gt;A&lt;a class="anchor" href="#a"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="adalora-adaptive-lora"&gt;AdaLoRA (Adaptive LoRA)&lt;a class="anchor" href="#adalora-adaptive-lora"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;自适应秩分配的LoRA变体，根据重要性动态调整不同层的秩参数，提升参数效率。
→ 详见 [Part 3 Ch 2: 微调你的专属模型]&lt;/p&gt;
&lt;h3 id="agent-智能体"&gt;Agent (智能体)&lt;a class="anchor" href="#agent-%e6%99%ba%e8%83%bd%e4%bd%93"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;能够感知环境、自主决策并执行行动以完成目标的LLM系统，通常结合ReAct模式与工具调用能力。
→ 详见 [Part 4 Ch 3: 智能体核心机制]&lt;/p&gt;
&lt;h3 id="alignment-对齐"&gt;Alignment (对齐)&lt;a class="anchor" href="#alignment-%e5%af%b9%e9%bd%90"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;使模型输出符合人类价值观和意图的过程，核心技术包括RLHF、DPO等。
→ 详见 [Part 3 Ch 3: 与人类对齐：偏好优化]&lt;/p&gt;
&lt;h3 id="attention-注意力机制"&gt;Attention (注意力机制)&lt;a class="anchor" href="#attention-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transformer的核心组件，通过Query-Key-Value机制动态加权聚合信息，实现上下文理解。
→ 详见 [Part 2 Ch 1: Transformer核心揭秘]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="b"&gt;B&lt;a class="anchor" href="#b"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="bert-bidirectional-encoder-representations-from-transformers"&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;a class="anchor" href="#bert-bidirectional-encoder-representations-from-transformers"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;基于Transformer编码器的双向预训练模型，擅长理解任务如文本分类、命名实体识别。
→ 详见 [Part 1 Ch 1: 初识大语言模型] / [Part 2 Ch 2: 模型家族谱系]&lt;/p&gt;</description></item><item><title>ROADMAP</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/</guid><description>&lt;h1 id="大模型技能树路线图-llm-skill-tree--learning-paths"&gt;大模型技能树路线图 (LLM Skill Tree &amp;amp; Learning Paths)&lt;a class="anchor" href="#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e6%8a%80%e8%83%bd%e6%a0%91%e8%b7%af%e7%ba%bf%e5%9b%be-llm-skill-tree--learning-paths"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;从入门到精通，根据职业目标选择最优学习路径。&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="技能树总览-skill-tree-overview"&gt;技能树总览 (Skill Tree Overview)&lt;a class="anchor" href="#%e6%8a%80%e8%83%bd%e6%a0%91%e6%80%bb%e8%a7%88-skill-tree-overview"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;pre class="mermaid"&gt;graph TD
 Start[开始学习] --&amp;gt; Foundation[基础阶段]

 Foundation --&amp;gt; P1_1[&amp;#34;第1章: 初识大语言模型&amp;#34;]
 Foundation --&amp;gt; P1_2[&amp;#34;第2章: 提示工程基础&amp;#34;]
 Foundation --&amp;gt; P1_3[&amp;#34;第3章: 分词与嵌入&amp;#34;]

 P1_1 --&amp;gt; Architecture[架构理解阶段]
 P1_2 --&amp;gt; Architecture
 P1_3 --&amp;gt; Architecture

 Architecture --&amp;gt; P2_1[&amp;#34;第1章: Transformer核心&amp;#34;]
 Architecture --&amp;gt; P2_2[&amp;#34;第2章: 模型家族谱系&amp;#34;]
 Architecture --&amp;gt; P2_3[&amp;#34;第3章: 预训练奥秘&amp;#34;]

 P2_1 --&amp;gt; Split{职业分流}
 P2_2 --&amp;gt; Split
 P2_3 --&amp;gt; Split

 Split --&amp;gt;|应用开发路线| AppDev[👨‍💻 应用开发]
 Split --&amp;gt;|算法研究路线| Research[🧪 算法研究]
 Split --&amp;gt;|MLOps路线| Ops[⚙️ MLOps工程]

 AppDev --&amp;gt; P4_RAG[&amp;#34;RAG应用开发&amp;#34;]
 AppDev --&amp;gt; P4_Agent[&amp;#34;Agent系统开发&amp;#34;]
 AppDev --&amp;gt; P6_Deploy[&amp;#34;生产部署&amp;#34;]

 Research --&amp;gt; P3_Data[&amp;#34;数据工程&amp;#34;]
 Research --&amp;gt; P3_FineTune[&amp;#34;微调技术&amp;#34;]
 Research --&amp;gt; P3_Align[&amp;#34;对齐与偏好优化&amp;#34;]
 Research --&amp;gt; P7_Advanced[&amp;#34;前沿技术&amp;#34;]

 Ops --&amp;gt; P5_Tools[&amp;#34;工具栈掌握&amp;#34;]
 Ops --&amp;gt; P6_Serving[&amp;#34;高性能推理&amp;#34;]
 Ops --&amp;gt; P6_Eval[&amp;#34;评估体系&amp;#34;]

 P4_RAG --&amp;gt; Master[精通阶段]
 P4_Agent --&amp;gt; Master
 P3_Align --&amp;gt; Master
 P7_Advanced --&amp;gt; Master
 P6_Serving --&amp;gt; Master

 style Start fill:#e1f5ff
 style Split fill:#fff4e1
 style Master fill:#d4f5d4
 style AppDev fill:#ffe1e1
 style Research fill:#e1ffe1
 style Ops fill:#f0e1ff&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id="三大职业路线详解-career-paths"&gt;三大职业路线详解 (Career Paths)&lt;a class="anchor" href="#%e4%b8%89%e5%a4%a7%e8%81%8c%e4%b8%9a%e8%b7%af%e7%ba%bf%e8%af%a6%e8%a7%a3-career-paths"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="-应用开发路线-application-developer-path"&gt;👨‍💻 应用开发路线 (Application Developer Path)&lt;a class="anchor" href="#-%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91%e8%b7%af%e7%ba%bf-application-developer-path"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;职业目标&lt;/strong&gt;：基于 LLM 构建应用（聊天机器人、RAG 系统、AI Agent）&lt;/p&gt;</description></item><item><title>skills</title><link>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/</guid><description>&lt;h1 id="机器学习笔记写作指南-writing-guidelines"&gt;机器学习笔记写作指南 (Writing Guidelines)&lt;a class="anchor" href="#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%e5%86%99%e4%bd%9c%e6%8c%87%e5%8d%97-writing-guidelines"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="核心原则"&gt;核心原则&lt;a class="anchor" href="#%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;顶级质量标准&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;教科书级的严谨&lt;/strong&gt;：数学定义必须精确，符号规范统一。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;小说级的可读性&lt;/strong&gt;：语言通俗易懂，避免枯燥的教科书式说教。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工程师级的实战&lt;/strong&gt;：每个概念都要回答&amp;quot;在机器学习中有什么用&amp;quot;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;内容结构规范&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义 (Definition)&lt;/strong&gt;：严格的数学表达。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直觉 (Intuition)&lt;/strong&gt;：用生活案例、几何图像或物理意义解释。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数学推导 (Derivation)&lt;/strong&gt;：核心公式必须推导，展示逻辑链条。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可视化 (Visualization)&lt;/strong&gt;：文字描述图形，帮助建立心理表征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ML应用 (Application)&lt;/strong&gt;：连接到具体的算法或模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;风格指南&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;避免&lt;/strong&gt;：口语化表达（如&amp;quot;老铁&amp;quot;、&amp;ldquo;搞定&amp;rdquo;）、模糊的描述（&amp;ldquo;显然&amp;rdquo;、&amp;ldquo;容易看出&amp;rdquo;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提倡&lt;/strong&gt;：学术专业术语、清晰的逻辑连接词、第一人称引导（&amp;ldquo;我们要解决&amp;hellip;&amp;quot;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;格式&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;向量用粗体小写 $\mathbf{x}$，矩阵用粗体大写 $\mathbf{A}$。&lt;/li&gt;
&lt;li&gt;重点内容使用引用块 &lt;code&gt;&amp;gt; &lt;/code&gt; 或加粗。&lt;/li&gt;
&lt;li&gt;公式使用 LaTeX 块 &lt;code&gt;$$ ... $$&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="章节模板"&gt;章节模板&lt;a class="anchor" href="#%e7%ab%a0%e8%8a%82%e6%a8%a1%e6%9d%bf"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-markdown" data-lang="markdown"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gh"&gt;# 第XX章：[章节标题]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;&amp;gt; &lt;/span&gt;&lt;span class="ge"&gt;**前言**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="ge"&gt;&amp;gt; [一段引人入胜的开场白，阐述本章的核心价值和学习目标。不仅要说学什么，更要说为什么学，以及学完后的思维升级。]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;## 目录
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;-&lt;/span&gt; [&lt;span class="nt"&gt;X.1 核心概念一&lt;/span&gt;](&lt;span class="na"&gt;#...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;-&lt;/span&gt; [&lt;span class="nt"&gt;X.2 核心概念二&lt;/span&gt;](&lt;span class="na"&gt;#...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;## X.1 [核心概念一]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;### 定义与直觉
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[严格数学定义]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;&amp;gt; &lt;/span&gt;&lt;span class="ge"&gt;**直觉/几何意义**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="ge"&gt;&amp;gt; [用通俗语言或几何视角解释。例如：投影就是向量在子空间上的&amp;#34;影子&amp;#34;。]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;### 数学推导
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[核心公式的推导过程]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;### 在机器学习中的应用
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;-&lt;/span&gt; **应用场景1**：[解释]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;-&lt;/span&gt; **应用场景2**：[解释]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;...&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="质量检查清单-checklist"&gt;质量检查清单 (Checklist)&lt;a class="anchor" href="#%e8%b4%a8%e9%87%8f%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95-checklist"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled="" type="checkbox"&gt; &lt;strong&gt;完整性&lt;/strong&gt;：是否覆盖了该主题的所有核心知识点？&lt;/li&gt;
&lt;li&gt;&lt;input disabled="" type="checkbox"&gt; &lt;strong&gt;深度&lt;/strong&gt;：是否触及了本质（Worldview），而不仅仅是表象？&lt;/li&gt;
&lt;li&gt;&lt;input disabled="" type="checkbox"&gt; &lt;strong&gt;连贯性&lt;/strong&gt;：章节之间是否流畅过渡，概念引用是否清晰？&lt;/li&gt;
&lt;li&gt;&lt;input disabled="" type="checkbox"&gt; &lt;strong&gt;准确性&lt;/strong&gt;：公式、符号、定理描述是否百分百正确？&lt;/li&gt;
&lt;li&gt;&lt;input disabled="" type="checkbox"&gt; &lt;strong&gt;实战性&lt;/strong&gt;：是否给出了具体的例子或数值计算过程？&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="常用-latex-符号规范"&gt;常用 LaTeX 符号规范&lt;a class="anchor" href="#%e5%b8%b8%e7%94%a8-latex-%e7%ac%a6%e5%8f%b7%e8%a7%84%e8%8c%83"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;向量：&lt;code&gt;\mathbf{x}&lt;/code&gt; ($\mathbf{x}$)&lt;/li&gt;
&lt;li&gt;矩阵：&lt;code&gt;\mathbf{A}&lt;/code&gt; ($\mathbf{A}$)&lt;/li&gt;
&lt;li&gt;此集：&lt;code&gt;\mathbb{R}^n&lt;/code&gt; ($\mathbb{R}^n$)&lt;/li&gt;
&lt;li&gt;梯度：&lt;code&gt;\nabla&lt;/code&gt; ($\nabla$)&lt;/li&gt;
&lt;li&gt;偏导：&lt;code&gt;\partial&lt;/code&gt; ($\partial$)&lt;/li&gt;
&lt;li&gt;期望：&lt;code&gt;\mathbb{E}&lt;/code&gt; ($\mathbb{E}$)&lt;/li&gt;
&lt;li&gt;损失函数：&lt;code&gt;\mathcal{L}&lt;/code&gt; ($\mathcal{L}$)&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>SUMMARY</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/</guid><description>&lt;h1 id="summary"&gt;Summary&lt;a class="anchor" href="#summary"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="README.md"&gt;封面&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第一部分大语言模型基础"&gt;第一部分：大语言模型基础&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"&gt;第1章 初识大语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/"&gt;第2章 与模型对话：提示工程基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/"&gt;第3章 语言的基石：分词与嵌入&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第二部分transformer架构揭秘"&gt;第二部分：Transformer架构揭秘&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%8c%e9%83%a8%e5%88%86transformer%e6%9e%b6%e6%9e%84%e6%8f%ad%e7%a7%98"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/"&gt;第1章 Transformer核心揭秘&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/"&gt;第2章 模型家族谱系：从编码器到解码器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/"&gt;第3章 预训练的奥秘：从数据到智能&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第三部分数据工程与定制化"&gt;第三部分：数据工程与定制化&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%89%e9%83%a8%e5%88%86%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b%e4%b8%8e%e5%ae%9a%e5%88%b6%e5%8c%96"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/"&gt;第1章 数据工程基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/"&gt;第2章 微调你的专属模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/"&gt;第3章 与人类对齐：偏好优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/"&gt;第4章 创建更优的嵌入模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第四部分大模型应用开发"&gt;第四部分：大模型应用开发&lt;a class="anchor" href="#%e7%ac%ac%e5%9b%9b%e9%83%a8%e5%88%86%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%ba%94%e7%94%a8%e5%bc%80%e5%8f%91"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/"&gt;第1章 提示工程与上下文学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/"&gt;第2章 检索增强生成（RAG）原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/"&gt;第3章 智能体（Agent）核心机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/"&gt;第4章 多模态大模型原理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第五部分工程实战工具栈"&gt;第五部分：工程实战工具栈&lt;a class="anchor" href="#%e7%ac%ac%e4%ba%94%e9%83%a8%e5%88%86%e5%b7%a5%e7%a8%8b%e5%ae%9e%e6%88%98%e5%b7%a5%e5%85%b7%e6%a0%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/"&gt;第1章 Hugging Face生态全景&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/"&gt;第2章 LLaMA-Factory微调工厂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"&gt;第3章 TRL与强化学习实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"&gt;第4章 DeepSpeed分布式训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"&gt;第5章 端到端LLM项目实战&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第六部分生产部署与评估"&gt;第六部分：生产部署与评估&lt;a class="anchor" href="#%e7%ac%ac%e5%85%ad%e9%83%a8%e5%88%86%e7%94%9f%e4%ba%a7%e9%83%a8%e7%bd%b2%e4%b8%8e%e8%af%84%e4%bc%b0"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/"&gt;第1章 模型压缩与推理加速&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/"&gt;第2章 vLLM高性能推理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/"&gt;第3章 模型评估体系&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="第七部分高级技术专题"&gt;第七部分：高级技术专题&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%83%e9%83%a8%e5%88%86%e9%ab%98%e7%ba%a7%e6%8a%80%e6%9c%af%e4%b8%93%e9%a2%98"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/"&gt;第1章 长上下文技术&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/"&gt;第2章 新型架构探索&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/"&gt;第3章 推理加速黑科技&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/"&gt;第4章 推理模型专题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/"&gt;第5章 模型安全与可解释性&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="附录"&gt;附录&lt;a class="anchor" href="#%e9%99%84%e5%bd%95"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/"&gt;完结报告&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>完结报告</title><link>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/</guid><description>&lt;h1 id="大模型笔记项目完结报告"&gt;大模型笔记项目完结报告&lt;a class="anchor" href="#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%ac%94%e8%ae%b0%e9%a1%b9%e7%9b%ae%e5%ae%8c%e7%bb%93%e6%8a%a5%e5%91%8a"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;blockquote class='book-hint '&gt;
&lt;p&gt;&lt;strong&gt;项目完成时间&lt;/strong&gt;: 20xx 年 x 月 x 日
&lt;strong&gt;项目状态&lt;/strong&gt;: ✅ 已完成
&lt;strong&gt;技术版本&lt;/strong&gt;: Latest SOTA Edition&lt;/p&gt;
&lt;/blockquote&gt;&lt;hr&gt;
&lt;h2 id="一项目概览"&gt;一、项目概览&lt;a class="anchor" href="#%e4%b8%80%e9%a1%b9%e7%9b%ae%e6%a6%82%e8%a7%88"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本项目是一份&lt;strong&gt;全栈大语言模型技术手册&lt;/strong&gt;，系统性覆盖从理论基础到生产实战的完整知识体系。项目采用&lt;strong&gt;模块化架构&lt;/strong&gt;，共分为 &lt;strong&gt;7 大部分 29 个章节&lt;/strong&gt;，总计超过 &lt;strong&gt;35 万字&lt;/strong&gt;的深度技术内容。&lt;/p&gt;
&lt;h3 id="项目规模统计"&gt;项目规模统计&lt;a class="anchor" href="#%e9%a1%b9%e7%9b%ae%e8%a7%84%e6%a8%a1%e7%bb%9f%e8%ae%a1"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;总章节数&lt;/strong&gt;: 29 章&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;覆盖部分&lt;/strong&gt;: 7 大技术领域&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术深度&lt;/strong&gt;: 从入门到生产级实战&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;内容形式&lt;/strong&gt;: 理论阐释 + 代码实战 + 案例分析&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代码量&lt;/strong&gt;: 500+ 个可运行代码片段&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;技术时效性&lt;/strong&gt;: SOTA 最新技术栈（DeepSeek-R1、SimPO、GraphRAG、MCP 等）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="知识体系架构"&gt;知识体系架构&lt;a class="anchor" href="#%e7%9f%a5%e8%af%86%e4%bd%93%e7%b3%bb%e6%9e%b6%e6%9e%84"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;大模型笔记
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 1: 大语言模型基础 (3章) # 历史演进、提示工程、分词嵌入
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 2: Transformer架构揭秘 (3章) # 注意力机制、模型谱系、预训练
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 3: 数据工程与定制化 (4章) # 数据工程、微调、对齐、嵌入模型
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 4: 大模型应用开发 (4章) # 分类聚类、RAG、Agent、多模态
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 5: 工程实战工具栈 (5章) # HF生态、LLaMA-Factory、TRL、DeepSpeed、项目实战
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;├── Part 6: 生产部署与评估 (4章) # 模型压缩、vLLM、部署、评估
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└── Part 7: 高级技术专题 (6章) # 长上下文、新架构、推理增强、安全、数据工程&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id="二核心亮点回顾"&gt;二、核心亮点回顾&lt;a class="anchor" href="#%e4%ba%8c%e6%a0%b8%e5%bf%83%e4%ba%ae%e7%82%b9%e5%9b%9e%e9%a1%be"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="-part-1-大语言模型基础--历史叙事与-api-实战"&gt;🎯 Part 1: 大语言模型基础 — 历史叙事与 API 实战&lt;a class="anchor" href="#-part-1-%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80--%e5%8e%86%e5%8f%b2%e5%8f%99%e4%ba%8b%e4%b8%8e-api-%e5%ae%9e%e6%88%98"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;第 1 章：初识大语言模型&lt;/strong&gt;&lt;/p&gt;</description></item></channel></rss>