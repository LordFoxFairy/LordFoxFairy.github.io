<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第六篇:生成模型(GAN/Diffusion)# 目标读者:掌握CNN和Transformer基础,希望深入理解生成式AI的读者
学习重点:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成
篇章概述# 生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。
为什么学习生成模型?# AIGC时代核心技术:Midjourney、Stable Diffusion、DALL-E等产品的底层技术 多模态理解基础:理解文生图是学习VLM的前置知识 实用价值高:图像生成、编辑、超分辨率等多种应用 技术快速迭代:从DDPM到FLUX,扩散模型仍在快速发展 技术演进时间线# 2014-2019: GAN时代 ├── 2014: GAN提出 (Goodfellow) ├── 2015: DCGAN - 稳定训练的GAN ├── 2018: StyleGAN - 高质量人脸生成 └── 2019: StyleGAN2 - 生成质量巅峰 2020-至今: Diffusion崛起 ├── 2020: DDPM提出 (Ho et al.) ├── 2021: DALL-E (OpenAI) ├── 2022: Stable Diffusion开源 ├── 2023: ControlNet、SDXL ├── 2024: Stable Diffusion 3、FLUX.1 └── 2025: 扩散模型持续迭代 章节安排# 第14章:生成对抗网络(GAN)# 快速回顾,不作为重点
14.1 GAN基础原理
生成器与判别器的对抗训练 GAN的损失函数 训练稳定性问题 14.2 DCGAN:深度卷积GAN
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第六篇 生成模型"><meta property="og:description" content="第六篇:生成模型(GAN/Diffusion)# 目标读者:掌握CNN和Transformer基础,希望深入理解生成式AI的读者
学习重点:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成
篇章概述# 生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。
为什么学习生成模型?# AIGC时代核心技术:Midjourney、Stable Diffusion、DALL-E等产品的底层技术 多模态理解基础:理解文生图是学习VLM的前置知识 实用价值高:图像生成、编辑、超分辨率等多种应用 技术快速迭代:从DDPM到FLUX,扩散模型仍在快速发展 技术演进时间线# 2014-2019: GAN时代 ├── 2014: GAN提出 (Goodfellow) ├── 2015: DCGAN - 稳定训练的GAN ├── 2018: StyleGAN - 高质量人脸生成 └── 2019: StyleGAN2 - 生成质量巅峰 2020-至今: Diffusion崛起 ├── 2020: DDPM提出 (Ho et al.) ├── 2021: DALL-E (OpenAI) ├── 2022: Stable Diffusion开源 ├── 2023: ControlNet、SDXL ├── 2024: Stable Diffusion 3、FLUX.1 └── 2025: 扩散模型持续迭代 章节安排# 第14章:生成对抗网络(GAN)# 快速回顾,不作为重点
14.1 GAN基础原理
生成器与判别器的对抗训练 GAN的损失函数 训练稳定性问题 14.2 DCGAN:深度卷积GAN"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第六篇 生成模型"><meta itemprop=description content="第六篇:生成模型(GAN/Diffusion)# 目标读者:掌握CNN和Transformer基础,希望深入理解生成式AI的读者
学习重点:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成
篇章概述# 生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。
为什么学习生成模型?# AIGC时代核心技术:Midjourney、Stable Diffusion、DALL-E等产品的底层技术 多模态理解基础:理解文生图是学习VLM的前置知识 实用价值高:图像生成、编辑、超分辨率等多种应用 技术快速迭代:从DDPM到FLUX,扩散模型仍在快速发展 技术演进时间线# 2014-2019: GAN时代 ├── 2014: GAN提出 (Goodfellow) ├── 2015: DCGAN - 稳定训练的GAN ├── 2018: StyleGAN - 高质量人脸生成 └── 2019: StyleGAN2 - 生成质量巅峰 2020-至今: Diffusion崛起 ├── 2020: DDPM提出 (Ho et al.) ├── 2021: DALL-E (OpenAI) ├── 2022: Stable Diffusion开源 ├── 2023: ControlNet、SDXL ├── 2024: Stable Diffusion 3、FLUX.1 └── 2025: 扩散模型持续迭代 章节安排# 第14章:生成对抗网络(GAN)# 快速回顾,不作为重点
14.1 GAN基础原理
生成器与判别器的对抗训练 GAN的损失函数 训练稳定性问题 14.2 DCGAN:深度卷积GAN"><meta itemprop=wordCount content="3002"><title>第六篇 生成模型 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle checked>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/ class=active>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第六篇 生成模型</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#篇章概述>篇章概述</a><ul><li><a href=#为什么学习生成模型>为什么学习生成模型?</a></li><li><a href=#技术演进时间线>技术演进时间线</a></li></ul></li><li><a href=#章节安排>章节安排</a><ul><li><a href=#第14章生成对抗网络gan>第14章:生成对抗网络(GAN)</a></li><li><a href=#第15章扩散模型diffusion>第15章:扩散模型(Diffusion)</a></li></ul></li><li><a href=#技术栈>技术栈</a><ul><li><a href=#环境要求>环境要求</a></li><li><a href=#核心依赖>核心依赖</a></li><li><a href=#验证安装>验证安装</a></li></ul></li><li><a href=#学习建议>学习建议</a><ul><li><a href=#1-重点放在diffusion>1. 重点放在Diffusion</a></li><li><a href=#2-理解扩散过程的数学原理>2. 理解扩散过程的数学原理</a></li><li><a href=#3-掌握提示词工程>3. 掌握提示词工程</a></li><li><a href=#4-循序渐进的实战>4. 循序渐进的实战</a></li></ul></li><li><a href=#模型资源>模型资源</a><ul><li><a href=#hugging-face-hub>Hugging Face Hub</a></li><li><a href=#国内下载加速>国内下载加速</a></li></ul></li><li><a href=#与前后篇的关系>与前后篇的关系</a></li><li><a href=#代码规范>代码规范</a><ul><li><a href=#目录结构>目录结构</a></li><li><a href=#代码风格>代码风格</a></li></ul></li><li><a href=#常见问题>常见问题</a><ul><li><a href=#q1-gan和diffusion哪个更重要>Q1: GAN和Diffusion哪个更重要?</a></li><li><a href=#q2-没有gpu能学习吗>Q2: 没有GPU能学习吗?</a></li><li><a href=#q3-stable-diffusion版本如何选择>Q3: Stable Diffusion版本如何选择?</a></li><li><a href=#q4-如何写出高质量提示词>Q4: 如何写出高质量提示词?</a></li><li><a href=#q5-生成的图像质量不好怎么办>Q5: 生成的图像质量不好怎么办?</a></li></ul></li><li><a href=#拓展资源>拓展资源</a><ul><li><a href=#官方文档>官方文档</a></li><li><a href=#论文阅读>论文阅读</a></li><li><a href=#社区资源>社区资源</a></li><li><a href=#实战平台>实战平台</a></li></ul></li><li><a href=#学习路线图>学习路线图</a></li><li><a href=#实战项目预告>实战项目预告</a></li></ul><ul><li><a href=#章节导读>章节导读</a><ul><li><a href=#为什么要学习gan>为什么要学习GAN?</a></li><li><a href=#本章学习策略>本章学习策略</a></li></ul></li><li><a href=#141-gan基础原理>14.1 GAN基础原理</a><ul><li><a href=#核心思想两人零和博弈>核心思想:两人零和博弈</a></li><li><a href=#数学形式>数学形式</a></li><li><a href=#训练流程>训练流程</a></li></ul></li><li><a href=#142-dcgan深度卷积gan>14.2 DCGAN:深度卷积GAN</a><ul><li><a href=#为什么需要dcgan>为什么需要DCGAN?</a></li><li><a href=#dcgan架构设计原则>DCGAN架构设计原则</a></li><li><a href=#生成器架构>生成器架构</a></li><li><a href=#判别器架构>判别器架构</a></li><li><a href=#权重初始化>权重初始化</a></li></ul></li><li><a href=#143-stylegan系列简介>14.3 StyleGAN系列(简介)</a><ul><li><a href=#stylegan的创新>StyleGAN的创新</a></li><li><a href=#stylegan2和stylegan3>StyleGAN2和StyleGAN3</a></li></ul></li><li><a href=#144-条件gan与应用>14.4 条件GAN与应用</a><ul><li><a href=#conditional-gan-cgan>Conditional GAN (cGAN)</a></li><li><a href=#经典应用>经典应用</a></li></ul></li><li><a href=#145-gan的主要问题>14.5 GAN的主要问题</a><ul><li><a href=#1-训练不稳定>1. 训练不稳定</a></li><li><a href=#2-梯度消失>2. 梯度消失</a></li><li><a href=#3-难以评估>3. 难以评估</a></li><li><a href=#4-超参数敏感>4. 超参数敏感</a></li></ul></li><li><a href=#146-实战dcgan生成人脸>14.6 实战:DCGAN生成人脸</a><ul><li><a href=#数据集celeba>数据集:CelebA</a></li><li><a href=#训练循环>训练循环</a></li><li><a href=#训练技巧>训练技巧</a></li></ul></li><li><a href=#147-gan与diffusion对比>14.7 GAN与Diffusion对比</a></li><li><a href=#148-总结与展望>14.8 总结与展望</a><ul><li><a href=#本章要点回顾>本章要点回顾</a></li><li><a href=#gan的现代应用>GAN的现代应用</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li><li><a href=#参考资源>参考资源</a><ul><li><a href=#论文>论文</a></li><li><a href=#代码资源>代码资源</a></li><li><a href=#在线课程>在线课程</a></li></ul></li></ul><ul><li><a href=#章节导读-1>章节导读</a><ul><li><a href=#为什么扩散模型如此重要>为什么扩散模型如此重要?</a></li><li><a href=#本章学习路线>本章学习路线</a></li></ul></li><li><a href=#151-扩散模型基础>15.1 扩散模型基础</a><ul><li><a href=#核心思想逐步加噪再去噪>核心思想:逐步加噪再去噪</a></li><li><a href=#前向扩散过程>前向扩散过程</a></li><li><a href=#反向去噪过程>反向去噪过程</a></li><li><a href=#ddpm训练目标>DDPM训练目标</a></li><li><a href=#训练流程伪代码>训练流程伪代码</a></li><li><a href=#采样流程生成图像>采样流程(生成图像)</a></li></ul></li><li><a href=#152-stable-diffusion架构>15.2 Stable Diffusion架构</a><ul><li><a href=#为什么需要latent-diffusion>为什么需要Latent Diffusion?</a></li><li><a href=#stable-diffusion三大组件>Stable Diffusion三大组件</a><ul><li><a href=#1-vae-variational-autoencoder>1. VAE (Variational Autoencoder)</a></li><li><a href=#2-u-net噪声预测器>2. U-Net:噪声预测器</a></li><li><a href=#3-clip-text-encoder>3. CLIP Text Encoder</a></li></ul></li><li><a href=#stable-diffusion生成流程>Stable Diffusion生成流程</a></li><li><a href=#采样器scheduler对比>采样器(Scheduler)对比</a></li><li><a href=#引导强度guidance-scale>引导强度(Guidance Scale)</a></li></ul></li><li><a href=#153-controlnet可控生成>15.3 ControlNet:可控生成</a><ul><li><a href=#为什么需要controlnet>为什么需要ControlNet?</a></li><li><a href=#controlnet原理>ControlNet原理</a></li><li><a href=#常用控制类型>常用控制类型</a><ul><li><a href=#1-canny-edge-边缘检测>1. Canny Edge (边缘检测)</a></li><li><a href=#2-depth-map-深度图>2. Depth Map (深度图)</a></li><li><a href=#3-openpose-人体姿态>3. OpenPose (人体姿态)</a></li><li><a href=#4-scribble-手绘草图>4. Scribble (手绘草图)</a></li><li><a href=#5-segmentation-语义分割>5. Segmentation (语义分割)</a></li></ul></li><li><a href=#多controlnet组合>多ControlNet组合</a></li><li><a href=#controlnet权重调节>ControlNet权重调节</a></li></ul></li><li><a href=#154-flux2024最新扩散模型>15.4 FLUX:2024最新扩散模型</a><ul><li><a href=#flux简介>FLUX简介</a></li><li><a href=#flux的创新点>FLUX的创新点</a></li><li><a href=#flux-vs-stable-diffusion>FLUX vs Stable Diffusion</a></li><li><a href=#使用flux>使用FLUX</a></li></ul></li><li><a href=#155-提示词工程prompt-engineering>15.5 提示词工程(Prompt Engineering)</a><ul><li><a href=#提示词的重要性>提示词的重要性</a></li><li><a href=#提示词结构>提示词结构</a></li><li><a href=#负向提示词negative-prompt>负向提示词(Negative Prompt)</a></li><li><a href=#提示词权重>提示词权重</a></li><li><a href=#风格提示词库>风格提示词库</a><ul><li><a href=#摄影风格>摄影风格</a></li><li><a href=#艺术风格>艺术风格</a></li><li><a href=#质量提升词>质量提升词</a></li></ul></li><li><a href=#提示词优化技巧>提示词优化技巧</a></li><li><a href=#常见问题与解决>常见问题与解决</a></li></ul></li><li><a href=#156-实战项目>15.6 实战项目</a><ul><li><a href=#项目1文生图text-to-image>项目1:文生图(Text-to-Image)</a></li><li><a href=#项目2图生图image-to-image>项目2:图生图(Image-to-Image)</a></li><li><a href=#项目3controlnet可控生成>项目3:ControlNet可控生成</a></li><li><a href=#项目4批量生成与自动化>项目4:批量生成与自动化</a></li></ul></li><li><a href=#157-高级技巧>15.7 高级技巧</a><ul><li><a href=#1-lora-low-rank-adaptation>1. LoRA (Low-Rank Adaptation)</a></li><li><a href=#2-textual-inversion>2. Textual Inversion</a></li><li><a href=#3-inpainting-局部重绘>3. Inpainting (局部重绘)</a></li><li><a href=#4-超分辨率upscaling>4. 超分辨率(Upscaling)</a></li></ul></li><li><a href=#158-性能优化>15.8 性能优化</a><ul><li><a href=#显存优化>显存优化</a></li><li><a href=#速度优化>速度优化</a></li><li><a href=#cpu-offloading>CPU Offloading</a></li></ul></li><li><a href=#159-总结>15.9 总结</a><ul><li><a href=#本章要点回顾-1>本章要点回顾</a></li><li><a href=#扩散模型-vs-gan>扩散模型 vs GAN</a></li><li><a href=#学习建议-1>学习建议</a></li><li><a href=#下一步方向>下一步方向</a></li></ul></li><li><a href=#参考资源-1>参考资源</a><ul><li><a href=#必读论文>必读论文</a></li><li><a href=#官方文档-1>官方文档</a></li><li><a href=#实战资源>实战资源</a></li><li><a href=#工具>工具</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第六篇生成模型gandiffusion>第六篇:生成模型(GAN/Diffusion)<a class=anchor href=#%e7%ac%ac%e5%85%ad%e7%af%87%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8bgandiffusion>#</a></h1><blockquote class=book-hint><p><strong>目标读者</strong>:掌握CNN和Transformer基础,希望深入理解生成式AI的读者</p><p><strong>学习重点</strong>:扩散模型(Diffusion)原理与实战、Stable Diffusion、ControlNet可控生成</p></blockquote><hr><h2 id=篇章概述>篇章概述<a class=anchor href=#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0>#</a></h2><p>生成式AI在2024年已成为计算机视觉最热门的方向。从早期的GAN到如今统治性的扩散模型,图像生成技术经历了巨大飞跃。本篇将快速回顾GAN,然后深入讲解扩散模型的原理与实战应用。</p><h3 id=为什么学习生成模型>为什么学习生成模型?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%ad%a6%e4%b9%a0%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b>#</a></h3><ul><li><strong>AIGC时代核心技术</strong>:Midjourney、Stable Diffusion、DALL-E等产品的底层技术</li><li><strong>多模态理解基础</strong>:理解文生图是学习VLM的前置知识</li><li><strong>实用价值高</strong>:图像生成、编辑、超分辨率等多种应用</li><li><strong>技术快速迭代</strong>:从DDPM到FLUX,扩散模型仍在快速发展</li></ul><h3 id=技术演进时间线>技术演进时间线<a class=anchor href=#%e6%8a%80%e6%9c%af%e6%bc%94%e8%bf%9b%e6%97%b6%e9%97%b4%e7%ba%bf>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>2014-2019: GAN时代
</span></span><span class=line><span class=cl>├── 2014: GAN提出 (Goodfellow)
</span></span><span class=line><span class=cl>├── 2015: DCGAN - 稳定训练的GAN
</span></span><span class=line><span class=cl>├── 2018: StyleGAN - 高质量人脸生成
</span></span><span class=line><span class=cl>└── 2019: StyleGAN2 - 生成质量巅峰
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2020-至今: Diffusion崛起
</span></span><span class=line><span class=cl>├── 2020: DDPM提出 (Ho et al.)
</span></span><span class=line><span class=cl>├── 2021: DALL-E (OpenAI)
</span></span><span class=line><span class=cl>├── 2022: Stable Diffusion开源
</span></span><span class=line><span class=cl>├── 2023: ControlNet、SDXL
</span></span><span class=line><span class=cl>├── 2024: Stable Diffusion 3、FLUX.1
</span></span><span class=line><span class=cl>└── 2025: 扩散模型持续迭代</span></span></code></pre></div><hr><h2 id=章节安排>章节安排<a class=anchor href=#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92>#</a></h2><h3 id=第14章生成对抗网络gan><a href=chapter14/README.md>第14章:生成对抗网络(GAN)</a><a class=anchor href=#%e7%ac%ac14%e7%ab%a0%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9cgan>#</a></h3><p><strong>快速回顾,不作为重点</strong></p><ul><li><p>14.1 GAN基础原理</p><ul><li>生成器与判别器的对抗训练</li><li>GAN的损失函数</li><li>训练稳定性问题</li></ul></li><li><p>14.2 DCGAN:深度卷积GAN</p><ul><li>网络架构设计</li><li>训练技巧与稳定性改进</li></ul></li><li><p>14.3 StyleGAN系列(简介)</p><ul><li>StyleGAN的创新点</li><li>风格迁移应用</li></ul></li><li><p>14.4 条件GAN与应用</p><ul><li>Conditional GAN</li><li>Pix2Pix、CycleGAN</li></ul></li><li><p><strong>实战</strong>:使用DCGAN生成人脸图像</p></li></ul><p><strong>核心技能</strong>:</p><ul><li>理解GAN的训练机制</li><li>掌握DCGAN的实现</li><li>了解GAN的局限性(为学习Diffusion做准备)</li></ul><p><strong>学习时间</strong>:1-2天(快速过一遍即可)</p><hr><h3 id=第15章扩散模型diffusion><a href=chapter15/README.md>第15章:扩散模型(Diffusion)</a><a class=anchor href=#%e7%ac%ac15%e7%ab%a0%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8bdiffusion>#</a></h3><p><strong>本篇重点,深入学习</strong></p><ul><li><p>15.1 扩散模型基础</p><ul><li>前向扩散过程:逐步加噪</li><li>反向去噪过程:学习噪声预测</li><li>DDPM数学原理(简化版)</li></ul></li><li><p>15.2 Stable Diffusion架构</p><ul><li>Latent Diffusion:潜空间扩散</li><li>VAE、U-Net、CLIP Text Encoder</li><li>采样器:DDIM、Euler、DPM-Solver++</li></ul></li><li><p>15.3 ControlNet:可控生成</p><ul><li>条件控制原理</li><li>常用控制类型:Canny、Depth、Pose等</li><li>多条件组合</li></ul></li><li><p>15.4 FLUX:2024最新扩散模型</p><ul><li>Black Forest Labs的新架构</li><li>FLUX.1-dev vs FLUX.1-schnell</li><li>性能对比与应用</li></ul></li><li><p>15.5 提示词工程(Prompt Engineering)</p><ul><li>高质量提示词结构</li><li>负向提示词技巧</li><li>提示词模板库</li></ul></li><li><p><strong>实战项目</strong>:</p><ul><li>文生图(Text-to-Image)完整流程</li><li>图生图(Image-to-Image)风格迁移</li><li>ControlNet可控生成</li><li>批量生成与自动化</li></ul></li></ul><p><strong>核心技能</strong>:</p><ul><li>深入理解扩散过程的数学原理</li><li>熟练使用Hugging Face diffusers库</li><li>掌握Stable Diffusion的各种应用</li><li>学会编写高质量提示词</li></ul><p><strong>学习时间</strong>:4-5天(重点学习)</p><hr><h2 id=技术栈>技术栈<a class=anchor href=#%e6%8a%80%e6%9c%af%e6%a0%88>#</a></h2><h3 id=环境要求>环境要求<a class=anchor href=#%e7%8e%af%e5%a2%83%e8%a6%81%e6%b1%82>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Python &gt;= 3.10</span>
</span></span><span class=line><span class=cl>python --version
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPU要求(强烈建议)</span>
</span></span><span class=line><span class=cl><span class=c1># - VRAM &gt;= 8GB (Stable Diffusion 1.5)</span>
</span></span><span class=line><span class=cl><span class=c1># - VRAM &gt;= 12GB (Stable Diffusion XL)</span>
</span></span><span class=line><span class=cl><span class=c1># - VRAM &gt;= 16GB (FLUX.1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看GPU信息</span>
</span></span><span class=line><span class=cl>nvidia-smi</span></span></code></pre></div><h3 id=核心依赖>核心依赖<a class=anchor href=#%e6%a0%b8%e5%bf%83%e4%be%9d%e8%b5%96>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># GAN相关(第14章)</span>
</span></span><span class=line><span class=cl>pip install torch torchvision
</span></span><span class=line><span class=cl>pip install matplotlib pillow
</span></span><span class=line><span class=cl>pip install tqdm
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Diffusion相关(第15章)</span>
</span></span><span class=line><span class=cl>pip install <span class=nv>diffusers</span><span class=o>==</span>0.35.1  <span class=c1># 最新版本(2024-11)</span>
</span></span><span class=line><span class=cl>pip install transformers accelerate
</span></span><span class=line><span class=cl>pip install safetensors
</span></span><span class=line><span class=cl>pip install controlnet-aux  <span class=c1># ControlNet预处理</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可选:模型下载加速</span>
</span></span><span class=line><span class=cl>pip install huggingface-hub
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>HF_ENDPOINT</span><span class=o>=</span>https://hf-mirror.com  <span class=c1># 国内镜像</span></span></span></code></pre></div><h3 id=验证安装>验证安装<a class=anchor href=#%e9%aa%8c%e8%af%81%e5%ae%89%e8%a3%85>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>diffusers</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>DiffusionPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;PyTorch: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Diffusers: </span><span class=si>{</span><span class=n>diffusers</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CUDA可用: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPU: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_name</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;VRAM: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_properties</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>total_memory</span> <span class=o>/</span> <span class=mf>1e9</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> GB&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=学习建议>学习建议<a class=anchor href=#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae>#</a></h2><h3 id=1-重点放在diffusion>1. 重点放在Diffusion<a class=anchor href=#1-%e9%87%8d%e7%82%b9%e6%94%be%e5%9c%a8diffusion>#</a></h3><p>GAN虽然是生成模型的开创性工作,但在2024年已基本被Diffusion取代:</p><table><thead><tr><th>维度</th><th>GAN</th><th>Diffusion</th></tr></thead><tbody><tr><td><strong>生成质量</strong></td><td>高但有模式崩溃风险</td><td>非常高且稳定</td></tr><tr><td><strong>训练稳定性</strong></td><td>难(生成器与判别器需平衡)</td><td>易(标准回归任务)</td></tr><tr><td><strong>多样性</strong></td><td>容易模式崩溃</td><td>多样性好</td></tr><tr><td><strong>可控性</strong></td><td>较难</td><td>容易(ControlNet等)</td></tr><tr><td><strong>主流应用</strong></td><td>较少</td><td>Midjourney、SD等</td></tr></tbody></table><p><strong>建议学习策略</strong>:</p><ul><li>第14章快速过一遍,理解GAN思想即可(1-2天)</li><li>第15章深入学习,动手实践各种应用(4-5天)</li></ul><h3 id=2-理解扩散过程的数学原理>2. 理解扩散过程的数学原理<a class=anchor href=#2-%e7%90%86%e8%a7%a3%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86>#</a></h3><p>扩散模型的核心是<strong>去噪扩散概率模型(DDPM)</strong>,其数学原理相对复杂。本笔记采用:</p><ul><li><strong>简化版数学推导</strong>:只讲核心思想,不展开复杂公式</li><li><strong>直观可视化</strong>:用代码和图示理解前向/反向过程</li><li><strong>工程实战为主</strong>:重点放在如何使用diffusers库</li></ul><p>即使不完全理解数学,也能掌握实战应用!</p><h3 id=3-掌握提示词工程>3. 掌握提示词工程<a class=anchor href=#3-%e6%8e%8c%e6%8f%a1%e6%8f%90%e7%a4%ba%e8%af%8d%e5%b7%a5%e7%a8%8b>#</a></h3><p>高质量图像生成的关键在于提示词(Prompt):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl># 低质量提示词
</span></span><span class=line><span class=cl>&#34;a cat&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 高质量提示词
</span></span><span class=line><span class=cl>&#34;a fluffy orange cat sitting on a wooden table,
</span></span><span class=line><span class=cl>soft natural lighting, shallow depth of field,
</span></span><span class=line><span class=cl>bokeh background, photorealistic, 8k uhd,
</span></span><span class=line><span class=cl>professional photography&#34;</span></span></code></pre></div><p><strong>学习路径</strong>:</p><ol><li>理解提示词的结构(主体、风格、质量词、负向词)</li><li>学习常用提示词模板</li><li>参考优秀案例(Civitai、PromptHero等)</li><li>大量实践,形成自己的提示词库</li></ol><h3 id=4-循序渐进的实战>4. 循序渐进的实战<a class=anchor href=#4-%e5%be%aa%e5%ba%8f%e6%b8%90%e8%bf%9b%e7%9a%84%e5%ae%9e%e6%88%98>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第一步:跑通Stable Diffusion基础代码
</span></span><span class=line><span class=cl>   ↓
</span></span><span class=line><span class=cl>第二步:尝试不同的提示词和参数
</span></span><span class=line><span class=cl>   ↓
</span></span><span class=line><span class=cl>第三步:学习ControlNet可控生成
</span></span><span class=line><span class=cl>   ↓
</span></span><span class=line><span class=cl>第四步:探索高级技巧(LoRA、IP-Adapter等)
</span></span><span class=line><span class=cl>   ↓
</span></span><span class=line><span class=cl>第五步:构建自己的图像生成应用</span></span></code></pre></div><hr><h2 id=模型资源>模型资源<a class=anchor href=#%e6%a8%a1%e5%9e%8b%e8%b5%84%e6%ba%90>#</a></h2><h3 id=hugging-face-hub>Hugging Face Hub<a class=anchor href=#hugging-face-hub>#</a></h3><p>本篇实战代码使用的模型:</p><ol><li><p><strong>Stable Diffusion 1.5</strong> (基础模型)</p><ul><li><code>runwayml/stable-diffusion-v1-5</code></li><li>VRAM需求:~5GB</li><li>速度:快</li></ul></li><li><p><strong>Stable Diffusion XL</strong> (高质量)</p><ul><li><code>stabilityai/stable-diffusion-xl-base-1.0</code></li><li>VRAM需求:~10GB</li><li>生成质量:显著提升</li></ul></li><li><p><strong>FLUX.1</strong> (2024最新)</p><ul><li><code>black-forest-labs/FLUX.1-dev</code></li><li><code>black-forest-labs/FLUX.1-schnell</code></li><li>VRAM需求:~16GB</li><li>生成质量:当前最佳</li></ul></li><li><p><strong>ControlNet模型</strong></p><ul><li><code>lllyasviel/control_v11p_sd15_canny</code></li><li><code>lllyasviel/control_v11f1p_sd15_depth</code></li><li>等多种控制类型</li></ul></li></ol><h3 id=国内下载加速>国内下载加速<a class=anchor href=#%e5%9b%bd%e5%86%85%e4%b8%8b%e8%bd%bd%e5%8a%a0%e9%80%9f>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 使用HuggingFace镜像</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>HF_ENDPOINT</span><span class=o>=</span>https://hf-mirror.com
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或使用ModelScope</span>
</span></span><span class=line><span class=cl>pip install modelscope
</span></span><span class=line><span class=cl><span class=c1># 代码中使用modelscope下载</span></span></span></code></pre></div><hr><h2 id=与前后篇的关系>与前后篇的关系<a class=anchor href=#%e4%b8%8e%e5%89%8d%e5%90%8e%e7%af%87%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第五篇:图像分割
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (判别式模型的最后一篇)
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>第六篇:生成模型 ← 当前篇
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (生成式模型:GAN → Diffusion)
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>第七篇:视觉大模型
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (多模态模型:CLIP、BLIP、LLaVA等)</span></span></code></pre></div><p><strong>知识衔接</strong>:</p><ul><li>Transformer基础(第3篇第6章) → Diffusion中的Attention机制</li><li>CNN架构(第3篇) → Diffusion中的U-Net</li><li>多模态模型(第7篇) → CLIP在SD中的文本编码器</li></ul><hr><h2 id=代码规范>代码规范<a class=anchor href=#%e4%bb%a3%e7%a0%81%e8%a7%84%e8%8c%83>#</a></h2><h3 id=目录结构>目录结构<a class=anchor href=#%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>part6_generation/
</span></span><span class=line><span class=cl>├── README.md                    # 本文件
</span></span><span class=line><span class=cl>├── chapter14/                   # GAN
</span></span><span class=line><span class=cl>│   ├── README.md
</span></span><span class=line><span class=cl>│   └── code/
</span></span><span class=line><span class=cl>│       └── dcgan_generation.py  # DCGAN完整实现
</span></span><span class=line><span class=cl>└── chapter15/                   # Diffusion
</span></span><span class=line><span class=cl>    ├── README.md
</span></span><span class=line><span class=cl>    └── code/
</span></span><span class=line><span class=cl>        ├── stable_diffusion_demo.py    # SD基础
</span></span><span class=line><span class=cl>        ├── controlnet_demo.py          # ControlNet
</span></span><span class=line><span class=cl>        └── utils/
</span></span><span class=line><span class=cl>            ├── prompt_templates.py     # 提示词模板
</span></span><span class=line><span class=cl>            └── image_utils.py          # 图像处理工具</span></span></code></pre></div><h3 id=代码风格>代码风格<a class=anchor href=#%e4%bb%a3%e7%a0%81%e9%a3%8e%e6%a0%bc>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>模块文档字符串:说明功能
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>DiffusionPipeline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_image</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>negative_prompt</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_inference_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>guidance_scale</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>7.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>seed</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    生成图像的核心函数
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        prompt: 正向提示词
</span></span></span><span class=line><span class=cl><span class=s2>        negative_prompt: 负向提示词
</span></span></span><span class=line><span class=cl><span class=s2>        num_inference_steps: 采样步数(越大越慢但质量越好)
</span></span></span><span class=line><span class=cl><span class=s2>        guidance_scale: 引导强度(7.5是常用值)
</span></span></span><span class=line><span class=cl><span class=s2>        seed: 随机种子(可复现结果)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        生成的图像Tensor
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 实现代码...</span>
</span></span><span class=line><span class=cl>    <span class=k>pass</span></span></span></code></pre></div><hr><h2 id=常见问题>常见问题<a class=anchor href=#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98>#</a></h2><h3 id=q1-gan和diffusion哪个更重要>Q1: GAN和Diffusion哪个更重要?<a class=anchor href=#q1-gan%e5%92%8cdiffusion%e5%93%aa%e4%b8%aa%e6%9b%b4%e9%87%8d%e8%a6%81>#</a></h3><p><strong>A</strong>: 2024年以后,Diffusion已成为主流。建议:</p><ul><li>GAN:了解基本原理即可,不必深入</li><li>Diffusion:重点学习,这是当前和未来的方向</li></ul><h3 id=q2-没有gpu能学习吗>Q2: 没有GPU能学习吗?<a class=anchor href=#q2-%e6%b2%a1%e6%9c%89gpu%e8%83%bd%e5%ad%a6%e4%b9%a0%e5%90%97>#</a></h3><p><strong>A</strong>:</p><ul><li>GAN部分:可以用CPU训练小模型(MNIST等)</li><li>Diffusion部分:强烈建议GPU,否则生成速度极慢</li><li>替代方案:使用Google Colab免费GPU或Hugging Face Spaces</li></ul><h3 id=q3-stable-diffusion版本如何选择>Q3: Stable Diffusion版本如何选择?<a class=anchor href=#q3-stable-diffusion%e7%89%88%e6%9c%ac%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9>#</a></h3><p><strong>A</strong>:</p><ul><li><strong>SD 1.5</strong>:轻量级,适合学习和低配GPU</li><li><strong>SDXL</strong>:质量更高,需要更多VRAM</li><li><strong>SD 3</strong>:最新版本,但生态还不完善</li><li><strong>FLUX</strong>:2024最强,但对硬件要求高</li></ul><p>建议从SD 1.5开始学习!</p><h3 id=q4-如何写出高质量提示词>Q4: 如何写出高质量提示词?<a class=anchor href=#q4-%e5%a6%82%e4%bd%95%e5%86%99%e5%87%ba%e9%ab%98%e8%b4%a8%e9%87%8f%e6%8f%90%e7%a4%ba%e8%af%8d>#</a></h3><p><strong>A</strong>: 遵循以下结构:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[主体描述] + [细节修饰] + [艺术风格] + [技术参数] + [质量词]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>示例:
</span></span><span class=line><span class=cl>&#34;a majestic lion standing on a cliff,
</span></span><span class=line><span class=cl>golden hour lighting, dramatic clouds,
</span></span><span class=line><span class=cl>digital painting, trending on artstation,
</span></span><span class=line><span class=cl>highly detailed, 8k uhd, masterpiece&#34;</span></span></code></pre></div><p>第15章会详细讲解提示词工程!</p><h3 id=q5-生成的图像质量不好怎么办>Q5: 生成的图像质量不好怎么办?<a class=anchor href=#q5-%e7%94%9f%e6%88%90%e7%9a%84%e5%9b%be%e5%83%8f%e8%b4%a8%e9%87%8f%e4%b8%8d%e5%a5%bd%e6%80%8e%e4%b9%88%e5%8a%9e>#</a></h3><p><strong>A</strong>: 依次检查:</p><ol><li><strong>提示词质量</strong>:是否足够详细和具体</li><li><strong>负向提示词</strong>:添加"low quality, blurry, distorted"等</li><li><strong>采样步数</strong>:提高num_inference_steps(20→50)</li><li><strong>引导强度</strong>:调整guidance_scale(5-10之间)</li><li><strong>采样器</strong>:尝试不同scheduler(Euler、DPM++等)</li><li><strong>模型选择</strong>:换用更高质量的模型</li></ol><hr><h2 id=拓展资源>拓展资源<a class=anchor href=#%e6%8b%93%e5%b1%95%e8%b5%84%e6%ba%90>#</a></h2><h3 id=官方文档>官方文档<a class=anchor href=#%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3>#</a></h3><ul><li><strong>Hugging Face Diffusers</strong>: <a href=https://huggingface.co/docs/diffusers>https://huggingface.co/docs/diffusers</a></li><li><strong>Stable Diffusion WebUI</strong>: <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui>https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></li><li><strong>ComfyUI</strong>: <a href=https://github.com/comfyanonymous/ComfyUI>https://github.com/comfyanonymous/ComfyUI</a> (节点式生成)</li></ul><h3 id=论文阅读>论文阅读<a class=anchor href=#%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb>#</a></h3><p><strong>必读论文</strong>(按时间顺序):</p><ol><li>GAN (2014) - Generative Adversarial Networks</li><li>DCGAN (2015) - Unsupervised Representation Learning with DCGAN</li><li>DDPM (2020) - Denoising Diffusion Probabilistic Models</li><li>Latent Diffusion (2022) - High-Resolution Image Synthesis</li><li>ControlNet (2023) - Adding Conditional Control to Text-to-Image</li></ol><h3 id=社区资源>社区资源<a class=anchor href=#%e7%a4%be%e5%8c%ba%e8%b5%84%e6%ba%90>#</a></h3><ul><li><strong>Civitai</strong>: 模型分享社区</li><li><strong>PromptHero</strong>: 提示词数据库</li><li><strong>Hugging Face Spaces</strong>: 在线Demo</li></ul><h3 id=实战平台>实战平台<a class=anchor href=#%e5%ae%9e%e6%88%98%e5%b9%b3%e5%8f%b0>#</a></h3><ul><li><strong>Midjourney</strong>: 商业化最成功的AI绘画工具</li><li><strong>Stable Diffusion WebUI</strong>: 本地部署,完全免费</li><li><strong>Leonardo.ai</strong>: 在线生成平台</li></ul><hr><h2 id=学习路线图>学习路线图<a class=anchor href=#%e5%ad%a6%e4%b9%a0%e8%b7%af%e7%ba%bf%e5%9b%be>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第1-2天: 第14章 GAN快速回顾
</span></span><span class=line><span class=cl>    ├── 理解GAN原理
</span></span><span class=line><span class=cl>    ├── 跑通DCGAN代码
</span></span><span class=line><span class=cl>    └── 了解GAN的局限性
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第3-7天: 第15章 Diffusion深入学习
</span></span><span class=line><span class=cl>    ├── Day 3: 理解扩散过程数学原理
</span></span><span class=line><span class=cl>    ├── Day 4: Stable Diffusion架构与使用
</span></span><span class=line><span class=cl>    ├── Day 5: 提示词工程与实战
</span></span><span class=line><span class=cl>    ├── Day 6: ControlNet可控生成
</span></span><span class=line><span class=cl>    └── Day 7: FLUX等前沿模型探索
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第8天: 综合实战项目
</span></span><span class=line><span class=cl>    └── 构建自己的图像生成应用</span></span></code></pre></div><hr><h2 id=实战项目预告>实战项目预告<a class=anchor href=#%e5%ae%9e%e6%88%98%e9%a1%b9%e7%9b%ae%e9%a2%84%e5%91%8a>#</a></h2><p>完成本篇学习后,你将能够:</p><ol><li><p><strong>基础应用</strong></p><ul><li>根据文本提示词生成高质量图像</li><li>使用ControlNet进行可控生成</li><li>图生图风格迁移</li></ul></li><li><p><strong>进阶应用</strong></p><ul><li>批量生成与自动化</li><li>与其他AI工具结合(如ChatGPT生成提示词)</li><li>构建Web应用(FastAPI + Diffusers)</li></ul></li><li><p><strong>商业应用方向</strong></p><ul><li>电商产品图生成</li><li>游戏素材制作</li><li>建筑效果图渲染</li><li>创意设计辅助</li></ul></li></ol><hr><p><strong>准备好进入生成式AI的奇妙世界了吗?让我们从第14章开始!</strong></p><hr><h1 id=第14章生成对抗网络gan-1>第14章:生成对抗网络(GAN)<a class=anchor href=#%e7%ac%ac14%e7%ab%a0%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9cgan-1>#</a></h1><blockquote class=book-hint><p><strong>学习目标</strong>:理解GAN的基本原理,掌握DCGAN实现,了解GAN的局限性</p><p><strong>建议学习时间</strong>:1-2天(快速回顾,不作为重点)</p><p><strong>前置知识</strong>:CNN基础、PyTorch基本操作</p></blockquote><hr><h2 id=章节导读>章节导读<a class=anchor href=#%e7%ab%a0%e8%8a%82%e5%af%bc%e8%af%bb>#</a></h2><p>生成对抗网络(GAN)在2014年由Ian Goodfellow提出,曾是生成模型的主流方向。虽然在2024年已基本被Diffusion模型取代,但理解GAN的思想对学习生成模型仍有重要意义。</p><h3 id=为什么要学习gan>为什么要学习GAN?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%ad%a6%e4%b9%a0gan>#</a></h3><ol><li><strong>历史意义</strong>:开创性的生成模型架构</li><li><strong>思想价值</strong>:对抗训练的思想影响深远</li><li><strong>知识衔接</strong>:理解GAN有助于理解Diffusion的优势</li><li><strong>特定应用</strong>:部分领域(如风格迁移)仍在使用</li></ol><h3 id=本章学习策略>本章学习策略<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%ad%a6%e4%b9%a0%e7%ad%96%e7%95%a5>#</a></h3><p><strong>快速过一遍,不必深入</strong>:</p><ul><li>理解GAN的核心思想</li><li>跑通一个DCGAN示例</li><li>了解GAN的主要问题</li><li>为学习Diffusion做准备</li></ul><hr><h2 id=141-gan基础原理>14.1 GAN基础原理<a class=anchor href=#141-gan%e5%9f%ba%e7%a1%80%e5%8e%9f%e7%90%86>#</a></h2><h3 id=核心思想两人零和博弈>核心思想:两人零和博弈<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e4%b8%a4%e4%ba%ba%e9%9b%b6%e5%92%8c%e5%8d%9a%e5%bc%88>#</a></h3><p>GAN的核心是**生成器(Generator)<strong>和</strong>判别器(Discriminator)**的对抗训练:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>生成器G: 学习生成逼真的假图像,试图欺骗判别器
</span></span><span class=line><span class=cl>判别器D: 学习区分真实图像和生成图像
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>训练过程:
</span></span><span class=line><span class=cl>1. G生成假图像
</span></span><span class=line><span class=cl>2. D学习区分真假(真实数据标记为1,假数据标记为0)
</span></span><span class=line><span class=cl>3. G根据D的反馈改进,生成更逼真的图像
</span></span><span class=line><span class=cl>4. D也在改进,提高鉴别能力
</span></span><span class=line><span class=cl>5. 最终达到纳什均衡:G生成的图像足够逼真,D无法区分</span></span></code></pre></div><h3 id=数学形式>数学形式<a class=anchor href=#%e6%95%b0%e5%ad%a6%e5%bd%a2%e5%bc%8f>#</a></h3><p>GAN的目标函数(简化版):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>其中:
</span></span><span class=line><span class=cl>- x: 真实数据
</span></span><span class=line><span class=cl>- z: 随机噪声(通常是高斯噪声)
</span></span><span class=line><span class=cl>- G(z): 生成器根据噪声z生成的假图像
</span></span><span class=line><span class=cl>- D(x): 判别器对真实图像的判断(接近1表示认为是真的)
</span></span><span class=line><span class=cl>- D(G(z)): 判别器对假图像的判断(接近0表示认为是假的)</span></span></code></pre></div><p><strong>直观理解</strong>:</p><ul><li>判别器D:最大化V,希望正确区分真假</li><li>生成器G:最小化V,希望骗过判别器</li></ul><h3 id=训练流程>训练流程<a class=anchor href=#%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>real_images</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># ===== 训练判别器 =====</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 真实图像送入判别器,标签为1</span>
</span></span><span class=line><span class=cl>        <span class=n>real_labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>real_outputs</span> <span class=o>=</span> <span class=n>discriminator</span><span class=p>(</span><span class=n>real_images</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_real</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>real_outputs</span><span class=p>,</span> <span class=n>real_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 生成假图像,标签为0</span>
</span></span><span class=line><span class=cl>        <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_images</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>noise</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_outputs</span> <span class=o>=</span> <span class=n>discriminator</span><span class=p>(</span><span class=n>fake_images</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_fake</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>fake_outputs</span><span class=p>,</span> <span class=n>fake_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 判别器总损失</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss</span> <span class=o>=</span> <span class=n>d_loss_real</span> <span class=o>+</span> <span class=n>d_loss_fake</span>
</span></span><span class=line><span class=cl>        <span class=c1># 反向传播更新判别器</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer_D</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ===== 训练生成器 =====</span>
</span></span><span class=line><span class=cl>        <span class=c1># 生成器希望判别器认为假图像是真的(标签为1)</span>
</span></span><span class=line><span class=cl>        <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_images</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>noise</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>discriminator</span><span class=p>(</span><span class=n>fake_images</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>g_loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 反向传播更新生成器</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer_G</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>g_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer_G</span><span class=o>.</span><span class=n>step</span><span class=p>()</span></span></span></code></pre></div><hr><h2 id=142-dcgan深度卷积gan>14.2 DCGAN:深度卷积GAN<a class=anchor href=#142-dcgan%e6%b7%b1%e5%ba%a6%e5%8d%b7%e7%a7%afgan>#</a></h2><h3 id=为什么需要dcgan>为什么需要DCGAN?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81dcgan>#</a></h3><p>原始GAN使用全连接层,存在以下问题:</p><ul><li>训练不稳定,容易模式崩溃</li><li>难以生成高分辨率图像</li><li>缺乏空间结构信息</li></ul><p><strong>DCGAN</strong> (Deep Convolutional GAN, 2015)通过引入卷积层解决了这些问题。</p><h3 id=dcgan架构设计原则>DCGAN架构设计原则<a class=anchor href=#dcgan%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1%e5%8e%9f%e5%88%99>#</a></h3><p>DCGAN提出了一套稳定训练的架构指南:</p><ol><li><p><strong>取消池化层</strong></p><ul><li>生成器:使用转置卷积(transposed convolution)进行上采样</li><li>判别器:使用步长卷积(strided convolution)进行下采样</li></ul></li><li><p><strong>使用BatchNorm</strong></p><ul><li>生成器:所有层都加BatchNorm(输出层除外)</li><li>判别器:所有层都加BatchNorm(输入层除外)</li></ul></li><li><p><strong>激活函数选择</strong></p><ul><li>生成器:隐藏层用ReLU,输出层用Tanh</li><li>判别器:所有层用LeakyReLU</li></ul></li><li><p><strong>去除全连接层</strong></p><ul><li>使用全卷积网络</li></ul></li></ol><h3 id=生成器架构>生成器架构<a class=anchor href=#%e7%94%9f%e6%88%90%e5%99%a8%e6%9e%b6%e6%9e%84>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Generator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>channels</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=c1># 输入: (batch, latent_dim, 1, 1)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 第一层:转置卷积 100 -&gt; 512x4x4</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>512</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 512x4x4 -&gt; 256x8x8</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 256x8x8 -&gt; 128x16x16</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 128x16x16 -&gt; 64x32x32</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 64x32x32 -&gt; 3x64x64</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>  <span class=c1># 输出范围[-1, 1]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># z: (batch, latent_dim, 1, 1)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>z</span><span class=p>)</span></span></span></code></pre></div><p><strong>关键点</strong>:</p><ul><li>输入是100维噪声向量,reshape为(100, 1, 1)</li><li>通过4次转置卷积,逐步上采样到64x64图像</li><li>最后用Tanh激活,输出范围[-1, 1]</li></ul><h3 id=判别器架构>判别器架构<a class=anchor href=#%e5%88%a4%e5%88%ab%e5%99%a8%e6%9e%b6%e6%9e%84>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Discriminator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>channels</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=c1># 输入: 3x64x64</span>
</span></span><span class=line><span class=cl>            <span class=c1># 3x64x64 -&gt; 64x32x32</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>channels</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 64x32x32 -&gt; 128x16x16</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 128x16x16 -&gt; 256x8x8</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 256x8x8 -&gt; 512x4x4</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>512</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=c1># 512x4x4 -&gt; 1x1x1</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>  <span class=c1># 输出范围[0, 1]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>img</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span></span></span></code></pre></div><p><strong>关键点</strong>:</p><ul><li>输入是64x64的图像</li><li>通过4次步长卷积,逐步下采样</li><li>最后输出单个值,表示判断为真的概率</li></ul><h3 id=权重初始化>权重初始化<a class=anchor href=#%e6%9d%83%e9%87%8d%e5%88%9d%e5%a7%8b%e5%8c%96>#</a></h3><p>DCGAN论文建议使用特定的权重初始化:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>weights_init</span><span class=p>(</span><span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    从均值0、标准差0.02的正态分布初始化权重
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>classname</span> <span class=o>=</span> <span class=n>m</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>classname</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;Conv&#39;</span><span class=p>)</span> <span class=o>!=</span> <span class=o>-</span><span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.02</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>classname</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;BatchNorm&#39;</span><span class=p>)</span> <span class=o>!=</span> <span class=o>-</span><span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>0.02</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>constant_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 应用初始化</span>
</span></span><span class=line><span class=cl><span class=n>generator</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>weights_init</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>weights_init</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=143-stylegan系列简介>14.3 StyleGAN系列(简介)<a class=anchor href=#143-stylegan%e7%b3%bb%e5%88%97%e7%ae%80%e4%bb%8b>#</a></h2><h3 id=stylegan的创新>StyleGAN的创新<a class=anchor href=#stylegan%e7%9a%84%e5%88%9b%e6%96%b0>#</a></h3><p><strong>StyleGAN</strong> (2018, NVIDIA)代表了GAN生成质量的巅峰:</p><ol><li><p><strong>风格迁移机制</strong></p><ul><li>将随机噪声映射到中间潜空间W</li><li>在不同层次注入风格信息</li><li>实现粗粒度到细粒度的风格控制</li></ul></li><li><p><strong>渐进式生长</strong></p><ul><li>从低分辨率逐步训练到高分辨率</li><li>训练更稳定</li></ul></li><li><p><strong>应用</strong></p><ul><li>高质量人脸生成</li><li>风格混合(style mixing)</li><li>图像编辑</li></ul></li></ol><h3 id=stylegan2和stylegan3>StyleGAN2和StyleGAN3<a class=anchor href=#stylegan2%e5%92%8cstylegan3>#</a></h3><ul><li><strong>StyleGAN2</strong> (2019):修复artifact问题,提升质量</li><li><strong>StyleGAN3</strong> (2021):解决纹理粘连问题</li></ul><p><strong>注意</strong>:StyleGAN系列架构复杂,训练难度大,本章不深入讲解。</p><hr><h2 id=144-条件gan与应用>14.4 条件GAN与应用<a class=anchor href=#144-%e6%9d%a1%e4%bb%b6gan%e4%b8%8e%e5%ba%94%e7%94%a8>#</a></h2><h3 id=conditional-gan-cgan>Conditional GAN (cGAN)<a class=anchor href=#conditional-gan-cgan>#</a></h3><p>在GAN中引入条件信息(如类别标签、文本描述):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 生成器和判别器都接受条件输入</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ConditionalGenerator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>label_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_classes</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 输入是噪声+条件拼接</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 将标签嵌入与噪声拼接</span>
</span></span><span class=line><span class=cl>        <span class=n>gen_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>noise</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>label_emb</span><span class=p>(</span><span class=n>labels</span><span class=p>)],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>gen_input</span><span class=p>)</span></span></span></code></pre></div><h3 id=经典应用>经典应用<a class=anchor href=#%e7%bb%8f%e5%85%b8%e5%ba%94%e7%94%a8>#</a></h3><ol><li><p><strong>Pix2Pix</strong> (2016)</p><ul><li>图像到图像的翻译(如素描→照片、白天→黑夜)</li><li>使用配对数据训练</li></ul></li><li><p><strong>CycleGAN</strong> (2017)</p><ul><li>无需配对数据的风格迁移</li><li>使用循环一致性损失</li></ul></li><li><p><strong>StarGAN</strong> (2018)</p><ul><li>多领域图像翻译</li><li>单个模型处理多种转换</li></ul></li></ol><p><strong>2024年现状</strong>:这些任务现在更多使用Diffusion模型(如ControlNet)。</p><hr><h2 id=145-gan的主要问题>14.5 GAN的主要问题<a class=anchor href=#145-gan%e7%9a%84%e4%b8%bb%e8%a6%81%e9%97%ae%e9%a2%98>#</a></h2><h3 id=1-训练不稳定>1. 训练不稳定<a class=anchor href=#1-%e8%ae%ad%e7%bb%83%e4%b8%8d%e7%a8%b3%e5%ae%9a>#</a></h3><p><strong>模式崩溃(Mode Collapse)</strong>:</p><ul><li>生成器只学会生成少数几种模式</li><li>缺乏多样性</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>理想情况: 数据分布有10个模式,生成器都能覆盖
</span></span><span class=line><span class=cl>模式崩溃: 生成器只生成其中2-3个模式</span></span></code></pre></div><h3 id=2-梯度消失>2. 梯度消失<a class=anchor href=#2-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1>#</a></h3><p>当判别器过强时,生成器梯度接近0,无法学习。</p><h3 id=3-难以评估>3. 难以评估<a class=anchor href=#3-%e9%9a%be%e4%bb%a5%e8%af%84%e4%bc%b0>#</a></h3><p>缺乏客观的评估指标,常用指标:</p><ul><li><strong>IS (Inception Score)</strong>:质量和多样性</li><li><strong>FID (Fréchet Inception Distance)</strong>:生成分布与真实分布的距离</li></ul><p>但这些指标都不完美。</p><h3 id=4-超参数敏感>4. 超参数敏感<a class=anchor href=#4-%e8%b6%85%e5%8f%82%e6%95%b0%e6%95%8f%e6%84%9f>#</a></h3><p>学习率、架构设计等超参数对结果影响巨大。</p><hr><h2 id=146-实战dcgan生成人脸>14.6 实战:DCGAN生成人脸<a class=anchor href=#146-%e5%ae%9e%e6%88%98dcgan%e7%94%9f%e6%88%90%e4%ba%ba%e8%84%b8>#</a></h2><h3 id=数据集celeba>数据集:CelebA<a class=anchor href=#%e6%95%b0%e6%8d%ae%e9%9b%86celeba>#</a></h3><p>CelebA包含20万张名人人脸图像,常用于GAN训练。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision</span> <span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>CenterCrop</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>([</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>])</span>  <span class=c1># 归一化到[-1, 1]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>CelebA</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>split</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>download</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_workers</span><span class=o>=</span><span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><h3 id=训练循环>训练循环<a class=anchor href=#%e8%ae%ad%e7%bb%83%e5%be%aa%e7%8e%af>#</a></h3><p>完整代码见: <a href=../../chapter29/code/chapter14_gan/dcgan_generation.py>code/chapter14_gan/dcgan_generation.py</a></p><h3 id=训练技巧>训练技巧<a class=anchor href=#%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7>#</a></h3><ol><li><p><strong>标签平滑</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 不使用硬标签1和0,而是0.9和0.1</span>
</span></span><span class=line><span class=cl><span class=n>real_labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl><span class=n>fake_labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=mf>0.1</span></span></span></code></pre></div></li><li><p><strong>单侧标签平滑</strong></p><ul><li>只对真实标签平滑,假标签保持0</li></ul></li><li><p><strong>特征匹配</strong></p><ul><li>生成器不仅要骗过判别器,还要匹配真实数据的统计特性</li></ul></li><li><p><strong>学习率调整</strong></p><ul><li>生成器和判别器使用不同学习率</li><li>常用:lr_D = 2e-4, lr_G = 2e-4</li></ul></li></ol><hr><h2 id=147-gan与diffusion对比>14.7 GAN与Diffusion对比<a class=anchor href=#147-gan%e4%b8%8ediffusion%e5%af%b9%e6%af%94>#</a></h2><table><thead><tr><th>维度</th><th>GAN</th><th>Diffusion</th></tr></thead><tbody><tr><td><strong>训练稳定性</strong></td><td>难,需要平衡G和D</td><td>稳定,标准去噪任务</td></tr><tr><td><strong>模式崩溃</strong></td><td>容易发生</td><td>不存在</td></tr><tr><td><strong>生成质量</strong></td><td>高(StyleGAN)</td><td>更高(Stable Diffusion)</td></tr><tr><td><strong>多样性</strong></td><td>有限</td><td>非常好</td></tr><tr><td><strong>可控性</strong></td><td>需要特殊设计(cGAN)</td><td>天然支持(ControlNet)</td></tr><tr><td><strong>训练时间</strong></td><td>相对快</td><td>慢</td></tr><tr><td><strong>推理时间</strong></td><td>快(单次前向)</td><td>慢(多步去噪)</td></tr><tr><td><strong>2024主流应用</strong></td><td>较少</td><td>Midjourney、SD等</td></tr></tbody></table><p><strong>结论</strong>:Diffusion在大多数方面优于GAN,这也是为什么本篇重点学习Diffusion。</p><hr><h2 id=148-总结与展望>14.8 总结与展望<a class=anchor href=#148-%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b>#</a></h2><h3 id=本章要点回顾>本章要点回顾<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e8%a6%81%e7%82%b9%e5%9b%9e%e9%a1%be>#</a></h3><ol><li><strong>GAN核心思想</strong>:生成器与判别器的对抗训练</li><li><strong>DCGAN架构</strong>:使用卷积层的稳定训练方案</li><li><strong>GAN的问题</strong>:训练不稳定、模式崩溃等</li><li><strong>历史地位</strong>:开创性工作,但已被Diffusion超越</li></ol><h3 id=gan的现代应用>GAN的现代应用<a class=anchor href=#gan%e7%9a%84%e7%8e%b0%e4%bb%a3%e5%ba%94%e7%94%a8>#</a></h3><p>虽然图像生成领域GAN已不是主流,但在以下领域仍有应用:</p><ul><li><strong>视频生成</strong>:部分工作仍使用GAN</li><li><strong>超分辨率</strong>:ESRGAN等</li><li><strong>特定领域</strong>:医学图像、工业检测等数据受限场景</li></ul><h3 id=下一章预告>下一章预告<a class=anchor href=#%e4%b8%8b%e4%b8%80%e7%ab%a0%e9%a2%84%e5%91%8a>#</a></h3><p>第15章我们将深入学习<strong>扩散模型(Diffusion)</strong>:</p><ul><li>理解前向扩散和反向去噪过程</li><li>掌握Stable Diffusion的使用</li><li>学习ControlNet可控生成</li><li>探索FLUX等最新模型</li></ul><p>扩散模型已成为生成式AI的核心技术,是本篇的重点内容!</p><hr><h2 id=参考资源>参考资源<a class=anchor href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%ba%90>#</a></h2><h3 id=论文>论文<a class=anchor href=#%e8%ae%ba%e6%96%87>#</a></h3><ol><li><p><strong>GAN</strong> (2014)</p><ul><li>Goodfellow et al., &ldquo;Generative Adversarial Networks&rdquo;</li><li><a href=https://arxiv.org/abs/1406.2661>arXiv:1406.2661</a></li></ul></li><li><p><strong>DCGAN</strong> (2015)</p><ul><li>Radford et al., &ldquo;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&rdquo;</li><li><a href=https://arxiv.org/abs/1511.06434>arXiv:1511.06434</a></li></ul></li><li><p><strong>StyleGAN</strong> (2018)</p><ul><li>Karras et al., &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;</li><li><a href=https://arxiv.org/abs/1812.04948>arXiv:1812.04948</a></li></ul></li></ol><h3 id=代码资源>代码资源<a class=anchor href=#%e4%bb%a3%e7%a0%81%e8%b5%84%e6%ba%90>#</a></h3><ul><li><strong>PyTorch官方DCGAN教程</strong>: <a href=https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html>https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html</a></li><li><strong>StyleGAN3 (NVIDIA)</strong>: <a href=https://github.com/NVlabs/stylegan3>https://github.com/NVlabs/stylegan3</a></li></ul><h3 id=在线课程>在线课程<a class=anchor href=#%e5%9c%a8%e7%ba%bf%e8%af%be%e7%a8%8b>#</a></h3><ul><li><strong>Coursera GAN Specialization</strong> (deeplearning.ai)</li><li><strong>Fast.ai Practical Deep Learning</strong> (Part 2包含GAN)</li></ul><hr><p><strong>下一步</strong>:进入<a href=../chapter15/README.md>第15章:扩散模型</a>,学习当前最先进的生成技术!</p><hr><h1 id=第15章扩散模型diffusion-models>第15章:扩散模型(Diffusion Models)<a class=anchor href=#%e7%ac%ac15%e7%ab%a0%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8bdiffusion-models>#</a></h1><blockquote class=book-hint><p><strong>学习目标</strong>:深入理解扩散模型原理,熟练使用Stable Diffusion,掌握ControlNet可控生成</p><p><strong>建议学习时间</strong>:4-5天(本篇重点,深入学习)</p><p><strong>前置知识</strong>:CNN基础、Transformer基础(第3篇第6章)</p></blockquote><hr><h2 id=章节导读-1>章节导读<a class=anchor href=#%e7%ab%a0%e8%8a%82%e5%af%bc%e8%af%bb-1>#</a></h2><p>扩散模型是2024年生成式AI的核心技术,Midjourney、Stable Diffusion、DALL-E 3等明星产品都基于扩散模型。本章将深入讲解扩散模型的原理与实战应用。</p><h3 id=为什么扩散模型如此重要>为什么扩散模型如此重要?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b%e5%a6%82%e6%ad%a4%e9%87%8d%e8%a6%81>#</a></h3><ol><li><strong>生成质量最高</strong>:超越GAN,成为当前最强生成技术</li><li><strong>训练稳定</strong>:不存在GAN的模式崩溃问题</li><li><strong>可控性强</strong>:通过ControlNet等技术实现精准控制</li><li><strong>生态完善</strong>:Hugging Face diffusers库,易于使用</li></ol><h3 id=本章学习路线>本章学习路线<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%ad%a6%e4%b9%a0%e8%b7%af%e7%ba%bf>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>15.1 扩散模型基础
</span></span><span class=line><span class=cl>  ↓ 理解前向加噪和反向去噪
</span></span><span class=line><span class=cl>15.2 Stable Diffusion架构
</span></span><span class=line><span class=cl>  ↓ 掌握Latent Diffusion的优势
</span></span><span class=line><span class=cl>15.3 ControlNet可控生成
</span></span><span class=line><span class=cl>  ↓ 学习条件控制技术
</span></span><span class=line><span class=cl>15.4 FLUX最新模型
</span></span><span class=line><span class=cl>  ↓ 了解2024前沿进展
</span></span><span class=line><span class=cl>15.5 提示词工程
</span></span><span class=line><span class=cl>  ↓ 编写高质量提示词
</span></span><span class=line><span class=cl>实战项目
</span></span><span class=line><span class=cl>  ↓ 文生图、图生图、ControlNet</span></span></code></pre></div><hr><h2 id=151-扩散模型基础>15.1 扩散模型基础<a class=anchor href=#151-%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80>#</a></h2><h3 id=核心思想逐步加噪再去噪>核心思想:逐步加噪再去噪<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e9%80%90%e6%ad%a5%e5%8a%a0%e5%99%aa%e5%86%8d%e5%8e%bb%e5%99%aa>#</a></h3><p>扩散模型的核心思想非常直观:</p><p><strong>前向过程(Forward Diffusion)</strong>:逐步向图像添加高斯噪声,直到变成纯噪声
<strong>反向过程(Reverse Diffusion)</strong>:学习如何一步步去除噪声,从纯噪声恢复图像</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始图像 → +噪声 → +噪声 → ... → 纯噪声  (前向过程,固定的)
</span></span><span class=line><span class=cl>          ↓        ↓        ↓        ↓
</span></span><span class=line><span class=cl>纯噪声 → -噪声 → -噪声 → ... → 生成图像  (反向过程,需要学习)</span></span></code></pre></div><h3 id=前向扩散过程>前向扩散过程<a class=anchor href=#%e5%89%8d%e5%90%91%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b>#</a></h3><p>给定一张图像 x₀,前向过程逐步添加高斯噪声:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x₀ → x₁ → x₂ → ... → x_T
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>每一步:
</span></span><span class=line><span class=cl>x_t = √(1-β_t) · x_{t-1} + √β_t · ε
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>其中:
</span></span><span class=line><span class=cl>- β_t: 第t步的噪声系数(通常从0.0001到0.02)
</span></span><span class=line><span class=cl>- ε ~ N(0, I): 标准高斯噪声
</span></span><span class=line><span class=cl>- T: 总步数(通常1000步)</span></span></code></pre></div><p><strong>关键性质</strong>:可以直接计算任意步的噪声图像,无需逐步计算:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x_t = √(ᾱ_t) · x₀ + √(1-ᾱ_t) · ε
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>其中:
</span></span><span class=line><span class=cl>ᾱ_t = ∏(1-β_i)  (累乘)</span></span></code></pre></div><p>这个性质使得训练时可以随机采样任意时间步,大大加速训练!</p><h3 id=反向去噪过程>反向去噪过程<a class=anchor href=#%e5%8f%8d%e5%90%91%e5%8e%bb%e5%99%aa%e8%bf%87%e7%a8%8b>#</a></h3><p>反向过程学习如何从噪声中恢复图像:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x_T → x_{T-1} → x_{T-2} → ... → x₀
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>每一步:
</span></span><span class=line><span class=cl>x_{t-1} = (1/√α_t) · (x_t - (β_t/√(1-ᾱ_t)) · ε_θ(x_t, t))</span></span></code></pre></div><p><strong>核心</strong>:神经网络 ε_θ(x_t, t) 学习预测每一步的噪声</p><h3 id=ddpm训练目标>DDPM训练目标<a class=anchor href=#ddpm%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87>#</a></h3><p><strong>Denoising Diffusion Probabilistic Models</strong> (DDPM, 2020)的训练目标非常简单:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>损失函数:
</span></span><span class=line><span class=cl>L = E[||ε - ε_θ(x_t, t)||²]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>其中:
</span></span><span class=line><span class=cl>- ε: 真实噪声
</span></span><span class=line><span class=cl>- ε_θ(x_t, t): 模型预测的噪声</span></span></code></pre></div><p><strong>直观理解</strong>:训练一个神经网络,让它学会预测"我们添加了什么噪声"。</p><h3 id=训练流程伪代码>训练流程伪代码<a class=anchor href=#%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b%e4%bc%aa%e4%bb%a3%e7%a0%81>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>x0</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>  <span class=c1># 原始图像</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 随机选择时间步 t ∈ [1, T]</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 生成随机噪声</span>
</span></span><span class=line><span class=cl>        <span class=n>epsilon</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 根据公式计算 x_t</span>
</span></span><span class=line><span class=cl>        <span class=n>xt</span> <span class=o>=</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>alpha_bar_t</span><span class=p>)</span> <span class=o>*</span> <span class=n>x0</span> <span class=o>+</span> <span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha_bar_t</span><span class=p>)</span> <span class=o>*</span> <span class=n>epsilon</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. 模型预测噪声</span>
</span></span><span class=line><span class=cl>        <span class=n>epsilon_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>xt</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 5. 计算损失(简单的MSE)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>epsilon</span> <span class=o>-</span> <span class=n>epsilon_pred</span><span class=p>)</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 6. 反向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span></span></span></code></pre></div><p><strong>关键点</strong>:</p><ul><li>每次训练只采样一个时间步,无需完整的T步</li><li>损失函数就是简单的L2损失</li><li>训练稳定,不需要GAN那样的对抗训练</li></ul><h3 id=采样流程生成图像>采样流程(生成图像)<a class=anchor href=#%e9%87%87%e6%a0%b7%e6%b5%81%e7%a8%8b%e7%94%9f%e6%88%90%e5%9b%be%e5%83%8f>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 从纯噪声开始</span>
</span></span><span class=line><span class=cl><span class=n>x_T</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 逐步去噪</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>T</span><span class=o>+</span><span class=mi>1</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 预测噪声</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_t</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 去噪一步</span>
</span></span><span class=line><span class=cl>    <span class=n>x_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>=</span> <span class=n>denoise_step</span><span class=p>(</span><span class=n>x_t</span><span class=p>,</span> <span class=n>epsilon_pred</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 最终得到 x_0,即生成的图像</span>
</span></span><span class=line><span class=cl><span class=n>generated_image</span> <span class=o>=</span> <span class=n>x_0</span></span></span></code></pre></div><p><strong>问题</strong>:需要1000步才能生成一张图像,太慢了!</p><p><strong>解决方案</strong>:DDIM、DPM-Solver等快速采样器,可以用20-50步达到相似质量。</p><hr><h2 id=152-stable-diffusion架构>15.2 Stable Diffusion架构<a class=anchor href=#152-stable-diffusion%e6%9e%b6%e6%9e%84>#</a></h2><h3 id=为什么需要latent-diffusion>为什么需要Latent Diffusion?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81latent-diffusion>#</a></h3><p>原始DDPM直接在像素空间操作,存在问题:</p><ul><li><strong>计算量大</strong>:1024x1024图像太大,显存吃不消</li><li><strong>效率低</strong>:高分辨率下训练和推理都慢</li></ul><p><strong>Latent Diffusion Model</strong> (LDM, 2022)的创新:
在**压缩的潜空间(Latent Space)**进行扩散,而非像素空间!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>像素空间(512x512x3) → VAE编码 → 潜空间(64x64x4)
</span></span><span class=line><span class=cl>                                    ↓
</span></span><span class=line><span class=cl>                              在这里进行扩散
</span></span><span class=line><span class=cl>                                    ↓
</span></span><span class=line><span class=cl>潜空间(64x64x4) → VAE解码 → 像素空间(512x512x3)</span></span></code></pre></div><p><strong>优势</strong>:</p><ul><li>潜空间维度小,计算快8倍以上</li><li>保留语义信息,生成质量不降低</li><li>可以在消费级GPU上运行</li></ul><h3 id=stable-diffusion三大组件>Stable Diffusion三大组件<a class=anchor href=#stable-diffusion%e4%b8%89%e5%a4%a7%e7%bb%84%e4%bb%b6>#</a></h3><p>Stable Diffusion由三个核心模块组成:</p><h4 id=1-vae-variational-autoencoder>1. VAE (Variational Autoencoder)<a class=anchor href=#1-vae-variational-autoencoder>#</a></h4><p><strong>作用</strong>:图像 ↔ 潜空间的相互转换</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 编码器:图像 → 潜空间</span>
</span></span><span class=line><span class=cl><span class=n>latent</span> <span class=o>=</span> <span class=n>vae</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>  <span class=c1># (3, 512, 512) → (4, 64, 64)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 解码器:潜空间 → 图像</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>vae</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>latent</span><span class=p>)</span>  <span class=c1># (4, 64, 64) → (3, 512, 512)</span></span></span></code></pre></div><p><strong>特点</strong>:</p><ul><li>压缩比8:1(在空间维度上)</li><li>4通道潜空间(不是RGB的3通道)</li><li>VAE提前训练好,扩散时冻结</li></ul><h4 id=2-u-net噪声预测器>2. U-Net:噪声预测器<a class=anchor href=#2-u-net%e5%99%aa%e5%a3%b0%e9%a2%84%e6%b5%8b%e5%99%a8>#</a></h4><p><strong>作用</strong>:预测噪声 ε_θ(z_t, t, c)</p><p>架构:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: 噪声潜空间 z_t + 时间步 t + 条件 c(文本嵌入)
</span></span><span class=line><span class=cl>        ↓
</span></span><span class=line><span class=cl>    Encoder (下采样)
</span></span><span class=line><span class=cl>        ↓
</span></span><span class=line><span class=cl>    Bottleneck (Attention层处理)
</span></span><span class=line><span class=cl>        ↓
</span></span><span class=line><span class=cl>    Decoder (上采样,带Skip Connection)
</span></span><span class=line><span class=cl>        ↓
</span></span><span class=line><span class=cl>输出: 预测的噪声</span></span></code></pre></div><p><strong>关键特性</strong>:</p><ul><li>ResNet块 + Attention层</li><li>Cross-Attention融合文本条件</li><li>时间步嵌入(类似Transformer的位置编码)</li></ul><h4 id=3-clip-text-encoder>3. CLIP Text Encoder<a class=anchor href=#3-clip-text-encoder>#</a></h4><p><strong>作用</strong>:将文本提示词转换为向量嵌入</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 文本 → 向量</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;a beautiful sunset over the ocean&#34;</span>
</span></span><span class=line><span class=cl><span class=n>text_embedding</span> <span class=o>=</span> <span class=n>clip_text_encoder</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span>  <span class=c1># (1, 77, 768)</span></span></span></code></pre></div><p><strong>特点</strong>:</p><ul><li>使用OpenAI的CLIP模型</li><li>最大长度77个token</li><li>768维嵌入向量</li><li>通过Cross-Attention影响U-Net</li></ul><h3 id=stable-diffusion生成流程>Stable Diffusion生成流程<a class=anchor href=#stable-diffusion%e7%94%9f%e6%88%90%e6%b5%81%e7%a8%8b>#</a></h3><p>完整的文生图流程:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 文本编码</span>
</span></span><span class=line><span class=cl><span class=n>text_embedding</span> <span class=o>=</span> <span class=n>clip_text_encoder</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 初始化随机噪声(在潜空间)</span>
</span></span><span class=line><span class=cl><span class=n>latent</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>  <span class=c1># 对应512x512图像</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 迭代去噪</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>timesteps</span><span class=p>:</span>  <span class=c1># 通常50步</span>
</span></span><span class=line><span class=cl>    <span class=c1># 预测噪声</span>
</span></span><span class=line><span class=cl>    <span class=n>noise_pred</span> <span class=o>=</span> <span class=n>unet</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>text_embedding</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 去噪一步</span>
</span></span><span class=line><span class=cl>    <span class=n>latent</span> <span class=o>=</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>noise_pred</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>latent</span><span class=p>)</span><span class=o>.</span><span class=n>prev_sample</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 解码为图像</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>vae</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>latent</span><span class=p>)</span></span></span></code></pre></div><h3 id=采样器scheduler对比>采样器(Scheduler)对比<a class=anchor href=#%e9%87%87%e6%a0%b7%e5%99%a8scheduler%e5%af%b9%e6%af%94>#</a></h3><p>不同采样器的速度和质量权衡:</p><table><thead><tr><th>采样器</th><th>步数</th><th>速度</th><th>质量</th><th>特点</th></tr></thead><tbody><tr><td><strong>DDPM</strong></td><td>1000</td><td>极慢</td><td>好</td><td>原始方法</td></tr><tr><td><strong>DDIM</strong></td><td>50</td><td>快</td><td>好</td><td>确定性采样</td></tr><tr><td><strong>Euler</strong></td><td>30-50</td><td>快</td><td>较好</td><td>简单稳定</td></tr><tr><td><strong>Euler A</strong></td><td>20-40</td><td>很快</td><td>较好</td><td>祖先采样</td></tr><tr><td><strong>DPM-Solver++</strong></td><td>20-30</td><td>很快</td><td>好</td><td>数值求解器</td></tr><tr><td><strong>UniPC</strong></td><td>20-30</td><td>很快</td><td>很好</td><td>2023新方法</td></tr></tbody></table><p><strong>推荐</strong>:</p><ul><li>日常使用:DPM-Solver++ (20-30步)</li><li>追求质量:DDIM (50步)</li><li>快速预览:Euler A (20步)</li></ul><h3 id=引导强度guidance-scale>引导强度(Guidance Scale)<a class=anchor href=#%e5%bc%95%e5%af%bc%e5%bc%ba%e5%ba%a6guidance-scale>#</a></h3><p><strong>Classifier-Free Guidance</strong>:控制生成结果对提示词的依赖程度</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>最终噪声预测 = 无条件预测 + guidance_scale × (有条件预测 - 无条件预测)</span></span></code></pre></div><p><strong>参数影响</strong>:</p><ul><li><strong>低值(1.0-5.0)</strong>:更随机,更有创意,但可能偏离提示词</li><li><strong>中值(7.0-9.0)</strong>:平衡,推荐默认值</li><li><strong>高值(10.0-20.0)</strong>:严格遵循提示词,但可能过饱和</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># guidance_scale = 1.0:完全忽略提示词</span>
</span></span><span class=line><span class=cl><span class=c1># guidance_scale = 7.5:标准值</span>
</span></span><span class=line><span class=cl><span class=c1># guidance_scale = 15.0:强烈遵循提示词</span></span></span></code></pre></div><hr><h2 id=153-controlnet可控生成>15.3 ControlNet:可控生成<a class=anchor href=#153-controlnet%e5%8f%af%e6%8e%a7%e7%94%9f%e6%88%90>#</a></h2><h3 id=为什么需要controlnet>为什么需要ControlNet?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81controlnet>#</a></h3><p>纯文本提示词的局限性:</p><ul><li>难以精确控制构图</li><li>难以指定物体位置</li><li>难以保持一致的姿态</li></ul><p><strong>ControlNet</strong> (2023)通过额外的视觉条件解决这些问题!</p><h3 id=controlnet原理>ControlNet原理<a class=anchor href=#controlnet%e5%8e%9f%e7%90%86>#</a></h3><p>核心思想:在U-Net基础上添加一个可训练的副本,接受视觉条件输入</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始U-Net(冻结)
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>  复制
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>ControlNet分支(可训练)
</span></span><span class=line><span class=cl>    ↑
</span></span><span class=line><span class=cl>视觉条件(Canny边缘、深度图等)</span></span></code></pre></div><p><strong>训练方式</strong>:</p><ol><li>复制U-Net的编码器权重</li><li>冻结原始U-Net</li><li>只训练ControlNet分支</li><li>通过零卷积(Zero Convolution)连接回主网络</li></ol><p><strong>零卷积的妙处</strong>:</p><ul><li>初始权重全为0,训练开始时ControlNet对结果无影响</li><li>逐渐学习如何融入条件信息</li><li>训练稳定,不破坏原模型</li></ul><h3 id=常用控制类型>常用控制类型<a class=anchor href=#%e5%b8%b8%e7%94%a8%e6%8e%a7%e5%88%b6%e7%b1%bb%e5%9e%8b>#</a></h3><p>ControlNet支持多种视觉条件:</p><h4 id=1-canny-edge-边缘检测>1. Canny Edge (边缘检测)<a class=anchor href=#1-canny-edge-%e8%be%b9%e7%bc%98%e6%a3%80%e6%b5%8b>#</a></h4><p><strong>用途</strong>:控制物体轮廓和边界</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>controlnet_aux</span> <span class=kn>import</span> <span class=n>CannyDetector</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>canny</span> <span class=o>=</span> <span class=n>CannyDetector</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>edge_image</span> <span class=o>=</span> <span class=n>canny</span><span class=p>(</span><span class=n>input_image</span><span class=p>)</span>  <span class=c1># 提取边缘</span></span></span></code></pre></div><p><strong>适用场景</strong>:</p><ul><li>保持物体形状</li><li>线稿上色</li><li>建筑设计</li></ul><h4 id=2-depth-map-深度图>2. Depth Map (深度图)<a class=anchor href=#2-depth-map-%e6%b7%b1%e5%ba%a6%e5%9b%be>#</a></h4><p><strong>用途</strong>:控制场景的3D结构</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>controlnet_aux</span> <span class=kn>import</span> <span class=n>DepthEstimator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>depth_estimator</span> <span class=o>=</span> <span class=n>DepthEstimator</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Intel/dpt-hybrid-midas&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>depth_map</span> <span class=o>=</span> <span class=n>depth_estimator</span><span class=p>(</span><span class=n>input_image</span><span class=p>)</span></span></span></code></pre></div><p><strong>适用场景</strong>:</p><ul><li>保持空间关系</li><li>3D场景转换</li><li>虚拟场景生成</li></ul><h4 id=3-openpose-人体姿态>3. OpenPose (人体姿态)<a class=anchor href=#3-openpose-%e4%ba%ba%e4%bd%93%e5%a7%bf%e6%80%81>#</a></h4><p><strong>用途</strong>:控制人物姿态和动作</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>controlnet_aux</span> <span class=kn>import</span> <span class=n>OpenposeDetector</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>openpose</span> <span class=o>=</span> <span class=n>OpenposeDetector</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;lllyasviel/ControlNet&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pose_image</span> <span class=o>=</span> <span class=n>openpose</span><span class=p>(</span><span class=n>input_image</span><span class=p>)</span></span></span></code></pre></div><p><strong>适用场景</strong>:</p><ul><li>动作指导</li><li>虚拟试衣</li><li>动画生成</li></ul><h4 id=4-scribble-手绘草图>4. Scribble (手绘草图)<a class=anchor href=#4-scribble-%e6%89%8b%e7%bb%98%e8%8d%89%e5%9b%be>#</a></h4><p><strong>用途</strong>:从简单涂鸦生成图像</p><p><strong>适用场景</strong>:</p><ul><li>快速概念设计</li><li>艺术创作辅助</li></ul><h4 id=5-segmentation-语义分割>5. Segmentation (语义分割)<a class=anchor href=#5-segmentation-%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2>#</a></h4><p><strong>用途</strong>:精确控制不同区域的内容</p><p><strong>适用场景</strong>:</p><ul><li>场景编辑</li><li>区域替换</li></ul><h3 id=多controlnet组合>多ControlNet组合<a class=anchor href=#%e5%a4%9acontrolnet%e7%bb%84%e5%90%88>#</a></h3><p>可以同时使用多个ControlNet,实现更精细的控制:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionControlNetPipeline</span><span class=p>,</span> <span class=n>ControlNetModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载多个ControlNet</span>
</span></span><span class=line><span class=cl><span class=n>controlnet_canny</span> <span class=o>=</span> <span class=n>ControlNetModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;lllyasviel/control_v11p_sd15_canny&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>controlnet_depth</span> <span class=o>=</span> <span class=n>ControlNetModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;lllyasviel/control_v11f1p_sd15_depth&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建pipeline</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionControlNetPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;runwayml/stable-diffusion-v1-5&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>controlnet</span><span class=o>=</span><span class=p>[</span><span class=n>controlnet_canny</span><span class=p>,</span> <span class=n>controlnet_depth</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成(同时使用边缘和深度控制)</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;...&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=o>=</span><span class=p>[</span><span class=n>canny_image</span><span class=p>,</span> <span class=n>depth_image</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>controlnet_conditioning_scale</span><span class=o>=</span><span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>]</span>  <span class=c1># 分别控制权重</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><h3 id=controlnet权重调节>ControlNet权重调节<a class=anchor href=#controlnet%e6%9d%83%e9%87%8d%e8%b0%83%e8%8a%82>#</a></h3><p><code>controlnet_conditioning_scale</code>参数控制条件的影响强度:</p><ul><li><strong>0.0</strong>:完全忽略ControlNet条件</li><li><strong>0.5-0.8</strong>:中等强度,推荐范围</li><li><strong>1.0-1.5</strong>:强烈遵循条件,可能损失创造性</li></ul><hr><h2 id=154-flux2024最新扩散模型>15.4 FLUX:2024最新扩散模型<a class=anchor href=#154-flux2024%e6%9c%80%e6%96%b0%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b>#</a></h2><h3 id=flux简介>FLUX简介<a class=anchor href=#flux%e7%ae%80%e4%bb%8b>#</a></h3><p><strong>FLUX</strong> 由Black Forest Labs(Stability AI创始团队成员创立)于2024年发布,代表了扩散模型的最新进展。</p><p><strong>主要版本</strong>:</p><ol><li><p><strong>FLUX.1-pro</strong> (商业API)</p><ul><li>最高质量</li><li>仅通过API访问</li></ul></li><li><p><strong>FLUX.1-dev</strong> (开源,非商用)</p><ul><li>接近pro的质量</li><li>适合研究和开发</li></ul></li><li><p><strong>FLUX.1-schnell</strong> (开源,Apache 2.0)</p><ul><li>优化速度,1-4步生成</li><li>可商用</li></ul></li></ol><h3 id=flux的创新点>FLUX的创新点<a class=anchor href=#flux%e7%9a%84%e5%88%9b%e6%96%b0%e7%82%b9>#</a></h3><ol><li><p><strong>架构升级</strong></p><ul><li>更大的模型(12B参数)</li><li>改进的Attention机制</li><li>更好的文本理解</li></ul></li><li><p><strong>生成质量</strong></p><ul><li>更准确的提示词理解</li><li>更细腻的细节</li><li>更自然的光影</li></ul></li><li><p><strong>速度优化</strong>(schnell版本)</p><ul><li>少步生成(1-4步)</li><li>保持高质量</li></ul></li></ol><h3 id=flux-vs-stable-diffusion>FLUX vs Stable Diffusion<a class=anchor href=#flux-vs-stable-diffusion>#</a></h3><table><thead><tr><th>维度</th><th>SD 1.5</th><th>SDXL</th><th>FLUX.1</th></tr></thead><tbody><tr><td><strong>参数量</strong></td><td>0.9B</td><td>2.6B</td><td>12B</td></tr><tr><td><strong>生成质量</strong></td><td>好</td><td>很好</td><td>极好</td></tr><tr><td><strong>提示词理解</strong></td><td>中等</td><td>好</td><td>优秀</td></tr><tr><td><strong>VRAM需求</strong></td><td>4-6GB</td><td>8-12GB</td><td>16-24GB</td></tr><tr><td><strong>速度</strong></td><td>快</td><td>中等</td><td>慢(dev)/快(schnell)</td></tr><tr><td><strong>开源许可</strong></td><td>CreativeML</td><td>CreativeML</td><td>Apache 2.0(schnell)</td></tr></tbody></table><h3 id=使用flux>使用FLUX<a class=anchor href=#%e4%bd%bf%e7%94%a8flux>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>FluxPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载FLUX.1-schnell(快速版本)</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>FluxPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;black-forest-labs/FLUX.1-schnell&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成(只需4步!)</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;a cat holding a sign that says &#39;hello world&#39;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_inference_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>0.0</span>  <span class=c1># schnell版本不需要guidance</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><p><strong>注意</strong>:FLUX需要较大显存,建议16GB+。</p><hr><h2 id=155-提示词工程prompt-engineering>15.5 提示词工程(Prompt Engineering)<a class=anchor href=#155-%e6%8f%90%e7%a4%ba%e8%af%8d%e5%b7%a5%e7%a8%8bprompt-engineering>#</a></h2><h3 id=提示词的重要性>提示词的重要性<a class=anchor href=#%e6%8f%90%e7%a4%ba%e8%af%8d%e7%9a%84%e9%87%8d%e8%a6%81%e6%80%a7>#</a></h3><p>同样的模型,提示词质量决定生成效果的80%!</p><p><strong>低质量提示词</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;a cat&#34;</span></span></code></pre></div><p><strong>高质量提示词</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;a fluffy orange tabby cat with green eyes,
</span></span><span class=line><span class=cl>sitting on a vintage wooden table,
</span></span><span class=line><span class=cl>warm golden hour lighting,
</span></span><span class=line><span class=cl>shallow depth of field,
</span></span><span class=line><span class=cl>bokeh background,
</span></span><span class=line><span class=cl>professional pet photography,
</span></span><span class=line><span class=cl>8k uhd, sharp focus,
</span></span><span class=line><span class=cl>highly detailed fur texture&#34;</span></span></code></pre></div><h3 id=提示词结构>提示词结构<a class=anchor href=#%e6%8f%90%e7%a4%ba%e8%af%8d%e7%bb%93%e6%9e%84>#</a></h3><p>标准的高质量提示词结构:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[主体描述] + [细节修饰] + [环境/背景] + [光照] +
</span></span><span class=line><span class=cl>[艺术风格] + [技术参数] + [质量词]</span></span></code></pre></div><p><strong>示例分解</strong>:</p><ol><li><p><strong>主体描述</strong>(必需)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>a majestic lion</span></span></code></pre></div></li><li><p><strong>细节修饰</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>with a flowing golden mane, piercing amber eyes</span></span></code></pre></div></li><li><p><strong>环境/背景</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>standing on a rocky cliff at sunset</span></span></code></pre></div></li><li><p><strong>光照</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>dramatic golden hour lighting, rim light</span></span></code></pre></div></li><li><p><strong>艺术风格</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>digital painting, cinematic, epic composition</span></span></code></pre></div></li><li><p><strong>技术参数</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>8k uhd, sharp focus, volumetric lighting</span></span></code></pre></div></li><li><p><strong>质量词</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>masterpiece, award winning, trending on artstation</span></span></code></pre></div></li></ol><p><strong>完整提示词</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>a majestic lion with a flowing golden mane and piercing amber eyes,
</span></span><span class=line><span class=cl>standing on a rocky cliff at sunset,
</span></span><span class=line><span class=cl>dramatic golden hour lighting with rim light,
</span></span><span class=line><span class=cl>digital painting, cinematic composition,
</span></span><span class=line><span class=cl>8k uhd, sharp focus, volumetric lighting,
</span></span><span class=line><span class=cl>masterpiece, award winning photography</span></span></code></pre></div><h3 id=负向提示词negative-prompt>负向提示词(Negative Prompt)<a class=anchor href=#%e8%b4%9f%e5%90%91%e6%8f%90%e7%a4%ba%e8%af%8dnegative-prompt>#</a></h3><p>告诉模型<strong>不要生成什么</strong>:</p><p><strong>常用负向提示词模板</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>低质量相关:
</span></span><span class=line><span class=cl>lowres, bad anatomy, bad hands, text, error,
</span></span><span class=line><span class=cl>missing fingers, extra digit, fewer digits,
</span></span><span class=line><span class=cl>cropped, worst quality, low quality,
</span></span><span class=line><span class=cl>normal quality, jpeg artifacts, signature,
</span></span><span class=line><span class=cl>watermark, username, blurry
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>不需要的元素:
</span></span><span class=line><span class=cl>ugly, duplicate, morbid, mutilated,
</span></span><span class=line><span class=cl>extra fingers, mutated hands, poorly drawn hands,
</span></span><span class=line><span class=cl>poorly drawn face, mutation, deformed</span></span></code></pre></div><p><strong>使用示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;a beautiful landscape...&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>negative_prompt</span><span class=o>=</span><span class=s2>&#34;blurry, low quality, distorted, watermark&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_inference_steps</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>7.5</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><h3 id=提示词权重>提示词权重<a class=anchor href=#%e6%8f%90%e7%a4%ba%e8%af%8d%e6%9d%83%e9%87%8d>#</a></h3><p>调整不同部分的重要性:</p><p><strong>语法</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>(keyword)     # 权重1.1
</span></span><span class=line><span class=cl>((keyword))   # 权重1.1 * 1.1 = 1.21
</span></span><span class=line><span class=cl>(keyword:1.5) # 权重1.5
</span></span><span class=line><span class=cl>[keyword]     # 权重0.9</span></span></code></pre></div><p><strong>示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;a ((highly detailed)) portrait,
</span></span><span class=line><span class=cl>(red hair:1.3),
</span></span><span class=line><span class=cl>[background]&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>解释:
</span></span><span class=line><span class=cl>- highly detailed: 权重1.21(强调)
</span></span><span class=line><span class=cl>- red hair: 权重1.3(非常强调红色头发)
</span></span><span class=line><span class=cl>- background: 权重0.9(弱化背景)</span></span></code></pre></div><h3 id=风格提示词库>风格提示词库<a class=anchor href=#%e9%a3%8e%e6%a0%bc%e6%8f%90%e7%a4%ba%e8%af%8d%e5%ba%93>#</a></h3><h4 id=摄影风格>摄影风格<a class=anchor href=#%e6%91%84%e5%bd%b1%e9%a3%8e%e6%a0%bc>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>portrait photography:
</span></span><span class=line><span class=cl>- &#34;professional portrait photography&#34;
</span></span><span class=line><span class=cl>- &#34;studio lighting, soft box&#34;
</span></span><span class=line><span class=cl>- &#34;85mm lens, f/1.4&#34;
</span></span><span class=line><span class=cl>- &#34;bokeh background&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>landscape photography:
</span></span><span class=line><span class=cl>- &#34;landscape photography&#34;
</span></span><span class=line><span class=cl>- &#34;golden hour, dramatic clouds&#34;
</span></span><span class=line><span class=cl>- &#34;wide angle lens&#34;
</span></span><span class=line><span class=cl>- &#34;high dynamic range&#34;</span></span></code></pre></div><h4 id=艺术风格>艺术风格<a class=anchor href=#%e8%89%ba%e6%9c%af%e9%a3%8e%e6%a0%bc>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>digital art:
</span></span><span class=line><span class=cl>- &#34;digital painting&#34;
</span></span><span class=line><span class=cl>- &#34;concept art&#34;
</span></span><span class=line><span class=cl>- &#34;trending on artstation&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>anime:
</span></span><span class=line><span class=cl>- &#34;anime style&#34;
</span></span><span class=line><span class=cl>- &#34;studio ghibli&#34;
</span></span><span class=line><span class=cl>- &#34;makoto shinkai&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>oil painting:
</span></span><span class=line><span class=cl>- &#34;oil painting&#34;
</span></span><span class=line><span class=cl>- &#34;impressionism&#34;
</span></span><span class=line><span class=cl>- &#34;visible brush strokes&#34;</span></span></code></pre></div><h4 id=质量提升词>质量提升词<a class=anchor href=#%e8%b4%a8%e9%87%8f%e6%8f%90%e5%8d%87%e8%af%8d>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>通用质量词:
</span></span><span class=line><span class=cl>- &#34;masterpiece&#34;
</span></span><span class=line><span class=cl>- &#34;best quality&#34;
</span></span><span class=line><span class=cl>- &#34;highly detailed&#34;
</span></span><span class=line><span class=cl>- &#34;8k uhd&#34;
</span></span><span class=line><span class=cl>- &#34;professional&#34;
</span></span><span class=line><span class=cl>- &#34;award winning&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>清晰度:
</span></span><span class=line><span class=cl>- &#34;sharp focus&#34;
</span></span><span class=line><span class=cl>- &#34;high resolution&#34;
</span></span><span class=line><span class=cl>- &#34;ultra detailed&#34;</span></span></code></pre></div><h3 id=提示词优化技巧>提示词优化技巧<a class=anchor href=#%e6%8f%90%e7%a4%ba%e8%af%8d%e4%bc%98%e5%8c%96%e6%8a%80%e5%b7%a7>#</a></h3><ol><li><p><strong>具体 > 抽象</strong></p><ul><li>差:&ldquo;a nice scene&rdquo;</li><li>好:&ldquo;a cozy coffee shop with warm lighting&rdquo;</li></ul></li><li><p><strong>使用艺术家名字</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;in the style of Greg Rutkowski&#34;
</span></span><span class=line><span class=cl>&#34;painted by Claude Monet&#34;
</span></span><span class=line><span class=cl>&#34;photograph by Annie Leibovitz&#34;</span></span></code></pre></div></li><li><p><strong>分段描述复杂场景</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>前景 + 中景 + 背景:
</span></span><span class=line><span class=cl>&#34;in the foreground, a red rose,
</span></span><span class=line><span class=cl>in the middle ground, a wooden table,
</span></span><span class=line><span class=cl>in the background, a blurred window&#34;</span></span></code></pre></div></li><li><p><strong>参考优秀案例</strong></p><ul><li>Civitai: <a href=https://civitai.com/>https://civitai.com/</a></li><li>Lexica: <a href=https://lexica.art/>https://lexica.art/</a></li><li>PromptHero: <a href=https://prompthero.com/>https://prompthero.com/</a></li></ul></li></ol><h3 id=常见问题与解决>常见问题与解决<a class=anchor href=#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98%e4%b8%8e%e8%a7%a3%e5%86%b3>#</a></h3><table><thead><tr><th>问题</th><th>原因</th><th>解决方案</th></tr></thead><tbody><tr><td><strong>生成模糊</strong></td><td>步数太少</td><td>增加num_inference_steps到50+</td></tr><tr><td><strong>不符合提示词</strong></td><td>guidance_scale太低</td><td>提高到7.5-10</td></tr><tr><td><strong>过度饱和</strong></td><td>guidance_scale太高</td><td>降低到7.5以下</td></tr><tr><td><strong>出现文字</strong></td><td>模型倾向</td><td>负向提示词加"text, words"</td></tr><tr><td><strong>手部畸形</strong></td><td>模型弱点</td><td>负向提示词加"bad hands, extra fingers"</td></tr><tr><td><strong>构图不佳</strong></td><td>提示词不够具体</td><td>明确描述构图和视角</td></tr></tbody></table><hr><h2 id=156-实战项目>15.6 实战项目<a class=anchor href=#156-%e5%ae%9e%e6%88%98%e9%a1%b9%e7%9b%ae>#</a></h2><h3 id=项目1文生图text-to-image>项目1:文生图(Text-to-Image)<a class=anchor href=#%e9%a1%b9%e7%9b%ae1%e6%96%87%e7%94%9f%e5%9b%betext-to-image>#</a></h3><p>完整代码见: <a href=../../chapter29/code/chapter15_diffusion/stable_diffusion_demo.py>code/chapter15_diffusion/stable_diffusion_demo.py</a></p><p><strong>核心代码</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionPipeline</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;runwayml/stable-diffusion-v1-5&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>a serene Japanese garden with a wooden bridge over a koi pond,
</span></span></span><span class=line><span class=cl><span class=s2>cherry blossoms falling, soft morning light,
</span></span></span><span class=line><span class=cl><span class=s2>traditional architecture in background,
</span></span></span><span class=line><span class=cl><span class=s2>photorealistic, 8k uhd, highly detailed
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>negative_prompt</span> <span class=o>=</span> <span class=s2>&#34;blurry, low quality, distorted, watermark&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>negative_prompt</span><span class=o>=</span><span class=n>negative_prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_inference_steps</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>7.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>height</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>width</span><span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;output.png&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=项目2图生图image-to-image>项目2:图生图(Image-to-Image)<a class=anchor href=#%e9%a1%b9%e7%9b%ae2%e5%9b%be%e7%94%9f%e5%9b%beimage-to-image>#</a></h3><p>基于参考图进行风格转换或修改:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionImg2ImgPipeline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionImg2ImgPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;runwayml/stable-diffusion-v1-5&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载参考图</span>
</span></span><span class=line><span class=cl><span class=n>init_image</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>&#34;input.jpg&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>resize</span><span class=p>((</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;turn this photo into an oil painting, impressionism style&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=o>=</span><span class=n>init_image</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>strength</span><span class=o>=</span><span class=mf>0.75</span><span class=p>,</span>  <span class=c1># 0.0-1.0,越高变化越大</span>
</span></span><span class=line><span class=cl>    <span class=n>guidance_scale</span><span class=o>=</span><span class=mf>7.5</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><p><strong>strength参数</strong>:</p><ul><li>0.0-0.3:细微调整</li><li>0.4-0.7:风格转换</li><li>0.8-1.0:大幅改变</li></ul><h3 id=项目3controlnet可控生成>项目3:ControlNet可控生成<a class=anchor href=#%e9%a1%b9%e7%9b%ae3controlnet%e5%8f%af%e6%8e%a7%e7%94%9f%e6%88%90>#</a></h3><p>完整代码见: <a href=../../chapter29/code/chapter15_diffusion/controlnet_demo.py>code/chapter15_diffusion/controlnet_demo.py</a></p><p><strong>Canny边缘控制</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionControlNetPipeline</span><span class=p>,</span> <span class=n>ControlNetModel</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>controlnet_aux</span> <span class=kn>import</span> <span class=n>CannyDetector</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 提取边缘</span>
</span></span><span class=line><span class=cl><span class=n>canny</span> <span class=o>=</span> <span class=n>CannyDetector</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>input_image</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>&#34;input.jpg&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>canny_image</span> <span class=o>=</span> <span class=n>canny</span><span class=p>(</span><span class=n>input_image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载ControlNet</span>
</span></span><span class=line><span class=cl><span class=n>controlnet</span> <span class=o>=</span> <span class=n>ControlNetModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;lllyasviel/control_v11p_sd15_canny&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionControlNetPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;runwayml/stable-diffusion-v1-5&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>controlnet</span><span class=o>=</span><span class=n>controlnet</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;a modern architectural rendering, glass and steel&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=o>=</span><span class=n>canny_image</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>controlnet_conditioning_scale</span><span class=o>=</span><span class=mf>0.8</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><h3 id=项目4批量生成与自动化>项目4:批量生成与自动化<a class=anchor href=#%e9%a1%b9%e7%9b%ae4%e6%89%b9%e9%87%8f%e7%94%9f%e6%88%90%e4%b8%8e%e8%87%aa%e5%8a%a8%e5%8c%96>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>batch_generate</span><span class=p>(</span><span class=n>prompts</span><span class=p>:</span> <span class=nb>list</span><span class=p>,</span> <span class=n>output_dir</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;批量生成图像&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>prompt</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>prompts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span><span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>output_dir</span><span class=si>}</span><span class=s2>/image_</span><span class=si>{</span><span class=n>i</span><span class=si>:</span><span class=s2>03d</span><span class=si>}</span><span class=s2>.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;生成 </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>prompts</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用</span>
</span></span><span class=line><span class=cl><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;a cat in a garden&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;a dog on a beach&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;a bird in the sky&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>batch_generate</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=s2>&#34;./outputs&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=157-高级技巧>15.7 高级技巧<a class=anchor href=#157-%e9%ab%98%e7%ba%a7%e6%8a%80%e5%b7%a7>#</a></h2><h3 id=1-lora-low-rank-adaptation>1. LoRA (Low-Rank Adaptation)<a class=anchor href=#1-lora-low-rank-adaptation>#</a></h3><p>快速微调模型,添加特定风格或概念:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载LoRA权重</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>load_lora_weights</span><span class=p>(</span><span class=s2>&#34;path/to/lora.safetensors&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 调整LoRA强度</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>set_adapters</span><span class=p>(</span><span class=s2>&#34;default&#34;</span><span class=p>,</span> <span class=n>adapter_weights</span><span class=o>=</span><span class=p>[</span><span class=mf>0.8</span><span class=p>])</span></span></span></code></pre></div><p><strong>LoRA资源</strong>:Civitai上有大量社区训练的LoRA</p><h3 id=2-textual-inversion>2. Textual Inversion<a class=anchor href=#2-textual-inversion>#</a></h3><p>添加新概念到模型词汇表:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>load_textual_inversion</span><span class=p>(</span><span class=s2>&#34;path/to/embedding.pt&#34;</span><span class=p>,</span> <span class=n>token</span><span class=o>=</span><span class=s2>&#34;&lt;my-concept&gt;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;a photo of &lt;my-concept&gt; in a forest&#34;</span></span></span></code></pre></div><h3 id=3-inpainting-局部重绘>3. Inpainting (局部重绘)<a class=anchor href=#3-inpainting-%e5%b1%80%e9%83%a8%e9%87%8d%e7%bb%98>#</a></h3><p>只修改图像的某个区域:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionInpaintPipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionInpaintPipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;a red car&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=o>=</span><span class=n>original_image</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>mask_image</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span>  <span class=c1># 白色区域会被重绘</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><h3 id=4-超分辨率upscaling>4. 超分辨率(Upscaling)<a class=anchor href=#4-%e8%b6%85%e5%88%86%e8%be%a8%e7%8e%87upscaling>#</a></h3><p>使用扩散模型进行图像放大:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>StableDiffusionUpscalePipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>StableDiffusionUpscalePipeline</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;stabilityai/stable-diffusion-x4-upscaler&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>upscaled</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span><span class=o>=</span><span class=s2>&#34;high quality, detailed&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=o>=</span><span class=n>low_res_image</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>images</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span></span></span></code></pre></div><hr><h2 id=158-性能优化>15.8 性能优化<a class=anchor href=#158-%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96>#</a></h2><h3 id=显存优化>显存优化<a class=anchor href=#%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>enable_attention_slicing</span><span class=p>()</span>  <span class=c1># 减少显存占用</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>enable_vae_slicing</span><span class=p>()</span>  <span class=c1># VAE分块处理</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>enable_xformers_memory_efficient_attention</span><span class=p>()</span>  <span class=c1># 使用xFormers</span></span></span></code></pre></div><h3 id=速度优化>速度优化<a class=anchor href=#%e9%80%9f%e5%ba%a6%e4%bc%98%e5%8c%96>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用torch.compile(PyTorch 2.0+)</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>unet</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>pipe</span><span class=o>.</span><span class=n>unet</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;reduce-overhead&#34;</span><span class=p>,</span> <span class=n>fullgraph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用更快的采样器</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>DPMSolverMultistepScheduler</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>scheduler</span> <span class=o>=</span> <span class=n>DPMSolverMultistepScheduler</span><span class=o>.</span><span class=n>from_config</span><span class=p>(</span><span class=n>pipe</span><span class=o>.</span><span class=n>scheduler</span><span class=o>.</span><span class=n>config</span><span class=p>)</span></span></span></code></pre></div><h3 id=cpu-offloading>CPU Offloading<a class=anchor href=#cpu-offloading>#</a></h3><p>在低配GPU上运行:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>enable_sequential_cpu_offload</span><span class=p>()</span>  <span class=c1># 组件轮流使用GPU</span>
</span></span><span class=line><span class=cl><span class=n>pipe</span><span class=o>.</span><span class=n>enable_model_cpu_offload</span><span class=p>()</span>  <span class=c1># 更智能的offload</span></span></span></code></pre></div><hr><h2 id=159-总结>15.9 总结<a class=anchor href=#159-%e6%80%bb%e7%bb%93>#</a></h2><h3 id=本章要点回顾-1>本章要点回顾<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e8%a6%81%e7%82%b9%e5%9b%9e%e9%a1%be-1>#</a></h3><ol><li><strong>扩散过程</strong>:前向加噪 + 反向去噪</li><li><strong>Latent Diffusion</strong>:在压缩空间进行扩散,高效实用</li><li><strong>ControlNet</strong>:通过视觉条件实现精准控制</li><li><strong>FLUX</strong>:2024最新模型,质量显著提升</li><li><strong>提示词工程</strong>:高质量生成的关键</li></ol><h3 id=扩散模型-vs-gan>扩散模型 vs GAN<a class=anchor href=#%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b-vs-gan>#</a></h3><table><thead><tr><th>维度</th><th>GAN</th><th>Diffusion</th></tr></thead><tbody><tr><td><strong>训练稳定性</strong></td><td>难</td><td>易</td></tr><tr><td><strong>生成质量</strong></td><td>好</td><td>更好</td></tr><tr><td><strong>多样性</strong></td><td>易模式崩溃</td><td>优秀</td></tr><tr><td><strong>可控性</strong></td><td>需特殊设计</td><td>天然支持</td></tr><tr><td><strong>2024地位</strong></td><td>边缘化</td><td>主流</td></tr></tbody></table><h3 id=学习建议-1>学习建议<a class=anchor href=#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae-1>#</a></h3><ol><li><strong>理解原理</strong>:前向/反向过程,DDPM训练目标</li><li><strong>熟练使用</strong>:diffusers库,各种pipeline</li><li><strong>掌握提示词</strong>:参考优秀案例,建立自己的提示词库</li><li><strong>实践项目</strong>:文生图、ControlNet等</li></ol><h3 id=下一步方向>下一步方向<a class=anchor href=#%e4%b8%8b%e4%b8%80%e6%ad%a5%e6%96%b9%e5%90%91>#</a></h3><p>完成本章后,你可以:</p><ul><li>进入第7篇学习<strong>视觉大模型</strong>(CLIP、LLaVA等)</li><li>深入研究<strong>模型微调</strong>(DreamBooth、LoRA训练)</li><li>探索<strong>视频生成</strong>(Animate Diff、Gen-2等)</li><li>学习<strong>3D生成</strong>(DreamFusion、Magic3D等)</li></ul><hr><h2 id=参考资源-1>参考资源<a class=anchor href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%ba%90-1>#</a></h2><h3 id=必读论文>必读论文<a class=anchor href=#%e5%bf%85%e8%af%bb%e8%ae%ba%e6%96%87>#</a></h3><ol><li><p><strong>DDPM</strong> (2020)</p><ul><li>Ho et al., &ldquo;Denoising Diffusion Probabilistic Models&rdquo;</li><li><a href=https://arxiv.org/abs/2006.11239>arXiv:2006.11239</a></li></ul></li><li><p><strong>Latent Diffusion</strong> (2022)</p><ul><li>Rombach et al., &ldquo;High-Resolution Image Synthesis with Latent Diffusion Models&rdquo;</li><li><a href=https://arxiv.org/abs/2112.10752>arXiv:2112.10752</a></li></ul></li><li><p><strong>ControlNet</strong> (2023)</p><ul><li>Zhang et al., &ldquo;Adding Conditional Control to Text-to-Image Diffusion Models&rdquo;</li><li><a href=https://arxiv.org/abs/2302.05543>arXiv:2302.05543</a></li></ul></li></ol><h3 id=官方文档-1>官方文档<a class=anchor href=#%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3-1>#</a></h3><ul><li><strong>Hugging Face Diffusers</strong>: <a href=https://huggingface.co/docs/diffusers>https://huggingface.co/docs/diffusers</a></li><li><strong>Stable Diffusion</strong>: <a href=https://stability.ai/stable-diffusion>https://stability.ai/stable-diffusion</a></li><li><strong>FLUX</strong>: <a href=https://blackforestlabs.ai/>https://blackforestlabs.ai/</a></li></ul><h3 id=实战资源>实战资源<a class=anchor href=#%e5%ae%9e%e6%88%98%e8%b5%84%e6%ba%90>#</a></h3><ul><li><strong>Civitai</strong>: <a href=https://civitai.com/>https://civitai.com/</a> (模型与LoRA)</li><li><strong>Lexica</strong>: <a href=https://lexica.art/>https://lexica.art/</a> (提示词库)</li><li><strong>PromptHero</strong>: <a href=https://prompthero.com/>https://prompthero.com/</a> (提示词数据库)</li></ul><h3 id=工具>工具<a class=anchor href=#%e5%b7%a5%e5%85%b7>#</a></h3><ul><li><strong>Stable Diffusion WebUI</strong>: <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui>https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></li><li><strong>ComfyUI</strong>: <a href=https://github.com/comfyanonymous/ComfyUI>https://github.com/comfyanonymous/ComfyUI</a></li></ul><hr><p><strong>恭喜完成第15章!你已经掌握了2024年最重要的生成式AI技术!</strong></p><hr></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第五篇 图像分割</span>
</a></span><span><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/ class="flex align-center"><span>第七篇 视觉大模型</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#篇章概述>篇章概述</a><ul><li><a href=#为什么学习生成模型>为什么学习生成模型?</a></li><li><a href=#技术演进时间线>技术演进时间线</a></li></ul></li><li><a href=#章节安排>章节安排</a><ul><li><a href=#第14章生成对抗网络gan>第14章:生成对抗网络(GAN)</a></li><li><a href=#第15章扩散模型diffusion>第15章:扩散模型(Diffusion)</a></li></ul></li><li><a href=#技术栈>技术栈</a><ul><li><a href=#环境要求>环境要求</a></li><li><a href=#核心依赖>核心依赖</a></li><li><a href=#验证安装>验证安装</a></li></ul></li><li><a href=#学习建议>学习建议</a><ul><li><a href=#1-重点放在diffusion>1. 重点放在Diffusion</a></li><li><a href=#2-理解扩散过程的数学原理>2. 理解扩散过程的数学原理</a></li><li><a href=#3-掌握提示词工程>3. 掌握提示词工程</a></li><li><a href=#4-循序渐进的实战>4. 循序渐进的实战</a></li></ul></li><li><a href=#模型资源>模型资源</a><ul><li><a href=#hugging-face-hub>Hugging Face Hub</a></li><li><a href=#国内下载加速>国内下载加速</a></li></ul></li><li><a href=#与前后篇的关系>与前后篇的关系</a></li><li><a href=#代码规范>代码规范</a><ul><li><a href=#目录结构>目录结构</a></li><li><a href=#代码风格>代码风格</a></li></ul></li><li><a href=#常见问题>常见问题</a><ul><li><a href=#q1-gan和diffusion哪个更重要>Q1: GAN和Diffusion哪个更重要?</a></li><li><a href=#q2-没有gpu能学习吗>Q2: 没有GPU能学习吗?</a></li><li><a href=#q3-stable-diffusion版本如何选择>Q3: Stable Diffusion版本如何选择?</a></li><li><a href=#q4-如何写出高质量提示词>Q4: 如何写出高质量提示词?</a></li><li><a href=#q5-生成的图像质量不好怎么办>Q5: 生成的图像质量不好怎么办?</a></li></ul></li><li><a href=#拓展资源>拓展资源</a><ul><li><a href=#官方文档>官方文档</a></li><li><a href=#论文阅读>论文阅读</a></li><li><a href=#社区资源>社区资源</a></li><li><a href=#实战平台>实战平台</a></li></ul></li><li><a href=#学习路线图>学习路线图</a></li><li><a href=#实战项目预告>实战项目预告</a></li></ul><ul><li><a href=#章节导读>章节导读</a><ul><li><a href=#为什么要学习gan>为什么要学习GAN?</a></li><li><a href=#本章学习策略>本章学习策略</a></li></ul></li><li><a href=#141-gan基础原理>14.1 GAN基础原理</a><ul><li><a href=#核心思想两人零和博弈>核心思想:两人零和博弈</a></li><li><a href=#数学形式>数学形式</a></li><li><a href=#训练流程>训练流程</a></li></ul></li><li><a href=#142-dcgan深度卷积gan>14.2 DCGAN:深度卷积GAN</a><ul><li><a href=#为什么需要dcgan>为什么需要DCGAN?</a></li><li><a href=#dcgan架构设计原则>DCGAN架构设计原则</a></li><li><a href=#生成器架构>生成器架构</a></li><li><a href=#判别器架构>判别器架构</a></li><li><a href=#权重初始化>权重初始化</a></li></ul></li><li><a href=#143-stylegan系列简介>14.3 StyleGAN系列(简介)</a><ul><li><a href=#stylegan的创新>StyleGAN的创新</a></li><li><a href=#stylegan2和stylegan3>StyleGAN2和StyleGAN3</a></li></ul></li><li><a href=#144-条件gan与应用>14.4 条件GAN与应用</a><ul><li><a href=#conditional-gan-cgan>Conditional GAN (cGAN)</a></li><li><a href=#经典应用>经典应用</a></li></ul></li><li><a href=#145-gan的主要问题>14.5 GAN的主要问题</a><ul><li><a href=#1-训练不稳定>1. 训练不稳定</a></li><li><a href=#2-梯度消失>2. 梯度消失</a></li><li><a href=#3-难以评估>3. 难以评估</a></li><li><a href=#4-超参数敏感>4. 超参数敏感</a></li></ul></li><li><a href=#146-实战dcgan生成人脸>14.6 实战:DCGAN生成人脸</a><ul><li><a href=#数据集celeba>数据集:CelebA</a></li><li><a href=#训练循环>训练循环</a></li><li><a href=#训练技巧>训练技巧</a></li></ul></li><li><a href=#147-gan与diffusion对比>14.7 GAN与Diffusion对比</a></li><li><a href=#148-总结与展望>14.8 总结与展望</a><ul><li><a href=#本章要点回顾>本章要点回顾</a></li><li><a href=#gan的现代应用>GAN的现代应用</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li><li><a href=#参考资源>参考资源</a><ul><li><a href=#论文>论文</a></li><li><a href=#代码资源>代码资源</a></li><li><a href=#在线课程>在线课程</a></li></ul></li></ul><ul><li><a href=#章节导读-1>章节导读</a><ul><li><a href=#为什么扩散模型如此重要>为什么扩散模型如此重要?</a></li><li><a href=#本章学习路线>本章学习路线</a></li></ul></li><li><a href=#151-扩散模型基础>15.1 扩散模型基础</a><ul><li><a href=#核心思想逐步加噪再去噪>核心思想:逐步加噪再去噪</a></li><li><a href=#前向扩散过程>前向扩散过程</a></li><li><a href=#反向去噪过程>反向去噪过程</a></li><li><a href=#ddpm训练目标>DDPM训练目标</a></li><li><a href=#训练流程伪代码>训练流程伪代码</a></li><li><a href=#采样流程生成图像>采样流程(生成图像)</a></li></ul></li><li><a href=#152-stable-diffusion架构>15.2 Stable Diffusion架构</a><ul><li><a href=#为什么需要latent-diffusion>为什么需要Latent Diffusion?</a></li><li><a href=#stable-diffusion三大组件>Stable Diffusion三大组件</a><ul><li><a href=#1-vae-variational-autoencoder>1. VAE (Variational Autoencoder)</a></li><li><a href=#2-u-net噪声预测器>2. U-Net:噪声预测器</a></li><li><a href=#3-clip-text-encoder>3. CLIP Text Encoder</a></li></ul></li><li><a href=#stable-diffusion生成流程>Stable Diffusion生成流程</a></li><li><a href=#采样器scheduler对比>采样器(Scheduler)对比</a></li><li><a href=#引导强度guidance-scale>引导强度(Guidance Scale)</a></li></ul></li><li><a href=#153-controlnet可控生成>15.3 ControlNet:可控生成</a><ul><li><a href=#为什么需要controlnet>为什么需要ControlNet?</a></li><li><a href=#controlnet原理>ControlNet原理</a></li><li><a href=#常用控制类型>常用控制类型</a><ul><li><a href=#1-canny-edge-边缘检测>1. Canny Edge (边缘检测)</a></li><li><a href=#2-depth-map-深度图>2. Depth Map (深度图)</a></li><li><a href=#3-openpose-人体姿态>3. OpenPose (人体姿态)</a></li><li><a href=#4-scribble-手绘草图>4. Scribble (手绘草图)</a></li><li><a href=#5-segmentation-语义分割>5. Segmentation (语义分割)</a></li></ul></li><li><a href=#多controlnet组合>多ControlNet组合</a></li><li><a href=#controlnet权重调节>ControlNet权重调节</a></li></ul></li><li><a href=#154-flux2024最新扩散模型>15.4 FLUX:2024最新扩散模型</a><ul><li><a href=#flux简介>FLUX简介</a></li><li><a href=#flux的创新点>FLUX的创新点</a></li><li><a href=#flux-vs-stable-diffusion>FLUX vs Stable Diffusion</a></li><li><a href=#使用flux>使用FLUX</a></li></ul></li><li><a href=#155-提示词工程prompt-engineering>15.5 提示词工程(Prompt Engineering)</a><ul><li><a href=#提示词的重要性>提示词的重要性</a></li><li><a href=#提示词结构>提示词结构</a></li><li><a href=#负向提示词negative-prompt>负向提示词(Negative Prompt)</a></li><li><a href=#提示词权重>提示词权重</a></li><li><a href=#风格提示词库>风格提示词库</a><ul><li><a href=#摄影风格>摄影风格</a></li><li><a href=#艺术风格>艺术风格</a></li><li><a href=#质量提升词>质量提升词</a></li></ul></li><li><a href=#提示词优化技巧>提示词优化技巧</a></li><li><a href=#常见问题与解决>常见问题与解决</a></li></ul></li><li><a href=#156-实战项目>15.6 实战项目</a><ul><li><a href=#项目1文生图text-to-image>项目1:文生图(Text-to-Image)</a></li><li><a href=#项目2图生图image-to-image>项目2:图生图(Image-to-Image)</a></li><li><a href=#项目3controlnet可控生成>项目3:ControlNet可控生成</a></li><li><a href=#项目4批量生成与自动化>项目4:批量生成与自动化</a></li></ul></li><li><a href=#157-高级技巧>15.7 高级技巧</a><ul><li><a href=#1-lora-low-rank-adaptation>1. LoRA (Low-Rank Adaptation)</a></li><li><a href=#2-textual-inversion>2. Textual Inversion</a></li><li><a href=#3-inpainting-局部重绘>3. Inpainting (局部重绘)</a></li><li><a href=#4-超分辨率upscaling>4. 超分辨率(Upscaling)</a></li></ul></li><li><a href=#158-性能优化>15.8 性能优化</a><ul><li><a href=#显存优化>显存优化</a></li><li><a href=#速度优化>速度优化</a></li><li><a href=#cpu-offloading>CPU Offloading</a></li></ul></li><li><a href=#159-总结>15.9 总结</a><ul><li><a href=#本章要点回顾-1>本章要点回顾</a></li><li><a href=#扩散模型-vs-gan>扩散模型 vs GAN</a></li><li><a href=#学习建议-1>学习建议</a></li><li><a href=#下一步方向>下一步方向</a></li></ul></li><li><a href=#参考资源-1>参考资源</a><ul><li><a href=#必读论文>必读论文</a></li><li><a href=#官方文档-1>官方文档</a></li><li><a href=#实战资源>实战资源</a></li><li><a href=#工具>工具</a></li></ul></li></ul></nav></div></aside></main></body></html>