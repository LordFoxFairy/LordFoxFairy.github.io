<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第3章：TRL 与强化学习实战 (SFT / DPO / PPO)# 本章定位：从微调（SFT）到对齐（Alignment）。我们将复现 Hugging Face 官方 Alignment Handbook 的核心流程，但为了让每位读者都能跑通，我们将基座模型替换为轻量级的 Qwen2-0.5B。无论你是在 Colab 还是单卡 3090，都能完整体验 RLHF 的全过程。
目录# 1. 完整的对齐流水线 (The Alignment Pipeline) 2. SFT：让 Qwen-0.5B 学会指令 2.1 数据格式与 Chat Template 2.2 核心技巧：Packing 加速 2.3 实战代码 3. DPO：工业界对齐首选 3.1 数据集：偏好对是如何构建的？ 3.2 关键超参：Beta 的魔法 3.3 实战：使用 Qwen-0.5B 跑通 DPO 4. PPO：经典 RLHF 三阶段 (进阶) 4.1 训练 Reward Model (RM) 4.2 PPO 流程详解 (Actor-Critic) 5.新兴趋势：ORPO 与 KTO 本章小结 1. 完整的对齐流水线 (The Alignment Pipeline)# 一个标准的工业级 LLM 训练流程包含三个阶段：
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第3章 TRL与强化学习实战"><meta property="og:description" content="第3章：TRL 与强化学习实战 (SFT / DPO / PPO)# 本章定位：从微调（SFT）到对齐（Alignment）。我们将复现 Hugging Face 官方 Alignment Handbook 的核心流程，但为了让每位读者都能跑通，我们将基座模型替换为轻量级的 Qwen2-0.5B。无论你是在 Colab 还是单卡 3090，都能完整体验 RLHF 的全过程。
目录# 1. 完整的对齐流水线 (The Alignment Pipeline) 2. SFT：让 Qwen-0.5B 学会指令 2.1 数据格式与 Chat Template 2.2 核心技巧：Packing 加速 2.3 实战代码 3. DPO：工业界对齐首选 3.1 数据集：偏好对是如何构建的？ 3.2 关键超参：Beta 的魔法 3.3 实战：使用 Qwen-0.5B 跑通 DPO 4. PPO：经典 RLHF 三阶段 (进阶) 4.1 训练 Reward Model (RM) 4.2 PPO 流程详解 (Actor-Critic) 5.新兴趋势：ORPO 与 KTO 本章小结 1. 完整的对齐流水线 (The Alignment Pipeline)# 一个标准的工业级 LLM 训练流程包含三个阶段："><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第3章 TRL与强化学习实战"><meta itemprop=description content="第3章：TRL 与强化学习实战 (SFT / DPO / PPO)# 本章定位：从微调（SFT）到对齐（Alignment）。我们将复现 Hugging Face 官方 Alignment Handbook 的核心流程，但为了让每位读者都能跑通，我们将基座模型替换为轻量级的 Qwen2-0.5B。无论你是在 Colab 还是单卡 3090，都能完整体验 RLHF 的全过程。
目录# 1. 完整的对齐流水线 (The Alignment Pipeline) 2. SFT：让 Qwen-0.5B 学会指令 2.1 数据格式与 Chat Template 2.2 核心技巧：Packing 加速 2.3 实战代码 3. DPO：工业界对齐首选 3.1 数据集：偏好对是如何构建的？ 3.2 关键超参：Beta 的魔法 3.3 实战：使用 Qwen-0.5B 跑通 DPO 4. PPO：经典 RLHF 三阶段 (进阶) 4.1 训练 Reward Model (RM) 4.2 PPO 流程详解 (Actor-Critic) 5.新兴趋势：ORPO 与 KTO 本章小结 1. 完整的对齐流水线 (The Alignment Pipeline)# 一个标准的工业级 LLM 训练流程包含三个阶段："><meta itemprop=wordCount content="559"><title>第3章 TRL与强化学习实战 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle checked>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/ class=active>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第3章 TRL与强化学习实战</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-完整的对齐流水线-the-alignment-pipeline>1. 完整的对齐流水线 (The Alignment Pipeline)</a></li><li><a href=#2-sft让-qwen-05b-学会指令>2. SFT：让 Qwen-0.5B 学会指令</a><ul><li><a href=#21-数据格式与-chat-template>2.1 数据格式与 Chat Template</a></li><li><a href=#22-核心技巧packing-序列打包>2.2 核心技巧：Packing (序列打包)</a></li><li><a href=#23-实战代码>2.3 实战代码</a></li></ul></li><li><a href=#3-dpo工业界对齐首选>3. DPO：工业界对齐首选</a><ul><li><a href=#31-数据集偏好对是如何构建的>3.1 数据集：偏好对是如何构建的？</a></li><li><a href=#32-关键超参beta-的魔法>3.2 关键超参：Beta 的魔法</a></li><li><a href=#33-实战使用-qwen-05b-跑通-dpo>3.3 实战：使用 Qwen-0.5B 跑通 DPO</a></li></ul></li><li><a href=#4-ppo经典-rlhf-三阶段-进阶>4. PPO：经典 RLHF 三阶段 (进阶)</a><ul><li><a href=#41-训练-reward-model-rm>4.1 训练 Reward Model (RM)</a></li><li><a href=#42-ppo-流程详解-actor-critic>4.2 PPO 流程详解 (Actor-Critic)</a></li></ul></li><li><a href=#5-新兴趋势orpo-与-kto>5. 新兴趋势：ORPO 与 KTO</a><ul><li><a href=#51-orpo-单阶段微调>5.1 ORPO (单阶段微调)</a></li><li><a href=#52-kto-非成对数据>5.2 KTO (非成对数据)</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第3章trl-与强化学习实战-sft--dpo--ppo>第3章：TRL 与强化学习实战 (SFT / DPO / PPO)<a class=anchor href=#%e7%ac%ac3%e7%ab%a0trl-%e4%b8%8e%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%ae%9e%e6%88%98-sft--dpo--ppo>#</a></h1><blockquote class=book-hint><p><strong>本章定位</strong>：从微调（SFT）到对齐（Alignment）。我们将复现 Hugging Face 官方 <strong>Alignment Handbook</strong> 的核心流程，但为了让每位读者都能跑通，我们将基座模型替换为轻量级的 <strong>Qwen2-0.5B</strong>。无论你是在 Colab 还是单卡 3090，都能完整体验 RLHF 的全过程。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#1-%e5%ae%8c%e6%95%b4%e7%9a%84%e5%af%b9%e9%bd%90%e6%b5%81%e6%b0%b4%e7%ba%bf-the-alignment-pipeline>1. 完整的对齐流水线 (The Alignment Pipeline)</a></li><li><a href=#2-sft%e8%ae%a9-qwen-05b-%e5%ad%a6%e4%bc%9a%e6%8c%87%e4%bb%a4>2. SFT：让 Qwen-0.5B 学会指令</a><ul><li><a href=#21-%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f%e4%b8%8e-chat-template>2.1 数据格式与 Chat Template</a></li><li><a href=#22-%e6%a0%b8%e5%bf%83%e6%8a%80%e5%b7%a7packing-%e5%8a%a0%e9%80%9f>2.2 核心技巧：Packing 加速</a></li><li><a href=#23-%e5%ae%9e%e6%88%98%e4%bb%a3%e7%a0%81>2.3 实战代码</a></li></ul></li><li><a href=#3-dpo%e5%b7%a5%e4%b8%9a%e7%95%8c%e5%af%b9%e9%bd%90%e9%a6%96%e9%80%89>3. DPO：工业界对齐首选</a><ul><li><a href=#31-%e6%95%b0%e6%8d%ae%e9%9b%86%e5%81%8f%e5%a5%bd%e5%af%b9%e6%98%af%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e7%9a%84>3.1 数据集：偏好对是如何构建的？</a></li><li><a href=#32-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82beta-%e7%9a%84%e9%ad%94%e6%b3%95>3.2 关键超参：Beta 的魔法</a></li><li><a href=#33-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-qwen-05b-%e8%b7%91%e9%80%9a-dpo>3.3 实战：使用 Qwen-0.5B 跑通 DPO</a></li></ul></li><li><a href=#4-ppo%e7%bb%8f%e5%85%b8-rlhf-%e4%b8%89%e9%98%b6%e6%ae%b5-%e8%bf%9b%e9%98%b6>4. PPO：经典 RLHF 三阶段 (进阶)</a><ul><li><a href=#41-%e8%ae%ad%e7%bb%83-reward-model-rm>4.1 训练 Reward Model (RM)</a></li><li><a href=#42-ppo-%e6%b5%81%e7%a8%8b%e8%af%a6%e8%a7%a3-actor-critic>4.2 PPO 流程详解 (Actor-Critic)</a></li></ul></li><li><a href=#5-2024-%e6%96%b0%e8%b6%8b%e5%8a%bforpo-%e4%b8%8e-kto>5.新兴趋势：ORPO 与 KTO</a></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li></ul><hr><h2 id=1-完整的对齐流水线-the-alignment-pipeline>1. 完整的对齐流水线 (The Alignment Pipeline)<a class=anchor href=#1-%e5%ae%8c%e6%95%b4%e7%9a%84%e5%af%b9%e9%bd%90%e6%b5%81%e6%b0%b4%e7%ba%bf-the-alignment-pipeline>#</a></h2><p>一个标准的工业级 LLM 训练流程包含三个阶段：</p><ol><li><strong>Pre-training (PT)</strong>: 海量文本，学习"续写"。</li><li><strong>Supervised Fine-Tuning (SFT)</strong>: 指令数据，学习"对话"。</li><li><strong>Preference Alignment (DPO/PPO)</strong>: 偏好数据，学习"价值观"。</li></ol><p>本章我们将使用 TRL 库，基于 <strong>Qwen2-0.5B</strong> 完成后两个阶段。</p><hr><h2 id=2-sft让-qwen-05b-学会指令>2. SFT：让 Qwen-0.5B 学会指令<a class=anchor href=#2-sft%e8%ae%a9-qwen-05b-%e5%ad%a6%e4%bc%9a%e6%8c%87%e4%bb%a4>#</a></h2><p>SFT 不仅仅是微调，更是让模型适应特定的<strong>对话格式</strong>。</p><h3 id=21-数据格式与-chat-template>2.1 数据格式与 Chat Template<a class=anchor href=#21-%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f%e4%b8%8e-chat-template>#</a></h3><p>对于 Qwen2，我们必须严格遵守 ChatML 格式：
<code>&lt;|im_start|>user\n{msg}&lt;|im_end|>\n&lt;|im_start|>assistant\n{msg}&lt;|im_end|></code></p><p>TRL 的 <code>SFTTrainer</code> 可以自动处理这个，前提是你配置好了 <code>chat_template</code>。</p><h3 id=22-核心技巧packing-序列打包>2.2 核心技巧：Packing (序列打包)<a class=anchor href=#22-%e6%a0%b8%e5%bf%83%e6%8a%80%e5%b7%a7packing-%e5%ba%8f%e5%88%97%e6%89%93%e5%8c%85>#</a></h3><p><code>SFTTrainer</code> 支持 <code>packing=True</code>。它将多个短对话拼接到 <code>max_seq_length</code> (如 2048)，用 <code>attention_mask</code> 隔开。</p><ul><li><strong>收益</strong>：训练速度通常提升 <strong>3-5 倍</strong>。</li><li><strong>代价</strong>：需要更多显存（但对于 0.5B 模型，这不是问题）。</li></ul><h3 id=23-实战代码>2.3 实战代码<a class=anchor href=#23-%e5%ae%9e%e6%88%98%e4%bb%a3%e7%a0%81>#</a></h3><p>以下代码可在单卡 T4 (Colab 免费版) 上运行。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>SFTTrainer</span><span class=p>,</span> <span class=n>SFTConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 准备模型与数据</span>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;Qwen/Qwen2-0.5B-Instruct&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 使用 HuggingFaceH4 的精选数据集 (Ultrafachat)</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;HuggingFaceH4/ultrachat_200k&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train_sft[:1%]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># ⚠️ Qwen 的 pad_token 有时需要手动指定，避免 loss 为 NaN</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 配置参数</span>
</span></span><span class=line><span class=cl><span class=n>args</span> <span class=o>=</span> <span class=n>SFTConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./qwen-sft&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>packing</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>                <span class=c1># 核心加速</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-5</span><span class=p>,</span>          <span class=c1># SFT 典型学习率</span>
</span></span><span class=line><span class=cl>    <span class=n>lr_scheduler_type</span><span class=o>=</span><span class=s2>&#34;cosine&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>                   <span class=c1># T4 用 fp16, A100 用 bf16</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 开始训练</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset_text_field</span><span class=o>=</span><span class=s2>&#34;messages&#34;</span><span class=p>,</span> <span class=c1># 数据集中的列名</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>args</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>save_model</span><span class=p>(</span><span class=s2>&#34;./qwen-sft-final&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=3-dpo工业界对齐首选>3. DPO：工业界对齐首选<a class=anchor href=#3-dpo%e5%b7%a5%e4%b8%9a%e7%95%8c%e5%af%b9%e9%bd%90%e9%a6%96%e9%80%89>#</a></h2><p>现在我们的 Qwen-0.5B 主要学会了说话，但可能还会胡说八道。我们要用 DPO (Direct Preference Optimization) 来对齐人类偏好。</p><h3 id=31-数据集偏好对是如何构建的>3.1 数据集：偏好对是如何构建的？<a class=anchor href=#31-%e6%95%b0%e6%8d%ae%e9%9b%86%e5%81%8f%e5%a5%bd%e5%af%b9%e6%98%af%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e7%9a%84>#</a></h3><p>DPO 数据必须是<strong>成对</strong>的：<code>(prompt, chosen, rejected)</code>。</p><ul><li><strong>Chosen (胜)</strong>：详细、有用、无害的回答。</li><li><strong>Rejected (负)</strong>：简短、错误或有害的回答。</li></ul><p>我们使用 <code>HuggingFaceH4/ultrafeedback_binarized</code>，这是目前质量最高的开源偏好数据集之一。</p><h3 id=32-关键超参beta-的魔法>3.2 关键超参：Beta 的魔法<a class=anchor href=#32-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82beta-%e7%9a%84%e9%ad%94%e6%b3%95>#</a></h3><p><code>beta</code> 是 DPO 损失函数中的 KL 惩罚系数。</p><ul><li><strong>Zephyr 配方</strong>：<code>beta=0.1</code>。</li><li><strong>直觉</strong>：beta 越大，模型越保守（贴近原始模型）；beta 越小，模型越激进（贴近 chosen 数据）。对于 Qwen-0.5B 这种小模型，建议 <code>beta=0.1</code> 以防止过度遗忘。</li></ul><h3 id=33-实战使用-qwen-05b-跑通-dpo>3.3 实战：使用 Qwen-0.5B 跑通 DPO<a class=anchor href=#33-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-qwen-05b-%e8%b7%91%e9%80%9a-dpo>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>DPOTrainer</span><span class=p>,</span> <span class=n>DPOConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载 SFT 后的模型 (作为 Policy Model)</span>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;./qwen-sft-final&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 加载数据</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;HuggingFaceH4/ultrafeedback_binarized&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train_prefs[:1000]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. DPO特殊的配置</span>
</span></span><span class=line><span class=cl><span class=c1># 注意：DPO 的学习率通常比 SFT 低一个数量级 (5e-6 vs 2e-5)</span>
</span></span><span class=line><span class=cl><span class=n>dpo_args</span> <span class=o>=</span> <span class=n>DPOConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./qwen-dpo&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>5e-6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 初始化 Trainer</span>
</span></span><span class=line><span class=cl><span class=c1># TRL 会自动加载 ref_model (也就是 model 的一份拷贝，冻结参数)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>DPOTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_model</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=c1># 自动处理</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>dpo_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_prompt_length</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span></span></span></code></pre></div><hr><h2 id=4-ppo经典-rlhf-三阶段-进阶>4. PPO：经典 RLHF 三阶段 (进阶)<a class=anchor href=#4-ppo%e7%bb%8f%e5%85%b8-rlhf-%e4%b8%89%e9%98%b6%e6%ae%b5-%e8%bf%9b%e9%98%b6>#</a></h2><p>虽然 DPO 很火，但 PPO (Proximal Policy Optimization) 依然是理解 RLHF 的基石。如果你的数据集没有成对的偏好，只有一个 Scalar Reward（比如代码通过没通过测试），那么 PPO 是唯一的选择。</p><h3 id=41-训练-reward-model-rm>4.1 训练 Reward Model (RM)<a class=anchor href=#41-%e8%ae%ad%e7%bb%83-reward-model-rm>#</a></h3><p>在 PPO 之前，我们需要一个裁判模型（Reward Model）。它通常是一个 BERT 或者同架构的 Decoder 模型，将最后输出层改为一个标量回归头。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>RewardTrainer</span><span class=p>,</span> <span class=n>RewardConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义模型：AutoModelForSequenceClassification (num_labels=1)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Qwen/Qwen2-0.5B&#34;</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>RewardTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>RewardConfig</span><span class=p>(</span><span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./qwen-rm&#34;</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span> <span class=c1># 包含 chosen/rejected</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span></span></span></code></pre></div><h3 id=42-ppo-流程详解-actor-critic>4.2 PPO 流程详解 (Actor-Critic)<a class=anchor href=#42-ppo-%e6%b5%81%e7%a8%8b%e8%af%a6%e8%a7%a3-actor-critic>#</a></h3><p>这部分的代码非常复杂，涉及 4 个模型：</p><ol><li><strong>Actor</strong>: 我们要训练的 Qwen-0.5B。</li><li><strong>Ref Model</strong>: 原始 Qwen-0.5B（冻结），用于计算 KL 散度，防止 Actor 跑偏（Reward Hacking）。</li><li><strong>Critic</strong>: 价值函数网络，估计 V(s)。</li><li><strong>Reward Model</strong>: 刚才训练好的裁判。</li></ol><p><strong>核心代码逻辑</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码流程</span>
</span></span><span class=line><span class=cl><span class=n>ppo_trainer</span> <span class=o>=</span> <span class=n>PPOTrainer</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;input_ids&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. Actor 生成回复</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>ppo_trainer</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. RM 打分</span>
</span></span><span class=line><span class=cl>    <span class=n>reward</span> <span class=o>=</span> <span class=n>reward_model</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. PPO Update Step</span>
</span></span><span class=line><span class=cl>    <span class=c1># 这一步会综合 reward 和 KL(actor, ref) 来更新 actor</span>
</span></span><span class=line><span class=cl>    <span class=n>stats</span> <span class=o>=</span> <span class=n>ppo_trainer</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>response</span><span class=p>,</span> <span class=n>reward</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=5-新兴趋势orpo-与-kto>5. 新兴趋势：ORPO 与 KTO<a class=anchor href=#5-%e6%96%b0%e5%85%b4%e8%b6%8b%e5%8a%bforpo-%e4%b8%8e-kto>#</a></h2><h3 id=51-orpo-单阶段微调>5.1 ORPO (单阶段微调)<a class=anchor href=#51-orpo-%e5%8d%95%e9%98%b6%e6%ae%b5%e5%be%ae%e8%b0%83>#</a></h3><p><strong>ORPO (Odds Ratio Preference Optimization)</strong> 试图将 SFT 和 DPO 合二为一。</p><ul><li><strong>原理</strong>：在 SFT 的 Loss 上增加一项，专门惩罚 rejected 生成的概率。</li><li><strong>优势</strong>：不需要 SFT -> DPO 两步走，<strong>一步到位</strong>。</li><li><strong>代码</strong>：将 <code>DPOTrainer</code> 替换为 <code>ORPOTrainer</code> 即可，接口几乎一致。</li></ul><h3 id=52-kto-非成对数据>5.2 KTO (非成对数据)<a class=anchor href=#52-kto-%e9%9d%9e%e6%88%90%e5%af%b9%e6%95%b0%e6%8d%ae>#</a></h3><p><strong>KTO (Kahneman-Tversky Optimization)</strong> 解决了 DPO 必须要有成对数据 <code>(A > B)</code> 的痛点。</p><ul><li>如果你的数据只有 &ldquo;A 是好的&rdquo; (点赞) 和 &ldquo;B 是坏的&rdquo; (点踩)，没有配对关系，KTO 是最佳选择。</li></ul><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>本章我们用最轻量的 <strong>Qwen2-0.5B</strong> 跑通了最硬核的 <strong>RLHF</strong> 流程：</p><ol><li><strong>SFT</strong>: 用 <code>packing=True</code> 高效教会模型指令格式。</li><li><strong>DPO</strong>: 用 <code>beta=0.1</code> 和成对数据，低成本实现偏好对齐（工业界 MVP）。</li><li><strong>PPO</strong>: 理解了 Actor/Critic/RM 的复杂博弈（学术界基石）。</li><li><strong>前沿</strong>: ORPO 和 KTO 提供了更灵活的选择。</li></ol><p>现在，你手中的 Qwen-0.5B 不仅能说话，还能说出“符合人类偏好”的话。下一章，我们将探讨如何利用 <strong>DeepSpeed</strong> 将这一套流程扩展到 7B、70B 甚至更大的模型上。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第2章 LLaMA-Factory微调工厂</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ class="flex align-center"><span>第4章 DeepSpeed分布式训练</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-完整的对齐流水线-the-alignment-pipeline>1. 完整的对齐流水线 (The Alignment Pipeline)</a></li><li><a href=#2-sft让-qwen-05b-学会指令>2. SFT：让 Qwen-0.5B 学会指令</a><ul><li><a href=#21-数据格式与-chat-template>2.1 数据格式与 Chat Template</a></li><li><a href=#22-核心技巧packing-序列打包>2.2 核心技巧：Packing (序列打包)</a></li><li><a href=#23-实战代码>2.3 实战代码</a></li></ul></li><li><a href=#3-dpo工业界对齐首选>3. DPO：工业界对齐首选</a><ul><li><a href=#31-数据集偏好对是如何构建的>3.1 数据集：偏好对是如何构建的？</a></li><li><a href=#32-关键超参beta-的魔法>3.2 关键超参：Beta 的魔法</a></li><li><a href=#33-实战使用-qwen-05b-跑通-dpo>3.3 实战：使用 Qwen-0.5B 跑通 DPO</a></li></ul></li><li><a href=#4-ppo经典-rlhf-三阶段-进阶>4. PPO：经典 RLHF 三阶段 (进阶)</a><ul><li><a href=#41-训练-reward-model-rm>4.1 训练 Reward Model (RM)</a></li><li><a href=#42-ppo-流程详解-actor-critic>4.2 PPO 流程详解 (Actor-Critic)</a></li></ul></li><li><a href=#5-新兴趋势orpo-与-kto>5. 新兴趋势：ORPO 与 KTO</a><ul><li><a href=#51-orpo-单阶段微调>5.1 ORPO (单阶段微调)</a></li><li><a href=#52-kto-非成对数据>5.2 KTO (非成对数据)</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></div></aside></main></body></html>