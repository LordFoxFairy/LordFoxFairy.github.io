<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第2章：模型家族谱系：从编码器到解码器 (Model Architectures)# “The best way to predict the future is to invent it.” - Alan Kay
本章将带你理解Transformer的三大架构分支，掌握每种架构的设计哲学、技术细节和当前的主流选择，助你在实际应用中做出明智的架构选型。
目录# 一、共同的祖先：编码器-解码器架构 1.1 原始Transformer的双塔设计 1.2 编码器-解码器的工作流程 1.3 T5：现代编码器-解码器的代表 二、分裂与演化：为何不都用编码器-解码器？ 2.1 计算效率考量 2.2 任务特性适配 三、仅编码器架构：双向的理解专家 3.1 BERT的革命性设计 3.2 掩码语言模型 (MLM) 3.3 为什么BERT不能生成文本？ 3.4 实战：BERT文本分类 四、仅解码器架构：生成的王者 4.1 GPT的单向设计哲学 4.2 因果注意力机制 (Causal Mask) 4.3 当前视角：Decoder-only的全面胜利 4.4 实战：GPT文本生成 五、架构选型指南 5.1 三大架构对比表 5.2 决策树：我该用哪个？ 5.3 黄金组合：Encoder做索引，Decoder做生成 六、深度问答 本章概览
在第1章中，我们深入学习了Transformer的核心机制。但你是否好奇：为什么BERT擅长理解文本，而GPT擅长生成文本？为什么现在的新模型（如DeepSeek-V3, LLaMA-3）几乎全都是Decoder-only架构？
这一切的答案，藏在Transformer的三大架构分支中：
graph TD A[Transformer 原始架构<br>Encoder-Decoder 2017] --> B[仅编码器<br>Encoder-only] A --> C[仅解码器<br>Decoder-only] A --> D[编码器-解码器<br>Encoder-Decoder] B --> B1[BERT 2018<br>RoBERTa 2019<br>Embedding Models 2025] C --> C1[GPT-3/4 2020-2023<br>LLaMA-3 2024<br>DeepSeek-V3 2024] D --> D1[T5 2020<br>BART 2020<br>GLM-130B 2022] style A fill:#FFE4E1,stroke:#E87461 style B fill:#E8F5E9,stroke:#81C784 style C fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px style D fill:#FFF9C4,stroke:#FDD835 style C1 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px难度级别：⭐⭐（进阶）- 需要理解第1章的Transformer基础
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第2章 模型家族谱系：从编码器到解码器"><meta property="og:description" content="第2章：模型家族谱系：从编码器到解码器 (Model Architectures)# “The best way to predict the future is to invent it.” - Alan Kay
本章将带你理解Transformer的三大架构分支，掌握每种架构的设计哲学、技术细节和当前的主流选择，助你在实际应用中做出明智的架构选型。
目录# 一、共同的祖先：编码器-解码器架构 1.1 原始Transformer的双塔设计 1.2 编码器-解码器的工作流程 1.3 T5：现代编码器-解码器的代表 二、分裂与演化：为何不都用编码器-解码器？ 2.1 计算效率考量 2.2 任务特性适配 三、仅编码器架构：双向的理解专家 3.1 BERT的革命性设计 3.2 掩码语言模型 (MLM) 3.3 为什么BERT不能生成文本？ 3.4 实战：BERT文本分类 四、仅解码器架构：生成的王者 4.1 GPT的单向设计哲学 4.2 因果注意力机制 (Causal Mask) 4.3 当前视角：Decoder-only的全面胜利 4.4 实战：GPT文本生成 五、架构选型指南 5.1 三大架构对比表 5.2 决策树：我该用哪个？ 5.3 黄金组合：Encoder做索引，Decoder做生成 六、深度问答 本章概览
在第1章中，我们深入学习了Transformer的核心机制。但你是否好奇：为什么BERT擅长理解文本，而GPT擅长生成文本？为什么现在的新模型（如DeepSeek-V3, LLaMA-3）几乎全都是Decoder-only架构？
这一切的答案，藏在Transformer的三大架构分支中：
graph TD A[Transformer 原始架构<br>Encoder-Decoder 2017] --> B[仅编码器<br>Encoder-only] A --> C[仅解码器<br>Decoder-only] A --> D[编码器-解码器<br>Encoder-Decoder] B --> B1[BERT 2018<br>RoBERTa 2019<br>Embedding Models 2025] C --> C1[GPT-3/4 2020-2023<br>LLaMA-3 2024<br>DeepSeek-V3 2024] D --> D1[T5 2020<br>BART 2020<br>GLM-130B 2022] style A fill:#FFE4E1,stroke:#E87461 style B fill:#E8F5E9,stroke:#81C784 style C fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px style D fill:#FFF9C4,stroke:#FDD835 style C1 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px难度级别：⭐⭐（进阶）- 需要理解第1章的Transformer基础"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第2章 模型家族谱系：从编码器到解码器"><meta itemprop=description content="第2章：模型家族谱系：从编码器到解码器 (Model Architectures)# “The best way to predict the future is to invent it.” - Alan Kay
本章将带你理解Transformer的三大架构分支，掌握每种架构的设计哲学、技术细节和当前的主流选择，助你在实际应用中做出明智的架构选型。
目录# 一、共同的祖先：编码器-解码器架构 1.1 原始Transformer的双塔设计 1.2 编码器-解码器的工作流程 1.3 T5：现代编码器-解码器的代表 二、分裂与演化：为何不都用编码器-解码器？ 2.1 计算效率考量 2.2 任务特性适配 三、仅编码器架构：双向的理解专家 3.1 BERT的革命性设计 3.2 掩码语言模型 (MLM) 3.3 为什么BERT不能生成文本？ 3.4 实战：BERT文本分类 四、仅解码器架构：生成的王者 4.1 GPT的单向设计哲学 4.2 因果注意力机制 (Causal Mask) 4.3 当前视角：Decoder-only的全面胜利 4.4 实战：GPT文本生成 五、架构选型指南 5.1 三大架构对比表 5.2 决策树：我该用哪个？ 5.3 黄金组合：Encoder做索引，Decoder做生成 六、深度问答 本章概览
在第1章中，我们深入学习了Transformer的核心机制。但你是否好奇：为什么BERT擅长理解文本，而GPT擅长生成文本？为什么现在的新模型（如DeepSeek-V3, LLaMA-3）几乎全都是Decoder-only架构？
这一切的答案，藏在Transformer的三大架构分支中：
graph TD A[Transformer 原始架构<br>Encoder-Decoder 2017] --> B[仅编码器<br>Encoder-only] A --> C[仅解码器<br>Decoder-only] A --> D[编码器-解码器<br>Encoder-Decoder] B --> B1[BERT 2018<br>RoBERTa 2019<br>Embedding Models 2025] C --> C1[GPT-3/4 2020-2023<br>LLaMA-3 2024<br>DeepSeek-V3 2024] D --> D1[T5 2020<br>BART 2020<br>GLM-130B 2022] style A fill:#FFE4E1,stroke:#E87461 style B fill:#E8F5E9,stroke:#81C784 style C fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px style D fill:#FFF9C4,stroke:#FDD835 style C1 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px难度级别：⭐⭐（进阶）- 需要理解第1章的Transformer基础"><meta itemprop=wordCount content="1693"><title>第2章 模型家族谱系：从编码器到解码器 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle checked>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/ class=active>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第2章 模型家族谱系：从编码器到解码器</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一共同的祖先编码器-解码器架构>一、共同的祖先：编码器-解码器架构</a><ul><li><a href=#-深度解析encoder与decoder命名的历史真相>🎯 深度解析：Encoder与Decoder命名的历史真相</a><ul><li><a href=#命名的历史遗留>命名的历史遗留</a></li><li><a href=#transformer继承了命名但改变了本质>Transformer继承了命名，但改变了本质</a></li><li><a href=#命名的崩塌bert和gpt时代>命名的崩塌：BERT和GPT时代</a></li><li><a href=#正确理解decoder--masked-encoder>正确理解：Decoder = Masked Encoder</a></li><li><a href=#为什么不重新命名>为什么不重新命名？</a></li><li><a href=#更准确的命名方案>更准确的命名方案</a></li><li><a href=#面试标准回答>面试标准回答</a></li><li><a href=#关键洞察>关键洞察</a></li></ul></li><li><a href=#11-原始transformer的双塔设计>1.1 原始Transformer的双塔设计</a></li><li><a href=#12-编码器-解码器的工作流程>1.2 编码器-解码器的工作流程</a></li><li><a href=#13-t5现代编码器-解码器的代表>1.3 T5：现代编码器-解码器的代表</a></li></ul></li><li><a href=#二分裂与演化为何不都用编码器-解码器>二、分裂与演化：为何不都用编码器-解码器？</a><ul><li><a href=#21-计算效率考量>2.1 计算效率考量</a></li><li><a href=#22-任务特性适配>2.2 任务特性适配</a></li></ul></li><li><a href=#三仅编码器架构双向的理解专家>三、仅编码器架构：双向的理解专家</a><ul><li><a href=#31-bert的革命性设计>3.1 BERT的革命性设计</a></li><li><a href=#32-掩码语言模型-mlm>3.2 掩码语言模型 (MLM)</a></li><li><a href=#33-为什么bert不能生成文本>3.3 为什么BERT不能生成文本？</a></li><li><a href=#34-实战bert文本分类>3.4 实战：BERT文本分类</a></li></ul></li><li><a href=#四仅解码器架构生成的王者>四、仅解码器架构：生成的王者</a><ul><li><a href=#41-gpt的单向设计哲学>4.1 GPT的单向设计哲学</a></li><li><a href=#42-因果注意力机制-causal-mask>4.2 因果注意力机制 (Causal Mask)</a></li><li><a href=#43-当前视角decoder-only的全面胜利>4.3 当前视角：Decoder-only的全面胜利</a></li><li><a href=#45-dense-vs-sparsemoe架构的崛起>4.5 Dense vs Sparse：MoE架构的崛起</a><ul><li><a href=#什么是moe>什么是MoE？</a></li><li><a href=#dense-vs-sparse-对比>Dense vs Sparse 对比</a></li></ul></li><li><a href=#46-超越transformerssm与mamba的挑战>4.6 超越Transformer：SSM与Mamba的挑战</a><ul><li><a href=#状态空间模型ssm线性时间的挑战者>状态空间模型（SSM）：线性时间的挑战者</a></li></ul></li><li><a href=#44-实战gpt文本生成>4.4 实战：GPT文本生成</a></li></ul></li><li><a href=#五架构选型指南>五、架构选型指南</a><ul><li><a href=#51-三大架构对比表>5.1 三大架构对比表</a></li><li><a href=#52-决策树我该用哪个>5.2 决策树：我该用哪个？</a></li><li><a href=#53-黄金组合encoder做索引decoder做生成>5.3 黄金组合：Encoder做索引，Decoder做生成</a></li></ul></li><li><a href=#六深度问答>六、深度问答</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第2章模型家族谱系从编码器到解码器-model-architectures>第2章：模型家族谱系：从编码器到解码器 (Model Architectures)<a class=anchor href=#%e7%ac%ac2%e7%ab%a0%e6%a8%a1%e5%9e%8b%e5%ae%b6%e6%97%8f%e8%b0%b1%e7%b3%bb%e4%bb%8e%e7%bc%96%e7%a0%81%e5%99%a8%e5%88%b0%e8%a7%a3%e7%a0%81%e5%99%a8-model-architectures>#</a></h1><blockquote class=book-hint><p>&ldquo;The best way to predict the future is to invent it.&rdquo; - Alan Kay</p><p>本章将带你理解Transformer的三大架构分支，掌握每种架构的设计哲学、技术细节和当前的主流选择，助你在实际应用中做出明智的架构选型。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e5%85%b1%e5%90%8c%e7%9a%84%e7%a5%96%e5%85%88%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84>一、共同的祖先：编码器-解码器架构</a><ul><li><a href=#11-%e5%8e%9f%e5%a7%8btransformer%e7%9a%84%e5%8f%8c%e5%a1%94%e8%ae%be%e8%ae%a1>1.1 原始Transformer的双塔设计</a></li><li><a href=#12-%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>1.2 编码器-解码器的工作流程</a></li><li><a href=#13-t5%e7%8e%b0%e4%bb%a3%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e4%bb%a3%e8%a1%a8>1.3 T5：现代编码器-解码器的代表</a></li></ul></li><li><a href=#%e4%ba%8c%e5%88%86%e8%a3%82%e4%b8%8e%e6%bc%94%e5%8c%96%e4%b8%ba%e4%bd%95%e4%b8%8d%e9%83%bd%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8>二、分裂与演化：为何不都用编码器-解码器？</a><ul><li><a href=#21-%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e8%80%83%e9%87%8f>2.1 计算效率考量</a></li><li><a href=#22-%e4%bb%bb%e5%8a%a1%e7%89%b9%e6%80%a7%e9%80%82%e9%85%8d>2.2 任务特性适配</a></li></ul></li><li><a href=#%e4%b8%89%e4%bb%85%e7%bc%96%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e5%8f%8c%e5%90%91%e7%9a%84%e7%90%86%e8%a7%a3%e4%b8%93%e5%ae%b6>三、仅编码器架构：双向的理解专家</a><ul><li><a href=#31-bert%e7%9a%84%e9%9d%a9%e5%91%bd%e6%80%a7%e8%ae%be%e8%ae%a1>3.1 BERT的革命性设计</a></li><li><a href=#32-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-mlm>3.2 掩码语言模型 (MLM)</a></li><li><a href=#33-%e4%b8%ba%e4%bb%80%e4%b9%88bert%e4%b8%8d%e8%83%bd%e7%94%9f%e6%88%90%e6%96%87%e6%9c%ac>3.3 为什么BERT不能生成文本？</a></li><li><a href=#34-%e5%ae%9e%e6%88%98bert%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb>3.4 实战：BERT文本分类</a></li></ul></li><li><a href=#%e5%9b%9b%e4%bb%85%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e7%94%9f%e6%88%90%e7%9a%84%e7%8e%8b%e8%80%85>四、仅解码器架构：生成的王者</a><ul><li><a href=#41-gpt%e7%9a%84%e5%8d%95%e5%90%91%e8%ae%be%e8%ae%a1%e5%93%b2%e5%ad%a6>4.1 GPT的单向设计哲学</a></li><li><a href=#42-%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-causal-mask>4.2 因果注意力机制 (Causal Mask)</a></li><li><a href=#43-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92decoder-only%e7%9a%84%e5%85%a8%e9%9d%a2%e8%83%9c%e5%88%a9>4.3 当前视角：Decoder-only的全面胜利</a></li><li><a href=#44-%e5%ae%9e%e6%88%98gpt%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90>4.4 实战：GPT文本生成</a></li></ul></li><li><a href=#%e4%ba%94%e6%9e%b6%e6%9e%84%e9%80%89%e5%9e%8b%e6%8c%87%e5%8d%97>五、架构选型指南</a><ul><li><a href=#51-%e4%b8%89%e5%a4%a7%e6%9e%b6%e6%9e%84%e5%af%b9%e6%af%94%e8%a1%a8>5.1 三大架构对比表</a></li><li><a href=#52-%e5%86%b3%e7%ad%96%e6%a0%91%e6%88%91%e8%af%a5%e7%94%a8%e5%93%aa%e4%b8%aa>5.2 决策树：我该用哪个？</a></li><li><a href=#53-%e9%bb%84%e9%87%91%e7%bb%84%e5%90%88encoder%e5%81%9a%e7%b4%a2%e5%bc%95decoder%e5%81%9a%e7%94%9f%e6%88%90>5.3 黄金组合：Encoder做索引，Decoder做生成</a></li></ul></li><li><a href=#%e5%85%ad%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94>六、深度问答</a></li></ul><hr><p><strong>本章概览</strong></p><p>在第1章中，我们深入学习了Transformer的核心机制。但你是否好奇：<strong>为什么BERT擅长理解文本，而GPT擅长生成文本？为什么现在的新模型（如DeepSeek-V3, LLaMA-3）几乎全都是Decoder-only架构？</strong></p><p>这一切的答案，藏在Transformer的<strong>三大架构分支</strong>中：</p><pre class=mermaid>graph TD
    A[Transformer 原始架构&lt;br&gt;Encoder-Decoder 2017] --&gt; B[仅编码器&lt;br&gt;Encoder-only]
    A --&gt; C[仅解码器&lt;br&gt;Decoder-only]
    A --&gt; D[编码器-解码器&lt;br&gt;Encoder-Decoder]

    B --&gt; B1[BERT 2018&lt;br&gt;RoBERTa 2019&lt;br&gt;Embedding Models 2025]
    C --&gt; C1[GPT-3/4 2020-2023&lt;br&gt;LLaMA-3 2024&lt;br&gt;DeepSeek-V3 2024]
    D --&gt; D1[T5 2020&lt;br&gt;BART 2020&lt;br&gt;GLM-130B 2022]

    style A fill:#FFE4E1,stroke:#E87461
    style B fill:#E8F5E9,stroke:#81C784
    style C fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px
    style D fill:#FFF9C4,stroke:#FDD835
    style C1 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><p><strong>难度级别</strong>：⭐⭐（进阶）- 需要理解第1章的Transformer基础</p><hr><h2 id=一共同的祖先编码器-解码器架构>一、共同的祖先：编码器-解码器架构<a class=anchor href=#%e4%b8%80%e5%85%b1%e5%90%8c%e7%9a%84%e7%a5%96%e5%85%88%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84>#</a></h2><h3 id=-深度解析encoder与decoder命名的历史真相>🎯 深度解析：Encoder与Decoder命名的历史真相<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90encoder%e4%b8%8edecoder%e5%91%bd%e5%90%8d%e7%9a%84%e5%8e%86%e5%8f%b2%e7%9c%9f%e7%9b%b8>#</a></h3><p><strong>核心困惑</strong>：为什么GPT被称为"Decoder-only"，明明它没有"解码"任何输入源？</p><h4 id=命名的历史遗留>命名的历史遗留<a class=anchor href=#%e5%91%bd%e5%90%8d%e7%9a%84%e5%8e%86%e5%8f%b2%e9%81%97%e7%95%99>#</a></h4><p>这是一个<strong>Seq2Seq时代的命名惯性</strong>。让我们追溯历史：</p><p><strong>2014-2017：Seq2Seq的黄金时代</strong></p><p>在Transformer出现之前，机器翻译的主流范式是**序列到序列（Sequence-to-Sequence）**模型，由两个RNN组成：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>源语言: &#34;I love AI&#34;
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>【Encoder RNN】
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>固定长度的上下文向量 (Context Vector)
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>【Decoder RNN】
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>目标语言: &#34;我爱AI&#34;</span></span></code></pre></div><p>在这个语境下，命名非常直观：</p><ul><li><strong>Encoder（编码器）</strong>：将可变长度的<strong>源序列</strong>压缩成固定长度的向量</li><li><strong>Decoder（解码器）</strong>：将固定向量<strong>展开</strong>成可变长度的<strong>目标序列</strong></li></ul><p>关键：Decoder的输入<strong>不是</strong>目标序列本身，而是来自Encoder的编码结果。</p><h4 id=transformer继承了命名但改变了本质>Transformer继承了命名，但改变了本质<a class=anchor href=#transformer%e7%bb%a7%e6%89%bf%e4%ba%86%e5%91%bd%e5%90%8d%e4%bd%86%e6%94%b9%e5%8f%98%e4%ba%86%e6%9c%ac%e8%b4%a8>#</a></h4><p><strong>2017：Transformer论文</strong></p><p>当Vaswani等人提出Transformer时，他们保持了这套命名：</p><ul><li><strong>Encoder</strong>：双向注意力，处理源序列（如英文句子）</li><li><strong>Decoder</strong>：带Causal Mask的自注意力 + Cross-Attention，生成目标序列（如中文句子）</li></ul><p>但注意<strong>关键变化</strong>：Transformer的Decoder现在有<strong>两个注意力机制</strong>：</p><ol><li><strong>Masked Self-Attention</strong>：对目标序列自己进行因果注意力</li><li><strong>Cross-Attention</strong>：从Encoder获取源序列信息</li></ol><h4 id=命名的崩塌bert和gpt时代>命名的崩塌：BERT和GPT时代<a class=anchor href=#%e5%91%bd%e5%90%8d%e7%9a%84%e5%b4%a9%e5%a1%8cbert%e5%92%8cgpt%e6%97%b6%e4%bb%a3>#</a></h4><p><strong>2018：BERT（Encoder-only）</strong></p><p>BERT砍掉了Decoder，只保留了Encoder的双向注意力。这个命名还算合理——它确实是在"编码"输入。</p><p><strong>2018：GPT（Decoder-only）</strong></p><p>GPT砍掉了Encoder和Cross-Attention，只保留了Decoder的<strong>Masked Self-Attention</strong>。</p><p><strong>这里就出现了命名混乱</strong>：</p><ul><li>GPT没有"源序列"和"目标序列"的区分</li><li>它的输入和输出是<strong>同一个序列</strong>（自回归生成）</li><li>它没有任何"解码"操作（没有从压缩向量还原信息）</li></ul><p><strong>那为什么还叫Decoder-only？</strong></p><p>因为它使用的是<strong>Transformer原始论文中Decoder部分的Self-Attention机制</strong>（Causal Mask）。</p><h4 id=正确理解decoder--masked-encoder>正确理解：Decoder = Masked Encoder<a class=anchor href=#%e6%ad%a3%e7%a1%ae%e7%90%86%e8%a7%a3decoder--masked-encoder>#</a></h4><p><strong>本质揭秘</strong>：</p><p>从技术实现看，GPT的"Decoder"就是<strong>加了Causal Mask的Encoder</strong>。</p><table><thead><tr><th>特性</th><th>BERT (Encoder)</th><th>GPT (Decoder)</th><th>本质区别</th></tr></thead><tbody><tr><td><strong>Self-Attention</strong></td><td>双向（Full Mask）</td><td>单向（Causal Mask）</td><td>注意力掩码不同</td></tr><tr><td><strong>Position Encoding</strong></td><td>✅</td><td>✅</td><td>相同</td></tr><tr><td><strong>FFN</strong></td><td>✅</td><td>✅</td><td>相同</td></tr><tr><td><strong>LayerNorm</strong></td><td>✅</td><td>✅</td><td>相同</td></tr><tr><td><strong>Cross-Attention</strong></td><td>❌</td><td>❌ (在GPT中被移除)</td><td>都没有</td></tr></tbody></table><p><strong>代码验证</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_attention_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>is_encoder</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;创建注意力掩码&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>is_encoder</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Encoder: 全1矩阵（双向）</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Decoder: 下三角矩阵（因果）</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对比Encoder和Decoder的唯一区别</span>
</span></span><span class=line><span class=cl><span class=n>encoder_mask</span> <span class=o>=</span> <span class=n>create_attention_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>is_encoder</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>decoder_mask</span> <span class=o>=</span> <span class=n>create_attention_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>is_encoder</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Encoder Mask (BERT):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>encoder_mask</span><span class=o>.</span><span class=n>int</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Decoder Mask (GPT):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decoder_mask</span><span class=o>.</span><span class=n>int</span><span class=p>())</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Encoder Mask (BERT):
</span></span><span class=line><span class=cl>tensor([[1, 1, 1, 1, 1],  ← 每个token可以看到所有token
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 1],
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 1],
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 1],
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 1]])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Decoder Mask (GPT):
</span></span><span class=line><span class=cl>tensor([[1, 0, 0, 0, 0],  ← 每个token只能看到左边
</span></span><span class=line><span class=cl>        [1, 1, 0, 0, 0],
</span></span><span class=line><span class=cl>        [1, 1, 1, 0, 0],
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 0],
</span></span><span class=line><span class=cl>        [1, 1, 1, 1, 1]])</span></span></code></pre></div><p><strong>结论</strong>：GPT的"Decoder"就是<strong>Masked Encoder</strong>。</p><h4 id=为什么不重新命名>为什么不重新命名？<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e9%87%8d%e6%96%b0%e5%91%bd%e5%90%8d>#</a></h4><p><strong>三个原因</strong>：</p><ol><li><strong>历史惯性</strong>：学术界已经习惯了这套术语</li><li><strong>论文引用链</strong>：保持命名一致性，方便追溯</li><li><strong>功能区分</strong>：虽然本质相同，但"Encoder-only"和"Decoder-only"的名字确实暗示了它们的<strong>训练目标</strong>不同：<ul><li>Encoder（BERT）：预测被Mask的token（双向上下文）</li><li>Decoder（GPT）：预测下一个token（单向因果）</li></ul></li></ol><h4 id=更准确的命名方案>更准确的命名方案<a class=anchor href=#%e6%9b%b4%e5%87%86%e7%a1%ae%e7%9a%84%e5%91%bd%e5%90%8d%e6%96%b9%e6%a1%88>#</a></h4><p>如果重新命名，学术界可能会这样称呼：</p><table><thead><tr><th>现在的名字</th><th>更准确的名字</th><th>核心特征</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>Bidirectional Transformer</strong></td><td>双向注意力</td></tr><tr><td><strong>Decoder-only</strong></td><td><strong>Causal Transformer</strong></td><td>因果注意力</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td><strong>Cross-Attention Transformer</strong></td><td>交叉注意力</td></tr></tbody></table><p>但这些新名字已经错过了历史时机。</p><h4 id=面试标准回答>面试标准回答<a class=anchor href=#%e9%9d%a2%e8%af%95%e6%a0%87%e5%87%86%e5%9b%9e%e7%ad%94>#</a></h4><p><strong>Q: 为什么GPT叫Decoder-only，它明明没有Encoder？</strong></p><p><strong>A</strong>: 这是Seq2Seq时代的命名遗留。在原始Transformer中，Decoder使用Causal Mask的Self-Attention + Cross-Attention。GPT移除了Cross-Attention，只保留了Causal Self-Attention，因此被称为"Decoder-only"。从技术本质上讲，GPT的"Decoder"就是<strong>加了Causal Mask的Encoder</strong>，两者的唯一区别是注意力掩码的形状（全1矩阵 vs 下三角矩阵）。</p><h4 id=关键洞察>关键洞察<a class=anchor href=#%e5%85%b3%e9%94%ae%e6%b4%9e%e5%af%9f>#</a></h4><p><strong>记住一句话</strong>：</p><blockquote class=book-hint><p><strong>Decoder-only ≠ 解码器，而是 Masked Encoder = 因果注意力的Transformer</strong></p></blockquote><p>这个命名更多是历史传承，而非技术本质。</p><hr><h3 id=11-原始transformer的双塔设计>1.1 原始Transformer的双塔设计<a class=anchor href=#11-%e5%8e%9f%e5%a7%8btransformer%e7%9a%84%e5%8f%8c%e5%a1%94%e8%ae%be%e8%ae%a1>#</a></h3><p>2017年，论文《Attention is All You Need》提出的Transformer，采用的是**编码器-解码器（Encoder-Decoder）**结构，最初是为了解决机器翻译任务：</p><pre class=mermaid>graph LR
    subgraph Encoder[编码器：理解源语言]
    A[输入: I love AI] --&gt; B[双向Attention]
    B --&gt; C[语义向量]
    end

    subgraph Decoder[解码器：生成目标语言]
    D[输入: &lt;start&gt;] --&gt; E[Masked Attention]
    E --&gt; F[Cross Attention]
    C -.-&gt; F
    F --&gt; G[输出: 我]
    G -.-&gt; D
    end

    style Encoder fill:#E8F5E9
    style Decoder fill:#E3F2FD</pre><p><strong>核心设计理念</strong>：</p><ul><li><strong>编码器（Encoder）</strong>：使用<strong>双向注意力</strong>，同时看到整个输入句子，提取深层语义。</li><li><strong>解码器（Decoder）</strong>：使用<strong>因果注意力</strong>（只看左边），自回归地生成目标句子。</li><li><strong>交叉注意力（Cross-Attention）</strong>：连接两座塔的桥梁，让解码器在生成每个字时都能"回头看"编码器的信息。</li></ul><h3 id=12-编码器-解码器的工作流程>1.2 编码器-解码器的工作流程<a class=anchor href=#12-%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>#</a></h3><p>以翻译 &ldquo;I love AI&rdquo; -> &ldquo;我爱AI&rdquo; 为例：</p><ol><li><strong>编码阶段</strong>：Encoder 读入完整句子 &ldquo;I love AI&rdquo;，将其转化为一系列高维向量（Memory）。</li><li><strong>解码阶段 - Step 1</strong>：Decoder 接收 <code>&lt;start></code>，此时通过 <strong>Cross-Attention</strong> 查询 Memory，重点关注 &ldquo;I&rdquo;，输出 &ldquo;我&rdquo;。</li><li><strong>解码阶段 - Step 2</strong>：Decoder 接收 <code>&lt;start> 我</code>，关注 &ldquo;love&rdquo;，输出 &ldquo;爱&rdquo;。</li><li><strong>解码阶段 - Step 3</strong>：Decoder 接收 <code>&lt;start> 我 爱</code>，关注 &ldquo;AI&rdquo;，输出 &ldquo;AI&rdquo;。</li></ol><h3 id=13-t5现代编码器-解码器的代表>1.3 T5：现代编码器-解码器的代表<a class=anchor href=#13-t5%e7%8e%b0%e4%bb%a3%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e7%9a%84%e4%bb%a3%e8%a1%a8>#</a></h3><p>Google 在 2020 年提出的 <strong>T5 (Text-to-Text Transfer Transformer)</strong> 将这种架构推向了极致。它提出一个核心观点：<strong>所有NLP任务都可以视为文本到文本的转换</strong>。</p><ul><li><strong>翻译</strong>：<code>translate English to German: That is good.</code> -> <code>Das ist gut.</code></li><li><strong>分类</strong>：<code>cola sentence: The course is jumping well.</code> -> <code>not acceptable</code></li><li><strong>摘要</strong>：<code>summarize: state authorities dispatched emergency crews...</code> -> <code>six people hospitalized...</code></li></ul><p>T5 证明了 Encoder-Decoder 架构在多任务统一上的强大能力，特别是对于输入输出都需要理解的任务（如翻译、摘要）。</p><hr><h2 id=二分裂与演化为何不都用编码器-解码器>二、分裂与演化：为何不都用编码器-解码器？<a class=anchor href=#%e4%ba%8c%e5%88%86%e8%a3%82%e4%b8%8e%e6%bc%94%e5%8c%96%e4%b8%ba%e4%bd%95%e4%b8%8d%e9%83%bd%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8>#</a></h2><p>既然 Encoder-Decoder 全能，为什么后来分化出了 BERT 和 GPT？</p><h3 id=21-计算效率考量>2.1 计算效率考量<a class=anchor href=#21-%e8%ae%a1%e7%ae%97%e6%95%88%e7%8e%87%e8%80%83%e9%87%8f>#</a></h3><p>Encoder-Decoder 需要维护两套参数（编码器和解码器），且推理时需要两个模块协同工作。</p><ul><li>对于<strong>分类任务</strong>（如情感分析），只需要输出一个标签，用 Decoder 是杀鸡用牛刀。</li><li>对于<strong>生成任务</strong>（如写小说），输入就是上文，不需要单独的 Encoder 去编码一个"源文本"，直接用 Decoder 自回归即可。</li></ul><h3 id=22-任务特性适配>2.2 任务特性适配<a class=anchor href=#22-%e4%bb%bb%e5%8a%a1%e7%89%b9%e6%80%a7%e9%80%82%e9%85%8d>#</a></h3><ul><li><strong>理解密集型</strong>：情感分析、实体识别、文本匹配。需要<strong>双向</strong>上下文（既看左也看右）。👉 <strong>Encoder-only (BERT)</strong></li><li><strong>生成密集型</strong>：创意写作、代码补全、对话。需要<strong>自回归</strong>生成（只能看左，不能剧透）。👉 <strong>Decoder-only (GPT)</strong></li></ul><hr><h2 id=三仅编码器架构双向的理解专家>三、仅编码器架构：双向的理解专家<a class=anchor href=#%e4%b8%89%e4%bb%85%e7%bc%96%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e5%8f%8c%e5%90%91%e7%9a%84%e7%90%86%e8%a7%a3%e4%b8%93%e5%ae%b6>#</a></h2><h3 id=31-bert的革命性设计>3.1 BERT的革命性设计<a class=anchor href=#31-bert%e7%9a%84%e9%9d%a9%e5%91%bd%e6%80%a7%e8%ae%be%e8%ae%a1>#</a></h3><p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong> 砍掉了 Transformer 的解码器，只保留编码器。</p><p><strong>核心武器：双向注意力 (Bidirectional Attention)</strong>
GPT 只能从左到右看（为了生成），这导致它在理解语境时有缺陷。例如：</p><blockquote class=book-hint><p>&ldquo;I went to the <strong>bank</strong> to deposit money.&rdquo;
&ldquo;I went to the <strong>bank</strong> of the river.&rdquo;</p></blockquote><p>BERT 能同时看到 &ldquo;bank&rdquo; 左右的词（&ldquo;deposit money&rdquo; 或 &ldquo;river&rdquo;），从而精准判断 &ldquo;bank&rdquo; 是"银行"还是"河岸"。</p><h3 id=32-掩码语言模型-mlm>3.2 掩码语言模型 (MLM)<a class=anchor href=#32-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-mlm>#</a></h3><p>为了训练双向模型，BERT 发明了 <strong>MLM (Masked Language Modeling)</strong> 任务，类似"完形填空"：</p><blockquote class=book-hint><p>原句：The cat sat on the mat.</p><p>输入：The cat [MASK] on the mat.</p><p>目标：预测 [MASK] 是 &ldquo;sat&rdquo;。</p></blockquote><p><strong>代码实战：BERT 做完形填空</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fill_mask</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;fill-mask&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;bert-base-uncased&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>fill_mask</span><span class=p>(</span><span class=s2>&#34;Paris is the [MASK] of France.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: [{&#39;score&#39;: 0.99, &#39;token_str&#39;: &#39;capital&#39;, ...}]</span></span></span></code></pre></div><h3 id=33-为什么bert不能生成文本>3.3 为什么BERT不能生成文本？<a class=anchor href=#33-%e4%b8%ba%e4%bb%80%e4%b9%88bert%e4%b8%8d%e8%83%bd%e7%94%9f%e6%88%90%e6%96%87%e6%9c%ac>#</a></h3><p>这是一个常见的面试题：<strong>BERT 既然理解能力这么强，为什么不能用来写文章？</strong></p><ol><li><strong>预训练目标不同</strong>：BERT 训练的是"填空"能力，不是"预测下一个词"的能力。它习惯了看到完整的上下文。</li><li><strong>双向注意力的泄露</strong>：在生成时，如果使用双向注意力，生成第 $t$ 个词时会看到第 $t+1$ 个词的信息（这在训练时是存在的，但在真实生成时通过 Mask 可以在技术上屏蔽，但模型并未适应这种单向语境）。</li><li><strong>独立性假设</strong>：BERT 预测 [MASK] 时假设各个 [MASK] 是独立的（在非自回归变体中），而文本生成强依赖词与词的顺序关联。</li></ol><h3 id=34-实战bert文本分类>3.4 实战：BERT文本分类<a class=anchor href=#34-%e5%ae%9e%e6%88%98bert%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb>#</a></h3><p>BERT 最擅长的是将变长的文本压缩成一个定长的向量（通常取 <code>[CLS]</code> token 的向量），用于分类。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span><span class=p>,</span> <span class=n>BertForSequenceClassification</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载模型</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;bert-base-uncased&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BertForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 准备输入</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;This movie is absolutely fantastic!&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 前向传播</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>    <span class=n>prediction</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预测类别: </span><span class=si>{</span><span class=n>prediction</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=四仅解码器架构生成的王者>四、仅解码器架构：生成的王者<a class=anchor href=#%e5%9b%9b%e4%bb%85%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84%e7%94%9f%e6%88%90%e7%9a%84%e7%8e%8b%e8%80%85>#</a></h2><h3 id=41-gpt的单向设计哲学>4.1 GPT的单向设计哲学<a class=anchor href=#41-gpt%e7%9a%84%e5%8d%95%e5%90%91%e8%ae%be%e8%ae%a1%e5%93%b2%e5%ad%a6>#</a></h3><p><strong>GPT (Generative Pre-trained Transformer)</strong> 砍掉了 Transformer 的编码器，只保留解码器。</p><p>它的哲学非常简单粗暴：<strong>预测下一个词 (Next Token Prediction)</strong>。
不需要复杂的 Mask 策略，不需要区分源文本和目标文本，就是把整个互联网的文本读一遍，尝试预测每一个词的下一个词是什么。</p><p>$$ P(\text{text}) = \prod_{i=1}^{n} P(w_i | w_1, &mldr;, w_{i-1}) $$</p><h3 id=42-因果注意力机制-causal-mask>4.2 因果注意力机制 (Causal Mask)<a class=anchor href=#42-%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-causal-mask>#</a></h3><p>GPT 的核心在于<strong>Causal Mask（因果掩码）</strong>，保证预测 $w_i$ 时只能看到 $w_1$ 到 $w_{i-1}$，绝不能看到后面。</p><p><strong>Causal Mask 的形状与实现</strong>：
它是一个下三角矩阵（上三角部分被处理为负无穷大，Softmax 后变为 0）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_causal_mask</span><span class=p>(</span><span class=n>size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 创建一个全1矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 提取下三角，上三角置0</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：长度为5的序列</span>
</span></span><span class=line><span class=cl><span class=c1># 行i表示第i个token，列j表示它关注的token</span>
</span></span><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>create_causal_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>输出:
</span></span></span><span class=line><span class=cl><span class=s2>tensor([[1., 0., 0., 0., 0.],  # 第1个词只能看自己
</span></span></span><span class=line><span class=cl><span class=s2>        [1., 1., 0., 0., 0.],  # 第2个词看前2个
</span></span></span><span class=line><span class=cl><span class=s2>        [1., 1., 1., 0., 0.],
</span></span></span><span class=line><span class=cl><span class=s2>        [1., 1., 1., 1., 0.],
</span></span></span><span class=line><span class=cl><span class=s2>        [1., 1., 1., 1., 1.]]) # 第5个词看全部
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><h3 id=43-当前视角decoder-only的全面胜利>4.3 当前视角：Decoder-only的全面胜利<a class=anchor href=#43-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92decoder-only%e7%9a%84%e5%85%a8%e9%9d%a2%e8%83%9c%e5%88%a9>#</a></h3><p>在 2018-2020 年，BERT 和 GPT 各领风骚。但在当前，<strong>Decoder-only 架构已经统治了通用大模型领域</strong>。</p><p><strong>主流模型清单 (SOTA)</strong>：</p><ul><li><strong>OpenAI GPT-4/o1</strong>：Decoder-only</li><li><strong>Meta LLaMA 3</strong>：Decoder-only</li><li><strong>DeepSeek-V3/R1</strong>：Decoder-only (配合 MoE)</li><li><strong>Anthropic Claude 3.5</strong>：Decoder-only</li></ul><p><strong>胜出原因</strong>：</p><ol><li><strong>Scaling Law (扩展定律)</strong>：研究发现，在极大规模参数下，Causal Decoder 的训练效率最高，最容易扩展。BERT 的 MLM 任务每个样本只能学习 15% 的 token（被 mask 的那些），而 GPT 的 Next Token Prediction 每个样本的所有 token 都能产生 loss，<strong>数据利用率更高</strong>。</li><li><strong>In-Context Learning (上下文学习)</strong>：这是 GPT-3 发现的涌现能力。Decoder-only 架构天然符合人类"说话"的顺序，给它几个示例（Few-shot），它就能依葫芦画瓢地生成后续内容。这种通用性通过 Prompt 实现了无需微调即可处理多种任务。</li><li><strong>推理优化的成熟</strong>：KV Cache 等技术让 Decoder-only 的生成速度大幅提升，工业界围绕这一架构建立的生态（vLLM, TensorRT-LLM）最为完善。</li></ol><blockquote class=book-hint><p><strong>注意</strong>：DeepSeek-V3 虽然引入了 MLA (Multi-head Latent Attention) 等改进，DeepSeek-R1 引入了强化学习训练推理能力，但它们的骨架依然是 <strong>Decoder-only Transformer</strong>。这一点至关重要。</p></blockquote><h3 id=45-dense-vs-sparsemoe架构的崛起>4.5 Dense vs Sparse：MoE架构的崛起<a class=anchor href=#45-dense-vs-sparsemoe%e6%9e%b6%e6%9e%84%e7%9a%84%e5%b4%9b%e8%b5%b7>#</a></h3><p>在 Decoder-only 架构的主流之下，近年出现了一个重要的分支创新：<strong>混合专家 (Mixture of Experts, MoE)</strong> 架构。</p><h4 id=什么是moe>什么是MoE？<a class=anchor href=#%e4%bb%80%e4%b9%88%e6%98%afmoe>#</a></h4><p>传统Transformer是<strong>密集模型（Dense Model）</strong>：每个token都经过所有参数的计算。而MoE是<strong>稀疏模型（Sparse Model）</strong>：模型拥有大量参数，但每个token只激活其中一小部分。</p><pre class=mermaid>graph LR
    subgraph Dense Model
    A[输入Token] --&gt; B[全部参数&lt;br&gt;70B]
    B --&gt; C[输出]
    end

    subgraph MoE Model
    D[输入Token] --&gt; E[路由器&lt;br&gt;Router]
    E --&gt; F1[专家1&lt;br&gt;7B]
    E --&gt; F2[专家2&lt;br&gt;7B]
    E --&gt; F8[...&lt;br&gt;专家8]
    F1 &amp; F2 --&gt; G[输出]
    end

    style Dense fill:#FFE4E1
    style MoE fill:#E3F2FD</pre><p><strong>核心原理</strong>：</p><ul><li><strong>路由机制（Gating）</strong>：对于每个token，通过一个小型路由网络决定激活哪几个专家。</li><li><strong>稀疏激活</strong>：通常只激活Top-K个专家（如K=2，即8个专家中选2个）。</li><li><strong>专家专业化</strong>：不同专家自然地学习到不同领域的知识（如代码、数学、语言等）。</li></ul><h4 id=dense-vs-sparse-对比>Dense vs Sparse 对比<a class=anchor href=#dense-vs-sparse-%e5%af%b9%e6%af%94>#</a></h4><table><thead><tr><th>特性</th><th>Dense Model (LLaMA 3)</th><th>Sparse MoE (DeepSeek-V3)</th></tr></thead><tbody><tr><td><strong>总参数量</strong></td><td>70B</td><td>671B</td></tr><tr><td><strong>激活参数量</strong></td><td>70B (100%)</td><td>37B (5.5%)</td></tr><tr><td><strong>推理成本</strong></td><td>高（所有参数参与）</td><td>中等（相当于37B Dense）</td></tr><tr><td><strong>训练成本</strong></td><td>中等</td><td>高（需要Expert Parallelism）</td></tr><tr><td><strong>性能</strong></td><td>优秀</td><td>卓越（超越GPT-4）</td></tr><tr><td><strong>显存占用</strong></td><td>140GB (FP16)</td><td>1.3TB (需要分布式)</td></tr></tbody></table><p><strong>代码示例：MoE的路由逻辑</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MoELayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;混合专家层&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_experts</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span> <span class=n>expert_dim</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>4096</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>top_k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_experts</span> <span class=o>=</span> <span class=n>num_experts</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>top_k</span> <span class=o>=</span> <span class=n>top_k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 路由器：为每个token选择专家</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gate</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>expert_dim</span><span class=p>,</span> <span class=n>num_experts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 专家网络（简化为单层FFN）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>experts</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>expert_dim</span><span class=p>,</span> <span class=n>expert_dim</span> <span class=o>*</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>expert_dim</span> <span class=o>*</span> <span class=mi>4</span><span class=p>,</span> <span class=n>expert_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_experts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        x: (batch_size, seq_len, expert_dim)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 路由决策：为每个token选择Top-K专家</span>
</span></span><span class=line><span class=cl>        <span class=n>gate_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gate</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B, L, num_experts)</span>
</span></span><span class=line><span class=cl>        <span class=n>gate_probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>gate_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 选择Top-K专家</span>
</span></span><span class=line><span class=cl>        <span class=n>top_k_probs</span><span class=p>,</span> <span class=n>top_k_indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>gate_probs</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>top_k</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>top_k_probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>top_k_probs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 重新归一化</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 稀疏激活：只计算被选中的专家</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>top_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>expert_idx</span> <span class=o>=</span> <span class=n>top_k_indices</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span>  <span class=c1># (B, L)</span>
</span></span><span class=line><span class=cl>            <span class=n>expert_weight</span> <span class=o>=</span> <span class=n>top_k_probs</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (B, L, 1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 批量处理：将同一专家的token收集在一起</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>expert_id</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_experts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>expert_idx</span> <span class=o>==</span> <span class=n>expert_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>mask</span><span class=o>.</span><span class=n>any</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                    <span class=n>expert_input</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                    <span class=n>expert_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>experts</span><span class=p>[</span><span class=n>expert_id</span><span class=p>](</span><span class=n>expert_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>output</span><span class=p>[</span><span class=n>mask</span><span class=p>]</span> <span class=o>+=</span> <span class=n>expert_output</span> <span class=o>*</span> <span class=n>expert_weight</span><span class=p>[</span><span class=n>mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>moe_layer</span> <span class=o>=</span> <span class=n>MoELayer</span><span class=p>(</span><span class=n>num_experts</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>expert_dim</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>4096</span><span class=p>)</span>  <span class=c1># (batch=2, seq_len=10, dim=4096)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>moe_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输入形状: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输出形状: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;激活参数比例: </span><span class=si>{</span><span class=mi>2</span><span class=o>/</span><span class=mi>8</span><span class=si>:</span><span class=s2>.1%</span><span class=si>}</span><span class=s2> (Top-2 / 8 experts)&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>MoE的训练挑战与解决方案</strong>：</p><ol><li><p><strong>负载均衡问题</strong>：所有token都选择同一个专家，导致其他专家"失业"。</p><ul><li><strong>解决</strong>：添加Load Balancing Loss，惩罚不均衡的专家选择。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_balancing_loss</span><span class=p>(</span><span class=n>gate_logits</span><span class=p>,</span> <span class=n>num_experts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    确保每个专家被平均使用
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算每个专家被选择的频率</span>
</span></span><span class=line><span class=cl>    <span class=n>expert_counts</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>gate_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>target_counts</span> <span class=o>=</span> <span class=n>gate_logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>gate_logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>num_experts</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># L2损失</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>((</span><span class=n>expert_counts</span> <span class=o>-</span> <span class=n>target_counts</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span></span></span></code></pre></div></li><li><p><strong>通信开销</strong>：专家分布在不同GPU上，需要频繁通信。</p><ul><li><strong>解决</strong>：Expert Parallelism（专家并行）+ All-to-All通信优化。</li></ul></li><li><p><strong>推理部署</strong>：显存占用大（需要加载所有专家）。</p><ul><li><strong>解决</strong>：量化压缩（INT8/INT4）+ 专家卸载到CPU/磁盘。</li></ul></li></ol><p><strong>实际案例</strong>：</p><table><thead><tr><th>模型</th><th>总参数</th><th>激活参数</th><th>专家数</th><th>Top-K</th><th>性能亮点</th></tr></thead><tbody><tr><td><strong>Mixtral 8x7B</strong></td><td>46.7B</td><td>13B</td><td>8</td><td>2</td><td>接近LLaMA-70B性能，推理快5倍</td></tr><tr><td><strong>DeepSeek-V3</strong></td><td>671B</td><td>37B</td><td>256</td><td>8</td><td>超越GPT-4，训练成本仅$5.5M</td></tr><tr><td><strong>Qwen2.5-MoE</strong></td><td>14.3B</td><td>2.7B</td><td>60</td><td>8</td><td>2.7B激活达到7B Dense性能</td></tr></tbody></table><p><strong>为什么MoE近年爆发？</strong></p><ol><li><strong>成本优势</strong>：训练671B MoE的成本 &lt; 训练175B Dense。</li><li><strong>推理效率</strong>：激活参数少，推理速度快。</li><li><strong>容量优势</strong>：更多参数意味着更大的知识容量。</li><li><strong>分布式训练成熟</strong>：DeepSpeed、Megatron等框架原生支持MoE。</li></ol><blockquote class=book-hint><p><strong>深入学习</strong>：MoE的完整训练策略、路由机制设计、专家并行实现详见 [Part 7 第2章：新型架构探索]。</p></blockquote><h3 id=46-超越transformerssm与mamba的挑战>4.6 超越Transformer：SSM与Mamba的挑战<a class=anchor href=#46-%e8%b6%85%e8%b6%8atransformerssm%e4%b8%8emamba%e7%9a%84%e6%8c%91%e6%88%98>#</a></h3><p>虽然Transformer统治了当前的LLM领域，但研究者们一直在探索<strong>非Transformer架构</strong>，试图解决Transformer的固有缺陷：</p><p><strong>Transformer的痛点</strong>：</p><ul><li><strong>二次方复杂度</strong>：注意力机制的计算量是 $O(n^2)$，处理长文本（如100K tokens）时非常慢。</li><li><strong>KV Cache膨胀</strong>：生成长文本时，KV Cache占用的显存随序列长度线性增长。</li></ul><h4 id=状态空间模型ssm线性时间的挑战者>状态空间模型（SSM）：线性时间的挑战者<a class=anchor href=#%e7%8a%b6%e6%80%81%e7%a9%ba%e9%97%b4%e6%a8%a1%e5%9e%8bssm%e7%ba%bf%e6%80%a7%e6%97%b6%e9%97%b4%e7%9a%84%e6%8c%91%e6%88%98%e8%80%85>#</a></h4><p><strong>核心思想</strong>：用状态空间方程替代注意力机制，将序列建模问题转化为线性系统。</p><p>$$
\begin{cases}
h_t = A h_{t-1} + B x_t \
y_t = C h_t + D x_t
\end{cases}
$$</p><p>其中：</p><ul><li>$h_t$：隐状态（类似RNN）</li><li>$A, B, C, D$：可学习的状态转移矩阵</li><li>关键：可以通过卷积形式高效实现，复杂度为 $O(n \log n)$</li></ul><p><strong>Mamba (2023)</strong>：最成功的SSM架构</p><p>Mamba通过**选择性SSM（Selective SSM）**解决了传统SSM无法处理长距离依赖的问题：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Mamba的核心思想（概念代码，非实际实现）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MambaBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Mamba块：选择性状态空间模型
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_state</span><span class=o>=</span><span class=mi>16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_state</span> <span class=o>=</span> <span class=n>d_state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 选择性参数：根据输入动态调整SSM参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_state</span> <span class=o>*</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 状态转移矩阵（可学习）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>A</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_state</span><span class=p>,</span> <span class=n>d_state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>B</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_state</span><span class=p>,</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>C</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        x: (batch, seq_len, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>L</span><span class=p>,</span> <span class=n>D</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 选择性机制：根据输入调整SSM参数</span>
</span></span><span class=line><span class=cl>        <span class=n>ssm_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B, L, d_state*3)</span>
</span></span><span class=line><span class=cl>        <span class=n>delta</span><span class=p>,</span> <span class=n>B_sel</span><span class=p>,</span> <span class=n>C_sel</span> <span class=o>=</span> <span class=n>ssm_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 状态空间递归（简化版）</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>L</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 选择性更新状态</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>delta</span><span class=p>[:,</span> <span class=n>t</span><span class=p>])</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>A</span> <span class=o>@</span> <span class=n>h</span><span class=p>)</span> <span class=o>+</span> <span class=n>B_sel</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span> <span class=o>*</span> <span class=n>x</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>y</span> <span class=o>=</span> <span class=n>C_sel</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span> <span class=o>@</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span></span></span></code></pre></div><p><strong>Transformer vs SSM vs Mamba 对比</strong></p><table><thead><tr><th>特性</th><th>Transformer</th><th>传统SSM (S4)</th><th>Mamba</th></tr></thead><tbody><tr><td><strong>时间复杂度</strong></td><td>$O(n^2)$</td><td>$O(n \log n)$</td><td>$O(n)$</td></tr><tr><td><strong>长文本处理</strong></td><td>慢（>8K困难）</td><td>中等</td><td>快（支持1M+）</td></tr><tr><td><strong>并行训练</strong></td><td>优秀</td><td>中等</td><td>优秀</td></tr><tr><td><strong>上下文学习</strong></td><td>强</td><td>弱</td><td>中等</td></tr><tr><td><strong>生态成熟度</strong></td><td>极高</td><td>低</td><td>低</td></tr></tbody></table><p><strong>Mamba的实际表现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 性能对比（来自Mamba论文，2024）</span>
</span></span><span class=line><span class=cl><span class=n>benchmark_results</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;模型&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;Transformer-7B&#34;</span><span class=p>,</span> <span class=s2>&#34;Mamba-7B&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;训练速度&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;1x&#34;</span><span class=p>,</span> <span class=s2>&#34;3x&#34;</span><span class=p>],</span>  <span class=c1># Mamba快3倍</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;推理吞吐量（8K seq）&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;1x&#34;</span><span class=p>,</span> <span class=s2>&#34;5x&#34;</span><span class=p>],</span>  <span class=c1># 长序列推理Mamba快5倍</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;显存占用（100K seq）&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;OOM&#34;</span><span class=p>,</span> <span class=s2>&#34;正常&#34;</span><span class=p>],</span>  <span class=c1># Transformer爆显存</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;语言建模PPL&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>10.5</span><span class=p>,</span> <span class=mf>10.8</span><span class=p>],</span>  <span class=c1># 性能接近</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>为什么Mamba当前还未取代Transformer？</strong></p><ol><li><strong>In-Context Learning弱</strong>：Mamba在Few-shot学习上不如Transformer，这是LLM的核心能力。</li><li><strong>生态不足</strong>：缺少成熟的工具链（如vLLM、TensorRT-LLM对Mamba的支持有限）。</li><li><strong>训练稳定性</strong>：大规模训练时，Mamba的稳定性不如Transformer。</li><li><strong>投资惯性</strong>：工业界已在Transformer上投入数十亿美元，迁移成本高。</li></ol><p><strong>未来展望</strong>：</p><ul><li><strong>混合架构</strong>：Transformer（处理短上下文）+ Mamba（处理长上下文），取长补短。</li><li><strong>多模态优势</strong>：Mamba在视频、音频等长序列模态上可能更有优势。</li></ul><blockquote class=book-hint><p><strong>深入学习</strong>：SSM的数学原理、Mamba的完整实现、混合架构设计详见 [Part 7 第2章：新型架构探索]。</p></blockquote><p><strong>本节小结</strong>：</p><p>当前的架构格局：</p><ul><li><strong>主流</strong>：Decoder-only Transformer（GPT、LLaMA系列）</li><li><strong>前沿</strong>：MoE稀疏架构（DeepSeek-V3、Mixtral）</li><li><strong>挑战者</strong>：SSM/Mamba（线性复杂度，长上下文优势）</li><li><strong>特定领域</strong>：Encoder-only（Embedding）、Encoder-Decoder（多模态）</li></ul><p>选择建议：</p><ul><li><strong>通用LLM</strong>：Decoder-only Transformer（生态最成熟）</li><li><strong>超大规模</strong>：MoE架构（成本效率高）</li><li><strong>超长上下文</strong>：混合架构（Transformer + Mamba）</li><li><strong>嵌入任务</strong>：Encoder-only（BERT变体）</li></ul><h3 id=44-实战gpt文本生成>4.4 实战：GPT文本生成<a class=anchor href=#44-%e5%ae%9e%e6%88%98gpt%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90>#</a></h3><p>使用 <code>AutoModelForCausalLM</code> 进行生成。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>GPT2LMHeadModel</span><span class=p>,</span> <span class=n>GPT2Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;gpt2&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>GPT2Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>GPT2LMHeadModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;The future of AI is&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成配置</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span>    <span class=c1># 控制随机性：越低越保守，越高越奔放</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>          <span class=c1># 核采样：保留累积概率90%的词</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span>      <span class=c1># 启用采样，否则是贪婪搜索</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 可能的输出: &#34;The future of AI is bright. With transformers leading the way...&#34;</span></span></span></code></pre></div><hr><h2 id=五架构选型指南>五、架构选型指南<a class=anchor href=#%e4%ba%94%e6%9e%b6%e6%9e%84%e9%80%89%e5%9e%8b%e6%8c%87%e5%8d%97>#</a></h2><p>在实际构建应用时，我们该如何选择？</p><h3 id=51-三大架构对比表>5.1 三大架构对比表<a class=anchor href=#51-%e4%b8%89%e5%a4%a7%e6%9e%b6%e6%9e%84%e5%af%b9%e6%af%94%e8%a1%a8>#</a></h3><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>Encoder-only (BERT类)</th><th style=text-align:left>Decoder-only (GPT类)</th><th style=text-align:left>Encoder-Decoder (T5类)</th></tr></thead><tbody><tr><td style=text-align:left><strong>可见范围</strong></td><td style=text-align:left>双向 (Bi-directional)</td><td style=text-align:left>单向 (Uni-directional)</td><td style=text-align:left>混合 (Mixed)</td></tr><tr><td style=text-align:left><strong>擅长任务</strong></td><td style=text-align:left>文本分类、命名实体识别、<strong>语义嵌入</strong></td><td style=text-align:left><strong>文本生成</strong>、对话、逻辑推理</td><td style=text-align:left>翻译、摘要 (逐渐被Decoder替代)</td></tr><tr><td style=text-align:left><strong>代表模型</strong></td><td style=text-align:left>BERT, RoBERTa, <strong>BGE-M3</strong></td><td style=text-align:left>GPT-4, LLaMA 3, <strong>DeepSeek-V3</strong></td><td style=text-align:left>T5, UL2</td></tr><tr><td style=text-align:left><strong>缺点</strong></td><td style=text-align:left>很难生成长文本</td><td style=text-align:left>只能看左边，嵌入质量略逊</td><td style=text-align:left>结构复杂，推理慢</td></tr></tbody></table><h3 id=52-决策树我该用哪个>5.2 决策树：我该用哪个？<a class=anchor href=#52-%e5%86%b3%e7%ad%96%e6%a0%91%e6%88%91%e8%af%a5%e7%94%a8%e5%93%aa%e4%b8%aa>#</a></h3><ol><li><p><strong>如果你要把文本变成向量 (Embedding)</strong> -> <strong>Encoder-only</strong></p><ul><li>场景：RAG 中的知识库索引、语义搜索、聚类。</li><li>选型：<code>bge-m3</code>, <code>nomic-embed-text</code>, <code>text-embedding-3-small</code> (OpenAI 虽然是闭源，但内部通常也是基于双向注意力优化的)。</li></ul></li><li><p><strong>如果你要生成文本、回答问题、写代码</strong> -> <strong>Decoder-only</strong></p><ul><li>场景：聊天机器人、写作助手、Agent。</li><li>选型：<code>DeepSeek-V3</code>, <code>Claude 3.5</code>, <code>Llama 3</code>, <code>GPT-4</code>。</li></ul></li><li><p><strong>如果你要处理极其不对称的转换</strong> -> <strong>Encoder-Decoder</strong> (较少见)</p><ul><li>场景：语音转文字 (Whisper)、复杂的机器翻译（尽管 GPT 也能做，但专用模型有时更精准）。</li></ul></li></ol><h3 id=53-黄金组合encoder做索引decoder做生成>5.3 黄金组合：Encoder做索引，Decoder做生成<a class=anchor href=#53-%e9%bb%84%e9%87%91%e7%bb%84%e5%90%88encoder%e5%81%9a%e7%b4%a2%e5%bc%95decoder%e5%81%9a%e7%94%9f%e6%88%90>#</a></h3><p>在最流行的 <strong>RAG (Retrieval-Augmented Generation)</strong> 架构中，我们通常结合使用两者：</p><ol><li>用 <strong>Encoder</strong> 模型（如 BERT 变体）将百万文档转化为向量库。</li><li>用户提问时，先检索出相关文档。</li><li>用 <strong>Decoder</strong> 模型（如 GPT-4）阅读检索到的文档并回答用户问题。</li></ol><p>这是目前企业级应用的标准范式。</p><hr><h2 id=六深度问答>六、深度问答<a class=anchor href=#%e5%85%ad%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94>#</a></h2><p><strong>Q1: 既然 Decoder-only 是主流，BERT 是不是被淘汰了？</strong>
A: <strong>没有</strong>。在 Embedding 领域（将文本转化为向量），Encoder-only 架构依然是绝对王者。因为理解一句话的语义，必须同时看到前后文。目前最强的开源 Embedding 模型（如 BGE, E5 等）本质上都是 BERT 的现代变体。</p><p><strong>Q2: T5 这种 Encoder-Decoder 还有人用吗？</strong>
A: 在多模态领域（如音频转文本 Whisper、图像生成 Imagen）依然广泛使用。但在纯文本生成领域，确实正在被 Decoder-only 取代，因为后者在 Scaling 上更具优势。</p><p><strong>Q3: DeepSeek-R1 / o1 是什么架构？</strong>
A: 它们的基础架构依然是 <strong>Decoder-only Transformer</strong>。它们的"推理能力"主要来自<strong>后训练阶段 (Post-training)</strong> 的强化学习（RL）策略，而不是改变了 Transformer 的底层积木。这些内容将在 [Part 7 第4章] 详细拆解。</p><p><strong>Q4: 我只做很简单的文本分类，该用什么？</strong>
A: 如果数据量少且算力受限，微调一个 <code>DistilBERT</code> (6层 BERT) 依然是最佳选择，速度快、效果好、成本低。如果不想微调，直接用 LLM (GPT) 做 Few-shot 也可以，但成本较高。</p><hr><blockquote class=book-hint><p><strong>下一章预告</strong>：知道了架构，这些模型是如何"炼"出来的？MLM 和 Next Token Prediction 到底是如何让模型产生智能的？下一章《预训练的奥秘》将揭晓。</p></blockquote></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第1章 Transformer核心揭秘</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/ class="flex align-center"><span>第3章 预训练的奥秘：从数据到智能</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一共同的祖先编码器-解码器架构>一、共同的祖先：编码器-解码器架构</a><ul><li><a href=#-深度解析encoder与decoder命名的历史真相>🎯 深度解析：Encoder与Decoder命名的历史真相</a><ul><li><a href=#命名的历史遗留>命名的历史遗留</a></li><li><a href=#transformer继承了命名但改变了本质>Transformer继承了命名，但改变了本质</a></li><li><a href=#命名的崩塌bert和gpt时代>命名的崩塌：BERT和GPT时代</a></li><li><a href=#正确理解decoder--masked-encoder>正确理解：Decoder = Masked Encoder</a></li><li><a href=#为什么不重新命名>为什么不重新命名？</a></li><li><a href=#更准确的命名方案>更准确的命名方案</a></li><li><a href=#面试标准回答>面试标准回答</a></li><li><a href=#关键洞察>关键洞察</a></li></ul></li><li><a href=#11-原始transformer的双塔设计>1.1 原始Transformer的双塔设计</a></li><li><a href=#12-编码器-解码器的工作流程>1.2 编码器-解码器的工作流程</a></li><li><a href=#13-t5现代编码器-解码器的代表>1.3 T5：现代编码器-解码器的代表</a></li></ul></li><li><a href=#二分裂与演化为何不都用编码器-解码器>二、分裂与演化：为何不都用编码器-解码器？</a><ul><li><a href=#21-计算效率考量>2.1 计算效率考量</a></li><li><a href=#22-任务特性适配>2.2 任务特性适配</a></li></ul></li><li><a href=#三仅编码器架构双向的理解专家>三、仅编码器架构：双向的理解专家</a><ul><li><a href=#31-bert的革命性设计>3.1 BERT的革命性设计</a></li><li><a href=#32-掩码语言模型-mlm>3.2 掩码语言模型 (MLM)</a></li><li><a href=#33-为什么bert不能生成文本>3.3 为什么BERT不能生成文本？</a></li><li><a href=#34-实战bert文本分类>3.4 实战：BERT文本分类</a></li></ul></li><li><a href=#四仅解码器架构生成的王者>四、仅解码器架构：生成的王者</a><ul><li><a href=#41-gpt的单向设计哲学>4.1 GPT的单向设计哲学</a></li><li><a href=#42-因果注意力机制-causal-mask>4.2 因果注意力机制 (Causal Mask)</a></li><li><a href=#43-当前视角decoder-only的全面胜利>4.3 当前视角：Decoder-only的全面胜利</a></li><li><a href=#45-dense-vs-sparsemoe架构的崛起>4.5 Dense vs Sparse：MoE架构的崛起</a><ul><li><a href=#什么是moe>什么是MoE？</a></li><li><a href=#dense-vs-sparse-对比>Dense vs Sparse 对比</a></li></ul></li><li><a href=#46-超越transformerssm与mamba的挑战>4.6 超越Transformer：SSM与Mamba的挑战</a><ul><li><a href=#状态空间模型ssm线性时间的挑战者>状态空间模型（SSM）：线性时间的挑战者</a></li></ul></li><li><a href=#44-实战gpt文本生成>4.4 实战：GPT文本生成</a></li></ul></li><li><a href=#五架构选型指南>五、架构选型指南</a><ul><li><a href=#51-三大架构对比表>5.1 三大架构对比表</a></li><li><a href=#52-决策树我该用哪个>5.2 决策树：我该用哪个？</a></li><li><a href=#53-黄金组合encoder做索引decoder做生成>5.3 黄金组合：Encoder做索引，Decoder做生成</a></li></ul></li><li><a href=#六深度问答>六、深度问答</a></li></ul></nav></div></aside></main></body></html>