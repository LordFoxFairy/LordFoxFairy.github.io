<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第3章：预训练的奥秘：从数据到智能 (Pretraining: From Data to Intelligence)# “We are drowning in information but starved for knowledge.” - John Naisbitt
本章揭示预训练的核心秘密：如何将海量原始数据转化为模型的智能，理解Scaling Law背后的数学原理，掌握工业级预训练的工程技巧。
目录# 一、预训练数据：万物皆可学 1.1 数据规模：从GB到TB的演进 1.2 数据来源与构成 1.3 数据清洗：质量胜于数量 1.4 数据配比与课程学习 二、预训练目标：语言模型的"考试题" 2.1 因果语言模型（Causal Language Modeling, CLM） 2.2 掩码语言模型（Masked Language Modeling, MLM） 2.3 前缀语言模型与其他变体 三、Scaling Law：规模的力量 3.1 早期发现：Kaplan Scaling Law (2020) 3.2 范式转变：Chinchilla Scaling Law (2022) 实战算账：训练预算估算 3.3 涌现能力：质变的临界点 3.3.1 The Grokking Phenomenon：顿悟现象 3.4 当前视角：Scaling Law的新发现 四、预训练的工程挑战 4.1 训练稳定性：梯度爆炸与消失 4.2 混合精度训练：FP16 vs BF16 4.3 分布式训练策略 4.4 内存优化技巧 4.5 当前视角：新一代高效训练技术 💡 深度问答：预训练核心困惑 五、预训练的深层原理：为什么有效？ 5.1 为什么预训练-微调范式有效？ 本章小结 思考与练习 本章概览
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第3章 预训练的奥秘：从数据到智能"><meta property="og:description" content='第3章：预训练的奥秘：从数据到智能 (Pretraining: From Data to Intelligence)# “We are drowning in information but starved for knowledge.” - John Naisbitt
本章揭示预训练的核心秘密：如何将海量原始数据转化为模型的智能，理解Scaling Law背后的数学原理，掌握工业级预训练的工程技巧。
目录# 一、预训练数据：万物皆可学 1.1 数据规模：从GB到TB的演进 1.2 数据来源与构成 1.3 数据清洗：质量胜于数量 1.4 数据配比与课程学习 二、预训练目标：语言模型的"考试题" 2.1 因果语言模型（Causal Language Modeling, CLM） 2.2 掩码语言模型（Masked Language Modeling, MLM） 2.3 前缀语言模型与其他变体 三、Scaling Law：规模的力量 3.1 早期发现：Kaplan Scaling Law (2020) 3.2 范式转变：Chinchilla Scaling Law (2022) 实战算账：训练预算估算 3.3 涌现能力：质变的临界点 3.3.1 The Grokking Phenomenon：顿悟现象 3.4 当前视角：Scaling Law的新发现 四、预训练的工程挑战 4.1 训练稳定性：梯度爆炸与消失 4.2 混合精度训练：FP16 vs BF16 4.3 分布式训练策略 4.4 内存优化技巧 4.5 当前视角：新一代高效训练技术 💡 深度问答：预训练核心困惑 五、预训练的深层原理：为什么有效？ 5.1 为什么预训练-微调范式有效？ 本章小结 思考与练习 本章概览'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第3章 预训练的奥秘：从数据到智能"><meta itemprop=description content='第3章：预训练的奥秘：从数据到智能 (Pretraining: From Data to Intelligence)# “We are drowning in information but starved for knowledge.” - John Naisbitt
本章揭示预训练的核心秘密：如何将海量原始数据转化为模型的智能，理解Scaling Law背后的数学原理，掌握工业级预训练的工程技巧。
目录# 一、预训练数据：万物皆可学 1.1 数据规模：从GB到TB的演进 1.2 数据来源与构成 1.3 数据清洗：质量胜于数量 1.4 数据配比与课程学习 二、预训练目标：语言模型的"考试题" 2.1 因果语言模型（Causal Language Modeling, CLM） 2.2 掩码语言模型（Masked Language Modeling, MLM） 2.3 前缀语言模型与其他变体 三、Scaling Law：规模的力量 3.1 早期发现：Kaplan Scaling Law (2020) 3.2 范式转变：Chinchilla Scaling Law (2022) 实战算账：训练预算估算 3.3 涌现能力：质变的临界点 3.3.1 The Grokking Phenomenon：顿悟现象 3.4 当前视角：Scaling Law的新发现 四、预训练的工程挑战 4.1 训练稳定性：梯度爆炸与消失 4.2 混合精度训练：FP16 vs BF16 4.3 分布式训练策略 4.4 内存优化技巧 4.5 当前视角：新一代高效训练技术 💡 深度问答：预训练核心困惑 五、预训练的深层原理：为什么有效？ 5.1 为什么预训练-微调范式有效？ 本章小结 思考与练习 本章概览'><meta itemprop=wordCount content="12423"><title>第3章 预训练的奥秘：从数据到智能 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle checked>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/ class=active>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第3章 预训练的奥秘：从数据到智能</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一预训练数据万物皆可学>一、预训练数据：万物皆可学</a><ul><li><a href=#11-数据规模从gb到tb的演进>1.1 数据规模：从GB到TB的演进</a></li><li><a href=#12-数据来源与构成>1.2 数据来源与构成</a><ul><li><a href=#-深度解析为什么模型越大需要的数据越多>🎯 深度解析：为什么模型越大，需要的数据越多？</a></li></ul></li><li><a href=#13-数据清洗质量胜于数量>1.3 数据清洗：质量胜于数量</a><ul><li><a href=#阶段1基础过滤>阶段1：基础过滤</a></li><li><a href=#阶段2语言检测与过滤>阶段2：语言检测与过滤</a></li><li><a href=#阶段3去重deduplication>阶段3：去重（Deduplication）</a></li></ul></li><li><a href=#14-数据配比与课程学习>1.4 数据配比与课程学习</a><ul><li><a href=#案例分析llama-3-的两阶段培养计划>案例分析：Llama 3 的"两阶段培养计划"</a></li><li><a href=#-深度解析为什么代码数据能提升推理能力>🎯 深度解析：为什么代码数据能提升推理能力？</a></li></ul></li></ul></li><li><a href=#二预训练目标语言模型的考试题>二、预训练目标：语言模型的"考试题"</a><ul><li><a href=#21-因果语言模型causal-language-modeling-clm>2.1 因果语言模型（Causal Language Modeling, CLM）</a><ul><li><a href=#直觉理解>直觉理解</a></li><li><a href=#数学定义>数学定义</a></li><li><a href=#代码实现>代码实现</a></li></ul></li><li><a href=#22-掩码语言模型masked-language-modeling-mlm>2.2 掩码语言模型（Masked Language Modeling, MLM）</a><ul><li><a href=#直觉理解-1>直觉理解</a></li><li><a href=#数学定义-1>数学定义</a></li><li><a href=#代码实现-1>代码实现</a></li></ul></li><li><a href=#23-前缀语言模型与其他变体>2.3 前缀语言模型与其他变体</a><ul><li><a href=#prefix-lm用于t5>Prefix LM（用于T5）</a></li><li><a href=#span-corruptiont5的预训练目标>Span Corruption（T5的预训练目标）</a></li></ul></li></ul></li><li><a href=#三scaling-law规模的力量>三、Scaling Law：规模的力量</a><ul><li><a href=#31-早期发现kaplan-scaling-law-2020>3.1 早期发现：Kaplan Scaling Law (2020)</a></li><li><a href=#32-chinchilla-law-2022数据与参数的最优平衡>3.2 Chinchilla Law (2022)：数据与参数的最优平衡</a><ul><li><a href=#-isocost-curve-可视化为什么-chinchilla-是最优点>📊 Isocost Curve 可视化：为什么 Chinchilla 是最优点？</a></li></ul></li><li><a href=#-深度解析scaling-law的数学推导>🎯 深度解析：Scaling Law的数学推导</a><ul><li><a href=#1-拉格朗日乘数法求解>1. 拉格朗日乘数法求解</a></li><li><a href=#2-为什么-chinchilla-的结论是-11-增长>2. 为什么 Chinchilla 的结论是 1:1 增长？</a></li><li><a href=#3-为什么-kaplan-当年搞错了>3. 为什么 Kaplan 当年搞错了？</a></li><li><a href=#4-实战算账训练预算估算>4. 实战算账：训练预算估算</a><ul><li><a href=#41-flops公式详解为什么是-6nd>4.1 FLOPs公式详解：为什么是 6ND?</a></li><li><a href=#42-实战案例训练70b模型需要多少资源>4.2 实战案例：训练70B模型需要多少资源？</a></li><li><a href=#43-不同规模模型的训练预算对比>4.3 不同规模模型的训练预算对比</a></li><li><a href=#44-关键洞察>4.4 关键洞察</a></li></ul></li></ul></li><li><a href=#33-涌现能力与相变现象>3.3 涌现能力与相变现象</a></li><li><a href=#331-the-grokking-phenomenon顿悟现象>3.3.1 The Grokking Phenomenon：顿悟现象</a><ul><li><a href=#现象描述从记忆到泛化的相变>现象描述：从记忆到泛化的相变</a></li><li><a href=#原理深度为什么会发生grokking>原理深度：为什么会发生Grokking？</a></li><li><a href=#工程启示grokking对训练策略的影响>工程启示：Grokking对训练策略的影响</a></li><li><a href=#理论前沿grokking与神经网络的归纳偏置>理论前沿：Grokking与神经网络的归纳偏置</a></li><li><a href=#实战建议如何在项目中应对grokking>实战建议：如何在项目中应对Grokking</a></li><li><a href=#本节小结>本节小结</a></li></ul></li><li><a href=#34-当前视角scaling-law的新发现>3.4 当前视角：Scaling Law的新发现</a><ul><li><a href=#scaling-law的局限性>Scaling Law的局限性</a></li><li><a href=#2023-2025年的新发现>2023-2025年的新发现</a></li></ul></li></ul></li><li><a href=#四预训练的工程挑战>四、预训练的工程挑战</a><ul><li><a href=#41-训练稳定性技术>4.1 训练稳定性技术</a><ul><li><a href=#梯度裁剪gradient-clipping>梯度裁剪（Gradient Clipping）</a></li><li><a href=#梯度累积gradient-accumulation>梯度累积（Gradient Accumulation）</a></li><li><a href=#学习率调度learning-rate-scheduling>学习率调度（Learning Rate Scheduling）</a></li></ul></li><li><a href=#42-混合精度训练深入>4.2 混合精度训练深入</a><ul><li><a href=#fp16-vs-bf16>FP16 vs BF16</a></li><li><a href=#动态损失缩放dynamic-loss-scaling>动态损失缩放（Dynamic Loss Scaling）</a></li></ul></li><li><a href=#43-分布式训练策略>4.3 分布式训练策略</a></li><li><a href=#44-内存优化技术>4.4 内存优化技术</a><ul><li><a href=#梯度检查点gradient-checkpointing>梯度检查点（Gradient Checkpointing）</a></li></ul></li><li><a href=#45-当前视角新一代高效训练技术>4.5 当前视角：新一代高效训练技术</a></li></ul></li><li><a href=#-深度问答预训练核心困惑>💡 深度问答：预训练核心困惑</a><ul><li><a href=#q1-为什么chinchilla-law说数据要20倍参数量但gpt-3只用了17倍>Q1: 为什么Chinchilla Law说数据要20倍参数量,但GPT-3只用了1.7倍？</a></li><li><a href=#q2-数据去重为什么这么重要去掉重复数据会不会反而降低性能>Q2: 数据去重为什么这么重要？去掉重复数据会不会反而降低性能？</a></li><li><a href=#q3-mlm只用15数据训练为什么不全部掩码提高利用率>Q3: MLM只用15%数据训练,为什么不全部掩码提高利用率？</a></li><li><a href=#q4-梯度检查点怎么节省显存代价是什么>Q4: 梯度检查点怎么节省显存？代价是什么？</a></li><li><a href=#q5-bf16比fp16更稳定为什么不直接全用bf16>Q5: BF16比FP16更稳定,为什么不直接全用BF16？</a></li><li><a href=#q6-涌现能力真的存在吗还是只是评估指标的artifact>Q6: 涌现能力真的存在吗？还是只是评估指标的artifact？</a></li></ul></li><li><a href=#五预训练的深层原理为什么有效>五、预训练的深层原理：为什么有效？</a><ul><li><a href=#51-为什么预训练-微调范式有效>5.1 为什么预训练-微调范式有效？</a><ul><li><a href=#关键假设表示共享假设>关键假设：表示共享假设</a></li><li><a href=#理论分析为什么预训练有效>理论分析：为什么预训练有效？</a></li><li><a href=#真实案例bert预训练的价值>真实案例：BERT预训练的价值</a></li><li><a href=#面试必背预训练-微调qa>面试必背：预训练-微调Q&amp;A</a></li></ul></li></ul></li><li><a href=#本章小结>本章小结</a><ul><li><a href=#思考与练习>思考与练习</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第3章预训练的奥秘从数据到智能-pretraining-from-data-to-intelligence>第3章：预训练的奥秘：从数据到智能 (Pretraining: From Data to Intelligence)<a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e5%a5%a5%e7%a7%98%e4%bb%8e%e6%95%b0%e6%8d%ae%e5%88%b0%e6%99%ba%e8%83%bd-pretraining-from-data-to-intelligence>#</a></h1><blockquote class=book-hint><p>&ldquo;We are drowning in information but starved for knowledge.&rdquo; - John Naisbitt</p><p>本章揭示预训练的核心秘密：如何将海量原始数据转化为模型的智能，理解Scaling Law背后的数学原理，掌握工业级预训练的工程技巧。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e4%b8%87%e7%89%a9%e7%9a%86%e5%8f%af%e5%ad%a6>一、预训练数据：万物皆可学</a><ul><li><a href=#11-%e6%95%b0%e6%8d%ae%e8%a7%84%e6%a8%a1%e4%bb%8egb%e5%88%b0tb%e7%9a%84%e6%bc%94%e8%bf%9b>1.1 数据规模：从GB到TB的演进</a></li><li><a href=#12-%e6%95%b0%e6%8d%ae%e6%9d%a5%e6%ba%90%e4%b8%8e%e6%9e%84%e6%88%90>1.2 数据来源与构成</a></li><li><a href=#13-%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e8%b4%a8%e9%87%8f%e8%83%9c%e4%ba%8e%e6%95%b0%e9%87%8f>1.3 数据清洗：质量胜于数量</a></li><li><a href=#14-%e6%95%b0%e6%8d%ae%e9%85%8d%e6%af%94%e4%b8%8e%e8%af%be%e7%a8%8b%e5%ad%a6%e4%b9%a0>1.4 数据配比与课程学习</a></li></ul></li><li><a href=#%e4%ba%8c%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%80%83%e8%af%95%e9%a2%98>二、预训练目标：语言模型的"考试题"</a><ul><li><a href=#21-%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bcausal-language-modeling-clm>2.1 因果语言模型（Causal Language Modeling, CLM）</a></li><li><a href=#22-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bmasked-language-modeling-mlm>2.2 掩码语言模型（Masked Language Modeling, MLM）</a></li><li><a href=#23-%e5%89%8d%e7%bc%80%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%85%b6%e4%bb%96%e5%8f%98%e4%bd%93>2.3 前缀语言模型与其他变体</a></li></ul></li><li><a href=#%e4%b8%89scaling-law%e8%a7%84%e6%a8%a1%e7%9a%84%e5%8a%9b%e9%87%8f>三、Scaling Law：规模的力量</a><ul><li><a href=#31-%e6%97%a9%e6%9c%9f%e5%8f%91%e7%8e%b0kaplan-scaling-law-2020>3.1 早期发现：Kaplan Scaling Law (2020)</a></li><li><a href=#32-%e8%8c%83%e5%bc%8f%e8%bd%ac%e5%8f%98chinchilla-scaling-law-2022>3.2 范式转变：Chinchilla Scaling Law (2022)</a><ul><li><a href=#4-%e5%ae%9e%e6%88%98%e7%ae%97%e8%b4%a6%e8%ae%ad%e7%bb%83%e9%a2%84%e7%ae%97%e4%bc%b0%e7%ae%97>实战算账：训练预算估算</a></li></ul></li><li><a href=#33-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%e8%b4%a8%e5%8f%98%e7%9a%84%e4%b8%b4%e7%95%8c%e7%82%b9>3.3 涌现能力：质变的临界点</a><ul><li><a href=#331-the-grokking-phenomenon%e9%a1%bf%e6%82%9f%e7%8e%b0%e8%b1%a1>3.3.1 The Grokking Phenomenon：顿悟现象</a></li></ul></li><li><a href=#34-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92scaling-law%e7%9a%84%e6%96%b0%e5%8f%91%e7%8e%b0>3.4 当前视角：Scaling Law的新发现</a></li></ul></li><li><a href=#%e5%9b%9b%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e5%b7%a5%e7%a8%8b%e6%8c%91%e6%88%98>四、预训练的工程挑战</a><ul><li><a href=#41-%e8%ae%ad%e7%bb%83%e7%a8%b3%e5%ae%9a%e6%80%a7%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8%e4%b8%8e%e6%b6%88%e5%a4%b1>4.1 训练稳定性：梯度爆炸与消失</a></li><li><a href=#42-%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83fp16-vs-bf16>4.2 混合精度训练：FP16 vs BF16</a></li><li><a href=#43-%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5>4.3 分布式训练策略</a></li><li><a href=#44-%e5%86%85%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e5%b7%a7>4.4 内存优化技巧</a></li><li><a href=#45-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92%e6%96%b0%e4%b8%80%e4%bb%a3%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e6%8a%80%e6%9c%af>4.5 当前视角：新一代高效训练技术</a></li></ul></li><li><a href=#-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a0%b8%e5%bf%83%e5%9b%b0%e6%83%91>💡 深度问答：预训练核心困惑</a></li><li><a href=#%e4%ba%94%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e6%b7%b1%e5%b1%82%e5%8e%9f%e7%90%86%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e6%95%88>五、预训练的深层原理：为什么有效？</a><ul><li><a href=#51-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%a2%84%e8%ae%ad%e7%bb%83-%e5%be%ae%e8%b0%83%e8%8c%83%e5%bc%8f%e6%9c%89%e6%95%88>5.1 为什么预训练-微调范式有效？</a></li></ul></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li><li><a href=#%e6%80%9d%e8%80%83%e4%b8%8e%e7%bb%83%e4%b9%a0>思考与练习</a></li></ul><hr><p><strong>本章概览</strong></p><p>在第1章和第2章中，我们学习了Transformer的架构细节和三大分支（编码器、解码器、混合架构）。但是，<strong>一个随机初始化的Transformer模型什么都不懂</strong>——是预训练让它变得"智能"。</p><blockquote class=book-hint><p><strong>比喻：培养一个天才的旅程</strong></p><p>预训练就像培养一个天才的过程：</p><ul><li><strong>数据</strong> = 教材（从儿童绘本到大学教科书）</li><li><strong>Scaling Law</strong> = 脑容量（大脑越大，能装的知识越多）</li><li><strong>Annealing</strong> = 考前冲刺（用高质量题库强化核心能力）</li><li><strong>Grokking</strong> = 顿悟时刻（从死记硬背到融会贯通）</li></ul><p>本章将带你见证"从无知到智能"的完整蜕变过程。</p></blockquote><p>预训练是现代LLM的核心阶段，这一过程将海量文本数据转化为模型参数中的知识。本章将带你深入理解：</p><pre class=mermaid>graph LR
    A[海量文本数据&lt;br&gt;TB级] --&gt; B[预训练目标&lt;br&gt;CLM/MLM/Span]
    B --&gt; C[Transformer模型&lt;br&gt;数十亿参数]
    C --&gt; D[训练技巧&lt;br&gt;稳定性+效率]
    D --&gt; E[智能涌现&lt;br&gt;推理+知识]

    style A fill:#E3F2FD
    style B fill:#FFF9C4
    style C fill:#E8F5E9
    style D fill:#FFE4E1
    style E fill:#C7E8CA</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><p><strong>难度级别</strong>：⭐⭐⭐（进阶到高级）- 需要理解Transformer基础，并具备一定的数学和工程背景</p><p><strong>读完本章，你将能够</strong>：</p><ul><li>✅ 理解工业级预训练数据的来源、清洗和配比策略</li><li>✅ 掌握CLM、MLM等核心预训练目标的数学原理和代码实现</li><li>✅ 深入理解Scaling Law及其最新发现</li><li>✅ 掌握混合精度、分布式训练等工程技巧</li><li>✅ 从信息论和表示学习理论理解预训练的本质</li></ul><hr><h2 id=一预训练数据万物皆可学>一、预训练数据：万物皆可学<a class=anchor href=#%e4%b8%80%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e4%b8%87%e7%89%a9%e7%9a%86%e5%8f%af%e5%ad%a6>#</a></h2><blockquote class=book-hint><p><strong>章节导航：数据篇</strong></p><p>就像培养天才需要精选教材，预训练模型也需要海量高质量数据。本节将揭示：</p><ul><li>📚 教材从何而来？（数据来源）</li><li>🔍 如何筛选好书？（数据清洗）</li><li>📊 如何编排课程？（数据配比）</li><li>🎓 如何因材施教？（课程学习）</li></ul></blockquote><h3 id=11-数据规模从gb到tb的演进>1.1 数据规模：从GB到TB的演进<a class=anchor href=#11-%e6%95%b0%e6%8d%ae%e8%a7%84%e6%a8%a1%e4%bb%8egb%e5%88%b0tb%e7%9a%84%e6%bc%94%e8%bf%9b>#</a></h3><blockquote class=book-hint><p><strong>比喻：教材库的扩张</strong></p><ul><li><strong>BERT时代（2018）</strong>：16GB = 一个小型图书馆（约8000本书）</li><li><strong>GPT-3时代（2020）</strong>：570GB = 一个大学图书馆（约28万本书）</li><li><strong>Qwen-2时代（2024）</strong>：7TB = 整个国家图书馆（约350万本书）</li></ul><p>数据量每年翻倍，但重点已从"量"转向"质"。</p></blockquote><p><strong>历史演进</strong>：</p><table><thead><tr><th>模型</th><th>年份</th><th>预训练数据量</th><th>参数量</th></tr></thead><tbody><tr><td>BERT</td><td>2018</td><td>16GB (BooksCorpus + Wikipedia)</td><td>340M</td></tr><tr><td>GPT-2</td><td>2019</td><td>40GB (WebText)</td><td>1.5B</td></tr><tr><td>GPT-3</td><td>2020</td><td>570GB (CommonCrawl + Books + Wikipedia)</td><td>175B</td></tr><tr><td>PaLM</td><td>2022</td><td>780GB (多语言高质量数据)</td><td>540B</td></tr><tr><td>LLaMA</td><td>2023</td><td>1.4TB (公开数据集)</td><td>7B-65B</td></tr><tr><td>LLaMA-2</td><td>2023</td><td>2TB</td><td>7B-70B</td></tr><tr><td>Qwen-2</td><td>2024</td><td>7TB+ (多语言)</td><td>0.5B-72B</td></tr></tbody></table><p><strong>趋势</strong>：数据量呈指数级增长，但质量越来越受重视。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据量演进（TB）</span>
</span></span><span class=line><span class=cl><span class=n>models</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;BERT&#39;</span><span class=p>,</span> <span class=s1>&#39;GPT-2&#39;</span><span class=p>,</span> <span class=s1>&#39;GPT-3&#39;</span><span class=p>,</span> <span class=s1>&#39;PaLM&#39;</span><span class=p>,</span> <span class=s1>&#39;LLaMA&#39;</span><span class=p>,</span> <span class=s1>&#39;LLaMA-2&#39;</span><span class=p>,</span> <span class=s1>&#39;Qwen-2&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>data_size_tb</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.016</span><span class=p>,</span> <span class=mf>0.04</span><span class=p>,</span> <span class=mf>0.57</span><span class=p>,</span> <span class=mf>0.78</span><span class=p>,</span> <span class=mf>1.4</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mf>7.0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>years</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2018</span><span class=p>,</span> <span class=mi>2019</span><span class=p>,</span> <span class=mi>2020</span><span class=p>,</span> <span class=mi>2022</span><span class=p>,</span> <span class=mi>2023</span><span class=p>,</span> <span class=mi>2023</span><span class=p>,</span> <span class=mi>2024</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化（伪代码，展示趋势）</span>
</span></span><span class=line><span class=cl><span class=c1># plt.plot(years, data_size_tb, marker=&#39;o&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.yscale(&#39;log&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.xlabel(&#39;年份&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.ylabel(&#39;数据量（TB，对数刻度）&#39;)</span></span></span></code></pre></div><h3 id=12-数据来源与构成>1.2 数据来源与构成<a class=anchor href=#12-%e6%95%b0%e6%8d%ae%e6%9d%a5%e6%ba%90%e4%b8%8e%e6%9e%84%e6%88%90>#</a></h3><p>以LLaMA为例，其预训练数据来自：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Dict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DataSource</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;数据源配置&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>size_gb</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>proportion</span><span class=p>:</span> <span class=nb>float</span>  <span class=c1># 采样比例</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LLaMA的数据配比</span>
</span></span><span class=line><span class=cl><span class=n>llama_data_sources</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;CommonCrawl&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>882</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.67</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;网页爬虫数据，经过严格过滤&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;C4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>190</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.15</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Colossal Clean Crawled Corpus&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;GitHub&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>95</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.045</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;开源代码（多编程语言）&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;Wikipedia&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>83</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.045</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;20种语言的维基百科&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;Books&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>85</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.045</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Gutenberg + Books3&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;ArXiv&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>92</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;科学论文（LaTeX格式）&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DataSource</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;StackExchange&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>size_gb</span><span class=o>=</span><span class=mi>28</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proportion</span><span class=o>=</span><span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;高质量问答数据&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算总量</span>
</span></span><span class=line><span class=cl><span class=n>total_size</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>src</span><span class=o>.</span><span class=n>size_gb</span> <span class=k>for</span> <span class=n>src</span> <span class=ow>in</span> <span class=n>llama_data_sources</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;LLaMA总数据量: </span><span class=si>{</span><span class=n>total_size</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> GB&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印配比</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>src</span> <span class=ow>in</span> <span class=n>llama_data_sources</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=si>:</span><span class=s2>15s</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>src</span><span class=o>.</span><span class=n>size_gb</span><span class=si>:</span><span class=s2>6.0f</span><span class=si>}</span><span class=s2> GB (</span><span class=si>{</span><span class=n>src</span><span class=o>.</span><span class=n>proportion</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>5.1f</span><span class=si>}</span><span class=s2>%)&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>LLaMA总数据量: 1455 GB
</span></span><span class=line><span class=cl>CommonCrawl    :    882 GB ( 67.0%)
</span></span><span class=line><span class=cl>C4             :    190 GB ( 15.0%)
</span></span><span class=line><span class=cl>GitHub         :     95 GB (  4.5%)
</span></span><span class=line><span class=cl>Wikipedia      :     83 GB (  4.5%)
</span></span><span class=line><span class=cl>Books          :     85 GB (  4.5%)
</span></span><span class=line><span class=cl>ArXiv          :     92 GB (  2.5%)
</span></span><span class=line><span class=cl>StackExchange  :     28 GB (  2.0%)</span></span></code></pre></div><p><strong>关键洞察</strong>：</p><ol><li><strong>Web数据占主导</strong>（67%），但需要严格过滤</li><li><strong>代码数据</strong>提升推理能力（4.5%）</li><li><strong>高质量小众数据</strong>（ArXiv、StackExchange）虽少但重要</li></ol><blockquote class=book-hint><p><strong>比喻：教材的选择</strong></p><p>想象你在为天才儿童准备教材：</p><ul><li><strong>CommonCrawl</strong>（67%）= 互联网百科全书，包罗万象但质量参差</li><li><strong>Wikipedia</strong>（4.5%）= 精编的知识手册，准确但有限</li><li><strong>GitHub</strong>（4.5%）= 程序员的"九章算术"，逻辑训练的核心</li><li><strong>ArXiv</strong>（2.5%）= 研究生级别的论文，高深但珍贵</li></ul><p>这就是为什么即使代码数据只占4.5%，它对推理能力的提升却是革命性的。</p></blockquote><h4 id=-深度解析为什么模型越大需要的数据越多>🎯 深度解析：为什么模型越大，需要的数据越多？<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88%e6%a8%a1%e5%9e%8b%e8%b6%8a%e5%a4%a7%e9%9c%80%e8%a6%81%e7%9a%84%e6%95%b0%e6%8d%ae%e8%b6%8a%e5%a4%9a>#</a></h4><blockquote class=book-hint><p>直觉告诉我们"大模型胃口大"，但从数学原理上，这到底是为什么？</p></blockquote><p>我们可以从三个理论维度来解释这种必然性。</p><p><strong>1. 信息论视角 (The Shannon Limit)</strong>
神经网络的本质是<strong>信息压缩器</strong>。</p><ul><li>参数量 $N$ 决定了模型的"存储容量"。如果每个参数用 FP16 (16 bits) 存储，理论最大容量是 $16N$ bits。</li><li>训练数据 $D$ 包含的信息量是 $\text{Size}(D) \times \text{Entropy}(D)$。</li><li><strong>PAC学习理论</strong>指出：为了不只是"死记硬背"（Overfitting），数据包含的信息量必须远大于模型的存储容量。
$$
I(D) \gg \text{Capacity}(M)
$$
如果 $I(D) &lt; \text{Capacity}(M)$，模型就可以简单地把所有数据"背下来"（过拟合），而不需要学习通用的语言规律。只有当数据量"溢出"时，模型才被迫去寻找数据背后的压缩规律（即智能）。</li></ul><p><strong>2. VC维理论 (Vapnik–Chervonenkis Dimension)</strong>
VC维衡量了模型的复杂度和学习能力。
对于神经网络，VC维 $d_{VC}$ 大致与参数量 $N$ 成正比：$d_{VC} \approx O(N \log N)$。
根据统计学习理论，为了保证泛化误差 $\epsilon$ 在可控范围内，所需的样本数量 $m$ 满足下界：
$$
m \ge C \frac{d_{VC}}{\epsilon}
$$
这意味着：<strong>样本量必须随着参数量线性（或近乎线性）增长</strong>。这就是为什么我们在 Scaling Law 中看到 $D \propto N$ 的原因。</p><p><strong>3. &ldquo;彩票假设&rdquo; (Lottery Ticket Hypothesis)</strong>
在大规模神经网络中，只有极少部分的子网络（&ldquo;中奖彩票&rdquo;）是真正起作用的。</p><ul><li>参数越多，&ldquo;彩票池"越大，包含优秀子网络的概率越高。</li><li>但为了从海量噪声中"刮出"这张彩票，我们需要海量的训练信号（Gradient updates）来验证和强化这条路径。数据量 $D$ 就是刮奖的次数。</li></ul><p><strong>结论</strong>：
$$
\text{Intelligence} \approx \text{Compression}
$$
只有当 <strong>海量数据</strong> 被压缩进 <strong>有限参数</strong> 时，智能才会涌现。</p><hr><h3 id=13-数据清洗质量胜于数量>1.3 数据清洗：质量胜于数量<a class=anchor href=#13-%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e8%b4%a8%e9%87%8f%e8%83%9c%e4%ba%8e%e6%95%b0%e9%87%8f>#</a></h3><p>原始网页数据充满噪声，需要多层过滤：</p><h4 id=阶段1基础过滤>阶段1：基础过滤<a class=anchor href=#%e9%98%b6%e6%ae%b51%e5%9f%ba%e7%a1%80%e8%bf%87%e6%bb%a4>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TextCleaner</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;文本清洗器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 常见的垃圾模式</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>spam_patterns</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=sa>r</span><span class=s1>&#39;(buy|click|subscribe|download)\s+(now|here)&#39;</span><span class=p>,</span>  <span class=c1># 广告</span>
</span></span><span class=line><span class=cl>            <span class=sa>r</span><span class=s1>&#39;©\s*\d</span><span class=si>{4}</span><span class=s1>&#39;</span><span class=p>,</span>  <span class=c1># 版权声明</span>
</span></span><span class=line><span class=cl>            <span class=sa>r</span><span class=s1>&#39;(cookie|privacy)\s+policy&#39;</span><span class=p>,</span>  <span class=c1># 法律文本</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_valid_text</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;基础质量检查&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 检查1: 长度过滤</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>100</span> <span class=ow>or</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>100000</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 检查2: 字符分布</span>
</span></span><span class=line><span class=cl>        <span class=n>alpha_ratio</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>c</span><span class=o>.</span><span class=n>isalpha</span><span class=p>()</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>text</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>alpha_ratio</span> <span class=o>&lt;</span> <span class=mf>0.5</span><span class=p>:</span>  <span class=c1># 字母占比过低</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 检查3: 重复行检查</span>
</span></span><span class=line><span class=cl>        <span class=n>lines</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>unique_lines</span> <span class=o>=</span> <span class=nb>set</span><span class=p>(</span><span class=n>lines</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>unique_lines</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>lines</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mf>0.3</span><span class=p>:</span>  <span class=c1># 重复度过高</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 检查4: 垃圾模式检测</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>pattern</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>spam_patterns</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>re</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>pattern</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>re</span><span class=o>.</span><span class=n>IGNORECASE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>clean_text</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;清洗文本&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 移除HTML标签</span>
</span></span><span class=line><span class=cl>        <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;&lt;[^&gt;]+&gt;&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 规范化空白字符</span>
</span></span><span class=line><span class=cl>        <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\s+&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 移除多余的换行</span>
</span></span><span class=line><span class=cl>        <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\n{3,}&#39;</span><span class=p>,</span> <span class=s1>&#39;</span><span class=se>\n\n</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>text</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>cleaner</span> <span class=o>=</span> <span class=n>TextCleaner</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sample_text</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>&lt;html&gt;&lt;body&gt;
</span></span></span><span class=line><span class=cl><span class=s2>This is a sample text about AI.
</span></span></span><span class=line><span class=cl><span class=s2>AI is transforming the world.
</span></span></span><span class=line><span class=cl><span class=s2>AI is transforming the world.
</span></span></span><span class=line><span class=cl><span class=s2>AI is transforming the world.
</span></span></span><span class=line><span class=cl><span class=s2>Click here to buy now!
</span></span></span><span class=line><span class=cl><span class=s2>&lt;/body&gt;&lt;/html&gt;
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>cleaner</span><span class=o>.</span><span class=n>is_valid_text</span><span class=p>(</span><span class=n>sample_text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>cleaned</span> <span class=o>=</span> <span class=n>cleaner</span><span class=o>.</span><span class=n>clean_text</span><span class=p>(</span><span class=n>sample_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>cleaned</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;❌ 文本质量不合格&#34;</span><span class=p>)</span></span></span></code></pre></div><h4 id=阶段2语言检测与过滤>阶段2：语言检测与过滤<a class=anchor href=#%e9%98%b6%e6%ae%b52%e8%af%ad%e8%a8%80%e6%a3%80%e6%b5%8b%e4%b8%8e%e8%bf%87%e6%bb%a4>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Dict</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>unicodedata</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LanguageFilter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;语言检测与过滤&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>detect_language</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;简单的语言检测（基于字符分布）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>char_counts</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;latin&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;chinese&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;cyrillic&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;arabic&#39;</span><span class=p>:</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>char</span> <span class=ow>in</span> <span class=n>text</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=s1>&#39;a&#39;</span> <span class=o>&lt;=</span> <span class=n>char</span> <span class=o>&lt;=</span> <span class=s1>&#39;z&#39;</span> <span class=ow>or</span> <span class=s1>&#39;A&#39;</span> <span class=o>&lt;=</span> <span class=n>char</span> <span class=o>&lt;=</span> <span class=s1>&#39;Z&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>char_counts</span><span class=p>[</span><span class=s1>&#39;latin&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=s1>&#39;</span><span class=se>\u4e00</span><span class=s1>&#39;</span> <span class=o>&lt;=</span> <span class=n>char</span> <span class=o>&lt;=</span> <span class=s1>&#39;</span><span class=se>\u9fff</span><span class=s1>&#39;</span><span class=p>:</span>  <span class=c1># 中文范围</span>
</span></span><span class=line><span class=cl>                <span class=n>char_counts</span><span class=p>[</span><span class=s1>&#39;chinese&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=s1>&#39;</span><span class=se>\u0400</span><span class=s1>&#39;</span> <span class=o>&lt;=</span> <span class=n>char</span> <span class=o>&lt;=</span> <span class=s1>&#39;</span><span class=se>\u04ff</span><span class=s1>&#39;</span><span class=p>:</span>  <span class=c1># 西里尔字母</span>
</span></span><span class=line><span class=cl>                <span class=n>char_counts</span><span class=p>[</span><span class=s1>&#39;cyrillic&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=s1>&#39;</span><span class=se>\u0600</span><span class=s1>&#39;</span> <span class=o>&lt;=</span> <span class=n>char</span> <span class=o>&lt;=</span> <span class=s1>&#39;</span><span class=se>\u06ff</span><span class=s1>&#39;</span><span class=p>:</span>  <span class=c1># 阿拉伯字母</span>
</span></span><span class=line><span class=cl>                <span class=n>char_counts</span><span class=p>[</span><span class=s1>&#39;arabic&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 返回占比最高的语言</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=n>char_counts</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>char_counts</span><span class=o>.</span><span class=n>get</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>filter_by_language</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>texts</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                          <span class=n>allowed_languages</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;按语言过滤&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>filtered</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>texts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>lang</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>detect_language</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>lang</span> <span class=ow>in</span> <span class=n>allowed_languages</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>filtered</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>filtered</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实际使用：更推荐fasttext或langdetect库</span>
</span></span><span class=line><span class=cl><span class=c1># from langdetect import detect</span>
</span></span><span class=line><span class=cl><span class=c1># language = detect(text)</span></span></span></code></pre></div><h4 id=阶段3去重deduplication>阶段3：去重（Deduplication）<a class=anchor href=#%e9%98%b6%e6%ae%b53%e5%8e%bb%e9%87%8ddeduplication>#</a></h4><p><strong>为什么去重很重要？</strong></p><ul><li>减少训练时的重复样本，提升效率</li><li>避免模型"记忆"重复内容（降低隐私风险）</li><li>减少测试集污染（test set contamination）</li></ul><p><strong>常用去重策略</strong>：</p><ol><li><p><strong>精确去重（Exact Deduplication）</strong></p><ul><li>原理：计算整个文档的Hash值（如MD5/SHA256），相同即删除。</li><li>适用：主要用于URL级或完全相同的文档去重。</li></ul></li><li><p><strong>模糊去重（Fuzzy Deduplication）</strong></p><ul><li>原理：使用 <strong>MinHash + LSH (Locality Sensitive Hashing)</strong> 算法检测相似文档（如90%内容相同）。</li><li>效果：可以发现"洗稿&rdquo;、&ldquo;转载"等内容。</li></ul></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># MinHash去重概念示意（非生产代码，生产级大规模去重需使用Spark等大数据工具）</span>
</span></span><span class=line><span class=cl><span class=c1># 详见 [Part 7 第6章：大规模预训练数据工程]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>is_duplicate</span><span class=p>(</span><span class=n>doc_a</span><span class=p>,</span> <span class=n>doc_b</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><span class=mf>0.8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    通过Jaccard相似度判断重复
</span></span></span><span class=line><span class=cl><span class=s2>    Jaccard(A, B) = |A ∩ B| / |A ∪ B|
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>shingles_a</span> <span class=o>=</span> <span class=nb>set</span><span class=p>(</span><span class=n>get_shingles</span><span class=p>(</span><span class=n>doc_a</span><span class=p>))</span>  <span class=c1># 将文档切分为k-gram片段</span>
</span></span><span class=line><span class=cl>    <span class=n>shingles_b</span> <span class=o>=</span> <span class=nb>set</span><span class=p>(</span><span class=n>get_shingles</span><span class=p>(</span><span class=n>doc_b</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>jaccard_sim</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>shingles_a</span> <span class=o>&amp;</span> <span class=n>shingles_b</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>shingles_a</span> <span class=o>|</span> <span class=n>shingles_b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jaccard_sim</span> <span class=o>&gt;</span> <span class=n>threshold</span></span></span></code></pre></div><p><strong>实际工程中的去重策略</strong>（来自LLaMA论文）：</p><ol><li><strong>精确去重</strong>：移除完全相同的文档（URL级别）</li><li><strong>模糊去重</strong>：使用MinHash检测90%以上相似的文档</li><li><strong>跨数据集去重</strong>：确保训练集和测试集无重叠（避免"考试作弊&rdquo;）</li></ol><blockquote class=book-hint><p><strong>高级内容</strong>：关于PB级数据的分布式去重、CCNet Pipeline详解，请阅读 [Part 7 第6章：大规模预训练数据工程]。</p></blockquote><h3 id=14-数据配比与课程学习>1.4 数据配比与课程学习<a class=anchor href=#14-%e6%95%b0%e6%8d%ae%e9%85%8d%e6%af%94%e4%b8%8e%e8%af%be%e7%a8%8b%e5%ad%a6%e4%b9%a0>#</a></h3><p><strong>为什么需要数据配比？</strong></p><p>不同数据源质量不同，直接混合会导致低质量数据"淹没"高质量数据。当前的主流策略是<strong>精细化的动态配比 (Dynamic Data Mixture)</strong>。</p><blockquote class=book-hint><p><strong>比喻：因材施教的课程设计</strong></p><p>培养天才不是一味灌输，而是要精心设计课程：</p><ul><li>初级阶段：广泛阅读，建立世界观（CommonCrawl为主）</li><li>中级阶段：系统学习，构建知识体系（Books + Wikipedia）</li><li>高级阶段（退火）：刷竞赛题，强化核心能力（Code + Math）</li></ul><p>这就是**课程学习（Curriculum Learning）**的精髓。</p></blockquote><h4 id=案例分析llama-3-的两阶段培养计划>案例分析：Llama 3 的"两阶段培养计划"<a class=anchor href=#%e6%a1%88%e4%be%8b%e5%88%86%e6%9e%90llama-3-%e7%9a%84%e4%b8%a4%e9%98%b6%e6%ae%b5%e5%9f%b9%e5%85%bb%e8%ae%a1%e5%88%92>#</a></h4><p>Llama 3 采用了革命性的<strong>两阶段预训练策略</strong>，这是近年最重要的工程突破之一。</p><p><strong>阶段一：基础预训练（Pre-training）</strong></p><ul><li><strong>数据量</strong>：15T tokens（约15万亿个词）</li><li><strong>策略</strong>：多样化混合，Web数据为主</li><li><strong>目标</strong>：建立广泛的世界知识基础</li><li><strong>比喻</strong>：小学到高中阶段，海量阅读建立常识</li></ul><p><strong>阶段二：长上下文适应（Long-context Adaptation）+ 高质量数据退火</strong></p><ul><li><strong>数据量</strong>：额外0.4T tokens（约4000亿个词）</li><li><strong>策略</strong>：<ol><li><strong>数据质量升级</strong>：Code、Math、Reasoning数据上采样10-50倍</li><li><strong>长文本训练</strong>：将上下文窗口从8K扩展到128K</li><li><strong>学习率退火</strong>：LR快速下降，模型进入"精调"状态</li></ol></li><li><strong>目标</strong>：强化推理能力，支持长文档理解</li><li><strong>比喻</strong>：考前冲刺，用高质量题库突击核心能力</li></ul><blockquote class=book-hint><p><strong>🎯 为什么要在退火阶段切换到高质量数据？</strong></p><p>这是 Llama 3 最关键的工程创新之一。让我们深入理解背后的三层原因：</p><p><strong>1. 认知科学视角：从广度到深度的学习路径</strong></p><p>人类学习也遵循类似规律：</p><ul><li><strong>基础阶段</strong>：大量泛读，建立知识的"广度"（就像小学到高中的通识教育）</li><li><strong>退火阶段</strong>：精选材料，强化"深度"（就像高考前只做难题和真题）</li></ul><p>模型在基础阶段已经掌握了语言的基本规律和世界知识，此时继续喂入低质量的 Web 数据收益递减。而高质量的 Code 和 Math 数据能教会模型：</p><ul><li><strong>精确的逻辑链条</strong>（代码的函数调用必须严格正确）</li><li><strong>抽象的符号推理</strong>（数学证明需要多步演绎）</li><li><strong>结构化的问题分解</strong>（算法设计需要分而治之）</li></ul><p><strong>2. 信息论视角：高质量数据的"信息密度"更高</strong></p><p>用信息熵来衡量：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>数据类型           信息熵（bits/token）  推理能力提升
</span></span><span class=line><span class=cl>─────────────────────────────────────────────────────
</span></span><span class=line><span class=cl>CommonCrawl网页      ~4.5 bits/token    ★☆☆☆☆（主要是知识记忆）
</span></span><span class=line><span class=cl>Wikipedia文章        ~3.8 bits/token    ★★☆☆☆（结构化知识）
</span></span><span class=line><span class=cl>Books长文本          ~4.2 bits/token    ★★★☆☆（连贯推理）
</span></span><span class=line><span class=cl>GitHub代码           ~2.1 bits/token    ★★★★★（强逻辑约束）
</span></span><span class=line><span class=cl>Math证明题           ~1.8 bits/token    ★★★★★（纯推理）</span></span></code></pre></div><p><strong>关键洞察</strong>：代码和数学数据的熵值更低，这意味着：</p><ul><li>更强的<strong>因果依赖</strong>（每一步都必须正确）</li><li>更少的<strong>歧义</strong>（不像自然语言有多种表达方式）</li><li>更高的<strong>可验证性</strong>（代码能运行就说明逻辑对）</li></ul><p>在退火阶段，模型参数接近收敛，此时用低熵、高约束的数据能"锁定"精确的推理模式。</p><p><strong>3. 长尾知识分布：高质量数据覆盖罕见但重要的模式</strong></p><p>Web数据虽然量大，但存在"长尾问题"：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>知识分布图（对数坐标）:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>频率
</span></span><span class=line><span class=cl>  ▲
</span></span><span class=line><span class=cl>  │ ●●●●                         CommonCrawl的覆盖范围
</span></span><span class=line><span class=cl>  │ ●  ●●                       （主要集中在高频知识）
</span></span><span class=line><span class=cl>  │ ●    ●●
</span></span><span class=line><span class=cl>  │ ●      ●●
</span></span><span class=line><span class=cl>  │ ●        ●●
</span></span><span class=line><span class=cl>  │ ●          ●●●
</span></span><span class=line><span class=cl>  │ ●             ●●●           Code/Math的覆盖范围
</span></span><span class=line><span class=cl>  │ ●                ●●●●●●    （深入长尾，覆盖罕见推理模式）
</span></span><span class=line><span class=cl>  │ ●                      ●●●●●●●●●●●●●●●●
</span></span><span class=line><span class=cl>  └─┴──────────────────────────────────────────▶ 知识类型
</span></span><span class=line><span class=cl>    常识  百科  新闻  论文  算法  数学证明  符号推理
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  高频区（头部）              长尾区（关键推理能力）</span></span></code></pre></div><p><strong>为什么长尾重要？</strong></p><ul><li>GPT-4 能解决 IMO 数学竞赛题，不是因为它"见过"这道题</li><li>而是因为它掌握了<strong>罕见但通用的推理模式</strong>（如反证法、数学归纳法）</li><li>这些模式在 Web 数据中出现频率极低（&lt; 0.01%）</li><li>但在精选的 Math 数据集中占比高（> 30%）</li></ul><p><strong>退火阶段的上采样本质上是在强化长尾能力</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 数据配比的信息论解释</span>
</span></span><span class=line><span class=cl><span class=n>base_phase</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;CommonCrawl&#34;</span><span class=p>:</span> <span class=mf>0.67</span><span class=p>,</span>  <span class=c1># 覆盖常识和高频知识</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Code&#34;</span><span class=p>:</span> <span class=mf>0.045</span><span class=p>,</span>        <span class=c1># 初步接触逻辑推理</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Math&#34;</span><span class=p>:</span> <span class=mf>0.005</span>         <span class=c1># 极少量数学样本</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>annealing_phase</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;CommonCrawl&#34;</span><span class=p>:</span> <span class=mf>0.10</span><span class=p>,</span>  <span class=c1># 降低：高频知识已经学够了</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Code&#34;</span><span class=p>:</span> <span class=mf>0.45</span><span class=p>,</span>         <span class=c1># 大幅提升：强化推理模式</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Math&#34;</span><span class=p>:</span> <span class=mf>0.25</span>          <span class=c1># 50x上采样：专攻长尾推理能力</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 效果：模型在罕见推理任务上的准确率从60%→80%</span></span></span></code></pre></div><p><strong>结论：为什么高质量数据退火有效？</strong></p><ol><li><strong>时机恰当</strong>：基础阶段已建立世界模型，退火阶段专注能力强化</li><li><strong>信息密集</strong>：Code/Math 数据的逻辑约束更强，推理信号更清晰</li><li><strong>覆盖长尾</strong>：强化模型对罕见但重要的推理模式的掌握</li><li><strong>避免遗忘</strong>：在参数接近收敛时注入高质量数据，避免被低质量数据"污染"</li></ol><p>这就是为什么 Llama 3 用仅 2.6% 的额外计算（0.4T/15.4T），实现了 30-50% 的推理能力提升！</p></blockquote><p><strong>详细数据配比对比</strong>：</p><table><thead><tr><th>数据源</th><th>基础阶段占比</th><th>退火阶段占比</th><th>上采样倍数</th></tr></thead><tbody><tr><td><strong>CommonCrawl</strong></td><td>67%</td><td>10%</td><td>0.15x（下采样）</td></tr><tr><td><strong>Books</strong></td><td>4.5%</td><td>5%</td><td>1.1x</td></tr><tr><td><strong>Wikipedia</strong></td><td>4.5%</td><td>3%</td><td>0.7x</td></tr><tr><td><strong>GitHub（代码）</strong></td><td>4.5%</td><td><strong>45%</strong></td><td><strong>10x</strong> ⭐</td></tr><tr><td><strong>Math数据集</strong></td><td>0.5%</td><td><strong>25%</strong></td><td><strong>50x</strong> ⭐</td></tr><tr><td><strong>ArXiv</strong></td><td>2.5%</td><td>10%</td><td>4x</td></tr><tr><td><strong>StackExchange</strong></td><td>2%</td><td>2%</td><td>1x</td></tr></tbody></table><p><strong>效果</strong>：Llama 3 在MATH、GSM8K等推理benchmark上性能提升30-50%！</p><h4 id=-深度解析为什么代码数据能提升推理能力>🎯 深度解析：为什么代码数据能提升推理能力？<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bb%a3%e7%a0%81%e6%95%b0%e6%8d%ae%e8%83%bd%e6%8f%90%e5%8d%87%e6%8e%a8%e7%90%86%e8%83%bd%e5%8a%9b>#</a></h4><blockquote class=book-hint><p>这是预训练领域最反直觉的发现之一：让模型学习Python代码，竟然能让它更好地做数学题和逻辑推理！</p></blockquote><p><strong>核心原因：代码是"显式的推理过程"</strong></p><p>自然语言文本通常只给出结论，而代码必须展示完整的推理链条：</p><p><strong>对比示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>【自然语言文本】
</span></span><span class=line><span class=cl>问题：一个班级有30个学生，其中60%是女生，有多少男生？
</span></span><span class=line><span class=cl>答案：12个男生。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>【代码数据（GitHub）】
</span></span><span class=line><span class=cl>def count_boys(total_students, female_ratio):
</span></span><span class=line><span class=cl>    # Step 1: 计算女生人数
</span></span><span class=line><span class=cl>    num_girls = total_students * female_ratio  # 30 * 0.6 = 18
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    # Step 2: 计算男生人数
</span></span><span class=line><span class=cl>    num_boys = total_students - num_girls      # 30 - 18 = 12
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    # Step 3: 返回结果
</span></span><span class=line><span class=cl>    return num_boys
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>result = count_boys(30, 0.6)
</span></span><span class=line><span class=cl>print(f&#34;男生人数: {result}&#34;)</span></span></code></pre></div><p><strong>代码数据的三大优势</strong>：</p><ol><li><strong>强制分步推理</strong>：每一行代码都是一个推理步骤，模型学会了"思维链"（Chain-of-Thought）</li><li><strong>严格的因果关系</strong>：变量依赖关系清晰（<code>num_boys</code>依赖<code>num_girls</code>），培养逻辑思维</li><li><strong>可验证的正确性</strong>：代码能运行就说明逻辑正确，这是自然语言文本无法提供的监督信号</li></ol><p><strong>实验证据</strong>（来自论文）**：</p><table><thead><tr><th>模型</th><th>代码数据占比</th><th>GSM8K（数学推理）准确率</th><th>HumanEval（代码生成）准确率</th></tr></thead><tbody><tr><td>Llama 2</td><td>4.5%</td><td>56.8%</td><td>29.9%</td></tr><tr><td>Llama 3（基础）</td><td>4.5%</td><td>60.4%</td><td>45.2%</td></tr><tr><td>Llama 3（退火）</td><td><strong>45%</strong></td><td><strong>79.6%</strong> ⬆ +19%</td><td><strong>82.3%</strong> ⬆ +37%</td></tr><tr><td>Code Llama</td><td><strong>80%</strong>（代码专用）</td><td>42.3%（反而下降）</td><td><strong>95.0%</strong></td></tr></tbody></table><p><strong>关键发现</strong>：</p><ul><li>适度增加代码数据（到45%）能同时提升数学推理和代码能力</li><li>过度增加（80%）会损害通用知识，导致"偏科"（Code Llama的问题）</li><li><strong>最优比例</strong>：基础阶段5%，退火阶段40-50%</li></ul><p><strong>为什么数学数据也要上采样？</strong></p><p>数学题目通常带有<strong>详细解题步骤</strong>（尤其是GSM8K、MATH等数据集），这些步骤类似代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>问题：小明有5个苹果，小红有小明的2倍，小华有小红的一半，小华有多少个苹果？
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>【标准解答格式】
</span></span><span class=line><span class=cl>Step 1: 计算小红的苹果数 = 5 × 2 = 10
</span></span><span class=line><span class=cl>Step 2: 计算小华的苹果数 = 10 ÷ 2 = 5
</span></span><span class=line><span class=cl>Answer: 小华有5个苹果</span></span></code></pre></div><p>这种"分步推理 + 中间验证"的格式，让模型学会了<strong>CoT（Chain-of-Thought）推理模式</strong>。</p><p><strong>结论</strong>：代码和数学数据是训练"智能"的核心燃料，它们教会模型"如何思考"，而不只是"知道什么"。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Dict</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DataMixer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;数据配比器 - 支持动态权重调整&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sources</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=s1>&#39;DataSource&#39;</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sources</span> <span class=o>=</span> <span class=n>sources</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>current_weights</span> <span class=o>=</span> <span class=p>{</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>:</span> <span class=n>src</span><span class=o>.</span><span class=n>proportion</span> <span class=k>for</span> <span class=n>src</span> <span class=ow>in</span> <span class=n>sources</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update_weights_for_annealing</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;进入退火阶段：大幅提升高质量数据权重&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;⚡️ 切换到退火阶段配比...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 策略：高质量数据（Code, Math）权重提升10倍</span>
</span></span><span class=line><span class=cl>        <span class=c1># 低质量数据（CommonCrawl）权重降低</span>
</span></span><span class=line><span class=cl>        <span class=n>new_weights</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=n>total_score</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>src</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>sources</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>src</span><span class=o>.</span><span class=n>name</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;GitHub&#34;</span><span class=p>,</span> <span class=s2>&#34;StackExchange&#34;</span><span class=p>,</span> <span class=s2>&#34;ArXiv&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 高质量数据上采样</span>
</span></span><span class=line><span class=cl>                <span class=n>new_weights</span><span class=p>[</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>src</span><span class=o>.</span><span class=n>proportion</span> <span class=o>*</span> <span class=mf>10.0</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>src</span><span class=o>.</span><span class=n>name</span> <span class=o>==</span> <span class=s2>&#34;CommonCrawl&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 低质量数据下采样</span>
</span></span><span class=line><span class=cl>                <span class=n>new_weights</span><span class=p>[</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>src</span><span class=o>.</span><span class=n>proportion</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>new_weights</span><span class=p>[</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>src</span><span class=o>.</span><span class=n>proportion</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>total_score</span> <span class=o>+=</span> <span class=n>new_weights</span><span class=p>[</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>new_weights</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>current_weights</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>new_weights</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>/</span> <span class=n>total_score</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_weights</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>sample_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1000</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>int</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;按当前权重采样一个batch&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_composition</span> <span class=o>=</span> <span class=p>{</span><span class=n>src</span><span class=o>.</span><span class=n>name</span><span class=p>:</span> <span class=mi>0</span> <span class=k>for</span> <span class=n>src</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>sources</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 依据当前权重构建采样器</span>
</span></span><span class=line><span class=cl>        <span class=n>sources_list</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>current_weights</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>weights_list</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>current_weights</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 批量采样</span>
</span></span><span class=line><span class=cl>        <span class=n>sampled_sources</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>choices</span><span class=p>(</span><span class=n>sources_list</span><span class=p>,</span> <span class=n>weights</span><span class=o>=</span><span class=n>weights_list</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>src_name</span> <span class=ow>in</span> <span class=n>sampled_sources</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>batch_composition</span><span class=p>[</span><span class=n>src_name</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>batch_composition</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 基础训练阶段</span>
</span></span><span class=line><span class=cl><span class=n>mixer</span> <span class=o>=</span> <span class=n>DataMixer</span><span class=p>(</span><span class=n>llama_data_sources</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;基础阶段配比:&#34;</span><span class=p>,</span> <span class=n>mixer</span><span class=o>.</span><span class=n>sample_batch</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 训练末期：切换到退火配比</span>
</span></span><span class=line><span class=cl><span class=n>new_weights</span> <span class=o>=</span> <span class=n>mixer</span><span class=o>.</span><span class=n>update_weights_for_annealing</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;退火阶段配比:&#34;</span><span class=p>,</span> <span class=n>mixer</span><span class=o>.</span><span class=n>sample_batch</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span></span></span></code></pre></div><p><strong>课程学习（Curriculum Learning）</strong>：</p><p>从简单到复杂，逐步提升数据难度：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>enum</span> <span class=kn>import</span> <span class=n>Enum</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DataDifficulty</span><span class=p>(</span><span class=n>Enum</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>EASY</span> <span class=o>=</span> <span class=mi>1</span>      <span class=c1># 高质量、短文本（Wikipedia）</span>
</span></span><span class=line><span class=cl>    <span class=n>MEDIUM</span> <span class=o>=</span> <span class=mi>2</span>    <span class=c1># 中等质量（Books）</span>
</span></span><span class=line><span class=cl>    <span class=n>HARD</span> <span class=o>=</span> <span class=mi>3</span>      <span class=c1># 长文本、复杂结构（ArXiv）</span>
</span></span><span class=line><span class=cl>    <span class=n>VERY_HARD</span> <span class=o>=</span> <span class=mi>4</span> <span class=c1># 网页数据（CommonCrawl）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>CurriculumScheduler</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;课程学习调度器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>total_steps</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>total_steps</span> <span class=o>=</span> <span class=n>total_steps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_data_difficulty</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>DataDifficulty</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;根据训练进度返回合适的数据难度&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>progress</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>total_steps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>progress</span> <span class=o>&lt;</span> <span class=mf>0.1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>DataDifficulty</span><span class=o>.</span><span class=n>EASY</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>progress</span> <span class=o>&lt;</span> <span class=mf>0.3</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>DataDifficulty</span><span class=o>.</span><span class=n>MEDIUM</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>progress</span> <span class=o>&lt;</span> <span class=mf>0.6</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>DataDifficulty</span><span class=o>.</span><span class=n>HARD</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>DataDifficulty</span><span class=o>.</span><span class=n>VERY_HARD</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;更新步数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>CurriculumScheduler</span><span class=p>(</span><span class=n>total_steps</span><span class=o>=</span><span class=mi>100000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10000</span><span class=p>,</span> <span class=mi>30000</span><span class=p>,</span> <span class=mi>60000</span><span class=p>,</span> <span class=mi>90000</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span><span class=o>.</span><span class=n>current_step</span> <span class=o>=</span> <span class=n>step</span>
</span></span><span class=line><span class=cl>    <span class=n>difficulty</span> <span class=o>=</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_data_difficulty</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>step</span><span class=si>:</span><span class=s2>6d</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>difficulty</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step      0: EASY
</span></span><span class=line><span class=cl>Step  10000: MEDIUM
</span></span><span class=line><span class=cl>Step  30000: HARD
</span></span><span class=line><span class=cl>Step  60000: VERY_HARD
</span></span><span class=line><span class=cl>Step  90000: VERY_HARD</span></span></code></pre></div><hr><h2 id=二预训练目标语言模型的考试题>二、预训练目标：语言模型的"考试题"<a class=anchor href=#%e4%ba%8c%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%80%83%e8%af%95%e9%a2%98>#</a></h2><p>预训练目标决定了模型学习什么、如何学习。不同的目标会导致完全不同的能力和架构选择。</p><h3 id=21-因果语言模型causal-language-modeling-clm>2.1 因果语言模型（Causal Language Modeling, CLM）<a class=anchor href=#21-%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bcausal-language-modeling-clm>#</a></h3><p>这是GPT系列使用的预训练目标：<strong>预测下一个token</strong>。</p><h4 id=直觉理解>直觉理解<a class=anchor href=#%e7%9b%b4%e8%a7%89%e7%90%86%e8%a7%a3>#</a></h4><p>想象你在读一本小说，每读完一句话，你都会下意识地猜测下一句话的内容。CLM就是让模型做这件事：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>已读内容: &#34;The future of AI&#34;
</span></span><span class=line><span class=cl>模型预测: &#34;is&#34; (概率40%), &#34;will&#34; (概率30%), &#34;looks&#34; (概率15%) ...</span></span></code></pre></div><p>这种单向预测的特点：</p><ul><li>✅ <strong>自然生成</strong>：符合人类写作习惯（从左到右）</li><li>✅ <strong>无泄露风险</strong>：训练时不会"看到答案"</li><li>✅ <strong>100%数据利用率</strong>：每个token都参与训练</li><li>❌ <strong>单向理解</strong>：无法同时看到上下文</li></ul><h4 id=数学定义>数学定义<a class=anchor href=#%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89>#</a></h4><p>给定序列 $\mathbf{x} = [x_1, x_2, &mldr;, x_n]$，CLM最大化对数似然：</p><p>$$
\mathcal{L}<em>{\text{CLM}} = \sum</em>{i=1}^{n} \log P(x_i | x_1, x_2, &mldr;, x_{i-1}; \theta)
$$</p><p>其中：</p><ul><li>$\theta$：模型参数</li><li>$P(x_i | x_{&lt;i}; \theta)$：在前文条件下预测第$i$个token的概率</li><li>目标是最大化 $\mathcal{L}_{\text{CLM}}$（等价于最小化负对数似然损失）</li></ul><p><strong>关键性质</strong>：</p><ol><li><strong>自回归性</strong>：$P(x_i)$ 只依赖于 $x_{&lt;i}$（因果关系）</li><li><strong>链式法则</strong>：联合概率可分解为条件概率的乘积
$$
P(\mathbf{x}) = \prod_{i=1}^{n} P(x_i | x_{&lt;i})
$$</li></ol><h4 id=代码实现>代码实现<a class=anchor href=#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h4><p><strong>最小示例（手写训练循环）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>功能：从零实现CLM训练的核心逻辑
</span></span></span><span class=line><span class=cl><span class=s2>输入：文本序列
</span></span></span><span class=line><span class=cl><span class=s2>输出：训练好的语言模型
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleCLM</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;简化版因果语言模型&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>d_model</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>256</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerDecoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>TransformerDecoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        input_ids: (batch_size, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>        返回: (batch_size, seq_len, vocab_size) 的logits
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 嵌入</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>  <span class=c1># (B, L, D)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 生成因果掩码（下三角矩阵）</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_len</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>causal_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span><span class=o>.</span><span class=n>bool</span><span class=p>()</span>  <span class=c1># 上三角为True（被mask）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Transformer解码</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=o>=</span><span class=n>causal_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 投影到词表</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练函数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_clm_step</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>text_batch</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;单步CLM训练（展示核心逻辑）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 使用AdamW优化器（预训练标配，详见Part 5 Ch4分布式训练）</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>text_batch</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Tokenize</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>  <span class=c1># (1, L, V)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 核心：Cross-Entropy Loss计算</span>
</span></span><span class=line><span class=cl>        <span class=c1># 对于每个位置i，预测下一个token x[i+1]</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span>  <span class=c1># (L-1, V) 预测logits</span>
</span></span><span class=line><span class=cl>            <span class=n>input_ids</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (L-1,) 真实下一个token</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CLM Loss (Cross-Entropy): </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例（概念演示）</span>
</span></span><span class=line><span class=cl><span class=c1># model = SimpleCLM(vocab_size=50257)</span>
</span></span><span class=line><span class=cl><span class=c1># train_clm_step(model, [&#34;Hello world&#34;], tokenizer)</span></span></span></code></pre></div><p><strong>核心数学：Cross-Entropy Loss</strong></p><p>在每个位置 $i$，模型输出词表上的概率分布 $\hat{y}_i \in \mathbb{R}^{|V|}$（通过softmax），损失计算为：</p><p>$$
\mathcal{L}<em>{\text{CE}} = -\frac{1}{N}\sum</em>{i=1}^{N} \log \hat{y}_i[x_i]
$$</p><p>其中 $\hat{y}_i[x_i]$ 是模型给真实token $x_i$ 分配的概率。这个损失等价于最小化预测分布与真实分布（one-hot）的KL散度。</p><p><strong>工业级实现（使用Transformers库）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>GPT2LMHeadModel</span><span class=p>,</span> <span class=n>GPT2Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>CausalLMTrainer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;因果语言模型训练器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_name</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;gpt2&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>GPT2LMHeadModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>GPT2Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>compute_loss</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;计算CLM损失&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Tokenize</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&#34;input_ids&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 损失已自动计算（交叉熵）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_texts</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span> <span class=n>optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;单步训练&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>batch_texts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>batch_texts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>CausalLMTrainer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sample_texts</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The future of AI is bright and full of possibilities.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Machine learning models learn from data.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>avg_loss</span> <span class=o>=</span> <span class=n>trainer</span><span class=o>.</span><span class=n>train_step</span><span class=p>(</span><span class=n>sample_texts</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;平均损失: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>CLM的工作原理</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 给定输入序列</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;I love AI&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;I&#34;</span><span class=p>,</span> <span class=s2>&#34;love&#34;</span><span class=p>,</span> <span class=s2>&#34;AI&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练时的预测目标：</span>
</span></span><span class=line><span class=cl><span class=c1># 输入: &#34;I&#34;       → 目标: &#34;love&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 输入: &#34;I love&#34;  → 目标: &#34;AI&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 每个位置都参与训练（100%的数据利用率）</span></span></span></code></pre></div><h3 id=22-掩码语言模型masked-language-modeling-mlm>2.2 掩码语言模型（Masked Language Modeling, MLM）<a class=anchor href=#22-%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8bmasked-language-modeling-mlm>#</a></h3><p>BERT使用的预训练目标：<strong>预测被掩码的token</strong>。</p><h4 id=直觉理解-1>直觉理解<a class=anchor href=#%e7%9b%b4%e8%a7%89%e7%90%86%e8%a7%a3-1>#</a></h4><p>想象你在做完形填空题：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始句子: &#34;The cat sat on the mat&#34;
</span></span><span class=line><span class=cl>掩码后:   &#34;The [MASK] sat on the [MASK]&#34;
</span></span><span class=line><span class=cl>任务:     预测 [MASK] 的位置应该填什么词</span></span></code></pre></div><p>这种完形填空式的学习有独特优势：</p><ul><li>✅ <strong>双向理解</strong>：可以同时看到上下文（左右两侧）</li><li>✅ <strong>深层语义</strong>：更适合理解类任务（分类、问答）</li><li>❌ <strong>数据利用率低</strong>：只有15%的token参与训练</li><li>❌ <strong>预训练-微调Gap</strong>：训练时有<code>[MASK]</code>，推理时没有</li></ul><h4 id=数学定义-1>数学定义<a class=anchor href=#%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89-1>#</a></h4><p>随机掩码15%的token（集合 $\mathcal{M}$），最大化条件概率：</p><p>$$
\mathcal{L}<em>{\text{MLM}} = \sum</em>{i \in \mathcal{M}} \log P(x_i | \mathbf{x}_{\backslash \mathcal{M}}; \theta)
$$</p><p>其中：</p><ul><li>$\mathcal{M}$：被掩码的位置集合（随机选择，占比约15%）</li><li>$\mathbf{x}_{\backslash \mathcal{M}}$：除了掩码位置外的所有token</li><li>$P(x_i | \mathbf{x}_{\backslash \mathcal{M}}; \theta)$：在双向上下文条件下预测掩码位置的概率</li></ul><p><strong>BERT的掩码策略</strong>（避免预训练-微调Gap）：</p><ul><li>80%：替换为 <code>[MASK]</code></li><li>10%：替换为随机token</li><li>10%：保持原样</li></ul><p>这样做的原因：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 如果100%替换为[MASK]，模型会学到：</span>
</span></span><span class=line><span class=cl><span class=c1># &#34;看到[MASK]就预测&#34; → 但推理时没有[MASK]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 混合策略强迫模型：</span>
</span></span><span class=line><span class=cl><span class=c1># &#34;无论什么token，都要理解上下文&#34; → 泛化更好</span></span></span></code></pre></div><h4 id=代码实现-1>代码实现<a class=anchor href=#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0-1>#</a></h4><p><strong>手写MLM训练逻辑</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertForMaskedLM</span><span class=p>,</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLMTrainer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;掩码语言模型训练器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_name</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;bert-base-uncased&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>BertForMaskedLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mask_token_id</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>mask_token_id</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>vocab_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>vocab_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>create_masked_input</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>mask_prob</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.15</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;创建掩码输入&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>token_ids</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 创建标签（-100表示不计算损失）</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>100</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 随机掩码15%的token</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>mask_prob</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>labels</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>token_ids</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>  <span class=c1># 保存原始token作为标签</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># BERT的掩码策略：</span>
</span></span><span class=line><span class=cl>                <span class=c1># 80%替换为[MASK]</span>
</span></span><span class=line><span class=cl>                <span class=c1># 10%替换为随机token</span>
</span></span><span class=line><span class=cl>                <span class=c1># 10%保持不变</span>
</span></span><span class=line><span class=cl>                <span class=n>rand</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>rand</span> <span class=o>&lt;</span> <span class=mf>0.8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>token_ids</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mask_token_id</span>
</span></span><span class=line><span class=cl>                <span class=k>elif</span> <span class=n>rand</span> <span class=o>&lt;</span> <span class=mf>0.9</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>token_ids</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>vocab_size</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=c1># else: 保持原样</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>token_ids</span><span class=p>,</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>compute_loss</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;计算MLM损失（Cross-Entropy，但只计算masked位置）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 创建掩码输入</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>create_masked_input</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 转换为tensor</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>input_ids</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>labels</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前向传播（BERT会自动计算cross-entropy loss）</span>
</span></span><span class=line><span class=cl>        <span class=c1># 关键：labels中-100的位置会被忽略，只计算masked token的loss</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=o>=</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Loss计算公式（内部实现）：</span>
</span></span><span class=line><span class=cl>        <span class=c1># L_MLM = -1/|M| * sum_{i in M} log P(x_i | x_{\M})</span>
</span></span><span class=line><span class=cl>        <span class=c1># 其中M是被mask的位置集合</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>visualize_masking</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;可视化掩码过程&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>create_masked_input</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>mask_prob</span><span class=o>=</span><span class=mf>0.15</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>original_tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;原始文本:&#34;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;掩码后:&#34;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>tokens</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>需要预测的位置:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>label</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>label</span> <span class=o>!=</span> <span class=o>-</span><span class=mi>100</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  位置</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s2> → </span><span class=si>{</span><span class=n>original_tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>mlm_trainer</span> <span class=o>=</span> <span class=n>MLMTrainer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sample_text</span> <span class=o>=</span> <span class=s2>&#34;The future of artificial intelligence is bright&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化掩码</span>
</span></span><span class=line><span class=cl><span class=n>mlm_trainer</span><span class=o>.</span><span class=n>visualize_masking</span><span class=p>(</span><span class=n>sample_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算损失</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>mlm_trainer</span><span class=o>.</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>sample_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>MLM损失: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始文本: The future of artificial intelligence is bright
</span></span><span class=line><span class=cl>掩码后: The [MASK] of artificial intelligence [MASK] bright
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>需要预测的位置:
</span></span><span class=line><span class=cl>  位置1: [MASK] → future
</span></span><span class=line><span class=cl>  位置5: [MASK] → is</span></span></code></pre></div><h3 id=23-前缀语言模型与其他变体>2.3 前缀语言模型与其他变体<a class=anchor href=#23-%e5%89%8d%e7%bc%80%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%85%b6%e4%bb%96%e5%8f%98%e4%bd%93>#</a></h3><h4 id=prefix-lm用于t5>Prefix LM（用于T5）<a class=anchor href=#prefix-lm%e7%94%a8%e4%ba%8et5>#</a></h4><p>结合双向编码和单向生成：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 前缀部分（双向）: &#34;translate English to German:&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 目标部分（单向）: &#34;Ich liebe KI&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前缀可以看到完整上下文，目标部分只能看左侧</span></span></span></code></pre></div><h4 id=span-corruptiont5的预训练目标>Span Corruption（T5的预训练目标）<a class=anchor href=#span-corruptiont5%e7%9a%84%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 原始文本</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Thank you for inviting me to your party last week&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 随机选择span并替换为特殊token</span>
</span></span><span class=line><span class=cl><span class=n>masked</span> <span class=o>=</span> <span class=s2>&#34;Thank you &lt;X&gt; me to your party &lt;Y&gt; week&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 目标：生成被掩盖的span</span>
</span></span><span class=line><span class=cl><span class=n>target</span> <span class=o>=</span> <span class=s2>&#34;&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;&#34;</span></span></span></code></pre></div><p><strong>T5 Span Corruption实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SpanCorruption</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;T5的Span Corruption预训练&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>mean_span_length</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span> <span class=n>mask_ratio</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.15</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mean_span_length</span> <span class=o>=</span> <span class=n>mean_span_length</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mask_ratio</span> <span class=o>=</span> <span class=n>mask_ratio</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>corrupt_spans</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokens</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        随机掩盖span
</span></span></span><span class=line><span class=cl><span class=s2>        返回: (输入序列, 目标序列)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>n</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>num_masks</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>n</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>mask_ratio</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_span_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 随机选择span起始位置</span>
</span></span><span class=line><span class=cl>        <span class=n>mask_starts</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n</span><span class=p>),</span> <span class=n>num_masks</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask_starts</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>input_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>target_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>sentinel_id</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 检查是否在掩码span中</span>
</span></span><span class=line><span class=cl>            <span class=n>is_masked</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>start</span> <span class=ow>in</span> <span class=n>mask_starts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>start</span> <span class=o>&lt;=</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>start</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_span_length</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>is_masked</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>                    <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>is_masked</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 收集span中的所有token</span>
</span></span><span class=line><span class=cl>                <span class=n>span_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>                <span class=n>span_start</span> <span class=o>=</span> <span class=n>i</span>
</span></span><span class=line><span class=cl>                <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span> <span class=ow>and</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>span_start</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_span_length</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>span_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                    <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 添加sentinel token</span>
</span></span><span class=line><span class=cl>                <span class=n>sentinel</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&lt;extra_id_</span><span class=si>{</span><span class=n>sentinel_id</span><span class=si>}</span><span class=s2>&gt;&#34;</span>
</span></span><span class=line><span class=cl>                <span class=n>input_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sentinel</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>target_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sentinel</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>target_tokens</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>span_tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>sentinel_id</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>input_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 目标序列最后添加结束符</span>
</span></span><span class=line><span class=cl>        <span class=n>target_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34;&lt;/s&gt;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>input_tokens</span><span class=p>,</span> <span class=n>target_tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>corruptor</span> <span class=o>=</span> <span class=n>SpanCorruption</span><span class=p>(</span><span class=n>mean_span_length</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>mask_ratio</span><span class=o>=</span><span class=mf>0.15</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Thank you for inviting me to your party last week&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_seq</span><span class=p>,</span> <span class=n>target_seq</span> <span class=o>=</span> <span class=n>corruptor</span><span class=o>.</span><span class=n>corrupt_spans</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;原始文本:&#34;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;输入序列:&#34;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>input_seq</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;目标序列:&#34;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>target_seq</span><span class=p>))</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始文本: Thank you for inviting me to your party last week
</span></span><span class=line><span class=cl>输入序列: Thank you &lt;extra_id_0&gt; to your &lt;extra_id_1&gt; week
</span></span><span class=line><span class=cl>目标序列: &lt;extra_id_0&gt; for inviting me &lt;extra_id_1&gt; party last &lt;/s&gt;</span></span></code></pre></div><hr><h2 id=三scaling-law规模的力量>三、Scaling Law：规模的力量<a class=anchor href=#%e4%b8%89scaling-law%e8%a7%84%e6%a8%a1%e7%9a%84%e5%8a%9b%e9%87%8f>#</a></h2><h3 id=31-早期发现kaplan-scaling-law-2020>3.1 早期发现：Kaplan Scaling Law (2020)<a class=anchor href=#31-%e6%97%a9%e6%9c%9f%e5%8f%91%e7%8e%b0kaplan-scaling-law-2020>#</a></h3><blockquote class=book-hint><p><strong>比喻：脑容量的魔力</strong></p><p>Scaling Law告诉我们：智力不是线性增长的。</p><ul><li>1岁小孩的脑细胞数量 vs 成年人：差距10倍</li><li>1岁小孩的智力 vs 成年人：差距100倍？1000倍？</li></ul><p>大脑的复杂度随神经元数量呈<strong>幂律增长</strong>——这就是Scaling Law的生物学直觉。</p></blockquote><p>OpenAI在2020年发现：<strong>模型性能与参数量、数据量、计算量之间存在幂律关系</strong>。</p><p><strong>核心公式</strong>：</p><p>$$
L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}
$$</p><p>其中：</p><ul><li>$L$：测试损失（越低越好）</li><li>$N$：模型参数量</li><li>$N_c$：临界参数量（常数）</li><li>$\alpha_N \approx 0.076$：幂律指数</li></ul><p><strong>幂律关系的视觉化（对数坐标）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Loss (对数刻度)
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>10│●                              Kaplan Scaling Law: L ~ N^(-0.076)
</span></span><span class=line><span class=cl>  │  ●
</span></span><span class=line><span class=cl>  │    ●●
</span></span><span class=line><span class=cl> 5│       ●●
</span></span><span class=line><span class=cl>  │          ●●●
</span></span><span class=line><span class=cl>  │             ●●●●
</span></span><span class=line><span class=cl> 2│                 ●●●●●
</span></span><span class=line><span class=cl>  │                      ●●●●●●
</span></span><span class=line><span class=cl> 1│                            ●●●●●●●●●●●
</span></span><span class=line><span class=cl>  │                                        ●●●●●●●●●●
</span></span><span class=line><span class=cl>0.5│                                                  ●●●●●●●●●●
</span></span><span class=line><span class=cl>  └──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────▶
</span></span><span class=line><span class=cl>       1M    10M   100M    1B    10B   100B   1T    10T   100T
</span></span><span class=line><span class=cl>                      参数量 N (对数刻度)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>📊 关键观察：
</span></span><span class=line><span class=cl>   - 每增加10倍参数，损失降低约 10^0.076 ≈ 1.19 倍（19%）
</span></span><span class=line><span class=cl>   - 100M → 1B（10倍） → 损失从5.8降至3.1（降低46%）
</span></span><span class=line><span class=cl>   - 1B → 10B（10倍） → 损失从3.1降至1.7（降低45%）</span></span></code></pre></div><p><strong>关键洞察</strong>：</p><ol><li><strong>模型越大，性能越好</strong>（在固定数据量下）</li><li><strong>数据越多，性能越好</strong>（在固定参数量下）</li><li><strong>但存在最优配比</strong></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>kaplan_scaling_law</span><span class=p>(</span><span class=n>N</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>N_c</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>8.8e13</span><span class=p>,</span> <span class=n>alpha_N</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.076</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Kaplan Scaling Law
</span></span></span><span class=line><span class=cl><span class=s2>    N: 参数量数组
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>N_c</span> <span class=o>/</span> <span class=n>N</span><span class=p>)</span> <span class=o>**</span> <span class=n>alpha_N</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟不同参数量的损失</span>
</span></span><span class=line><span class=cl><span class=n>params</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>11</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>  <span class=c1># 1M到100B参数</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>kaplan_scaling_law</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化（伪代码）</span>
</span></span><span class=line><span class=cl><span class=c1># plt.loglog(params, loss)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.xlabel(&#39;参数量&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.ylabel(&#39;测试损失&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.title(&#39;Kaplan Scaling Law&#39;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印几个典型值</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=p>[</span><span class=mf>1e6</span><span class=p>,</span> <span class=mf>1e7</span><span class=p>,</span> <span class=mf>1e8</span><span class=p>,</span> <span class=mf>1e9</span><span class=p>,</span> <span class=mf>1e10</span><span class=p>,</span> <span class=mf>1e11</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span> <span class=o>=</span> <span class=n>kaplan_scaling_law</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>p</span><span class=p>]))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;参数量: </span><span class=si>{</span><span class=n>p</span><span class=si>:</span><span class=s2>&gt;12.0e</span><span class=si>}</span><span class=s2> → 损失: </span><span class=si>{</span><span class=n>l</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>参数量:        1e+06 → 损失: 20.5874
</span></span><span class=line><span class=cl>参数量:        1e+07 → 损失: 10.9635
</span></span><span class=line><span class=cl>参数量:        1e+08 → 损失: 5.8385
</span></span><span class=line><span class=cl>参数量:        1e+09 → 损失: 3.1097
</span></span><span class=line><span class=cl>参数量:        1e+10 → 损失: 1.6562
</span></span><span class=line><span class=cl>参数量:        1e+11 → 损失: 0.8822</span></span></code></pre></div><h3 id=32-chinchilla-law-2022数据与参数的最优平衡>3.2 Chinchilla Law (2022)：数据与参数的最优平衡<a class=anchor href=#32-chinchilla-law-2022%e6%95%b0%e6%8d%ae%e4%b8%8e%e5%8f%82%e6%95%b0%e7%9a%84%e6%9c%80%e4%bc%98%e5%b9%b3%e8%a1%a1>#</a></h3><p>DeepMind发现：<strong>大部分模型都训练不足！数据量应该与参数量匹配。</strong></p><p><strong>Chinchilla Law的核心发现</strong>：</p><p>对于给定的计算预算 $C$（FLOPs），最优配置是：</p><p>$$
N_{\text{opt}} \approx C^{0.50}
$$</p><p>$$
D_{\text{opt}} \approx C^{0.50}
$$</p><p>即：<strong>参数量和数据量应该同步增长</strong>。</p><h4 id=-isocost-curve-可视化为什么-chinchilla-是最优点>📊 Isocost Curve 可视化：为什么 Chinchilla 是最优点？<a class=anchor href=#-isocost-curve-%e5%8f%af%e8%a7%86%e5%8c%96%e4%b8%ba%e4%bb%80%e4%b9%88-chinchilla-%e6%98%af%e6%9c%80%e4%bc%98%e7%82%b9>#</a></h4><blockquote class=book-hint><p><strong>核心洞察</strong>：在相同的计算预算下,有无数种 $(N, D)$ 组合可以选择（它们都在同一条 Isocost 曲线上）,但只有一个点能让 Loss 最小——这就是 Chinchilla 点。</p></blockquote><p><strong>Isocost Curve（等成本曲线）</strong>：满足 $C = 6ND$ 的所有 $(N, D)$ 点的集合。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=err>数据量</span> <span class=n>D</span> <span class=p>(</span><span class=n>Tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>▲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>                    <span class=n>Isocost</span> <span class=n>Curve</span><span class=p>:</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>6</span><span class=n>ND</span> <span class=o>=</span> <span class=n>constant</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>                   <span class=p>(</span><span class=err>所有点的计算成本相同</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>
</span></span><span class=line><span class=cl><span class=mi>7</span><span class=n>T</span><span class=err>│</span>  <span class=n>GPT</span><span class=o>-</span><span class=mi>3</span><span class=err>配置区</span>                              <span class=err>⭐</span> <span class=n>Chinchilla最优点</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>  <span class=p>(</span><span class=err>数据不足</span><span class=p>)</span>                              <span class=p>(</span><span class=n>N</span><span class=o>=</span><span class=mi>70</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=o>=</span><span class=mf>1.4</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>                                          <span class=n>Loss</span> <span class=o>=</span> <span class=mf>1.69</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>                  <span class=err>╱╲</span>
</span></span><span class=line><span class=cl><span class=mi>4</span><span class=n>T</span><span class=err>│</span>                 <span class=err>╱</span>  <span class=err>╲</span>  <span class=n>Loss等高线</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>                <span class=err>╱</span>    <span class=err>╲</span>  <span class=p>(</span><span class=n>Loss</span><span class=o>=</span><span class=mf>1.7</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>  <span class=err>●</span><span class=n>GPT</span><span class=o>-</span><span class=mi>3</span>      <span class=err>╱</span>  <span class=err>⭐</span>  <span class=err>╲</span>                     <span class=n>Gopher配置区</span>
</span></span><span class=line><span class=cl><span class=mi>2</span><span class=n>T</span><span class=err>│</span>  <span class=p>(</span><span class=n>N</span><span class=o>=</span><span class=mi>175</span><span class=n>B</span>    <span class=err>╱</span>        <span class=err>╲</span>                   <span class=p>(</span><span class=err>更差：</span><span class=n>N大D小</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>   <span class=n>D</span><span class=o>=</span><span class=mi>300</span><span class=n>B</span><span class=p>)</span>  <span class=err>╱</span>          <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>   <span class=n>Loss</span><span class=o>=</span><span class=mf>1.85</span>           <span class=err>╲</span>                  <span class=n>Loss</span><span class=o>=</span><span class=mf>1.90</span>
</span></span><span class=line><span class=cl><span class=mi>1</span><span class=n>T</span><span class=err>│</span>           <span class=err>╱</span>    <span class=err>最优点</span>   <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>          <span class=err>╱</span>   <span class=p>(</span><span class=err>谷底</span><span class=p>)</span>     <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>         <span class=err>╱</span>                <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>        <span class=err>╱</span>                  <span class=err>╲</span>     <span class=n>Loss等高线</span>
</span></span><span class=line><span class=cl><span class=mi>300</span><span class=n>B</span><span class=err>│</span>      <span class=err>╱╲</span>                    <span class=err>╲╲</span>  <span class=p>(</span><span class=n>Loss</span><span class=o>=</span><span class=mf>2.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>     <span class=err>╱</span>  <span class=err>╲</span>                     <span class=err>╲╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>    <span class=err>╱</span>    <span class=err>╲</span>                      <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>   <span class=err>╱</span>      <span class=err>╲</span>                      <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>│</span>  <span class=err>╱</span>        <span class=err>╲</span>                      <span class=err>╲</span>
</span></span><span class=line><span class=cl>  <span class=err>└─┴──────────┴──────────────────────┴────────▶</span> <span class=err>参数量</span> <span class=n>N</span>
</span></span><span class=line><span class=cl>   <span class=mi>10</span><span class=n>B</span>       <span class=mi>70</span><span class=n>B</span>      <span class=mi>100</span><span class=n>B</span>       <span class=mi>175</span><span class=n>B</span>      <span class=mi>300</span><span class=n>B</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>📍</span> <span class=err>关键观察：</span>
</span></span><span class=line><span class=cl><span class=mf>1.</span> <span class=n>GPT</span><span class=o>-</span><span class=mi>3</span><span class=err>点（</span><span class=mi>175</span><span class=n>B</span><span class=p>,</span> <span class=mi>300</span><span class=n>B</span><span class=err>）：参数太多，数据太少</span> <span class=err>→</span> <span class=n>Loss</span> <span class=o>=</span> <span class=mf>1.85</span>
</span></span><span class=line><span class=cl><span class=mf>2.</span> <span class=n>Chinchilla点</span><span class=err>（</span><span class=mi>70</span><span class=n>B</span><span class=p>,</span> <span class=mf>1.4</span><span class=n>T</span><span class=err>）：平衡配置</span> <span class=err>→</span> <span class=n>Loss</span> <span class=o>=</span> <span class=mf>1.69</span><span class=err>（最优！）</span>
</span></span><span class=line><span class=cl><span class=mf>3.</span> <span class=n>Gopher点</span><span class=err>（</span><span class=mi>280</span><span class=n>B</span><span class=p>,</span> <span class=mi>300</span><span class=n>B</span><span class=err>）：严重失衡</span> <span class=err>→</span> <span class=n>Loss</span> <span class=o>=</span> <span class=mf>1.90</span><span class=err>（最差）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>💡</span> <span class=err>直觉理解：</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>向左移动（增加</span><span class=n>N</span><span class=err>，减少</span><span class=n>D</span><span class=err>）：模型容量大但</span><span class=s2>&#34;营养不良&#34;</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=err>向右移动（减少</span><span class=n>N</span><span class=err>，增加</span><span class=n>D</span><span class=err>）：数据充足但模型容量不够</span>
</span></span><span class=line><span class=cl><span class=o>-</span> <span class=n>Chinchilla点</span><span class=err>：恰好平衡，</span><span class=n>Loss最低</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>数学本质：</span>
</span></span><span class=line><span class=cl><span class=err>在约束</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>6</span><span class=n>ND</span> <span class=err>下，最小化</span> <span class=n>L</span><span class=p>(</span><span class=n>N</span><span class=p>,</span><span class=n>D</span><span class=p>)</span> <span class=o>=</span> <span class=n>E</span> <span class=o>+</span> <span class=n>A</span><span class=o>/</span><span class=n>N</span><span class=o>^</span><span class=mf>0.5</span> <span class=o>+</span> <span class=n>B</span><span class=o>/</span><span class=n>D</span><span class=o>^</span><span class=mf>0.5</span>
</span></span><span class=line><span class=cl><span class=err>拉格朗日乘数法求解</span> <span class=err>→</span> <span class=err>得到最优点在</span> <span class=n>N</span> <span class=err>≈</span> <span class=n>D</span><span class=err>（</span><span class=mi>20</span><span class=p>:</span><span class=mi>1</span><span class=err>比例）</span></span></span></code></pre></div><p><strong>为什么 GPT-3 &ldquo;训练不足&rdquo;？</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 对比分析：相同计算预算下的不同策略</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compare_configurations</span><span class=p>(</span><span class=n>compute_budget_flops</span><span class=o>=</span><span class=mf>3.14e23</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    计算预算：3.14e23 FLOPs（训练GPT-3的实际成本）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># GPT-3的实际配置</span>
</span></span><span class=line><span class=cl>    <span class=n>gpt3_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=mf>175e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens&#34;</span><span class=p>:</span> <span class=mf>300e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_per_param&#34;</span><span class=p>:</span> <span class=mf>1.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;actual_compute&#34;</span><span class=p>:</span> <span class=mi>6</span> <span class=o>*</span> <span class=mf>175e9</span> <span class=o>*</span> <span class=mf>300e9</span><span class=p>,</span>  <span class=c1># 3.15e23 FLOPs</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=mf>1.85</span>  <span class=c1># 估计值</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Chinchilla建议的最优配置（相同计算预算）</span>
</span></span><span class=line><span class=cl>    <span class=n>optimal_N</span> <span class=o>=</span> <span class=p>(</span><span class=n>compute_budget_flops</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1.2e10</span>  <span class=c1># ≈ 70B</span>
</span></span><span class=line><span class=cl>    <span class=n>optimal_D</span> <span class=o>=</span> <span class=p>(</span><span class=n>compute_budget_flops</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=mf>7.5</span>     <span class=c1># ≈ 1.4T</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chinchilla_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=n>optimal_N</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens&#34;</span><span class=p>:</span> <span class=n>optimal_D</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_per_param&#34;</span><span class=p>:</span> <span class=n>optimal_D</span> <span class=o>/</span> <span class=n>optimal_N</span><span class=p>,</span>  <span class=c1># ≈ 20</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;actual_compute&#34;</span><span class=p>:</span> <span class=n>compute_budget_flops</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=mf>1.69</span>  <span class=c1># 实测值</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;相同计算预算下的配置对比&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;计算预算: </span><span class=si>{</span><span class=n>compute_budget_flops</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;【GPT-3配置】（2020年）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  参数量: </span><span class=si>{</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  训练Token数: </span><span class=si>{</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;tokens&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token/参数比: </span><span class=si>{</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;tokens_per_param&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Loss: </span><span class=si>{</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  问题: ❌ 参数太多，数据太少（</span><span class=se>\&#34;</span><span class=s2>大头娃娃</span><span class=se>\&#34;</span><span class=s2>）</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;【Chinchilla最优配置】（2022年）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  参数量: </span><span class=si>{</span><span class=n>chinchilla_config</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  训练Token数: </span><span class=si>{</span><span class=n>chinchilla_config</span><span class=p>[</span><span class=s1>&#39;tokens&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e12</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>T&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token/参数比: </span><span class=si>{</span><span class=n>chinchilla_config</span><span class=p>[</span><span class=s1>&#39;tokens_per_param&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Loss: </span><span class=si>{</span><span class=n>chinchilla_config</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  优势: ✅ 平衡配置，Loss降低 </span><span class=si>{</span><span class=p>(</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span><span class=o>-</span><span class=n>chinchilla_config</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>])</span><span class=o>/</span><span class=n>gpt3_config</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;【关键洞察】&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  用相同的钱，Chinchilla策略比GPT-3策略：&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - 参数量减少 </span><span class=si>{</span><span class=mi>175</span><span class=o>/</span><span class=mi>70</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x（从175B到70B）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - 数据量增加 </span><span class=si>{</span><span class=mi>1400</span><span class=o>/</span><span class=mi>300</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x（从300B到1.4T）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - 性能提升显著（Loss降低9%）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 运行对比</span>
</span></span><span class=line><span class=cl><span class=c1># compare_configurations()</span></span></span></code></pre></div><p><strong>关键对比</strong>：</p><table><thead><tr><th>模型</th><th>参数量</th><th>训练Token数</th><th>Token/参数比</th><th>是否最优？</th></tr></thead><tbody><tr><td>GPT-3</td><td>175B</td><td>300B</td><td>1.7x</td><td>❌ 数据不足</td></tr><tr><td>Gopher</td><td>280B</td><td>300B</td><td>1.1x</td><td>❌ 数据严重不足</td></tr><tr><td>Chinchilla</td><td>70B</td><td>1.4T</td><td><strong>20x</strong></td><td>✅ 最优</td></tr><tr><td>LLaMA</td><td>65B</td><td>1.4T</td><td>21.5x</td><td>✅ 接近最优</td></tr><tr><td>LLaMA-2</td><td>70B</td><td>2T</td><td>28.6x</td><td>✅ 接近最优</td></tr></tbody></table><p><strong>实验验证</strong>：Chinchilla（70B参数，1.4T token）性能超越Gopher（280B参数，300B token）！</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>chinchilla_optimal_config</span><span class=p>(</span><span class=n>compute_budget_flops</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    根据计算预算计算最优模型配置
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        compute_budget_flops: 计算预算（FLOPs）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回:
</span></span></span><span class=line><span class=cl><span class=s2>        包含最优参数量和数据量的字典
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Chinchilla Law的经验公式</span>
</span></span><span class=line><span class=cl>    <span class=c1># N_opt ≈ C^0.50 / 1.2e10</span>
</span></span><span class=line><span class=cl>    <span class=c1># D_opt ≈ C^0.50 / 7.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>=</span> <span class=n>compute_budget_flops</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>N_opt</span> <span class=o>=</span> <span class=p>(</span><span class=n>C</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1.2e10</span>  <span class=c1># 最优参数量</span>
</span></span><span class=line><span class=cl>    <span class=n>D_opt</span> <span class=o>=</span> <span class=p>(</span><span class=n>C</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=mf>7.5</span>      <span class=c1># 最优Token数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;optimal_params&#34;</span><span class=p>:</span> <span class=n>N_opt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;optimal_tokens&#34;</span><span class=p>:</span> <span class=n>D_opt</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_per_param&#34;</span><span class=p>:</span> <span class=n>D_opt</span> <span class=o>/</span> <span class=n>N_opt</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：不同计算预算的最优配置</span>
</span></span><span class=line><span class=cl><span class=n>budgets</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;小模型&#34;</span><span class=p>,</span> <span class=mf>1e20</span><span class=p>),</span>    <span class=c1># ~0.1B params</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;中模型&#34;</span><span class=p>,</span> <span class=mf>1e22</span><span class=p>),</span>    <span class=c1># ~8B params</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;大模型&#34;</span><span class=p>,</span> <span class=mf>1e24</span><span class=p>),</span>    <span class=c1># ~80B params</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;超大模型&#34;</span><span class=p>,</span> <span class=mf>1e25</span><span class=p>),</span>  <span class=c1># ~260B params</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;计算预算与最优配置:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>budget</span> <span class=ow>in</span> <span class=n>budgets</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=n>chinchilla_optimal_config</span><span class=p>(</span><span class=n>budget</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>name</span><span class=si>:</span><span class=s2>8s</span><span class=si>}</span><span class=s2> (C=</span><span class=si>{</span><span class=n>budget</span><span class=si>:</span><span class=s2>.0e</span><span class=si>}</span><span class=s2> FLOPs):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  最优参数量: </span><span class=si>{</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;optimal_params&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>6.1f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  最优Token数: </span><span class=si>{</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;optimal_tokens&#39;</span><span class=p>]</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>6.0f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token/参数比: </span><span class=si>{</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;tokens_per_param&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>()</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>计算预算与最优配置:
</span></span><span class=line><span class=cl>----------------------------------------------------------------------
</span></span><span class=line><span class=cl>小模型   (C=1e+20 FLOPs):
</span></span><span class=line><span class=cl>  最优参数量:    0.8B
</span></span><span class=line><span class=cl>  最优Token数:     13B
</span></span><span class=line><span class=cl>  Token/参数比: 16.0x
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>中模型   (C=1e+22 FLOPs):
</span></span><span class=line><span class=cl>  最优参数量:    8.3B
</span></span><span class=line><span class=cl>  最优Token数:    133B
</span></span><span class=line><span class=cl>  Token/参数比: 16.0x
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>大模型   (C=1e+24 FLOPs):
</span></span><span class=line><span class=cl>  最优参数量:   83.3B
</span></span><span class=line><span class=cl>  最优Token数:   1333B
</span></span><span class=line><span class=cl>  Token/参数比: 16.0x
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>超大模型 (C=1e+25 FLOPs):
</span></span><span class=line><span class=cl>  最优参数量:  263.5B
</span></span><span class=line><span class=cl>  最优Token数:   4216B
</span></span><span class=line><span class=cl>  Token/参数比: 16.0x</span></span></code></pre></div><h3 id=-深度解析scaling-law的数学推导>🎯 深度解析：Scaling Law的数学推导<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90scaling-law%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc>#</a></h3><blockquote class=book-hint><p>为什么 Chinchilla Law 得出 &ldquo;20 tokens per param&rdquo; 的结论？让我们用微积分来证明它。</p></blockquote><p>我们定义大模型的损失函数 $L$ 与参数量 $N$ 和数据量 $D$ 的关系为幂律分布：</p><p>$$
L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
$$</p><p>其中：</p><ul><li>$E$：不可约减的损失（贝叶斯误差，即语言本身的熵）</li><li>$A, B$：常数系数</li><li>$\alpha, \beta$：幂律指数（实验测得 $\alpha \approx 0.34, \beta \approx 0.28$ for Kaplan; $\alpha \approx \beta \approx 0.5$ for Chinchilla）</li></ul><p><strong>优化目标</strong>：在给定计算预算 $C$ 的约束下，最小化损失 $L$。</p><p><strong>约束条件</strong>：
训练一个Transformer模型的计算量（FLOPs）近似公式为：</p><p>$$
C \approx 6 N D
$$</p><p>其中：</p><ul><li>$N$ = 模型参数量</li><li>$D$ = 训练Token数量</li><li>系数6来自：Forward Pass（$2N$ FLOPs/token）+ Backward Pass（$4N$ FLOPs/token）</li></ul><blockquote class=book-hint><p><strong>为什么是6？</strong> 详细推导见下方3.2.4节"实战算账"部分。</p></blockquote><h4 id=1-拉格朗日乘数法求解>1. 拉格朗日乘数法求解<a class=anchor href=#1-%e6%8b%89%e6%a0%bc%e6%9c%97%e6%97%a5%e4%b9%98%e6%95%b0%e6%b3%95%e6%b1%82%e8%a7%a3>#</a></h4><p>构造拉格朗日函数：</p><p>$$
\mathcal{L}(N, D, \lambda) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} + \lambda (6ND - C)
$$</p><p>分别对 $N$ 和 $D$ 求偏导并令其为0：</p><p>$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial N} = -\frac{\alpha A}{N^{\alpha+1}} + 6\lambda D = 0 \
\frac{\partial \mathcal{L}}{\partial D} = -\frac{\beta B}{D^{\beta+1}} + 6\lambda N = 0
\end{cases}
$$</p><p>化简得到：</p><p>$$
\begin{cases}
6\lambda = \frac{\alpha A}{N^{\alpha+1} D} \
6\lambda = \frac{\beta B}{D^{\beta+1} N}
\end{cases}
$$</p><p>联立方程：</p><p>$$
\frac{\alpha A}{N^{\alpha+1} D} = \frac{\beta B}{D^{\beta+1} N}
$$</p><p>$$
\frac{\alpha A}{N^{\alpha}} = \frac{\beta B}{D^{\beta}}
$$</p><p>整理得到最优参数量 $N_{opt}$ 与最优数据量 $D_{opt}$ 的比例关系：</p><p>$$
D_{opt} = \left( \frac{\beta B}{\alpha A} \right)^{1/\beta} N_{opt}^{\alpha/\beta}
$$</p><h4 id=2-为什么-chinchilla-的结论是-11-增长>2. 为什么 Chinchilla 的结论是 1:1 增长？<a class=anchor href=#2-%e4%b8%ba%e4%bb%80%e4%b9%88-chinchilla-%e7%9a%84%e7%bb%93%e8%ae%ba%e6%98%af-11-%e5%a2%9e%e9%95%bf>#</a></h4><p>DeepMind 团队通过对 400 多个模型的实验拟合，发现对于现在的 Transformer 架构：</p><p>$$
\alpha \approx 0.50, \quad \beta \approx 0.50
$$</p><p>代入上面的比例关系：</p><p>$$
D_{opt} \propto N_{opt}^{0.5/0.5} \implies D_{opt} \propto N_{opt}
$$</p><p>这证明了：<strong>参数量和数据量应该线性同步增长</strong>。</p><p>进一步，将 $N \propto D$ 代入约束条件 $C \approx 6ND$：</p><p>$$
C \propto N \cdot N = N^2 \implies N_{opt} \propto \sqrt{C} = C^{0.5}
$$
$$
C \propto D \cdot D = D^2 \implies D_{opt} \propto \sqrt{C} = C^{0.5}
$$</p><p><strong>结论</strong>：当计算预算 $C$ 增加 100 倍时，参数量 $N$ 和数据量 $D$ 应该分别增加 10 倍（即 $\sqrt{100}$）。</p><h4 id=3-为什么-kaplan-当年搞错了>3. 为什么 Kaplan 当年搞错了？<a class=anchor href=#3-%e4%b8%ba%e4%bb%80%e4%b9%88-kaplan-%e5%bd%93%e5%b9%b4%e6%90%9e%e9%94%99%e4%ba%86>#</a></h4><p>Kaplan (OpenAI 2020) 当时测得 $\alpha \approx 0.076, \beta \approx 0.095$（注意这里定义的 $L$ 形式略有不同，导致指数数值不同，但核心结论是 $\alpha &lt; \beta$）。</p><p>这导致他们认为：<strong>参数量的增加比数据量的增加更重要</strong>。所以 GPT-3 做到了 175B 这么大，但数据量只有 300B（比例 1.7:1），这在当时被认为是合理的，但按 Chinchilla 标准看是严重的"大头娃娃"（参数虚高，训练不足）。</p><p>Chinchilla 指出 Kaplan 的实验主要基于较小的学习率调度，导致模型在固定数据量并未收敛，从而高估了参数量的作用。</p><h4 id=4-实战算账训练预算估算>4. 实战算账：训练预算估算<a class=anchor href=#4-%e5%ae%9e%e6%88%98%e7%ae%97%e8%b4%a6%e8%ae%ad%e7%bb%83%e9%a2%84%e7%ae%97%e4%bc%b0%e7%ae%97>#</a></h4><blockquote class=book-hint><p><strong>老板最关心的问题：训练一个70B模型要花多少钱？</strong></p></blockquote><p>作为架构师，你必须会算这笔账。让我们从FLOPs公式开始，一步步算清楚。</p><h5 id=41-flops公式详解为什么是-6nd>4.1 FLOPs公式详解：为什么是 6ND?<a class=anchor href=#41-flops%e5%85%ac%e5%bc%8f%e8%af%a6%e8%a7%a3%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af-6nd>#</a></h5><p>训练Transformer模型的计算量公式：</p><p>$$
C \approx 6ND
$$</p><p>其中：</p><ul><li>$N$ = 模型参数量</li><li>$D$ = 训练Token数量</li><li>$C$ = 总计算量（FLOPs）</li></ul><p><strong>为什么系数是6？</strong> 让我们拆解计算过程：</p><p><strong>Forward Pass（前向传播）</strong>：约 $2N$ FLOPs per token</p><ul><li>每个参数需要1次乘法 + 1次加法 = 2次浮点运算</li><li>例如：一个 $d \times d$ 的矩阵乘法需要 $2d^2$ FLOPs</li></ul><p><strong>Backward Pass（反向传播）</strong>：约 $4N$ FLOPs per token</p><ul><li>梯度计算需要重新计算激活值：$2N$ FLOPs</li><li>参数梯度累积：$2N$ FLOPs</li><li>总计：$4N$ FLOPs</li></ul><p><strong>总计</strong>：Forward $2N$ + Backward $4N$ = $6N$ FLOPs per token</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>detailed_flops_breakdown</span><span class=p>(</span><span class=n>params_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>tokens_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    详细分解训练FLOPs计算
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数：
</span></span></span><span class=line><span class=cl><span class=s2>        params_b: 参数量（十亿）
</span></span></span><span class=line><span class=cl><span class=s2>        tokens_b: Token数量（十亿）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回：
</span></span></span><span class=line><span class=cl><span class=s2>        详细的FLOPs分解
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=n>params_b</span> <span class=o>*</span> <span class=mf>1e9</span>  <span class=c1># 转换为实际参数数量</span>
</span></span><span class=line><span class=cl>    <span class=n>D</span> <span class=o>=</span> <span class=n>tokens_b</span> <span class=o>*</span> <span class=mf>1e9</span>  <span class=c1># 转换为实际Token数量</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Forward Pass</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_per_token</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>N</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_total</span> <span class=o>=</span> <span class=n>forward_per_token</span> <span class=o>*</span> <span class=n>D</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Backward Pass</span>
</span></span><span class=line><span class=cl>    <span class=n>backward_per_token</span> <span class=o>=</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>N</span>
</span></span><span class=line><span class=cl>    <span class=n>backward_total</span> <span class=o>=</span> <span class=n>backward_per_token</span> <span class=o>*</span> <span class=n>D</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 总计</span>
</span></span><span class=line><span class=cl>    <span class=n>total</span> <span class=o>=</span> <span class=n>forward_total</span> <span class=o>+</span> <span class=n>backward_total</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;forward_per_token&#34;</span><span class=p>:</span> <span class=n>forward_per_token</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;forward_total&#34;</span><span class=p>:</span> <span class=n>forward_total</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;backward_per_token&#34;</span><span class=p>:</span> <span class=n>backward_per_token</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;backward_total&#34;</span><span class=p>:</span> <span class=n>backward_total</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;total_flops&#34;</span><span class=p>:</span> <span class=n>total</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;multiplier&#34;</span><span class=p>:</span> <span class=n>total</span> <span class=o>/</span> <span class=p>(</span><span class=n>N</span> <span class=o>*</span> <span class=n>D</span><span class=p>)</span>  <span class=c1># 验证是否接近6</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：GPT-3</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>detailed_flops_breakdown</span><span class=p>(</span><span class=mi>175</span><span class=p>,</span> <span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;GPT-3 (175B参数, 300B Tokens) FLOPs分解：&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Forward Pass (每token)：  </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;forward_per_token&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Backward Pass (每token)： </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;backward_per_token&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;合计 (每token)：          </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;forward_per_token&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;backward_per_token&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Forward Pass (总计)：     </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;forward_total&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Backward Pass (总计)：    </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;backward_total&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;训练总FLOPs：             </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;total_flops&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;验证系数 (C/ND)：         </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;multiplier&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> ✓&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>GPT-3 (175B参数, 300B Tokens) FLOPs分解：
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>Forward Pass (每token)：  3.50e+11 FLOPs
</span></span><span class=line><span class=cl>Backward Pass (每token)： 7.00e+11 FLOPs
</span></span><span class=line><span class=cl>合计 (每token)：          1.05e+12 FLOPs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Forward Pass (总计)：     1.05e+23 FLOPs
</span></span><span class=line><span class=cl>Backward Pass (总计)：    2.10e+23 FLOPs
</span></span><span class=line><span class=cl>训练总FLOPs：             3.15e+23 FLOPs
</span></span><span class=line><span class=cl>验证系数 (C/ND)：         6.0 ✓</span></span></code></pre></div><h5 id=42-实战案例训练70b模型需要多少资源>4.2 实战案例：训练70B模型需要多少资源？<a class=anchor href=#42-%e5%ae%9e%e6%88%98%e6%a1%88%e4%be%8b%e8%ae%ad%e7%bb%8370b%e6%a8%a1%e5%9e%8b%e9%9c%80%e8%a6%81%e5%a4%9a%e5%b0%91%e8%b5%84%e6%ba%90>#</a></h5><p>假设我们要训练一个<strong>LLaMA-70B</strong>规模的模型，使用<strong>Chinchilla Law</strong>的最优配比。</p><p><strong>第一步：确定数据量</strong></p><p>根据Chinchilla Law：$D = 20N$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calculate_training_budget</span><span class=p>(</span><span class=n>params_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>gpu_tflops</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                             <span class=n>num_gpus</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>utilization</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    计算训练预算：FLOPs、GPU卡数、训练时间
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数：
</span></span></span><span class=line><span class=cl><span class=s2>        params_b: 模型参数量（十亿）
</span></span></span><span class=line><span class=cl><span class=s2>        gpu_tflops: 单卡算力（TFLOPS, FP16/BF16）
</span></span></span><span class=line><span class=cl><span class=s2>        num_gpus: GPU数量
</span></span></span><span class=line><span class=cl><span class=s2>        utilization: GPU利用率（考虑通信开销等，通常0.4-0.6）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回：
</span></span></span><span class=line><span class=cl><span class=s2>        训练预算详情
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 根据Chinchilla Law计算最优Token数</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens_b</span> <span class=o>=</span> <span class=mi>20</span> <span class=o>*</span> <span class=n>params_b</span>  <span class=c1># 20 tokens per parameter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 计算总FLOPs</span>
</span></span><span class=line><span class=cl>    <span class=n>total_flops</span> <span class=o>=</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>params_b</span> <span class=o>*</span> <span class=mf>1e9</span> <span class=o>*</span> <span class=n>tokens_b</span> <span class=o>*</span> <span class=mf>1e9</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 计算有效算力（考虑利用率）</span>
</span></span><span class=line><span class=cl>    <span class=n>effective_tflops_per_gpu</span> <span class=o>=</span> <span class=n>gpu_tflops</span> <span class=o>*</span> <span class=n>utilization</span>
</span></span><span class=line><span class=cl>    <span class=n>total_tflops</span> <span class=o>=</span> <span class=n>effective_tflops_per_gpu</span> <span class=o>*</span> <span class=n>num_gpus</span>
</span></span><span class=line><span class=cl>    <span class=n>total_flops_per_second</span> <span class=o>=</span> <span class=n>total_tflops</span> <span class=o>*</span> <span class=mf>1e12</span>  <span class=c1># 转换为FLOPS</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. 计算训练时间</span>
</span></span><span class=line><span class=cl>    <span class=n>training_seconds</span> <span class=o>=</span> <span class=n>total_flops</span> <span class=o>/</span> <span class=n>total_flops_per_second</span>
</span></span><span class=line><span class=cl>    <span class=n>training_hours</span> <span class=o>=</span> <span class=n>training_seconds</span> <span class=o>/</span> <span class=mi>3600</span>
</span></span><span class=line><span class=cl>    <span class=n>training_days</span> <span class=o>=</span> <span class=n>training_hours</span> <span class=o>/</span> <span class=mi>24</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5. 计算GPU时（GPU-hours）</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu_hours</span> <span class=o>=</span> <span class=n>training_hours</span> <span class=o>*</span> <span class=n>num_gpus</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;params_b&#34;</span><span class=p>:</span> <span class=n>params_b</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_b&#34;</span><span class=p>:</span> <span class=n>tokens_b</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;total_flops&#34;</span><span class=p>:</span> <span class=n>total_flops</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;num_gpus&#34;</span><span class=p>:</span> <span class=n>num_gpus</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gpu_tflops&#34;</span><span class=p>:</span> <span class=n>gpu_tflops</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;utilization&#34;</span><span class=p>:</span> <span class=n>utilization</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;effective_tflops&#34;</span><span class=p>:</span> <span class=n>total_tflops</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;training_hours&#34;</span><span class=p>:</span> <span class=n>training_hours</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;training_days&#34;</span><span class=p>:</span> <span class=n>training_days</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gpu_hours&#34;</span><span class=p>:</span> <span class=n>gpu_hours</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 案例：训练70B模型，使用256张A100（80GB）</span>
</span></span><span class=line><span class=cl><span class=n>budget</span> <span class=o>=</span> <span class=n>calculate_training_budget</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params_b</span><span class=o>=</span><span class=mi>70</span><span class=p>,</span>           <span class=c1># 70B参数</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu_tflops</span><span class=o>=</span><span class=mi>312</span><span class=p>,</span>        <span class=c1># A100的BF16算力</span>
</span></span><span class=line><span class=cl>    <span class=n>num_gpus</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span>          <span class=c1># 256张卡</span>
</span></span><span class=line><span class=cl>    <span class=n>utilization</span><span class=o>=</span><span class=mf>0.5</span>        <span class=c1># 50%利用率（考虑通信、IO等开销）</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;训练70B模型的资源预算：&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;模型参数量：        </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;params_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;训练Token数：       </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;tokens_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B (遵循Chinchilla Law: 20x参数量)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;总计算量：          </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;total_flops&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2> FLOPs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;硬件配置：          </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;num_gpus&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2> x A100 (80GB)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;单卡算力：          </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;gpu_tflops&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2> TFLOPS (BF16)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPU利用率：         </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;utilization&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>% (考虑通信/IO开销)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;有效总算力：        </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;effective_tflops&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> TFLOPS&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;训练时间：          </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;training_days&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> 天 (</span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;training_hours&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2> 小时)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPU总时：           </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;gpu_hours&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>,.0f</span><span class=si>}</span><span class=s2> GPU-hours&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;成本估算 (AWS按需)：  $</span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;gpu_hours&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mf>4.1</span><span class=si>:</span><span class=s2>,.0f</span><span class=si>}</span><span class=s2> USD&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;              (A100 80GB: ~$4.1/GPU-hour)&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练70B模型的资源预算：
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>模型参数量：        70B
</span></span><span class=line><span class=cl>训练Token数：       1400B (遵循Chinchilla Law: 20x参数量)
</span></span><span class=line><span class=cl>总计算量：          5.88e+23 FLOPs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>硬件配置：          256 x A100 (80GB)
</span></span><span class=line><span class=cl>单卡算力：          312 TFLOPS (BF16)
</span></span><span class=line><span class=cl>GPU利用率：         50% (考虑通信/IO开销)
</span></span><span class=line><span class=cl>有效总算力：        39936.0 TFLOPS
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>训练时间：          17.1 天 (410 小时)
</span></span><span class=line><span class=cl>GPU总时：           104,960 GPU-hours
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>成本估算 (AWS按需)：  $430,336 USD
</span></span><span class=line><span class=cl>              (A100 80GB: ~$4.1/GPU-hour)</span></span></code></pre></div><h5 id=43-不同规模模型的训练预算对比>4.3 不同规模模型的训练预算对比<a class=anchor href=#43-%e4%b8%8d%e5%90%8c%e8%a7%84%e6%a8%a1%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%ad%e7%bb%83%e9%a2%84%e7%ae%97%e5%af%b9%e6%af%94>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 对比不同规模模型的训练成本</span>
</span></span><span class=line><span class=cl><span class=n>model_configs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Small (7B)&#34;</span><span class=p>,</span>   <span class=mi>7</span><span class=p>,</span>   <span class=mi>128</span><span class=p>,</span>  <span class=mi>312</span><span class=p>),</span>  <span class=c1># 128张A100</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Medium (13B)&#34;</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span>  <span class=mi>256</span><span class=p>,</span>  <span class=mi>312</span><span class=p>),</span>  <span class=c1># 256张A100</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Large (70B)&#34;</span><span class=p>,</span>  <span class=mi>70</span><span class=p>,</span>  <span class=mi>256</span><span class=p>,</span>  <span class=mi>312</span><span class=p>),</span>  <span class=c1># 256张A100</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;XLarge (175B)&#34;</span><span class=p>,</span> <span class=mi>175</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>312</span><span class=p>),</span>  <span class=c1># 512张A100</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;不同规模模型的训练预算对比：&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;模型&#39;</span><span class=si>:</span><span class=s2>&lt;15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;参数&#39;</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;Token数&#39;</span><span class=si>:</span><span class=s2>&lt;10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;GPU卡数&#39;</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>      <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;训练天数&#39;</span><span class=si>:</span><span class=s2>&lt;10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;GPU-hours&#39;</span><span class=si>:</span><span class=s2>&lt;12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;成本($)&#39;</span><span class=si>:</span><span class=s2>&lt;12</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>gpus</span><span class=p>,</span> <span class=n>tflops</span> <span class=ow>in</span> <span class=n>model_configs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>budget</span> <span class=o>=</span> <span class=n>calculate_training_budget</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>tflops</span><span class=p>,</span> <span class=n>gpus</span><span class=p>,</span> <span class=n>utilization</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cost</span> <span class=o>=</span> <span class=n>budget</span><span class=p>[</span><span class=s1>&#39;gpu_hours&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mf>4.1</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>name</span><span class=si>:</span><span class=s2>&lt;15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>params</span><span class=si>:</span><span class=s2>&lt;8.0f</span><span class=si>}</span><span class=s2>B | </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;tokens_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;10.0f</span><span class=si>}</span><span class=s2>B | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpus</span><span class=si>:</span><span class=s2>&lt;8</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;training_days&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;10.1f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>budget</span><span class=p>[</span><span class=s1>&#39;gpu_hours&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>&lt;12,.0f</span><span class=si>}</span><span class=s2> | $</span><span class=si>{</span><span class=n>cost</span><span class=si>:</span><span class=s2>&lt;11,.0f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>不同规模模型的训练预算对比：
</span></span><span class=line><span class=cl>====================================================================================================
</span></span><span class=line><span class=cl>模型             | 参数     | Token数    | GPU卡数  | 训练天数   | GPU-hours    | 成本($)
</span></span><span class=line><span class=cl>----------------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>Small (7B)      | 7B       | 140B       | 128      | 3.9        | 6,323        | $25,924
</span></span><span class=line><span class=cl>Medium (13B)    | 13B      | 260B       | 256      | 3.1        | 12,052       | $49,413
</span></span><span class=line><span class=cl>Large (70B)     | 70B      | 1400B      | 256      | 17.1       | 104,960      | $430,336
</span></span><span class=line><span class=cl>XLarge (175B)   | 175B     | 3500B      | 512      | 21.3       | 261,856      | $1,073,610</span></span></code></pre></div><h5 id=44-关键洞察>4.4 关键洞察<a class=anchor href=#44-%e5%85%b3%e9%94%ae%e6%b4%9e%e5%af%9f>#</a></h5><ol><li><p><strong>训练时间与GPU数量的权衡</strong></p><ul><li>70B模型用256张A100训练17天</li><li>如果只用128张A100，训练时间翻倍至34天</li><li>但总GPU-hours保持不变（约10.5万）</li></ul></li><li><p><strong>Chinchilla Law的成本含义</strong></p><ul><li>70B模型需要1.4T Tokens（20x参数量）</li><li>如果按GPT-3的1.7x配比，只需119B Tokens</li><li>但模型性能会显著下降（参考3.2节对比）</li></ul></li><li><p><strong>成本优化策略</strong></p><ul><li><strong>Spot实例</strong>：AWS Spot可节省70%成本（$430K → $130K）</li><li><strong>混合精度</strong>：BF16训练比FP32快2倍，内存减半</li><li><strong>Flash Attention</strong>：可提升利用率从50%到60%+</li></ul></li><li><p><strong>工业界的实际数字</strong></p><ul><li>LLaMA-65B：使用2048张A100训练约21天</li><li>GPT-3 (175B)：使用约1万张V100训练数月</li><li>Chinchilla (70B)：使用1536张TPUv4训练数周</li></ul></li></ol><p><strong>实战建议</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>recommend_training_plan</span><span class=p>(</span><span class=n>params_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>budget_usd</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                           <span class=n>deadline_days</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    根据预算和截止日期推荐训练方案
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数：
</span></span></span><span class=line><span class=cl><span class=s2>        params_b: 目标参数量
</span></span></span><span class=line><span class=cl><span class=s2>        budget_usd: 预算（美元）
</span></span></span><span class=line><span class=cl><span class=s2>        deadline_days: 截止日期（天）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回：
</span></span></span><span class=line><span class=cl><span class=s2>        推荐的GPU配置
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># A100价格（AWS按需/Spot）</span>
</span></span><span class=line><span class=cl>    <span class=n>a100_ondemand</span> <span class=o>=</span> <span class=mf>4.1</span>  <span class=c1># $/GPU-hour</span>
</span></span><span class=line><span class=cl>    <span class=n>a100_spot</span> <span class=o>=</span> <span class=mf>1.2</span>      <span class=c1># $/GPU-hour (70%折扣)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算所需GPU-hours</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens_b</span> <span class=o>=</span> <span class=mi>20</span> <span class=o>*</span> <span class=n>params_b</span>
</span></span><span class=line><span class=cl>    <span class=n>total_flops</span> <span class=o>=</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>params_b</span> <span class=o>*</span> <span class=mf>1e9</span> <span class=o>*</span> <span class=n>tokens_b</span> <span class=o>*</span> <span class=mf>1e9</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 根据预算计算可用GPU-hours</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu_hours_ondemand</span> <span class=o>=</span> <span class=n>budget_usd</span> <span class=o>/</span> <span class=n>a100_ondemand</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu_hours_spot</span> <span class=o>=</span> <span class=n>budget_usd</span> <span class=o>/</span> <span class=n>a100_spot</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算所需GPU数量（基于截止日期）</span>
</span></span><span class=line><span class=cl>    <span class=n>required_gpu_hours</span> <span class=o>=</span> <span class=n>total_flops</span> <span class=o>/</span> <span class=p>(</span><span class=mi>312</span> <span class=o>*</span> <span class=mf>1e12</span> <span class=o>*</span> <span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 50%利用率</span>
</span></span><span class=line><span class=cl>    <span class=n>min_gpus</span> <span class=o>=</span> <span class=n>required_gpu_hours</span> <span class=o>/</span> <span class=p>(</span><span class=n>deadline_days</span> <span class=o>*</span> <span class=mi>24</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 推荐方案</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>gpu_hours_spot</span> <span class=o>&gt;=</span> <span class=n>required_gpu_hours</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>use_spot</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=n>num_gpus</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>min_gpus</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>actual_days</span> <span class=o>=</span> <span class=n>required_gpu_hours</span> <span class=o>/</span> <span class=p>(</span><span class=n>num_gpus</span> <span class=o>*</span> <span class=mi>24</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>actual_cost</span> <span class=o>=</span> <span class=n>required_gpu_hours</span> <span class=o>*</span> <span class=n>a100_spot</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;预算不足，无法在截止日期前完成训练&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;params_b&#34;</span><span class=p>:</span> <span class=n>params_b</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_b&#34;</span><span class=p>:</span> <span class=n>tokens_b</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;required_gpu_hours&#34;</span><span class=p>:</span> <span class=n>required_gpu_hours</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;recommended_gpus&#34;</span><span class=p>:</span> <span class=n>num_gpus</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;training_days&#34;</span><span class=p>:</span> <span class=n>actual_days</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;estimated_cost&#34;</span><span class=p>:</span> <span class=n>actual_cost</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;use_spot_instances&#34;</span><span class=p>:</span> <span class=n>use_spot</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：预算50万美元，3周内训练70B模型</span>
</span></span><span class=line><span class=cl><span class=n>plan</span> <span class=o>=</span> <span class=n>recommend_training_plan</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params_b</span><span class=o>=</span><span class=mi>70</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>budget_usd</span><span class=o>=</span><span class=mi>500000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>deadline_days</span><span class=o>=</span><span class=mi>21</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=s2>&#34;error&#34;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>plan</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;训练方案推荐：&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;目标模型：         </span><span class=si>{</span><span class=n>plan</span><span class=p>[</span><span class=s1>&#39;params_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B参数, </span><span class=si>{</span><span class=n>plan</span><span class=p>[</span><span class=s1>&#39;tokens_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B Tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;推荐配置：         </span><span class=si>{</span><span class=n>plan</span><span class=p>[</span><span class=s1>&#39;recommended_gpus&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2> x A100 (Spot实例)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预计训练时间：     </span><span class=si>{</span><span class=n>plan</span><span class=p>[</span><span class=s1>&#39;training_days&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> 天&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预计成本：         $</span><span class=si>{</span><span class=n>plan</span><span class=p>[</span><span class=s1>&#39;estimated_cost&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>,.0f</span><span class=si>}</span><span class=s2> USD&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预算剩余：         $</span><span class=si>{</span><span class=mi>500000</span> <span class=o>-</span> <span class=n>plan</span><span class=p>[</span><span class=s1>&#39;estimated_cost&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>,.0f</span><span class=si>}</span><span class=s2> USD&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>plan</span><span class=p>[</span><span class=s2>&#34;error&#34;</span><span class=p>])</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练方案推荐：
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>目标模型：         70B参数, 1400B Tokens
</span></span><span class=line><span class=cl>推荐配置：         209 x A100 (Spot实例)
</span></span><span class=line><span class=cl>预计训练时间：     20.9 天
</span></span><span class=line><span class=cl>预计成本：         $125,902 USD
</span></span><span class=line><span class=cl>预算剩余：         $374,098 USD</span></span></code></pre></div><hr><h3 id=33-涌现能力与相变现象>3.3 涌现能力与相变现象<a class=anchor href=#33-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%e4%b8%8e%e7%9b%b8%e5%8f%98%e7%8e%b0%e8%b1%a1>#</a></h3><blockquote class=book-hint><p><strong>比喻：智力的临界点</strong></p><p>就像人类大脑的发育：</p><ul><li><strong>婴儿（100M参数）</strong>：只能识别简单图形</li><li><strong>儿童（1B参数）</strong>：突然学会说话（语言涌现）</li><li><strong>少年（10B参数）</strong>：突然理解数学概念（抽象思维涌现）</li><li><strong>成年（100B+参数）</strong>：突然能进行复杂推理（逻辑能力涌现）</li></ul><p>这些能力不是线性增长，而是在某个"临界点"突然爆发！</p></blockquote><p><strong>涌现能力（Emergent Abilities）</strong>：当模型规模超过某个阈值时，突然出现的新能力。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EmergentAbility</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;涌现能力建模&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>threshold_params</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>performance_curve</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>name</span> <span class=o>=</span> <span class=n>name</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>threshold</span> <span class=o>=</span> <span class=n>threshold_params</span>  <span class=c1># 涌现阈值（参数量）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>performance_curve</span> <span class=o>=</span> <span class=n>performance_curve</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>evaluate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_params</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;评估给定规模模型的能力&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>performance_curve</span><span class=p>(</span><span class=n>model_params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义几个典型的涌现能力</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>few_shot_learning_curve</span><span class=p>(</span><span class=n>N</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Few-shot学习能力（在1B参数时涌现）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>N</span> <span class=o>&lt;</span> <span class=mf>1e9</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.0</span>  <span class=c1># 几乎没有few-shot能力</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>min</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=p>(</span><span class=n>N</span> <span class=o>-</span> <span class=mf>1e9</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1e11</span><span class=p>)</span>  <span class=c1># 逐渐增强</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>arithmetic_reasoning_curve</span><span class=p>(</span><span class=n>N</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;算术推理能力（在10B参数时涌现）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>N</span> <span class=o>&lt;</span> <span class=mf>10e9</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>min</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=p>(</span><span class=n>N</span> <span class=o>-</span> <span class=mf>10e9</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1e11</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>multi_step_reasoning_curve</span><span class=p>(</span><span class=n>N</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;多步推理能力（在100B参数时涌现）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>N</span> <span class=o>&lt;</span> <span class=mf>100e9</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>min</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=p>(</span><span class=n>N</span> <span class=o>-</span> <span class=mf>100e9</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1e11</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建涌现能力对象</span>
</span></span><span class=line><span class=cl><span class=n>emergent_abilities</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergentAbility</span><span class=p>(</span><span class=s2>&#34;Few-shot Learning&#34;</span><span class=p>,</span> <span class=mf>1e9</span><span class=p>,</span> <span class=n>few_shot_learning_curve</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergentAbility</span><span class=p>(</span><span class=s2>&#34;Arithmetic Reasoning&#34;</span><span class=p>,</span> <span class=mf>10e9</span><span class=p>,</span> <span class=n>arithmetic_reasoning_curve</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergentAbility</span><span class=p>(</span><span class=s2>&#34;Multi-step Reasoning&#34;</span><span class=p>,</span> <span class=mf>100e9</span><span class=p>,</span> <span class=n>multi_step_reasoning_curve</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 评估不同规模模型的能力</span>
</span></span><span class=line><span class=cl><span class=n>model_sizes</span> <span class=o>=</span> <span class=p>[</span><span class=mf>1e8</span><span class=p>,</span> <span class=mf>1e9</span><span class=p>,</span> <span class=mf>10e9</span><span class=p>,</span> <span class=mf>100e9</span><span class=p>,</span> <span class=mf>175e9</span><span class=p>]</span>  <span class=c1># 100M到175B</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;模型规模与涌现能力:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>model_sizes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>模型规模: </span><span class=si>{</span><span class=n>size</span><span class=o>/</span><span class=mf>1e9</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>B参数&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>ability</span> <span class=ow>in</span> <span class=n>emergent_abilities</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>performance</span> <span class=o>=</span> <span class=n>ability</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>status</span> <span class=o>=</span> <span class=s2>&#34;✅&#34;</span> <span class=k>if</span> <span class=n>performance</span> <span class=o>&gt;</span> <span class=mf>0.5</span> <span class=k>else</span> <span class=s2>&#34;❌&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=n>status</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>ability</span><span class=o>.</span><span class=n>name</span><span class=si>:</span><span class=s2>25s</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>performance</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>5.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>模型规模与涌现能力:
</span></span><span class=line><span class=cl>--------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型规模: 0.1B参数
</span></span><span class=line><span class=cl>  ❌ Few-shot Learning       :   0.0%
</span></span><span class=line><span class=cl>  ❌ Arithmetic Reasoning    :   0.0%
</span></span><span class=line><span class=cl>  ❌ Multi-step Reasoning    :   0.0%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型规模: 1.0B参数
</span></span><span class=line><span class=cl>  ❌ Few-shot Learning       :   0.0%
</span></span><span class=line><span class=cl>  ❌ Arithmetic Reasoning    :   0.0%
</span></span><span class=line><span class=cl>  ❌ Multi-step Reasoning    :   0.0%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型规模: 10.0B参数
</span></span><span class=line><span class=cl>  ✅ Few-shot Learning       :  90.0%
</span></span><span class=line><span class=cl>  ❌ Arithmetic Reasoning    :   0.0%
</span></span><span class=line><span class=cl>  ❌ Multi-step Reasoning    :   0.0%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型规模: 100.0B参数
</span></span><span class=line><span class=cl>  ✅ Few-shot Learning       : 100.0%
</span></span><span class=line><span class=cl>  ✅ Few Arithmetic Reasoning    :  90.0%
</span></span><span class=line><span class=cl>  ❌ Multi-step Reasoning    :   0.0%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型规模: 175.0B参数
</span></span><span class=line><span class=cl>  ✅ Few-shot Learning       : 100.0%
</span></span><span class=line><span class=cl>  ✅ Arithmetic Reasoning    : 100.0%
</span></span><span class=line><span class=cl>  ✅ Multi-step Reasoning    :  75.0%</span></span></code></pre></div><p><strong>真实案例</strong>（来自论文）：</p><table><thead><tr><th>能力</th><th>GPT-2 (1.5B)</th><th>GPT-3 (175B)</th><th>性能提升</th></tr></thead><tbody><tr><td>3-digit加法</td><td>0%</td><td>80%</td><td><strong>从无到有</strong></td></tr><tr><td>单词重组</td><td>5%</td><td>67%</td><td>13倍</td></tr><tr><td>多步推理</td><td>2%</td><td>58%</td><td>29倍</td></tr></tbody></table><hr><p><strong>涌现能力的微观视角</strong>：在宏观上，我们看到了模型规模扩大时能力的突然涌现；在微观上，训练过程中也存在一种类似的"顿悟"现象——这就是接下来要探讨的Grokking。它可以被视为<strong>时间维度上的涌现</strong>：模型在训练的某个时刻突然"理解"了任务的本质。</p><h3 id=331-the-grokking-phenomenon顿悟现象>3.3.1 The Grokking Phenomenon：顿悟现象<a class=anchor href=#331-the-grokking-phenomenon%e9%a1%bf%e6%82%9f%e7%8e%b0%e8%b1%a1>#</a></h3><blockquote class=book-hint><p><strong>比喻：从背诵到理解的"顿悟时刻"</strong></p><p>想象一个学生学习九九乘法表：</p><p><strong>阶段1（记忆期）</strong>：前100遍练习</p><ul><li>学生能快速背出 &ldquo;3×7=21&rdquo;，考试满分</li><li>但如果你问 &ldquo;为什么3×7=21？"，他一脸茫然</li><li><strong>状态</strong>：训练集100分，验证集0分</li></ul><p><strong>阶段2（停滞期）</strong>：100-1000遍练习</p><ul><li>继续重复练习，看似毫无进步</li><li>内心（神经网络参数）在缓慢重组</li><li><strong>状态</strong>：训练集100分，验证集依然0分</li></ul><p><strong>阶段3（顿悟时刻）</strong>：第1000遍后的某一天</p><ul><li>突然理解：&ldquo;哦！3×7 其实是 7+7+7！&rdquo;</li><li>现在可以做任何乘法题，因为理解了乘法的本质</li><li><strong>状态</strong>：训练集100分，验证集也100分！</li></ul><p>这就是<strong>Grokking</strong>——从死记硬背到融会贯通的"质变"时刻。</p></blockquote><p><strong>Grokking</strong>（顿悟现象）是深度学习训练中最令人着迷的现象之一：模型在训练集上已经完美拟合（Loss趋近于0）很久之后，验证集Loss突然在某个时刻急剧下降，实现真正的泛化。这就像学生"刷题"刷了很久，突然有一天"开窍"了，理解了背后的规律。</p><h4 id=现象描述从记忆到泛化的相变>现象描述：从记忆到泛化的相变<a class=anchor href=#%e7%8e%b0%e8%b1%a1%e6%8f%8f%e8%bf%b0%e4%bb%8e%e8%ae%b0%e5%bf%86%e5%88%b0%e6%b3%9b%e5%8c%96%e7%9a%84%e7%9b%b8%e5%8f%98>#</a></h4><p>在2022年OpenAI的研究论文《Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets》中，研究者发现了这一反直觉的现象：</p><p><strong>典型的Grokking训练曲线</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Loss
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>1.2│                                                 Grokking现象：
</span></span><span class=line><span class=cl>  │                                            从记忆到泛化的&#34;顿悟时刻&#34;
</span></span><span class=line><span class=cl>1.0│ ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●  Validation Loss
</span></span><span class=line><span class=cl>  │                                       ╲   (长期停滞在高位)
</span></span><span class=line><span class=cl>0.8│                                        ╲
</span></span><span class=line><span class=cl>  │                                         ╲
</span></span><span class=line><span class=cl>0.6│                                          ╲
</span></span><span class=line><span class=cl>  │                                           ╲
</span></span><span class=line><span class=cl>0.4│                                            ╲╲╲  突然下降！
</span></span><span class=line><span class=cl>  │                                               ╲╲╲╲╲
</span></span><span class=line><span class=cl>0.2│ ╲╲╲╲╲╲╲╲╲╲╲╲╲╲╲                                ●●●●●●●●●●●
</span></span><span class=line><span class=cl>  │             ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
</span></span><span class=line><span class=cl>0.0│              Training Loss (1000步就收敛到0)
</span></span><span class=line><span class=cl>  └────┬────────┬────────────┬────────────┬────────────┬────────▶
</span></span><span class=line><span class=cl>     500      1000         5000        10000        50000     Steps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      ▲                                   ▲
</span></span><span class=line><span class=cl>      │                                   │
</span></span><span class=line><span class=cl>  记忆阶段                             顿悟时刻
</span></span><span class=line><span class=cl>(Memorization)                      (Grokking Moment)
</span></span><span class=line><span class=cl> &#34;背会了题目&#34;                        &#34;理解了规律&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>📊 三个阶段：
</span></span><span class=line><span class=cl>   1️⃣ 快速记忆（0-1000步）：训练Loss暴跌，验证Loss不变
</span></span><span class=line><span class=cl>   2️⃣ 长期停滞（1000-10000步）：两条曲线都不变，看似&#34;死&#34;了
</span></span><span class=line><span class=cl>   3️⃣ 突然顿悟（10000+步）：验证Loss突然暴跌，实现真正泛化</span></span></code></pre></div><p><strong>关键观察</strong>：</p><ol><li><strong>训练集Loss早早收敛</strong>（~1000步）：模型已经"背会"了所有训练样本</li><li><strong>验证集Loss长期平台期</strong>（1000-10000步）：看似陷入过拟合</li><li><strong>突然的顿悟时刻</strong>（~10000-50000步）：验证Loss急剧下降，模型突然"理解"了规律</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModularAdditionDataset</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;模运算数据集 - Grokking现象的经典场景&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>97</span><span class=p>,</span> <span class=n>frac_train</span><span class=o>=</span><span class=mf>0.5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        p: 模数（质数，通常取97）
</span></span></span><span class=line><span class=cl><span class=s2>        frac_train: 训练集比例
</span></span></span><span class=line><span class=cl><span class=s2>        任务：学习 (a + b) mod p
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>p</span> <span class=o>=</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>        <span class=c1># 生成所有可能的 (a, b) 对</span>
</span></span><span class=line><span class=cl>        <span class=n>all_pairs</span> <span class=o>=</span> <span class=p>[(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=k>for</span> <span class=n>a</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>b</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>p</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>all_pairs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>split</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>all_pairs</span><span class=p>)</span> <span class=o>*</span> <span class=n>frac_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>train_pairs</span> <span class=o>=</span> <span class=n>all_pairs</span><span class=p>[:</span><span class=n>split</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>val_pairs</span> <span class=o>=</span> <span class=n>all_pairs</span><span class=p>[</span><span class=n>split</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;获取一个批次&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>pairs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>train_pairs</span> <span class=k>if</span> <span class=n>split</span> <span class=o>==</span> <span class=s1>&#39;train&#39;</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>val_pairs</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>pairs</span><span class=p>),</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>pairs</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>pairs</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([(</span><span class=n>pairs</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>pairs</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>])</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>p</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GrokkingTransformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;用于Grokking实验的简化Transformer&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>97</span><span class=p>,</span> <span class=n>d_model</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>encoder_layer</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 简化：只用最后一个token的输出</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>b</span><span class=p>)],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># [B, 2, D]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>  <span class=c1># [2, B, D]</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># [B, vocab_size]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_grokking_experiment</span><span class=p>(</span><span class=n>steps</span><span class=o>=</span><span class=mi>50000</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>log_interval</span><span class=o>=</span><span class=mi>500</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Grokking训练实验
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    关键：weight_decay（权重衰减）对Grokking至关重要！
</span></span></span><span class=line><span class=cl><span class=s2>    - 无weight_decay: 模型永远停留在记忆阶段
</span></span></span><span class=line><span class=cl><span class=s2>    - 有weight_decay: 迫使模型找到更简洁的泛化解
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span> <span class=o>=</span> <span class=n>ModularAdditionDataset</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mi>97</span><span class=p>,</span> <span class=n>frac_train</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>GrokkingTransformer</span><span class=p>(</span><span class=n>vocab_size</span><span class=o>=</span><span class=mi>97</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 关键：AdamW + 较大的weight_decay</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>train_losses</span><span class=p>,</span> <span class=n>val_losses</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>train_accs</span><span class=p>,</span> <span class=n>val_accs</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 训练步</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>get_batch</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 评估</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=n>log_interval</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=c1># 训练集评估</span>
</span></span><span class=line><span class=cl>                <span class=n>train_correct</span> <span class=o>=</span> <span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>train_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                <span class=n>train_accs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>train_correct</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 验证集评估</span>
</span></span><span class=line><span class=cl>                <span class=n>val_a</span><span class=p>,</span> <span class=n>val_b</span><span class=p>,</span> <span class=n>val_labels</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>get_batch</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s1>&#39;val&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>val_logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>val_a</span><span class=p>,</span> <span class=n>val_b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>val_loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>val_logits</span><span class=p>,</span> <span class=n>val_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>val_correct</span> <span class=o>=</span> <span class=p>(</span><span class=n>val_logits</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=n>val_labels</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>val_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>val_loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                <span class=n>val_accs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>val_correct</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=p>(</span><span class=n>log_interval</span> <span class=o>*</span> <span class=mi>10</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>step</span><span class=si>:</span><span class=s2>5d</span><span class=si>}</span><span class=s2> | Train Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc: </span><span class=si>{</span><span class=n>train_correct</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>                          <span class=sa>f</span><span class=s2>&#34;Val Loss: </span><span class=si>{</span><span class=n>val_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc: </span><span class=si>{</span><span class=n>val_correct</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>train_losses</span><span class=p>,</span> <span class=n>val_losses</span><span class=p>,</span> <span class=n>train_accs</span><span class=p>,</span> <span class=n>val_accs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 运行实验（注释掉，实际运行需要环境）</span>
</span></span><span class=line><span class=cl><span class=c1># train_losses, val_losses, train_accs, val_accs = train_grokking_experiment()</span></span></span></code></pre></div><p><strong>典型输出</strong>（模拟）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step     0 | Train Loss: 4.5234, Acc: 1.56% | Val Loss: 4.5189, Acc: 1.48%
</span></span><span class=line><span class=cl>Step  5000 | Train Loss: 0.0001, Acc: 100.00% | Val Loss: 4.4982, Acc: 1.52%  ← 记忆阶段
</span></span><span class=line><span class=cl>Step 10000 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 4.4891, Acc: 1.61%  ← 仍在记忆
</span></span><span class=line><span class=cl>Step 15000 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 3.2145, Acc: 15.23% ← 开始顿悟！
</span></span><span class=line><span class=cl>Step 20000 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.5821, Acc: 85.47% ← 快速泛化
</span></span><span class=line><span class=cl>Step 25000 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0123, Acc: 99.61% ← 完全泛化
</span></span><span class=line><span class=cl>Step 30000 | Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0002, Acc: 100.00% ← 顿悟完成</span></span></code></pre></div><h4 id=原理深度为什么会发生grokking>原理深度：为什么会发生Grokking？<a class=anchor href=#%e5%8e%9f%e7%90%86%e6%b7%b1%e5%ba%a6%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e5%8f%91%e7%94%9fgrokking>#</a></h4><p>Grokking本质上是<strong>从记忆解到泛化解的相变过程</strong>：</p><p><strong>1. 两种解的竞争</strong></p><p>神经网络的参数空间中存在两类极小值点：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>参数空间示意图:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>           High Loss Region (高损失区域)
</span></span><span class=line><span class=cl>    ╱╲    ╱╲    ╱╲    ╱╲    ╱╲    ╱╲
</span></span><span class=line><span class=cl>   ╱  ╲  ╱  ╲  ╱  ╲  ╱  ╲  ╱  ╲  ╱  ╲
</span></span><span class=line><span class=cl>  ╱    ╲╱    ╲╱    ╲╱    ╲╱    ╲╱    ╲
</span></span><span class=line><span class=cl>─┴──────┴──────┴──────┴──────┴──────┴───▶ Weight Space
</span></span><span class=line><span class=cl>  ▲                          ▲
</span></span><span class=line><span class=cl>  │                          │
</span></span><span class=line><span class=cl>记忆解 (Memorization)      泛化解 (Generalization)
</span></span><span class=line><span class=cl>• 快速到达                 • 缓慢到达
</span></span><span class=line><span class=cl>• 高权重范数               • 低权重范数
</span></span><span class=line><span class=cl>• 仅拟合训练集             • 拟合真实规律
</span></span><span class=line><span class=cl>• ||W|| ≈ 100             • ||W|| ≈ 10</span></span></code></pre></div><p><strong>记忆解</strong>：</p><ul><li>模型"硬编码"了训练集的每个样本映射</li><li>权重很大，网络容量被用来记忆查找表</li><li>训练Loss快速降低，但验证Loss居高不下</li></ul><p><strong>泛化解</strong>：</p><ul><li>模型学到了底层的数学规律（如模运算的代数结构）</li><li>权重较小，网络学习的是简洁的算法</li><li>训练Loss和验证Loss都很低</li></ul><p><strong>2. Weight Decay的关键作用</strong></p><p>权重衰减（$L_2$正则化）在Grokking中扮演关键角色：</p><p>$$
\mathcal{L}<em>{\text{total}} = \mathcal{L}</em>{\text{task}} + \lambda |\mathbf{W}|^2
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SolutionComparison</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;两种解的对比&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>memorization_solution</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;记忆解：查找表式的映射&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;train_loss&#34;</span><span class=p>:</span> <span class=mf>0.0001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;val_loss&#34;</span><span class=p>:</span> <span class=mf>4.5</span><span class=p>,</span>  <span class=c1># 泛化失败</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;weight_norm&#34;</span><span class=p>:</span> <span class=mf>125.3</span><span class=p>,</span>  <span class=c1># 高权重范数</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;为每个训练样本学习独立的路径&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;parameters_used&#34;</span><span class=p>:</span> <span class=s2>&#34;大量参数用于存储映射关系&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generalization_solution</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;泛化解：规律式的理解&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;train_loss&#34;</span><span class=p>:</span> <span class=mf>0.0001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;val_loss&#34;</span><span class=p>:</span> <span class=mf>0.0001</span><span class=p>,</span>  <span class=c1># 完美泛化</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;weight_norm&#34;</span><span class=p>:</span> <span class=mf>8.7</span><span class=p>,</span>  <span class=c1># 低权重范数</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;学习到模运算的代数结构&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;parameters_used&#34;</span><span class=p>:</span> <span class=s2>&#34;少量参数编码算法逻辑&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>why_weight_decay_helps</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Weight Decay如何促进Grokking&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        1. 早期（0-1000步）:
</span></span></span><span class=line><span class=cl><span class=s2>           - 记忆解和泛化解都在优化
</span></span></span><span class=line><span class=cl><span class=s2>           - 记忆解更快（梯度更大）→ 先到达局部最优
</span></span></span><span class=line><span class=cl><span class=s2>           - 模型陷入记忆解
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        2. 中期（1000-10000步）:
</span></span></span><span class=line><span class=cl><span class=s2>           - 训练Loss已经≈0，任务损失无梯度
</span></span></span><span class=line><span class=cl><span class=s2>           - 但weight_decay仍在惩罚高权重范数
</span></span></span><span class=line><span class=cl><span class=s2>           - ∇L_total = 0 + λ·2W（只剩正则项的梯度）
</span></span></span><span class=line><span class=cl><span class=s2>           - 模型被迫&#34;压缩&#34;权重
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        3. 晚期（10000-20000步）:
</span></span></span><span class=line><span class=cl><span class=s2>           - 高权重的记忆解被破坏（正则化压力）
</span></span></span><span class=line><span class=cl><span class=s2>           - 优化器被迫探索新路径
</span></span></span><span class=line><span class=cl><span class=s2>           - 发现低权重的泛化解（更简洁，正则损失小）
</span></span></span><span class=line><span class=cl><span class=s2>           - 快速切换到泛化解 → Grokking！
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        关键洞察：
</span></span></span><span class=line><span class=cl><span class=s2>        Weight decay将优化目标从&#34;拟合训练集&#34;变成&#34;用最小权重拟合训练集&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        这自然偏向于简洁的泛化解！
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 无Weight Decay vs 有Weight Decay</span>
</span></span><span class=line><span class=cl><span class=n>comparison</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>┌─────────────────────────────────────────────────────────────────┐
</span></span></span><span class=line><span class=cl><span class=s2>│               无Weight Decay (λ=0)                              │
</span></span></span><span class=line><span class=cl><span class=s2>├─────────────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Loss │ ●●●●●●●●●● Train &amp; Val都快速收敛                      │
</span></span></span><span class=line><span class=cl><span class=s2>│       │          ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●      │
</span></span></span><span class=line><span class=cl><span class=s2>│       │ 但是：验证集是靠记忆，换个测试集就崩溃                   │
</span></span></span><span class=line><span class=cl><span class=s2>│       └─────────────────────────────────────────────▶ Steps     │
</span></span></span><span class=line><span class=cl><span class=s2>│  结论：永远停留在记忆阶段，无Grokking                           │
</span></span></span><span class=line><span class=cl><span class=s2>└─────────────────────────────────────────────────────────────────┘
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>┌─────────────────────────────────────────────────────────────────┐
</span></span></span><span class=line><span class=cl><span class=s2>│            有Weight Decay (λ=1.0)                               │
</span></span></span><span class=line><span class=cl><span class=s2>├─────────────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Loss │ ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●  Val Loss高位平台        │
</span></span></span><span class=line><span class=cl><span class=s2>│       │ ╲╲╲╲╲╲╲╲╲╲                  ╲╲╲╲╲╲╲                   │
</span></span></span><span class=line><span class=cl><span class=s2>│       │         ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●  突然下降！      │
</span></span></span><span class=line><span class=cl><span class=s2>│       │ Train Loss快速收敛                                      │
</span></span></span><span class=line><span class=cl><span class=s2>│       └─────────────────────────────────────────────▶ Steps     │
</span></span></span><span class=line><span class=cl><span class=s2>│  结论：经历记忆→泛化的相变，实现真正的Grokking                  │
</span></span></span><span class=line><span class=cl><span class=s2>└─────────────────────────────────────────────────────────────────┘</span></span></span></code></pre></div><p><strong>3. 数学视角：Loss Landscape的拓扑变化</strong></p><p>Grokking可以理解为优化轨迹在Loss Landscape上的非凡旅程：</p><p>$$
\begin{aligned}
\text{阶段1（记忆）:} \quad & \theta_t \to \theta_{\text{mem}}, \quad \mathcal{L}<em>{\text{train}} \downarrow, \quad \mathcal{L}</em>{\text{val}} \text{ 不变} \
\text{阶段2（徘徊）:} \quad & \theta_t \approx \theta_{\text{mem}}, \quad \text{正则化缓慢改造参数} \
\text{阶段3（顿悟）:} \quad & \theta_t \to \theta_{\text{gen}}, \quad \mathcal{L}_{\text{val}} \downarrow\downarrow\downarrow
\end{aligned}
$$</p><h4 id=工程启示grokking对训练策略的影响>工程启示：Grokking对训练策略的影响<a class=anchor href=#%e5%b7%a5%e7%a8%8b%e5%90%af%e7%a4%bagrokking%e5%af%b9%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5%e7%9a%84%e5%bd%b1%e5%93%8d>#</a></h4><p><strong>1. 不要过早Early Stopping！</strong></p><p>传统的Early Stopping策略：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 错误的策略（会错过Grokking）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TraditionalEarlyStopping</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patience</span> <span class=o>=</span> <span class=n>patience</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>should_stop</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>val_loss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>val_loss</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patience</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>True</span>  <span class=c1># ❌ 会在Grokking前终止训练！</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Grokking时代的策略</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GrokkingAwareEarlyStopping</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;考虑Grokking现象的Early Stopping&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>min_steps</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>50</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>min_steps</span> <span class=o>=</span> <span class=n>min_steps</span>  <span class=c1># 最小训练步数（等待Grokking）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patience</span> <span class=o>=</span> <span class=n>patience</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>should_stop</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>step</span><span class=p>,</span> <span class=n>val_loss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 关键：在min_steps之前不允许停止</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>False</span>  <span class=c1># ✅ 给Grokking足够的时间</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 之后才应用传统逻辑</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>val_loss</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>best_val_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patience</span></span></span></code></pre></div><p><strong>实际案例</strong>：</p><ul><li><strong>GPT-3训练</strong>：300B tokens后仍在改进（相当于"超长期训练&rdquo;）</li><li><strong>LLaMA训练</strong>：1.4T tokens，远超传统Early Stopping的容忍度</li><li><strong>启示</strong>：大模型预训练本质上是在"等待Grokking"</li></ul><p><strong>2. 小数据集上更容易观察到Grokking</strong></p><p>Grokking为什么在小数据集上更明显？</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DatasetSizeEffect</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;数据集大小对Grokking的影响&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>small_dataset_scenario</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;小数据集（如模运算）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;train_samples&#34;</span><span class=p>:</span> <span class=mi>4704</span><span class=p>,</span>  <span class=c1># 97x97 * 50% = 4704</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model_capacity&#34;</span><span class=p>:</span> <span class=s2>&#34;128维Transformer&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;capacity_ratio&#34;</span><span class=p>:</span> <span class=s2>&#34;过参数化（模型容量 &gt;&gt; 数据复杂度）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;memorization_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;极低（几秒钟就能背会）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;generalization_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;较高（需要发现代数结构）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;grokking_visibility&#34;</span><span class=p>:</span> <span class=s2>&#34;⭐⭐⭐⭐⭐ 非常明显&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;grokking_delay&#34;</span><span class=p>:</span> <span class=s2>&#34;10000-50000步&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>large_dataset_scenario</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;大数据集（如GPT预训练）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;train_samples&#34;</span><span class=p>:</span> <span class=s2>&#34;300B tokens&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model_capacity&#34;</span><span class=p>:</span> <span class=s2>&#34;175B参数Transformer&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;capacity_ratio&#34;</span><span class=p>:</span> <span class=s2>&#34;仍然过参数化，但差距小&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;memorization_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;极高（不可能完全记忆）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;generalization_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;持续学习模式&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;grokking_visibility&#34;</span><span class=p>:</span> <span class=s2>&#34;⭐⭐ 不明显（渐进式改进）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;grokking_delay&#34;</span><span class=p>:</span> <span class=s2>&#34;可能存在，但被平滑&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>why_small_data_shows_grokking</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        小数据集的Grokking更清晰的原因：
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        1. 记忆成本低：
</span></span></span><span class=line><span class=cl><span class=s2>           - 模型可以快速&#34;背会&#34;所有训练样本
</span></span></span><span class=line><span class=cl><span class=s2>           - 记忆解快速达成 → 训练Loss迅速归零
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        2. 记忆与泛化的Gap大：
</span></span></span><span class=line><span class=cl><span class=s2>           - 记忆解：Train Acc 100%, Val Acc ~1%
</span></span></span><span class=line><span class=cl><span class=s2>           - 泛化解：Train Acc 100%, Val Acc ~100%
</span></span></span><span class=line><span class=cl><span class=s2>           - 两者性能差异巨大，相变明显
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        3. 泛化规律简洁：
</span></span></span><span class=line><span class=cl><span class=s2>           - 底层规律（如模运算）可以用很少的参数编码
</span></span></span><span class=line><span class=cl><span class=s2>           - 一旦发现，快速切换 → 验证Loss断崖式下跌
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        大数据集的Grokking为什么不明显：
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        1. 无法完全记忆：
</span></span></span><span class=line><span class=cl><span class=s2>           - 训练集太大，模型容量不足以记忆
</span></span></span><span class=line><span class=cl><span class=s2>           - 必须从一开始就学习某种压缩/规律
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        2. 渐进式学习：
</span></span></span><span class=line><span class=cl><span class=s2>           - 从简单模式→复杂模式逐步学习
</span></span></span><span class=line><span class=cl><span class=s2>           - 没有突变，而是持续改进
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        3. 多任务混合：
</span></span></span><span class=line><span class=cl><span class=s2>           - 语言模型同时学习语法、知识、推理等
</span></span></span><span class=line><span class=cl><span class=s2>           - 不同能力的Grokking时刻不同，整体平滑
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实验证据</span>
</span></span><span class=line><span class=cl><span class=n>experiments</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;模运算(97x97)&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;数据量&#34;</span><span class=p>:</span> <span class=s2>&#34;9K样本&#34;</span><span class=p>,</span> <span class=s2>&#34;Grokking步数&#34;</span><span class=p>:</span> <span class=s2>&#34;10K-50K&#34;</span><span class=p>,</span> <span class=s2>&#34;明显程度&#34;</span><span class=p>:</span> <span class=s2>&#34;极高&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;布尔函数&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;数据量&#34;</span><span class=p>:</span> <span class=s2>&#34;256样本&#34;</span><span class=p>,</span> <span class=s2>&#34;Grokking步数&#34;</span><span class=p>:</span> <span class=s2>&#34;5K-20K&#34;</span><span class=p>,</span> <span class=s2>&#34;明显程度&#34;</span><span class=p>:</span> <span class=s2>&#34;极高&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;MNIST&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;数据量&#34;</span><span class=p>:</span> <span class=s2>&#34;60K样本&#34;</span><span class=p>,</span> <span class=s2>&#34;Grokking步数&#34;</span><span class=p>:</span> <span class=s2>&#34;罕见&#34;</span><span class=p>,</span> <span class=s2>&#34;明显程度&#34;</span><span class=p>:</span> <span class=s2>&#34;低&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ImageNet&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;数据量&#34;</span><span class=p>:</span> <span class=s2>&#34;1.2M样本&#34;</span><span class=p>,</span> <span class=s2>&#34;Grokking步数&#34;</span><span class=p>:</span> <span class=s2>&#34;未观察到&#34;</span><span class=p>,</span> <span class=s2>&#34;明显程度&#34;</span><span class=p>:</span> <span class=s2>&#34;无&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;GPT预训练&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;数据量&#34;</span><span class=p>:</span> <span class=s2>&#34;300B tokens&#34;</span><span class=p>,</span> <span class=s2>&#34;Grokking步数&#34;</span><span class=p>:</span> <span class=s2>&#34;可能存在但模糊&#34;</span><span class=p>,</span> <span class=s2>&#34;明显程度&#34;</span><span class=p>:</span> <span class=s2>&#34;低&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>3. Weight Decay是Grokking的催化剂</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Grokking训练的最佳实践</span>
</span></span><span class=line><span class=cl><span class=n>best_practices</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;优化器&#34;</span><span class=p>:</span> <span class=s2>&#34;AdamW（带weight decay）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;学习率&#34;</span><span class=p>:</span> <span class=s2>&#34;1e-3 到 1e-4（适中）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;weight_decay&#34;</span><span class=p>:</span> <span class=s2>&#34;0.1 到 1.0（关键！）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;批次大小&#34;</span><span class=p>:</span> <span class=s2>&#34;较大（512+），稳定梯度&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;训练步数&#34;</span><span class=p>:</span> <span class=s2>&#34;远超传统（10x-100x）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Early Stopping&#34;</span><span class=p>:</span> <span class=s2>&#34;延迟或禁用&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对比实验</span>
</span></span><span class=line><span class=cl><span class=n>ablation_study</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>┌───────────────────────────────────────────────────────────┐
</span></span></span><span class=line><span class=cl><span class=s2>│  Weight Decay = 0.0                                       │
</span></span></span><span class=line><span class=cl><span class=s2>│  → 永远停留在记忆阶段，Val Acc ≈ 1%                        │
</span></span></span><span class=line><span class=cl><span class=s2>├───────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Weight Decay = 0.01                                      │
</span></span></span><span class=line><span class=cl><span class=s2>│  → 轻微Grokking，需要200K步，Val Acc → 60%                │
</span></span></span><span class=line><span class=cl><span class=s2>├───────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Weight Decay = 0.1                                       │
</span></span></span><span class=line><span class=cl><span class=s2>│  → 明显Grokking，50K步，Val Acc → 90%                     │
</span></span></span><span class=line><span class=cl><span class=s2>├───────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Weight Decay = 1.0  ✅ 最佳                              │
</span></span></span><span class=line><span class=cl><span class=s2>│  → 显著Grokking，20K步，Val Acc → 99%+                    │
</span></span></span><span class=line><span class=cl><span class=s2>├───────────────────────────────────────────────────────────┤
</span></span></span><span class=line><span class=cl><span class=s2>│  Weight Decay = 10.0                                      │
</span></span></span><span class=line><span class=cl><span class=s2>│  → 过度正则化，训练也受影响，Val Acc → 70%                 │
</span></span></span><span class=line><span class=cl><span class=s2>└───────────────────────────────────────────────────────────┘</span></span></span></code></pre></div><h4 id=理论前沿grokking与神经网络的归纳偏置>理论前沿：Grokking与神经网络的归纳偏置<a class=anchor href=#%e7%90%86%e8%ae%ba%e5%89%8d%e6%b2%bfgrokking%e4%b8%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%bd%92%e7%ba%b3%e5%81%8f%e7%bd%ae>#</a></h4><p>Grokking现象揭示了深度学习的深层真相：</p><p><strong>1. 简洁性偏好（Simplicity Bias）</strong></p><p>神经网络天然偏好"简洁"的解（低权重范数、低频函数）：</p><p>$$
\text{Implicit Regularization: } \min_{\theta} \mathcal{L}(\theta) \implies \min_{\theta} \left[ \mathcal{L}(\theta) + \text{Complexity}(\theta) \right]
$$</p><p>即使没有显式正则化，SGD本身也倾向于简单解（Neyshabur et al., 2014）。</p><p><strong>2. 双下降与Grokking的关联</strong></p><p>Grokking可以看作"时间维度的双下降"：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>传统双下降（横轴：模型容量）:
</span></span><span class=line><span class=cl>  Test Error
</span></span><span class=line><span class=cl>      │     ╱╲
</span></span><span class=line><span class=cl>      │    ╱  ╲        ╱────
</span></span><span class=line><span class=cl>      │   ╱    ╲      ╱
</span></span><span class=line><span class=cl>      │  ╱      ╲    ╱
</span></span><span class=line><span class=cl>      └──────────┴───┴──────▶ Model Capacity
</span></span><span class=line><span class=cl>         欠拟合  插值阈值  过参数化
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Grokking（横轴：训练时间）:
</span></span><span class=line><span class=cl>  Test Error
</span></span><span class=line><span class=cl>      │ ─────────────╲
</span></span><span class=line><span class=cl>      │               ╲╲╲
</span></span><span class=line><span class=cl>      │                  ●●●●●●
</span></span><span class=line><span class=cl>      │
</span></span><span class=line><span class=cl>      └──────────────────────▶ Training Steps
</span></span><span class=line><span class=cl>        记忆阶段        顿悟</span></span></code></pre></div><p><strong>3. 开放问题</strong></p><p>Grokking仍有许多未解之谜：</p><ul><li>❓ <strong>Grokking的时刻能否预测？</strong> 目前只能事后观察，无法提前知道何时发生</li><li>❓ <strong>大模型预训练中是否存在Grokking？</strong> 可能存在但被平滑/掩盖</li><li>❓ <strong>如何加速Grokking？</strong> 除了weight decay，是否有其他催化剂？</li><li>❓ <strong>Grokking与涌现能力的关系？</strong> 涌现是否是某种形式的Grokking？</li></ul><h4 id=实战建议如何在项目中应对grokking>实战建议：如何在项目中应对Grokking<a class=anchor href=#%e5%ae%9e%e6%88%98%e5%bb%ba%e8%ae%ae%e5%a6%82%e4%bd%95%e5%9c%a8%e9%a1%b9%e7%9b%ae%e4%b8%ad%e5%ba%94%e5%af%b9grokking>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GrokkingTrainingStrategy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;融入Grokking意识的训练策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>strategies</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;小数据集微调&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>small_data_strategy</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;中等规模预训练&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>medium_scale_strategy</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;大模型预训练&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>large_scale_strategy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>small_data_strategy</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;小数据集（&lt;10K样本）策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;现象&#34;</span><span class=p>:</span> <span class=s2>&#34;Grokking极可能出现&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;策略&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 使用AdamW，weight_decay=0.5-1.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 训练步数至少10K，不设Early Stopping&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 监控训练/验证Loss的分离度&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 保存多个checkpoint，选择验证集最优&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;⚠️  不要在训练Loss归零后立即停止&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;案例&#34;</span><span class=p>:</span> <span class=s2>&#34;Few-shot学习、数学推理任务&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>medium_scale_strategy</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;中等数据集（10K-1M样本）策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;现象&#34;</span><span class=p>:</span> <span class=s2>&#34;Grokking可能出现，但不明显&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;策略&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 适度weight_decay (0.01-0.1)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 训练步数按传统经验的2-3倍&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 使用Learning Rate Warmup + Cosine Decay&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 可以设置宽松的Early Stopping（patience=50）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;⚠️  关注验证Loss的长期趋势，不只看短期&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;案例&#34;</span><span class=p>:</span> <span class=s2>&#34;领域微调、代码生成&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>large_scale_strategy</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;大规模预训练（&gt;100M样本）策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;现象&#34;</span><span class=p>:</span> <span class=s2>&#34;Grokking被平滑，但泛化仍需时间&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;策略&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 小weight_decay (0.01-0.1)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 按Chinchilla Law估算，但训练步数可以超预算20%&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 不设Early Stopping，按计划训完&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;✅ 监控下游任务性能（如Few-shot），而非只看Loss&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;⚠️  Loss收敛后，模型仍在学习更高级能力&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;案例&#34;</span><span class=p>:</span> <span class=s2>&#34;GPT预训练、多模态大模型&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span></span></span></code></pre></div><h4 id=本节小结>本节小结<a class=anchor href=#%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h4><p><strong>Grokking现象的核心要点</strong>：</p><ol><li><strong>现象</strong>：训练Loss早早收敛，验证Loss却在很久之后突然下降</li><li><strong>本质</strong>：从记忆解到泛化解的相变过程</li><li><strong>机制</strong>：Weight decay破坏高权重的记忆解，迫使模型发现简洁的泛化解</li><li><strong>启示</strong>：<ul><li>不要过早Early Stopping</li><li>小数据集上Grokking更明显</li><li>大模型预训练需要足够耐心</li><li>简洁性是神经网络的内在偏好</li></ul></li></ol><p><strong>与其他概念的联系</strong>：</p><ul><li><strong>涌现能力</strong>（3.3节）：可能是规模维度的Grokking</li><li><strong>Scaling Law</strong>（3.1-3.2节）：预测Loss下降，但无法预测Grokking时刻</li><li><strong>训练稳定性</strong>（4.1节）：Grokking需要稳定的优化过程</li></ul><blockquote class=book-hint><p><strong>深入阅读</strong>：</p><ul><li>原论文：Power et al. (2022) &ldquo;Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets&rdquo;</li><li>理论分析：Nanda et al. (2023) &ldquo;Progress Measures for Grokking via Mechanistic Interpretability&rdquo;</li><li>实战案例：[Part 6 第4章] 小样本学习中的Grokking应用</li></ul></blockquote><h3 id=34-当前视角scaling-law的新发现>3.4 当前视角：Scaling Law的新发现<a class=anchor href=#34-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92scaling-law%e7%9a%84%e6%96%b0%e5%8f%91%e7%8e%b0>#</a></h3><h4 id=scaling-law的局限性>Scaling Law的局限性<a class=anchor href=#scaling-law%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7>#</a></h4><p>经过几年的实践，我们发现经典Scaling Law存在一些局限：</p><ol><li><p><strong>无法预测涌现能力的精确阈值</strong></p><ul><li>指令遵循、代码生成等能力在某个规模突然出现</li><li>现有公式只能预测perplexity，无法预测能力涌现</li></ul></li><li><p><strong>数据质量的影响未充分建模</strong></p><ul><li>Chinchilla假设数据质量均匀</li><li>实际上高质量数据价值远超低质量数据</li></ul></li><li><p><strong>指令微调后的性能变化难以预测</strong></p><ul><li>预训练损失低 ≠ 对话能力强</li><li>RLHF后的性能提升难以量化</li></ul></li></ol><h4 id=2023-2025年的新发现>2023-2025年的新发现<a class=anchor href=#2023-2025%e5%b9%b4%e7%9a%84%e6%96%b0%e5%8f%91%e7%8e%b0>#</a></h4><p><strong>1. Mixture of Experts (MoE) 打破传统Scaling Law</strong></p><p>MoE架构通过<strong>稀疏激活</strong>实现"虚拟大模型"：总参数量很大，但每次推理只激活部分专家。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 传统Dense模型：100B参数 = 100B激活</span>
</span></span><span class=line><span class=cl><span class=n>dense_model</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;total_params&#34;</span><span class=p>:</span> <span class=mf>100e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;active_params&#34;</span><span class=p>:</span> <span class=mf>100e9</span><span class=p>,</span>  <span class=c1># 每次前向传播都用全部参数</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inference_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;高&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;training_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;极高&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MoE模型：100B总参数，只激活13B</span>
</span></span><span class=line><span class=cl><span class=n>moe_model</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;total_params&#34;</span><span class=p>:</span> <span class=mf>100e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;active_params&#34;</span><span class=p>:</span> <span class=mf>13e9</span><span class=p>,</span>  <span class=c1># 路由机制选择2/8个专家</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inference_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;中等（降低87%）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;training_cost&#34;</span><span class=p>:</span> <span class=s2>&#34;高（需Expert Parallelism）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;performance&#34;</span><span class=p>:</span> <span class=s2>&#34;≈ 100B Dense模型&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实际案例对比</span>
</span></span><span class=line><span class=cl><span class=n>models</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;LLaMA-70B&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=mf>70e9</span><span class=p>,</span> <span class=s2>&#34;active&#34;</span><span class=p>:</span> <span class=mf>70e9</span><span class=p>,</span> <span class=s2>&#34;perf&#34;</span><span class=p>:</span> <span class=mi>100</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Mixtral-8x7B&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=mf>46.7e9</span><span class=p>,</span> <span class=s2>&#34;active&#34;</span><span class=p>:</span> <span class=mf>13e9</span><span class=p>,</span> <span class=s2>&#34;perf&#34;</span><span class=p>:</span> <span class=mi>98</span><span class=p>},</span>  <span class=c1># 性能接近，推理快5x</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;DeepSeek-V3&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=mf>671e9</span><span class=p>,</span> <span class=s2>&#34;active&#34;</span><span class=p>:</span> <span class=mf>37e9</span><span class=p>,</span> <span class=s2>&#34;perf&#34;</span><span class=p>:</span> <span class=mi>120</span><span class=p>}</span>   <span class=c1># 超越GPT-4，成本更低</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>MoE对Scaling Law的影响</strong>：</p><p>传统Dense模型：
$$
\text{Compute} \propto N \times D
$$</p><p>MoE模型：
$$
\text{Compute} \propto N_{\text{active}} \times D \ll N_{\text{total}} \times D
$$</p><p>这意味着：<strong>相同算力预算下，MoE可以训练更"大"的模型</strong>（总参数量大，但实际激活小）。</p><p><strong>预训练MoE的关键挑战</strong>：</p><ol><li><strong>负载均衡</strong>：避免所有数据只激活少数专家（需要Load Balancing Loss）</li><li><strong>专家并行</strong>：需要特殊的分布式训练策略（与数据并行不同）</li><li><strong>路由机制</strong>：如何高效地选择专家（Top-K Routing）</li></ol><blockquote class=book-hint><p><strong>深入学习</strong>：MoE的完整架构、路由机制、训练技巧详见 [Part 7 第2章：新型架构探索]</p></blockquote><p><strong>2. Test-Time Compute：推理时计算的新维度</strong></p><p>OpenAI o1、DeepSeek-R1等模型证明：<strong>推理时"多想一会儿"能显著提升性能</strong>，这开启了Scaling Law的新维度。</p><p><strong>传统Scaling Law的三要素</strong>：</p><ul><li><strong>参数量</strong> $N$（Parameters）</li><li><strong>数据量</strong> $D$（Data）</li><li><strong>训练计算</strong> $C$（Training Compute）</li></ul><p><strong>新Scaling Law的第四要素</strong>：</p><ul><li><strong>推理时计算</strong> $C_{\text{test}}$（Test-Time Compute）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 传统Scaling Law（2020-2022）</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>traditional_scaling</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>train_compute</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;性能只依赖训练阶段&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>performance</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>train_compute</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 新Scaling Law（2024-2025）：引入推理时计算</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>new_scaling</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>train_compute</span><span class=p>,</span> <span class=n>inference_compute</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    inference_compute: 推理时的思考步数
</span></span></span><span class=line><span class=cl><span class=s2>    - o1: 数千个token的思考链（内部CoT）
</span></span></span><span class=line><span class=cl><span class=s2>    - DeepSeek-R1: 自我验证、回溯推理
</span></span></span><span class=line><span class=cl><span class=s2>    - 关键：小模型 + 长思考 ≈ 大模型 + 短思考
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>base_perf</span> <span class=o>=</span> <span class=n>traditional_scaling</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>train_compute</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 推理增益：对数增长（经验发现）</span>
</span></span><span class=line><span class=cl>    <span class=c1># 每增加10倍推理计算，性能提升约1个数量级</span>
</span></span><span class=line><span class=cl>    <span class=n>reasoning_boost</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log10</span><span class=p>(</span><span class=n>inference_compute</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>base_perf</span> <span class=o>+</span> <span class=n>reasoning_boost</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对比实验：小模型 vs 大模型</span>
</span></span><span class=line><span class=cl><span class=n>gpt4_performance</span> <span class=o>=</span> <span class=n>traditional_scaling</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params</span><span class=o>=</span><span class=mf>1.8e12</span><span class=p>,</span>  <span class=c1># 1.8T参数</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span><span class=o>=</span><span class=mf>10e12</span><span class=p>,</span>     <span class=c1># 10T tokens</span>
</span></span><span class=line><span class=cl>    <span class=n>train_compute</span><span class=o>=</span><span class=mf>1e25</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>inference_compute</span><span class=o>=</span><span class=mi>100</span>  <span class=c1># 标准推理（几乎不思考）</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>o1_mini_performance</span> <span class=o>=</span> <span class=n>new_scaling</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params</span><span class=o>=</span><span class=mf>7e9</span><span class=p>,</span>     <span class=c1># 7B参数（小260倍）</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span><span class=o>=</span><span class=mf>1e12</span><span class=p>,</span>      <span class=c1># 1T tokens</span>
</span></span><span class=line><span class=cl>    <span class=n>train_compute</span><span class=o>=</span><span class=mf>1e23</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>inference_compute</span><span class=o>=</span><span class=mi>5000</span>  <span class=c1># 平均5000 token思考链</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPT-4性能（大模型，短推理）: </span><span class=si>{</span><span class=n>gpt4_performance</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;o1-mini性能（小模型，长推理）: </span><span class=si>{</span><span class=n>o1_mini_performance</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;结论：在数学推理任务上，o1-mini超越GPT-4！&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>核心洞察：Scaling Law的新公式</strong></p><p>传统：
$$
L(N, D, C) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}
$$</p><p>新增推理维度：
$$
L(N, D, C, C_{\text{test}}) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}} - \gamma \log(C_{\text{test}})
$$</p><p>其中 $\gamma$ 是推理增益系数（实验测得 $\gamma \approx 0.1 \sim 0.3$，取决于任务类型）。</p><p><strong>实际意义</strong>：</p><ul><li><strong>推理密集型任务</strong>（数学、编程、推理）：$\gamma$ 大，长思考收益显著</li><li><strong>记忆密集型任务</strong>（知识问答）：$\gamma$ 小，增加思考时间收益有限</li><li><strong>开销</strong>：推理时计算成本更高（latency增加，但不需要重新训练）</li></ul><blockquote class=book-hint><p>详见 [Part 7 第3章] 推理时计算增强 和 [Part 7 第4章] 推理模型专题（o1/R1技术解密）</p></blockquote><p><strong>3. 数据质量的非线性价值</strong></p><p>2024年研究发现：<strong>1条高质量数据 = 100-1000条低质量数据</strong>。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DataQuality</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;数据质量量化&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>raw_size</span><span class=p>:</span> <span class=nb>float</span>  <span class=c1># 原始大小（token数）</span>
</span></span><span class=line><span class=cl>    <span class=n>quality_score</span><span class=p>:</span> <span class=nb>float</span>  <span class=c1># 质量分数 (0-1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>effective_size</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;有效数据量（考虑质量）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 非线性权重：高质量数据价值指数增长</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>raw_size</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>quality_score</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：质量对有效数据量的影响</span>
</span></span><span class=line><span class=cl><span class=n>datasets</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>DataQuality</span><span class=p>(</span><span class=n>raw_size</span><span class=o>=</span><span class=mf>1e12</span><span class=p>,</span> <span class=n>quality_score</span><span class=o>=</span><span class=mf>0.3</span><span class=p>),</span>  <span class=c1># 低质量网页</span>
</span></span><span class=line><span class=cl>    <span class=n>DataQuality</span><span class=p>(</span><span class=n>raw_size</span><span class=o>=</span><span class=mf>1e11</span><span class=p>,</span> <span class=n>quality_score</span><span class=o>=</span><span class=mf>0.7</span><span class=p>),</span>  <span class=c1># 过滤后的书籍</span>
</span></span><span class=line><span class=cl>    <span class=n>DataQuality</span><span class=p>(</span><span class=n>raw_size</span><span class=o>=</span><span class=mf>1e10</span><span class=p>,</span> <span class=n>quality_score</span><span class=o>=</span><span class=mf>0.95</span><span class=p>),</span> <span class=c1># 精选教科书</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>ds</span> <span class=ow>in</span> <span class=n>datasets</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;原始大小: </span><span class=si>{</span><span class=n>ds</span><span class=o>.</span><span class=n>raw_size</span><span class=si>:</span><span class=s2>.0e</span><span class=si>}</span><span class=s2>, &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;质量: </span><span class=si>{</span><span class=n>ds</span><span class=o>.</span><span class=n>quality_score</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;有效大小: </span><span class=si>{</span><span class=n>ds</span><span class=o>.</span><span class=n>effective_size</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出：</span>
</span></span><span class=line><span class=cl><span class=c1># 原始大小: 1e+12, 质量: 0.30, 有效大小: 9.00e+10</span>
</span></span><span class=line><span class=cl><span class=c1># 原始大小: 1e+11, 质量: 0.70, 有效大小: 4.90e+10</span>
</span></span><span class=line><span class=cl><span class=c1># 原始大小: 1e+10, 质量: 0.95, 有效大小: 9.02e+09</span></span></span></code></pre></div><p><strong>关键洞察</strong>：</p><ul><li>📉 <strong>低质量海量数据</strong>（1T tokens @ 0.3质量）有效价值仅90B</li><li>📈 <strong>高质量精选数据</strong>（10B tokens @ 0.95质量）有效价值9B</li><li>🎯 <strong>2025年趋势</strong>：从"数据量竞赛"转向"数据质量工程"</li></ul><p><strong>4. 合成数据的崛起</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 2025年的新发现：模型生成的合成数据可以提升性能</span>
</span></span><span class=line><span class=cl><span class=n>synthetic_data_effect</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>传统观点: 模型不能从自己生成的数据中学习（模型坍塌）
</span></span></span><span class=line><span class=cl><span class=s2>新发现（2024-2025）:
</span></span></span><span class=line><span class=cl><span class=s2>  1. 高质量合成数据（经过验证）&gt; 低质量真实数据
</span></span></span><span class=line><span class=cl><span class=s2>  2. DeepSeek-V3: 40%训练数据为合成代码+数学题
</span></span></span><span class=line><span class=cl><span class=s2>  3. Nemotron: 98%合成对话数据，性能不降反升
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>关键: 必须有可靠的验证机制（如代码可执行、数学可验证）
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><hr><h2 id=四预训练的工程挑战>四、预训练的工程挑战<a class=anchor href=#%e5%9b%9b%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e5%b7%a5%e7%a8%8b%e6%8c%91%e6%88%98>#</a></h2><h3 id=41-训练稳定性技术>4.1 训练稳定性技术<a class=anchor href=#41-%e8%ae%ad%e7%bb%83%e7%a8%b3%e5%ae%9a%e6%80%a7%e6%8a%80%e6%9c%af>#</a></h3><h4 id=梯度裁剪gradient-clipping>梯度裁剪（Gradient Clipping）<a class=anchor href=#%e6%a2%af%e5%ba%a6%e8%a3%81%e5%89%aagradient-clipping>#</a></h4><p>防止梯度爆炸：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GradientClipper</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;梯度裁剪工具&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>max_norm</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>norm_type</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>2.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_norm</span> <span class=o>=</span> <span class=n>max_norm</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm_type</span> <span class=o>=</span> <span class=n>norm_type</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>clip_gradients</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        裁剪模型梯度
</span></span></span><span class=line><span class=cl><span class=s2>        返回: 裁剪前的梯度范数
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>total_norm</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_norm_</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>max_norm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>max_norm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>norm_type</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>norm_type</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>total_norm</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>should_skip_update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grad_norm</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>threshold</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>100.0</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;判断是否跳过此次更新（梯度异常）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>grad_norm</span> <span class=o>&gt;</span> <span class=n>threshold</span> <span class=ow>or</span> <span class=n>torch</span><span class=o>.</span><span class=n>isnan</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>grad_norm</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>clipper</span> <span class=o>=</span> <span class=n>GradientClipper</span><span class=p>(</span><span class=n>max_norm</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练循环中</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 假设的损失</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 裁剪梯度</span>
</span></span><span class=line><span class=cl><span class=n>grad_norm</span> <span class=o>=</span> <span class=n>clipper</span><span class=o>.</span><span class=n>clip_gradients</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;梯度范数: </span><span class=si>{</span><span class=n>grad_norm</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 检查是否应该跳过更新</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=ow>not</span> <span class=n>clipper</span><span class=o>.</span><span class=n>should_skip_update</span><span class=p>(</span><span class=n>grad_norm</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;⚠️ 检测到异常梯度，跳过本次更新&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span></span></span></code></pre></div><h4 id=梯度累积gradient-accumulation>梯度累积（Gradient Accumulation）<a class=anchor href=#%e6%a2%af%e5%ba%a6%e7%b4%af%e7%a7%afgradient-accumulation>#</a></h4><p>模拟更大的batch size：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GradientAccumulator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;梯度累积训练器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>accumulation_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optimizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>accumulation_steps</span> <span class=o>=</span> <span class=n>accumulation_steps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>step_count</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_data</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        单步训练（自动处理梯度累积）
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 归一化损失（重要！）</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>accumulation_steps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 反向传播（梯度累积）</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>step_count</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 每accumulation_steps步更新一次参数</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>step_count</span> <span class=o>%</span> <span class=bp>self</span><span class=o>.</span><span class=n>accumulation_steps</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=kc>True</span>  <span class=c1># 表示参数已更新</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>False</span>  <span class=c1># 参数未更新</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>accumulator</span> <span class=o>=</span> <span class=n>GradientAccumulator</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟训练</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_fn</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>updated</span> <span class=o>=</span> <span class=n>accumulator</span><span class=o>.</span><span class=n>train_step</span><span class=p>(</span><span class=n>batch_data</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>updated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>: 参数已更新&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>为什么需要梯度累积？</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 没有梯度累积：</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>8</span>  <span class=c1># 受限于显存</span>
</span></span><span class=line><span class=cl><span class=n>effective_batch_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用梯度累积：</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>accumulation_steps</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>effective_batch_size</span> <span class=o>=</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>=</span> <span class=mi>32</span>  <span class=c1># 模拟更大batch</span></span></span></code></pre></div><h4 id=学习率调度learning-rate-scheduling>学习率调度（Learning Rate Scheduling）<a class=anchor href=#%e5%ad%a6%e4%b9%a0%e7%8e%87%e8%b0%83%e5%ba%a6learning-rate-scheduling>#</a></h4><p>在2025年，<strong>WSD (Warmup-Stable-Decay)</strong> 调度器已经取代 Cosine Annealing 成为训练大模型的标准选择（如 MiniCPM, Llama 3 都在使用）。</p><p><strong>Cosine vs WSD 对比</strong>：</p><ul><li><strong>Cosine</strong>：LR 达到峰值后持续下降。缺点是训练过程中任何时刻停止都不是最优。</li><li><strong>WSD</strong>：<ol><li><strong>Warmup</strong>：线性预热。</li><li><strong>Stable</strong>：保持恒定高学习率（如训练过程的80%-90%时间）。这意味着可以无限期训练下去，直到觉得差不多了。</li><li><strong>Decay (Annealing)</strong>：在最后阶段（如10%步骤）快速下降，并配合更高质量的数据。</li></ol></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Callable</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>WSDScheduler</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;WSD (Warmup-Stable-Decay) 调度器 - 2025年主流选择&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>total_steps</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>warmup_steps</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>decay_steps</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>max_lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>min_lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optimizer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>total_steps</span> <span class=o>=</span> <span class=n>total_steps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>warmup_steps</span> <span class=o>=</span> <span class=n>warmup_steps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decay_steps</span> <span class=o>=</span> <span class=n>decay_steps</span>  <span class=c1># 退火阶段步数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>stable_steps</span> <span class=o>=</span> <span class=n>total_steps</span> <span class=o>-</span> <span class=n>warmup_steps</span> <span class=o>-</span> <span class=n>decay_steps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_lr</span> <span class=o>=</span> <span class=n>max_lr</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>min_lr</span> <span class=o>=</span> <span class=n>min_lr</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_lr</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;计算当前学习率&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 1. Warmup阶段：线性增长</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_lr</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>&lt;</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>warmup_steps</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>stable_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 2. Stable阶段：保持恒定最大值</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_lr</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 3. Decay (Annealing)阶段：快速下降</span>
</span></span><span class=line><span class=cl>            <span class=c1># 计算退火阶段的进度 (0.0 -&gt; 1.0)</span>
</span></span><span class=line><span class=cl>            <span class=n>decay_progress</span> <span class=o>=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>warmup_steps</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>stable_steps</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>decay_steps</span>
</span></span><span class=line><span class=cl>            <span class=n>decay_progress</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>decay_progress</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 使用Cosine曲线快速下降</span>
</span></span><span class=line><span class=cl>            <span class=n>cosine_decay</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>math</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>decay_progress</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_lr</span> <span class=o>+</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>max_lr</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_lr</span><span class=p>)</span> <span class=o>*</span> <span class=n>cosine_decay</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;更新学习率&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>lr</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_lr</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>param_group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param_group</span><span class=p>[</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>lr</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>current_step</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>lr</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=c1># 假设训练10000步：1000步预热 -&gt; 8000步稳定 -&gt; 1000步退火</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>WSDScheduler</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>total_steps</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_steps</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>decay_steps</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>min_lr</span><span class=o>=</span><span class=mf>1e-5</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化学习率曲线</span>
</span></span><span class=line><span class=cl><span class=n>lrs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>lr</span> <span class=o>=</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>lrs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>key_steps</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>999</span><span class=p>,</span> <span class=mi>5000</span><span class=p>,</span> <span class=mi>8999</span><span class=p>,</span> <span class=mi>9500</span><span class=p>,</span> <span class=mi>9999</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;WSD学习率变化:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>key_steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>phase</span> <span class=o>=</span> <span class=s2>&#34;Warmup&#34;</span> <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=mi>1000</span> <span class=k>else</span> <span class=p>(</span><span class=s2>&#34;Stable&#34;</span> <span class=k>if</span> <span class=n>step</span> <span class=o>&lt;</span> <span class=mi>9000</span> <span class=k>else</span> <span class=s2>&#34;Decay&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>step</span><span class=si>:</span><span class=s2>5d</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>phase</span><span class=si>}</span><span class=s2>): LR = </span><span class=si>{</span><span class=n>lrs</span><span class=p>[</span><span class=n>step</span><span class=p>]</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>WSD学习率变化:
</span></span><span class=line><span class=cl>Step     0 (Warmup): LR = 0.000000
</span></span><span class=line><span class=cl>Step   999 (Warmup): LR = 0.000999  ← 线性上升
</span></span><span class=line><span class=cl>Step  5000 (Stable): LR = 0.001000  ← 保持恒定
</span></span><span class=line><span class=cl>Step  8999 (Stable): LR = 0.001000  ← 保持恒定直到最后
</span></span><span class=line><span class=cl>Step  9500 (Decay) : LR = 0.000505  ← 快速下降
</span></span><span class=line><span class=cl>Step  9999 (Decay) : LR = 0.000010  ← 降至最低</span></span></code></pre></div><p><strong>WSD学习率曲线可视化（ASCII）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Learning Rate
</span></span><span class=line><span class=cl>  │
</span></span><span class=line><span class=cl>1.0│          ●━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━●  Stable Phase
</span></span><span class=line><span class=cl>  │         ╱                                        ╲  (80% 时间保持最大LR)
</span></span><span class=line><span class=cl>  │        ╱                                          ╲
</span></span><span class=line><span class=cl>0.8│       ╱                                            ╲
</span></span><span class=line><span class=cl>  │      ╱                                              ╲
</span></span><span class=line><span class=cl>  │     ╱                                                ╲╲
</span></span><span class=line><span class=cl>0.6│    ╱                                                  ╲╲
</span></span><span class=line><span class=cl>  │   ╱                                                     ╲╲
</span></span><span class=line><span class=cl>  │  ╱                                                       ╲╲╲
</span></span><span class=line><span class=cl>0.4│ ╱                                                         ╲╲╲
</span></span><span class=line><span class=cl>  │╱                                                            ╲╲╲
</span></span><span class=line><span class=cl>0.2│    Warmup                                                   ●●●● Decay
</span></span><span class=line><span class=cl>  │   (10% 时间)                                           (10% 时间 + 高质量数据)
</span></span><span class=line><span class=cl>0.0│                                                                  ●
</span></span><span class=line><span class=cl>  └─────┬────────────┬───────────────────────────────┬───────────┬───▶
</span></span><span class=line><span class=cl>      1K          10K                              90K        100K  Steps
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   ▲                                                  ▲
</span></span><span class=line><span class=cl>   │                                                  │
</span></span><span class=line><span class=cl> 广泛学习                                    🔥 退火阶段 🔥
</span></span><span class=line><span class=cl> (Web + Books)                            (Code + Math数据上采样)
</span></span><span class=line><span class=cl>                                          此时切换到高质量数据！</span></span></code></pre></div><p><strong>为什么 WSD + Annealing 有奇效？</strong></p><blockquote class=book-hint><p><strong>比喻：考前冲刺的科学</strong></p><ul><li><strong>Warmup（预热）</strong>：运动员热身，让肌肉适应高强度</li><li><strong>Stable（稳定期）</strong>：马拉松主体阶段，保持稳定配速跑完全程</li><li><strong>Decay（退火）</strong>：最后冲刺阶段，虽然速度（LR）下降，但此时换成"冲刺跑道"（高质量数据）</li></ul></blockquote><p><strong>Annealing阶段的三大变化</strong>：</p><ol><li><strong>学习率下降</strong>：从1e-3降至1e-5，模型进入"微调"状态</li><li><strong>数据质量提升</strong>：Code、Math数据占比从5%提升到45%</li><li><strong>参数收敛稳定</strong>：大幅降低的LR让参数不再剧烈震荡，精细调整到最优解</li></ol><p><strong>为什么要在Decay阶段换数据？</strong></p><p>在 Stable 阶段，模型快速吸收大量知识，但处于"高能态"（High Energy State），就像海绵吸满了水但还在滴滴答答。
在 Annealing 阶段，虽然 LR 快速下降，但我们通常会<strong>切换到更高质量的数据（如代码、数学题、教科书）</strong>。这就像考试前的"重点复习"，让模型在参数收敛的同时，强化最核心的逻辑能力。</p><p><strong>关键技巧</strong>：Decay阶段的数据要与目标任务强相关！</p><ul><li>想提升代码能力？→ 上采样GitHub数据</li><li>想提升数学推理？→ 上采样MATH、GSM8K数据</li><li>想提升长文档理解？→ 上采样Books、ArXiv数据</li></ul><p>这个过程往往能带来 5-10% 的 Benchmark 提升。</p><p><strong>实验数据</strong>（来自Llama 3论文）：</p><table><thead><tr><th>指标</th><th>基础预训练结束</th><th>Annealing后</th><th>提升</th></tr></thead><tbody><tr><td>GSM8K（数学推理）</td><td>72.3%</td><td>79.6%</td><td>+7.3%</td></tr><tr><td>HumanEval（代码）</td><td>75.8%</td><td>82.3%</td><td>+6.5%</td></tr><tr><td>MMLU（通用知识）</td><td>82.1%</td><td>86.0%</td><td>+3.9%</td></tr></tbody></table><h3 id=42-混合精度训练深入>4.2 混合精度训练深入<a class=anchor href=#42-%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83%e6%b7%b1%e5%85%a5>#</a></h3><h4 id=fp16-vs-bf16>FP16 vs BF16<a class=anchor href=#fp16-vs-bf16>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MixedPrecisionTrainer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;混合精度训练器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>precision</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;fp16&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>precision</span> <span class=o>=</span> <span class=n>precision</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>precision</span> <span class=o>==</span> <span class=s2>&#34;fp16&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float16</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_loss_scaling</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>loss_scale</span> <span class=o>=</span> <span class=mf>65536.0</span>  <span class=c1># 初始损失缩放因子</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>precision</span> <span class=o>==</span> <span class=s2>&#34;bf16&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_loss_scaling</span> <span class=o>=</span> <span class=kc>False</span>  <span class=c1># BF16不需要损失缩放</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>use_loss_scaling</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 使用autocast</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>amp</span><span class=o>.</span><span class=n>GradScaler</span><span class=p>(</span><span class=n>enabled</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>use_loss_scaling</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>loss_fn</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;混合精度训练步骤&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前向传播（自动混合精度）</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>amp</span><span class=o>.</span><span class=n>autocast</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dtype</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 反向传播（自动缩放梯度）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaler</span><span class=o>.</span><span class=n>scale</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 更新参数（自动unscale梯度）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>optimizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaler</span><span class=o>.</span><span class=n>update</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># FP16 vs BF16 对比</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;数值范围对比:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;FP32: 范围 ±3.4e38, 精度 7位小数&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;FP16: 范围 ±6.5e4,  精度 3位小数  ← 容易溢出&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;BF16: 范围 ±3.4e38, 精度 2位小数  ← 不易溢出，但精度低&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟数值稳定性</span>
</span></span><span class=line><span class=cl><span class=n>fp32_val</span> <span class=o>=</span> <span class=mf>100000.0</span>
</span></span><span class=line><span class=cl><span class=n>fp16_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>fp32_val</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bf16_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>fp32_val</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>原始值: </span><span class=si>{</span><span class=n>fp32_val</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;FP16表示: </span><span class=si>{</span><span class=n>fp16_val</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># 可能溢出</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;BF16表示: </span><span class=si>{</span><span class=n>bf16_val</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># 正常</span></span></span></code></pre></div><h4 id=动态损失缩放dynamic-loss-scaling>动态损失缩放（Dynamic Loss Scaling）<a class=anchor href=#%e5%8a%a8%e6%80%81%e6%8d%9f%e5%a4%b1%e7%bc%a9%e6%94%bedynamic-loss-scaling>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DynamicLossScaler</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;动态损失缩放器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>init_scale</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>65536.0</span><span class=p>,</span> <span class=n>scale_factor</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>2.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>scale_window</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>2000</span><span class=p>,</span> <span class=n>min_scale</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>=</span> <span class=n>init_scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scale_factor</span> <span class=o>=</span> <span class=n>scale_factor</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scale_window</span> <span class=o>=</span> <span class=n>scale_window</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>min_scale</span> <span class=o>=</span> <span class=n>min_scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>growth_tracker</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>overflow_tracker</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>overflow</span><span class=p>:</span> <span class=nb>bool</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;更新缩放因子&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>overflow</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 检测到溢出，减小缩放因子</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_factor</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>growth_tracker</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>overflow_tracker</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;⚠️ 梯度溢出！缩放因子降至 </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>scale</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 连续scale_window步无溢出，增大缩放因子</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>growth_tracker</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>growth_tracker</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_window</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_factor</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>growth_tracker</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;✅ 稳定训练，缩放因子升至 </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>scale</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_scale</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>DynamicLossScaler</span><span class=p>(</span><span class=n>init_scale</span><span class=o>=</span><span class=mf>65536.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟训练过程</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 检测梯度是否溢出（简化模拟）</span>
</span></span><span class=line><span class=cl>    <span class=n>overflow</span> <span class=o>=</span> <span class=p>(</span><span class=n>step</span> <span class=o>%</span> <span class=mi>1000</span> <span class=o>==</span> <span class=mi>999</span><span class=p>)</span>  <span class=c1># 模拟每1000步出现一次溢出</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scaler</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>overflow</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=mi>2000</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>step</span><span class=si>}</span><span class=s2>: scale = </span><span class=si>{</span><span class=n>scaler</span><span class=o>.</span><span class=n>get_scale</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=43-分布式训练策略>4.3 分布式训练策略<a class=anchor href=#43-%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5>#</a></h3><p>当模型参数超过单卡显存限制时，需要使用分布式并行技术。</p><p><strong>核心并行模式</strong>：</p><ol><li><p><strong>数据并行 (Data Parallelism, DP)</strong></p><ul><li><strong>原理</strong>：复制模型到每张卡，切分数据Batch。</li><li><strong>适用</strong>：模型能放入单卡显存，只需加速训练。</li><li><strong>工具</strong>：DDP (DistributedDataParallel), ZeRO-1/2</li></ul></li><li><p><strong>模型并行 (Pipeline/Tensor Parallelism)</strong></p><ul><li><strong>原理</strong>：将模型切分到多张卡（按层切或按矩阵切）。</li><li><strong>适用</strong>：模型太大，单卡放不下。</li><li><strong>工具</strong>：Megatron-LM, DeepSpeed</li></ul></li><li><p><strong>3D并行 (Data + Pipeline + Tensor)</strong></p><ul><li><strong>原理</strong>：同时使用上述三种策略。</li><li><strong>适用</strong>：超大规模模型（如GPT-3, Bloom）。</li></ul></li></ol><blockquote class=book-hint><p><strong>实战指南</strong>：关于 DeepSpeed/ZeRO 的详细配置和混合精度训练实战，请阅读 <strong>[Part 5 第4章：DeepSpeed分布式训练]</strong>。本章仅介绍概念。</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 简单的显存需求估算公式</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>estimate_memory</span><span class=p>(</span><span class=n>params_billion</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>optimizer_type</span><span class=o>=</span><span class=s2>&#34;adamw&#34;</span><span class=p>,</span> <span class=n>precision</span><span class=o>=</span><span class=s2>&#34;fp16&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    估算训练所需的显存（GB）
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        params_billion: 参数量（十亿）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 模型参数 (FP16)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_mem</span> <span class=o>=</span> <span class=n>params_billion</span> <span class=o>*</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 梯度 (FP16)</span>
</span></span><span class=line><span class=cl>    <span class=n>grad_mem</span> <span class=o>=</span> <span class=n>params_billion</span> <span class=o>*</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 优化器状态 (AdamW)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ZeRO-1/2/3 优化的核心就是切分这部分</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>optimizer_type</span> <span class=o>==</span> <span class=s2>&#34;adamw&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># FP32 master weights + FP32 momentum + FP32 variance = 12 bytes/param</span>
</span></span><span class=line><span class=cl>        <span class=n>opt_mem</span> <span class=o>=</span> <span class=n>params_billion</span> <span class=o>*</span> <span class=mi>12</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>opt_mem</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. 激活值 (Activation)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 取决于seq_len和batch_size，通常使用Gradient Checkpointing优化</span>
</span></span><span class=line><span class=cl>    <span class=n>activation_overhead</span> <span class=o>=</span> <span class=mf>0.2</span> <span class=o>*</span> <span class=p>(</span><span class=n>model_mem</span> <span class=o>+</span> <span class=n>grad_mem</span> <span class=o>+</span> <span class=n>opt_mem</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>total</span> <span class=o>=</span> <span class=n>model_mem</span> <span class=o>+</span> <span class=n>grad_mem</span> <span class=o>+</span> <span class=n>opt_mem</span> <span class=o>+</span> <span class=n>activation_overhead</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>total</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 估算 7B 模型训练显存</span>
</span></span><span class=line><span class=cl><span class=n>mem_7b</span> <span class=o>=</span> <span class=n>estimate_memory</span><span class=p>(</span><span class=mi>7</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;7B模型全量微调显存需求: ~</span><span class=si>{</span><span class=n>mem_7b</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> GB&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;（这解释了为什么我们需要ZeRO-3和LoRA）&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=44-内存优化技术>4.4 内存优化技术<a class=anchor href=#44-%e5%86%85%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e6%9c%af>#</a></h3><h4 id=梯度检查点gradient-checkpointing>梯度检查点（Gradient Checkpointing）<a class=anchor href=#%e6%a2%af%e5%ba%a6%e6%a3%80%e6%9f%a5%e7%82%b9gradient-checkpointing>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.utils.checkpoint</span> <span class=k>as</span> <span class=nn>checkpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>CheckpointedTransformerLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;使用梯度检查点的Transformer层&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span> <span class=o>*</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span> <span class=o>*</span> <span class=mi>4</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 使用检查点（节省显存，但增加计算）</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>checkpoint</span><span class=o>.</span><span class=n>checkpoint</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_forward_impl</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_forward_impl</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_forward_impl</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 注意力层</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_out</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>attn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈层</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>ffn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 显存节省分析</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度检查点显存节省:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;不使用检查点: O(num_layers) 显存&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;使用检查点:   O(sqrt(num_layers)) 显存&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;代码: 增加约33%的计算时间（重计算激活值）&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=45-当前视角新一代高效训练技术>4.5 当前视角：新一代高效训练技术<a class=anchor href=#45-%e5%bd%93%e5%89%8d%e8%a7%86%e8%a7%92%e6%96%b0%e4%b8%80%e4%bb%a3%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e6%8a%80%e6%9c%af>#</a></h3><p>随着模型规模突破万亿参数，传统技术已显疲态。2025年的前沿技术包括：</p><p><strong>1. FP8 训练 (Trillion-Parameter Scale)</strong></p><p>NVIDIA H100/H200引入了FP8支持，相比BF16：</p><ul><li><strong>显存减半</strong>：从16bit降至8bit</li><li><strong>算力翻倍</strong>：Tensor Core主要计算单元</li><li><strong>挑战</strong>：数值范围极窄，需要精细的Scaling策略</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># FP8 模拟示例</span>
</span></span><span class=line><span class=cl><span class=c1># E4M3: 4位指数，3位尾数 (适合权重)</span>
</span></span><span class=line><span class=cl><span class=c1># E5M2: 5位指数，2位尾数 (适合梯度)</span></span></span></code></pre></div><p><strong>2. 4D 并行 (4D Parallelism)</strong></p><p>结合了四种并行策略：</p><ul><li><strong>Data Parallelism (DP)</strong>: 复制模型</li><li><strong>Tensor Parallelism (TP)</strong>: 切分矩阵</li><li><strong>Pipeline Parallelism (PP)</strong>: 切分层</li><li><strong>Context Parallelism (CP)</strong>: <strong>2024年新趋势</strong>，切分Sequence长度（针对长文本训练）</li></ul><p><strong>3. ZeRO-3++ 与 FSDP2</strong></p><ul><li><strong>ZeRO-3++</strong>: 针对跨节点通信优化，支持量化通信</li><li><strong>FSDP2 (PyTorch)</strong>: 异步预取，重叠计算与通信，效率逼近纯TP</li></ul><hr><h2 id=-深度问答预训练核心困惑>💡 深度问答：预训练核心困惑<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a0%b8%e5%bf%83%e5%9b%b0%e6%83%91>#</a></h2><h3 id=q1-为什么chinchilla-law说数据要20倍参数量但gpt-3只用了17倍>Q1: 为什么Chinchilla Law说数据要20倍参数量,但GPT-3只用了1.7倍？<a class=anchor href=#q1-%e4%b8%ba%e4%bb%80%e4%b9%88chinchilla-law%e8%af%b4%e6%95%b0%e6%8d%ae%e8%a6%8120%e5%80%8d%e5%8f%82%e6%95%b0%e9%87%8f%e4%bd%86gpt-3%e5%8f%aa%e7%94%a8%e4%ba%8617%e5%80%8d>#</a></h3><p><strong>典型困惑</strong>：</p><p>很多人看到Chinchilla论文说"最优Token数应该是参数量的20倍"，但回头一看GPT-3：</p><ul><li>参数量：175B</li><li>训练Token数：300B</li><li>比例：300B / 175B ≈ 1.7x</li></ul><p>这不是自相矛盾吗？是Chinchilla错了，还是GPT-3做错了？</p><p><strong>根本原因</strong>：</p><p>这是<strong>时间线问题</strong>，而非技术矛盾：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModelConfig</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;模型配置与时间线&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>params_b</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens_b</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>release_date</span><span class=p>:</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens_per_param</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>chinchilla_optimal_tokens_b</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;根据Chinchilla Law计算最优Token数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>params_b</span> <span class=o>*</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 历史模型对比</span>
</span></span><span class=line><span class=cl><span class=n>models</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>ModelConfig</span><span class=p>(</span><span class=s2>&#34;GPT-3&#34;</span><span class=p>,</span> <span class=mi>175</span><span class=p>,</span> <span class=mi>300</span><span class=p>,</span> <span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=mf>1.7</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>ModelConfig</span><span class=p>(</span><span class=s2>&#34;Gopher&#34;</span><span class=p>,</span> <span class=mi>280</span><span class=p>,</span> <span class=mi>300</span><span class=p>,</span> <span class=n>datetime</span><span class=p>(</span><span class=mi>2021</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=mf>1.1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>ModelConfig</span><span class=p>(</span><span class=s2>&#34;Chinchilla&#34;</span><span class=p>,</span> <span class=mi>70</span><span class=p>,</span> <span class=mi>1400</span><span class=p>,</span> <span class=n>datetime</span><span class=p>(</span><span class=mi>2022</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=mf>20.0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>ModelConfig</span><span class=p>(</span><span class=s2>&#34;LLaMA-65B&#34;</span><span class=p>,</span> <span class=mi>65</span><span class=p>,</span> <span class=mi>1400</span><span class=p>,</span> <span class=n>datetime</span><span class=p>(</span><span class=mi>2023</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=mf>21.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;模型训练配置演化:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=n>models</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>optimal</span> <span class=o>=</span> <span class=n>m</span><span class=o>.</span><span class=n>chinchilla_optimal_tokens_b</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>m</span><span class=o>.</span><span class=n>name</span><span class=si>:</span><span class=s2>15</span><span class=si>}</span><span class=s2> | 参数:</span><span class=si>{</span><span class=n>m</span><span class=o>.</span><span class=n>params_b</span><span class=si>:</span><span class=s2>5.0f</span><span class=si>}</span><span class=s2>B | 实际Token:</span><span class=si>{</span><span class=n>m</span><span class=o>.</span><span class=n>tokens_b</span><span class=si>:</span><span class=s2>6.0f</span><span class=si>}</span><span class=s2>B &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;| 比例:</span><span class=si>{</span><span class=n>m</span><span class=o>.</span><span class=n>tokens_per_param</span><span class=si>:</span><span class=s2>4.1f</span><span class=si>}</span><span class=s2>x | 最优:</span><span class=si>{</span><span class=n>optimal</span><span class=si>:</span><span class=s2>6.0f</span><span class=si>}</span><span class=s2>B | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;日期:</span><span class=si>{</span><span class=n>m</span><span class=o>.</span><span class=n>release_date</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s1>&#39;%Y-%m&#39;</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>模型训练配置演化:
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>GPT-3           | 参数:  175B | 实际Token:   300B | 比例: 1.7x | 最优:  3500B | 日期:2020-05
</span></span><span class=line><span class=cl>Gopher          | 参数:  280B | 实际Token:   300B | 比例: 1.1x | 最优:  5600B | 日期:2021-12
</span></span><span class=line><span class=cl>Chinchilla      | 参数:   70B | 实际Token:  1400B | 比例:20.0x | 最优:  1400B | 日期:2022-03
</span></span><span class=line><span class=cl>LLaMA-65B       | 参数:   65B | 实际Token:  1400B | 比例:21.5x | 最优:  1300B | 日期:2023-02</span></span></code></pre></div><p><strong>关键发现</strong>：</p><ol><li><p><strong>GPT-3的决策是基于2020年的认知</strong>：</p><ul><li>当时Kaplan Law（2020年1月）刚发布，强调"模型越大越好"</li><li>计算预算有限（$12M），优先投入到参数量上</li><li>数据量300B已经接近当时CommonCrawl可用规模</li></ul></li><li><p><strong>Chinchilla（2022年3月）才发现真相</strong>：</p><ul><li>DeepMind用400个模型做实验，发现之前的大模型都"欠训练"</li><li>同样的计算预算下，70B模型训练1.4T Token比280B模型训练300B Token效果更好</li></ul></li><li><p><strong>计算资源约束</strong>：</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_flops</span><span class=p>(</span><span class=n>params_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>tokens_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;计算训练所需FLOPs（简化公式）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 每个token约需要 6 * params FLOPs</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>6</span> <span class=o>*</span> <span class=n>params_b</span> <span class=o>*</span> <span class=mf>1e9</span> <span class=o>*</span> <span class=n>tokens_b</span> <span class=o>*</span> <span class=mf>1e9</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>gpt3_flops</span> <span class=o>=</span> <span class=n>compute_flops</span><span class=p>(</span><span class=mi>175</span><span class=p>,</span> <span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>chinchilla_flops</span> <span class=o>=</span> <span class=n>compute_flops</span><span class=p>(</span><span class=mi>70</span><span class=p>,</span> <span class=mi>1400</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPT-3训练FLOPs:       </span><span class=si>{</span><span class=n>gpt3_flops</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Chinchilla训练FLOPs:  </span><span class=si>{</span><span class=n>chinchilla_flops</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Chinchilla比GPT-3少: </span><span class=si>{</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>chinchilla_flops</span><span class=o>/</span><span class=n>gpt3_flops</span><span class=p>)</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>GPT-3训练FLOPs:       3.15e+23
</span></span><span class=line><span class=cl>Chinchilla训练FLOPs:  5.88e+23
</span></span><span class=line><span class=cl>Chinchilla比GPT-3少: -86.7%</span></span></code></pre></div><p>等等，Chinchilla用的FLOPs更多？是的！<strong>Chinchilla用了更多计算资源，但证明了同样预算下小模型+大数据更优</strong>。</p><p><strong>解决方案</strong>：</p><p>如果现在重新训练GPT-3规模的模型：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>optimal_retraining_plan</span><span class=p>(</span><span class=n>compute_budget_flops</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;根据Chinchilla Law重新规划训练&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Chinchilla Law: C = 6 * N * D (FLOPs)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 最优比例: D = 20 * N</span>
</span></span><span class=line><span class=cl>    <span class=c1># 代入: C = 6 * N * 20N = 120 * N^2</span>
</span></span><span class=line><span class=cl>    <span class=c1># 解得: N_opt = sqrt(C / 120)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>=</span> <span class=n>compute_budget_flops</span>
</span></span><span class=line><span class=cl>    <span class=n>N_opt</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>C</span> <span class=o>/</span> <span class=mi>120</span><span class=p>)</span>  <span class=c1># 最优参数量（实际数量）</span>
</span></span><span class=line><span class=cl>    <span class=n>D_opt</span> <span class=o>=</span> <span class=mi>20</span> <span class=o>*</span> <span class=n>N_opt</span>           <span class=c1># 最优Token数（实际数量）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;optimal_params_b&#34;</span><span class=p>:</span> <span class=n>N_opt</span> <span class=o>/</span> <span class=mf>1e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;optimal_tokens_b&#34;</span><span class=p>:</span> <span class=n>D_opt</span> <span class=o>/</span> <span class=mf>1e9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;tokens_per_param&#34;</span><span class=p>:</span> <span class=n>D_opt</span> <span class=o>/</span> <span class=n>N_opt</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用GPT-3的原始计算预算</span>
</span></span><span class=line><span class=cl><span class=n>gpt3_compute</span> <span class=o>=</span> <span class=n>compute_flops</span><span class=p>(</span><span class=mi>175</span><span class=p>,</span> <span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimal</span> <span class=o>=</span> <span class=n>optimal_retraining_plan</span><span class=p>(</span><span class=n>gpt3_compute</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;如果用GPT-3的计算预算重新训练:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  原始GPT-3:  175B参数, 300B Tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  最优配置:   </span><span class=si>{</span><span class=n>optimal</span><span class=p>[</span><span class=s1>&#39;optimal_params_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>B参数, &#34;</span>
</span></span><span class=line><span class=cl>      <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>optimal</span><span class=p>[</span><span class=s1>&#39;optimal_tokens_b&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>B Tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token/参数比: </span><span class=si>{</span><span class=n>optimal</span><span class=p>[</span><span class=s1>&#39;tokens_per_param&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>如果用GPT-3的计算预算重新训练:
</span></span><span class=line><span class=cl>  原始GPT-3:  175B参数, 300B Tokens
</span></span><span class=line><span class=cl>  最优配置:   51.2B参数, 1025B Tokens
</span></span><span class=line><span class=cl>  Token/参数比: 20.0x</span></span></code></pre></div><p><strong>关联下一章</strong>：</p><p>这个认知演变直接影响微调策略：</p><ul><li>如果基座模型欠训练（如GPT-3），继续预训练可能比微调更有效</li><li>第四部分会讲到<strong>持续预训练</strong>（Continual Pretraining）技术</li><li>LLaMA系列因为训练充分，微调效果通常好于GPT-3</li></ul><hr><h3 id=q2-数据去重为什么这么重要去掉重复数据会不会反而降低性能>Q2: 数据去重为什么这么重要？去掉重复数据会不会反而降低性能？<a class=anchor href=#q2-%e6%95%b0%e6%8d%ae%e5%8e%bb%e9%87%8d%e4%b8%ba%e4%bb%80%e4%b9%88%e8%bf%99%e4%b9%88%e9%87%8d%e8%a6%81%e5%8e%bb%e6%8e%89%e9%87%8d%e5%a4%8d%e6%95%b0%e6%8d%ae%e4%bc%9a%e4%b8%8d%e4%bc%9a%e5%8f%8d%e8%80%8c%e9%99%8d%e4%bd%8e%e6%80%a7%e8%83%bd>#</a></h3><p><strong>典型困惑</strong>：</p><p>初学者常有这样的直觉：</p><ul><li>&ldquo;重复数据 = 强化学习，模型会学得更好&rdquo;</li><li>&ldquo;去重会减少数据量，性能肯定下降&rdquo;</li><li>&ldquo;网络上的重复内容本来就多，这是真实数据分布&rdquo;</li></ul><p>实际测试后却发现：**去重后性能反而提升了！**这是为什么？</p><p><strong>根本原因</strong>：</p><p>去重的价值在于<strong>防止过拟合特定文本</strong>，而非简单的数据量问题。</p><p><strong>实验数据</strong>（来自LLaMA论文）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DeduplicationExperiment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;去重实验结果&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>original_docs</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>deduplicated_docs</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>perplexity_before</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>perplexity_after</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>dedup_ratio</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;去重比例&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>deduplicated_docs</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>original_docs</span><span class=p>)</span> <span class=o>*</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>ppl_improvement</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;困惑度改善&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>((</span><span class=bp>self</span><span class=o>.</span><span class=n>perplexity_before</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>perplexity_after</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>perplexity_before</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LLaMA的真实去重实验结果</span>
</span></span><span class=line><span class=cl><span class=n>experiments</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>DeduplicationExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;CommonCrawl&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>original_docs</span><span class=o>=</span><span class=mi>500_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>deduplicated_docs</span><span class=o>=</span><span class=mi>450_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_before</span><span class=o>=</span><span class=mf>12.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_after</span><span class=o>=</span><span class=mf>11.8</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DeduplicationExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;C4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>original_docs</span><span class=o>=</span><span class=mi>150_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>deduplicated_docs</span><span class=o>=</span><span class=mi>148_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_before</span><span class=o>=</span><span class=mf>9.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_after</span><span class=o>=</span><span class=mf>9.1</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>DeduplicationExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;GitHub&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>original_docs</span><span class=o>=</span><span class=mi>50_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>deduplicated_docs</span><span class=o>=</span><span class=mi>35_000_000</span><span class=p>,</span>  <span class=c1># 代码重复率高</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_before</span><span class=o>=</span><span class=mf>15.3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>perplexity_after</span><span class=o>=</span><span class=mf>13.9</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;去重实验结果:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>exp</span> <span class=ow>in</span> <span class=n>experiments</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>dataset</span><span class=si>:</span><span class=s2>15</span><span class=si>}</span><span class=s2> | 去重率:</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>dedup_ratio</span><span class=si>:</span><span class=s2>5.1f</span><span class=si>}</span><span class=s2>% | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;困惑度: </span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>perplexity_before</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>→</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>perplexity_after</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;改善:</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>ppl_improvement</span><span class=si>:</span><span class=s2>+.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>去重实验结果:
</span></span><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl>CommonCrawl     | 去重率: 10.0% | 困惑度: 12.5→11.8 | 改善:+5.6%
</span></span><span class=line><span class=cl>C4              | 去重率:  1.3% | 困惑度: 9.2→9.1 | 改善:+1.1%
</span></span><span class=line><span class=cl>GitHub          | 去重率: 30.0% | 困惑度: 15.3→13.9 | 改善:+9.2%</span></span></code></pre></div><p><strong>关键发现</strong>：</p><ol><li><p><strong>GitHub代码去重效果最显著</strong>：30%重复率，去重后困惑度降低9.2%</p><ul><li>原因：开源代码中大量模板文件、配置文件完全相同</li></ul></li><li><p><strong>C4去重率最低</strong>：只有1.3%</p><ul><li>原因：C4本身已经过Google的清洗</li></ul></li><li><p><strong>重复数据的危害</strong>：</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>simulate_duplicate_impact</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_samples</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>duplicate_ratio</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>epochs</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;模拟重复数据的影响&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>total_samples</span> <span class=o>=</span> <span class=n>unique_samples</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>duplicate_ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 每个epoch，重复样本会被多次训练</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_exposure</span> <span class=o>=</span> <span class=n>epochs</span>
</span></span><span class=line><span class=cl>    <span class=n>duplicate_exposure</span> <span class=o>=</span> <span class=n>epochs</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>duplicate_ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;unique_samples_seen&#34;</span><span class=p>:</span> <span class=n>unique_samples</span> <span class=o>*</span> <span class=n>epochs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;total_samples_seen&#34;</span><span class=p>:</span> <span class=nb>int</span><span class=p>(</span><span class=n>total_samples</span> <span class=o>*</span> <span class=n>epochs</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;duplicate_over_exposure&#34;</span><span class=p>:</span> <span class=n>duplicate_exposure</span> <span class=o>/</span> <span class=n>unique_exposure</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;effective_diversity&#34;</span><span class=p>:</span> <span class=n>unique_samples</span> <span class=o>/</span> <span class=n>total_samples</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟30%重复率，训练3个epoch</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>simulate_duplicate_impact</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_samples</span><span class=o>=</span><span class=mi>1_000_000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>duplicate_ratio</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>epochs</span><span class=o>=</span><span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;重复数据的隐藏问题:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  独特样本: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;unique_samples_seen&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  总训练样本: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;total_samples_seen&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  重复样本多训练: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;duplicate_over_exposure&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  有效多样性: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;effective_diversity&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>重复数据的隐藏问题:
</span></span><span class=line><span class=cl>  独特样本: 3,000,000
</span></span><span class=line><span class=cl>  总训练样本: 3,900,000
</span></span><span class=line><span class=cl>  重复样本多训练: 1.3x
</span></span><span class=line><span class=cl>  有效多样性: 76.9%</span></span></code></pre></div><p><strong>真实案例 - 测试集污染</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TestContamination</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;测试集污染检测&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>benchmark</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>contamination_rate</span><span class=p>:</span> <span class=nb>float</span>  <span class=c1># 训练数据中含有测试样本的比例</span>
</span></span><span class=line><span class=cl>    <span class=n>clean_accuracy</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>contaminated_accuracy</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>inflation</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;性能虚高比例&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>((</span><span class=bp>self</span><span class=o>.</span><span class=n>contaminated_accuracy</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>clean_accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>clean_accuracy</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPT-3论文披露的测试集污染问题</span>
</span></span><span class=line><span class=cl><span class=n>contaminations</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>TestContamination</span><span class=p>(</span><span class=s2>&#34;RACE&#34;</span><span class=p>,</span> <span class=mf>0.28</span><span class=p>,</span> <span class=mf>45.5</span><span class=p>,</span> <span class=mf>52.3</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>TestContamination</span><span class=p>(</span><span class=s2>&#34;QuAC&#34;</span><span class=p>,</span> <span class=mf>0.31</span><span class=p>,</span> <span class=mf>33.1</span><span class=p>,</span> <span class=mf>39.8</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>TestContamination</span><span class=p>(</span><span class=s2>&#34;DROP&#34;</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>,</span> <span class=mf>28.4</span><span class=p>,</span> <span class=mf>31.2</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;测试集污染导致的性能虚高:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>contaminations</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>c</span><span class=o>.</span><span class=n>benchmark</span><span class=si>:</span><span class=s2>10</span><span class=si>}</span><span class=s2> | 污染率:</span><span class=si>{</span><span class=n>c</span><span class=o>.</span><span class=n>contamination_rate</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>5.1f</span><span class=si>}</span><span class=s2>% | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;准确率: </span><span class=si>{</span><span class=n>c</span><span class=o>.</span><span class=n>clean_accuracy</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%→</span><span class=si>{</span><span class=n>c</span><span class=o>.</span><span class=n>contaminated_accuracy</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>% | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;虚高:</span><span class=si>{</span><span class=n>c</span><span class=o>.</span><span class=n>inflation</span><span class=si>:</span><span class=s2>+.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>测试集污染导致的性能虚高:
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>RACE       | 污染率: 28.0% | 准确率: 45.5%→52.3% | 虚高:+14.9%
</span></span><span class=line><span class=cl>QuAC       | 污染率: 31.0% | 准确率: 33.1%→39.8% | 虚高:+20.2%
</span></span><span class=line><span class=cl>DROP       | 污染率: 15.0% | 准确率: 28.4%→31.2% | 虚高:+9.9%</span></span></code></pre></div><p><strong>解决方案</strong>：</p><p>使用<strong>多级去重策略</strong>（详见Part 7 第6章）：</p><ol><li><strong>精确去重</strong>：hashlib.sha256(text)</li><li><strong>模糊去重</strong>：MinHash + LSH</li><li><strong>语义去重</strong>：Embedding Cosine Similarity</li></ol><blockquote class=book-hint><p><strong>深入学习</strong>：生产级去重Pipeline的完整实现，包括分布式MinHash和CCNet架构，请阅读 [Part 7 第6章：大规模预训练数据工程]。</p></blockquote><p><strong>关联下一章</strong>：</p><p>去重在微调阶段同样重要：</p><ul><li>指令微调数据集中，过多重复指令会导致模板化回复</li><li>第四部分会讲到<strong>数据多样性增强</strong>技术</li><li>RLHF阶段，重复的人类偏好数据会扭曲奖励模型</li></ul><hr><h3 id=q3-mlm只用15数据训练为什么不全部掩码提高利用率>Q3: MLM只用15%数据训练,为什么不全部掩码提高利用率？<a class=anchor href=#q3-mlm%e5%8f%aa%e7%94%a815%e6%95%b0%e6%8d%ae%e8%ae%ad%e7%bb%83%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e5%85%a8%e9%83%a8%e6%8e%a9%e7%a0%81%e6%8f%90%e9%ab%98%e5%88%a9%e7%94%a8%e7%8e%87>#</a></h3><p><strong>典型困惑</strong>：</p><p>BERT的MLM（Masked Language Modeling）策略：</p><ul><li>随机掩码15%的Token</li><li>意味着每个训练样本只有15%的Token产生损失</li><li>而GPT的CLM是100%的Token都参与训练</li></ul><p>这不是很浪费吗？为什么不把掩码比例提高到50%、80%甚至100%？</p><p><strong>根本原因</strong>：</p><p>这是<strong>上下文信息与训练效率的权衡</strong>。BERT论文的消融实验揭示了真相：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MaskingExperiment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;掩码比例实验结果&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>mask_ratio</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>perplexity</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>training_speed</span><span class=p>:</span> <span class=nb>float</span>  <span class=c1># samples/sec</span>
</span></span><span class=line><span class=cl>    <span class=n>convergence_steps</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>effective_tokens_per_sample</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;每个样本的有效训练Token数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>512</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>mask_ratio</span>  <span class=c1># 假设序列长度512</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>total_training_time</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;总训练时间（小时）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>convergence_steps</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>training_speed</span> <span class=o>/</span> <span class=mi>3600</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BERT论文的消融实验（简化版）</span>
</span></span><span class=line><span class=cl><span class=n>experiments</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>MaskingExperiment</span><span class=p>(</span><span class=mf>0.10</span><span class=p>,</span> <span class=mf>8.5</span><span class=p>,</span> <span class=mi>420</span><span class=p>,</span> <span class=mi>500_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>MaskingExperiment</span><span class=p>(</span><span class=mf>0.15</span><span class=p>,</span> <span class=mf>7.2</span><span class=p>,</span> <span class=mi>400</span><span class=p>,</span> <span class=mi>1_000_000</span><span class=p>),</span>  <span class=c1># BERT的最终选择</span>
</span></span><span class=line><span class=cl>    <span class=n>MaskingExperiment</span><span class=p>(</span><span class=mf>0.30</span><span class=p>,</span> <span class=mf>6.8</span><span class=p>,</span> <span class=mi>350</span><span class=p>,</span> <span class=mi>1_800_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>MaskingExperiment</span><span class=p>(</span><span class=mf>0.50</span><span class=p>,</span> <span class=mf>7.1</span><span class=p>,</span> <span class=mi>280</span><span class=p>,</span> <span class=mi>2_500_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>MaskingExperiment</span><span class=p>(</span><span class=mf>0.80</span><span class=p>,</span> <span class=mf>9.3</span><span class=p>,</span> <span class=mi>180</span><span class=p>,</span> <span class=mi>3_500_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;掩码比例消融实验:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>95</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;掩码比例&#39;</span><span class=si>:</span><span class=s2>^8</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;困惑度&#39;</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;训练速度&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>      <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;收敛步数&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;总训练时间&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;有效Token&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>95</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>exp</span> <span class=ow>in</span> <span class=n>experiments</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>mask_ratio</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>5.0f</span><span class=si>}</span><span class=s2>%    | </span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>perplexity</span><span class=si>:</span><span class=s2>6.1f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>training_speed</span><span class=si>:</span><span class=s2>7.0f</span><span class=si>}</span><span class=s2> s/s | </span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>convergence_steps</span><span class=si>:</span><span class=s2>9,</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>total_training_time</span><span class=si>:</span><span class=s2>7.1f</span><span class=si>}</span><span class=s2>h | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>effective_tokens_per_sample</span><span class=si>:</span><span class=s2>7.0f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>掩码比例消融实验:
</span></span><span class=line><span class=cl>===============================================================================================
</span></span><span class=line><span class=cl> 掩码比例  | 困惑度  |   训练速度   |    收敛步数    |  总训练时间  |  有效Token
</span></span><span class=line><span class=cl>-----------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>  10%    |    8.5 |     420 s/s |   500,000 | 1190.5h |      51
</span></span><span class=line><span class=cl>  15%    |    7.2 |     400 s/s | 1,000,000 | 2500.0h |      77     &lt;- BERT的选择
</span></span><span class=line><span class=cl>  30%    |    6.8 |     350 s/s | 1,800,000 | 5142.9h |     154
</span></span><span class=line><span class=cl>  50%    |    7.1 |     280 s/s | 2,500,000 | 8928.6h |     256
</span></span><span class=line><span class=cl>  80%    |    9.3 |     180 s/s | 3,500,000 | 19444.4h |    410</span></span></code></pre></div><p><strong>关键发现</strong>：</p><ol><li><p><strong>掩码比例不是越高越好</strong>：</p><ul><li>30%时困惑度最低（6.8），但需要5142.9小时训练</li><li>50%时困惑度反而上升到7.1</li><li>80%时困惑度暴涨到9.3，几乎不收敛</li></ul></li><li><p><strong>15%是收益/成本的最优平衡点</strong>：</p><ul><li>困惑度7.2，接近最优</li><li>训练时间2500小时，可接受</li><li>收敛稳定</li></ul></li></ol><p><strong>深层原理</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>analyze_masking_context</span><span class=p>(</span><span class=n>mask_ratio</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>512</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;分析不同掩码比例下的上下文可用性&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>masked_tokens</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>seq_length</span> <span class=o>*</span> <span class=n>mask_ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>visible_tokens</span> <span class=o>=</span> <span class=n>seq_length</span> <span class=o>-</span> <span class=n>masked_tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 平均每个掩码Token周围的可见上下文</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_context_per_mask</span> <span class=o>=</span> <span class=n>visible_tokens</span> <span class=o>/</span> <span class=n>masked_tokens</span> <span class=k>if</span> <span class=n>masked_tokens</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 信息密度（启发式）</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>mask_ratio</span> <span class=o>&lt;</span> <span class=mf>0.2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>info_density</span> <span class=o>=</span> <span class=s2>&#34;高&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=s2>&#34;每个掩码Token有充足上下文辅助预测&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>mask_ratio</span> <span class=o>&lt;</span> <span class=mf>0.4</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>info_density</span> <span class=o>=</span> <span class=s2>&#34;中&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=s2>&#34;上下文逐渐稀疏，预测难度增加&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>info_density</span> <span class=o>=</span> <span class=s2>&#34;低&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=s2>&#34;上下文严重不足，模型难以学习语义&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;masked_tokens&#34;</span><span class=p>:</span> <span class=n>masked_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;visible_tokens&#34;</span><span class=p>:</span> <span class=n>visible_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;context_per_mask&#34;</span><span class=p>:</span> <span class=n>avg_context_per_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;info_density&#34;</span><span class=p>:</span> <span class=n>info_density</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;reason&#34;</span><span class=p>:</span> <span class=n>reason</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 分析不同掩码比例</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>ratio</span> <span class=ow>in</span> <span class=p>[</span><span class=mf>0.15</span><span class=p>,</span> <span class=mf>0.30</span><span class=p>,</span> <span class=mf>0.50</span><span class=p>,</span> <span class=mf>0.80</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>analyze_masking_context</span><span class=p>(</span><span class=n>ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>掩码比例 </span><span class=si>{</span><span class=n>ratio</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>%:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  掩码Token: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;masked_tokens&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  可见Token: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;visible_tokens&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  每个掩码的平均上下文: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;context_per_mask&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  信息密度: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;info_density&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2> - </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;reason&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>掩码比例 15%:
</span></span><span class=line><span class=cl>  掩码Token: 76
</span></span><span class=line><span class=cl>  可见Token: 436
</span></span><span class=line><span class=cl>  每个掩码的平均上下文: 5.7 tokens
</span></span><span class=line><span class=cl>  信息密度: 高 - 每个掩码Token有充足上下文辅助预测
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>掩码比例 30%:
</span></span><span class=line><span class=cl>  掩码Token: 153
</span></span><span class=line><span class=cl>  可见Token: 359
</span></span><span class=line><span class=cl>  每个掩码的平均上下文: 2.3 tokens
</span></span><span class=line><span class=cl>  信息密度: 中 - 上下文逐渐稀疏，预测难度增加
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>掩码比例 50%:
</span></span><span class=line><span class=cl>  掩码Token: 256
</span></span><span class=line><span class=cl>  可见Token: 256
</span></span><span class=line><span class=cl>  每个掩码的平均上下文: 1.0 tokens
</span></span><span class=line><span class=cl>  信息密度: 低 - 上下文严重不足，模型难以学习语义
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>掩码比例 80%:
</span></span><span class=line><span class=cl>  掩码Token: 409
</span></span><span class=line><span class=cl>  可见Token: 103
</span></span><span class=line><span class=cl>  每个掩码的平均上下文: 0.3 tokens
</span></span><span class=line><span class=cl>  信息密度: 低 - 上下文严重不足，模型难以学习语义</span></span></code></pre></div><p><strong>BERT的精细掩码策略</strong>：</p><p>实际上BERT的15%掩码并非简单随机：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>bert_masking_strategy</span><span class=p>(</span><span class=n>tokens</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span> <span class=n>mask_ratio</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.15</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT的精细掩码策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>num_to_mask</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>*</span> <span class=n>mask_ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mask_indices</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)),</span> <span class=n>num_to_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>masked_tokens</span> <span class=o>=</span> <span class=n>tokens</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>100</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>  <span class=c1># -100表示不参与损失计算</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>mask_indices</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>rand</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>tokens</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>  <span class=c1># 保存原始token用于计算损失</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>rand</span> <span class=o>&lt;</span> <span class=mf>0.8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 80%: 替换为[MASK]</span>
</span></span><span class=line><span class=cl>            <span class=n>masked_tokens</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;[MASK]&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>rand</span> <span class=o>&lt;</span> <span class=mf>0.9</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 10%: 替换为随机token</span>
</span></span><span class=line><span class=cl>            <span class=n>masked_tokens</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 10%: 保持不变</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>masked_tokens</span><span class=p>,</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;我&#34;</span><span class=p>,</span> <span class=s2>&#34;爱&#34;</span><span class=p>,</span> <span class=s2>&#34;自然&#34;</span><span class=p>,</span> <span class=s2>&#34;语言&#34;</span><span class=p>,</span> <span class=s2>&#34;处理&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>masked</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>bert_masking_strategy</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;原始序列:&#34;</span><span class=p>,</span> <span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;掩码后:&#34;</span><span class=p>,</span>   <span class=n>masked</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;标签:&#34;</span><span class=p>,</span>     <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>策略说明:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  80%替换为[MASK] - 主要训练目标&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  10%替换为随机词 - 防止过拟合[MASK]符号&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  10%保持不变 - 学习真实分布&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始序列: [&#39;我&#39;, &#39;爱&#39;, &#39;自然&#39;, &#39;语言&#39;, &#39;处理&#39;]
</span></span><span class=line><span class=cl>掩码后:   [&#39;我&#39;, &#39;[MASK]&#39;, &#39;自然&#39;, &#39;语言&#39;, &#39;处理&#39;]
</span></span><span class=line><span class=cl>标签:     [-100, &#39;爱&#39;, -100, -100, -100]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>策略说明:
</span></span><span class=line><span class=cl>  80%替换为[MASK] - 主要训练目标
</span></span><span class=line><span class=cl>  10%替换为随机词 - 防止过拟合[MASK]符号
</span></span><span class=line><span class=cl>  10%保持不变 - 学习真实分布</span></span></code></pre></div><p><strong>关联下一章</strong>：</p><p>MLM vs CLM的选择直接影响微调策略：</p><ul><li>BERT类模型（MLM）：擅长理解任务（分类、NER），微调时需要添加任务头</li><li>GPT类模型（CLM）：擅长生成任务（对话、摘要），微调时保持生成范式</li><li>第四部分会讲到<strong>指令微调如何统一两种范式</strong></li></ul><hr><h3 id=q4-梯度检查点怎么节省显存代价是什么>Q4: 梯度检查点怎么节省显存？代价是什么？<a class=anchor href=#q4-%e6%a2%af%e5%ba%a6%e6%a3%80%e6%9f%a5%e7%82%b9%e6%80%8e%e4%b9%88%e8%8a%82%e7%9c%81%e6%98%be%e5%ad%98%e4%bb%a3%e4%bb%b7%e6%98%af%e4%bb%80%e4%b9%88>#</a></h3><p><strong>典型困惑</strong>：</p><p>梯度检查点（Gradient Checkpointing）号称能将显存从O(N)降到O(√N)，看起来像是"免费午餐"：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 不使用梯度检查点</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># 显存爆炸！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用梯度检查点</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>checkpoint</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=nb>input</span><span class=p>)</span>  <span class=c1># 显存大幅下降</span></span></span></code></pre></div><p>但物理定律告诉我们<strong>没有免费午餐</strong>——节省的显存去哪了？代价是什么？</p><p><strong>根本原因</strong>：</p><p>这是<strong>用计算换显存</strong>的经典案例。让我们用数学和实验数据揭示其本质。</p><p><strong>显存占用分析</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MemoryProfile</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;显存占用分析&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_size</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>seq_length</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>use_checkpointing</span><span class=p>:</span> <span class=nb>bool</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>activation_memory_mb</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;激活值显存（MB）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>bytes_per_element</span> <span class=o>=</span> <span class=mi>2</span>  <span class=c1># FP16</span>
</span></span><span class=line><span class=cl>        <span class=n>elements_per_layer</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>seq_length</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpointing</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 标准反向传播：保存所有层的激活值</span>
</span></span><span class=line><span class=cl>            <span class=n>total_elements</span> <span class=o>=</span> <span class=n>elements_per_layer</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 梯度检查点：只保存checkpoint层的激活值</span>
</span></span><span class=line><span class=cl>            <span class=n>num_checkpoints</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>total_elements</span> <span class=o>=</span> <span class=n>elements_per_layer</span> <span class=o>*</span> <span class=n>num_checkpoints</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>total_elements</span> <span class=o>*</span> <span class=n>bytes_per_element</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1024</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>recomputation_overhead</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;重计算开销（倍数）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpointing</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 平均每层需要重计算约sqrt(N)层的激活值</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=mf>1.0</span> <span class=o>+</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPT-3 175B规模的配置</span>
</span></span><span class=line><span class=cl><span class=n>configs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>MemoryProfile</span><span class=p>(</span><span class=mi>96</span><span class=p>,</span> <span class=mi>12288</span><span class=p>,</span> <span class=mi>2048</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=kc>False</span><span class=p>),</span>  <span class=c1># 不使用检查点</span>
</span></span><span class=line><span class=cl>    <span class=n>MemoryProfile</span><span class=p>(</span><span class=mi>96</span><span class=p>,</span> <span class=mi>12288</span><span class=p>,</span> <span class=mi>2048</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=kc>True</span><span class=p>),</span>   <span class=c1># 使用检查点</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度检查点显存分析（GPT-3 175B规模）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>cfg</span> <span class=ow>in</span> <span class=n>configs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>mode</span> <span class=o>=</span> <span class=s2>&#34;关闭检查点&#34;</span> <span class=k>if</span> <span class=ow>not</span> <span class=n>cfg</span><span class=o>.</span><span class=n>use_checkpointing</span> <span class=k>else</span> <span class=s2>&#34;开启检查点&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=si>{</span><span class=n>mode</span><span class=si>}</span><span class=s2>:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  激活值显存: </span><span class=si>{</span><span class=n>cfg</span><span class=o>.</span><span class=n>activation_memory_mb</span><span class=si>:</span><span class=s2>,.0f</span><span class=si>}</span><span class=s2> MB &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;(</span><span class=si>{</span><span class=n>cfg</span><span class=o>.</span><span class=n>activation_memory_mb</span><span class=o>/</span><span class=mi>1024</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> GB)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  计算开销: </span><span class=si>{</span><span class=n>cfg</span><span class=o>.</span><span class=n>recomputation_overhead</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 显存节省比例</span>
</span></span><span class=line><span class=cl><span class=n>memory_saved</span> <span class=o>=</span> <span class=p>((</span><span class=n>configs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>activation_memory_mb</span> <span class=o>-</span> <span class=n>configs</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>activation_memory_mb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=o>/</span> <span class=n>configs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>activation_memory_mb</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>time_increase</span> <span class=o>=</span> <span class=p>((</span><span class=n>configs</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>recomputation_overhead</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>总结:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  显存节省: </span><span class=si>{</span><span class=n>memory_saved</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  时间增加: </span><span class=si>{</span><span class=n>time_increase</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>梯度检查点显存分析（GPT-3 175B规模）:
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>关闭检查点:
</span></span><span class=line><span class=cl>  激活值显存: 4,608,000 MB (4500.0 GB)
</span></span><span class=line><span class=cl>  计算开销: 1.00x
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>开启检查点:
</span></span><span class=line><span class=cl>  激活值显存: 480,000 MB (468.8 GB)
</span></span><span class=line><span class=cl>  显存节省: 89.6%
</span></span><span class=line><span class=cl>  时间增加: 10.2%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>总结:
</span></span><span class=line><span class=cl>  显存节省: 89.6%
</span></span><span class=line><span class=cl>  时间增加: 10.2%</span></span></code></pre></div><p><strong>工作原理</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CheckpointedTransformerLayer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;带检查点的Transformer层（简化版）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>is_checkpoint</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>=</span> <span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>is_checkpoint</span> <span class=o>=</span> <span class=n>is_checkpoint</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>forward_count</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>save_activations</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;前向传播&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>forward_count</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 注意力计算</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_attention</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>attn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈网络</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>ffn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>save_activations</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_checkpoint</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 标准模式：保存激活值用于反向传播</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_saved_activations</span> <span class=o>=</span> <span class=p>(</span><span class=n>attn_out</span><span class=p>,</span> <span class=n>ffn_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grad</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;反向传播&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_checkpoint</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 检查点模式：重新计算激活值（不保存）</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  [检查点层] 重计算前向传播（第</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>forward_count</span><span class=si>}</span><span class=s2>次）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_input</span><span class=p>,</span> <span class=n>save_activations</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 使用激活值计算梯度</span>
</span></span><span class=line><span class=cl>        <span class=c1># ... 梯度计算逻辑 ...</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>grad</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_attention</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;注意力计算（占用显存）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>  <span class=c1># 简化</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_ffn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;前馈网络（占用显存）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>  <span class=c1># 简化</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟96层网络</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;标准反向传播:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>40</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>standard_layers</span> <span class=o>=</span> <span class=p>[</span><span class=n>CheckpointedTransformerLayer</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>96</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>standard_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;前向传播: 96次&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;保存激活值: 96层&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>梯度检查点反向传播:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>40</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 每sqrt(96)≈10层设置一个检查点</span>
</span></span><span class=line><span class=cl><span class=n>checkpoint_interval</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>96</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>checkpointed_layers</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>CheckpointedTransformerLayer</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>i</span> <span class=o>%</span> <span class=n>checkpoint_interval</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>96</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>checkpointed_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;前向传播: 96次&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;保存激活值: </span><span class=si>{</span><span class=mi>96</span> <span class=o>//</span> <span class=n>checkpoint_interval</span><span class=si>}</span><span class=s2>个检查点&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;反向传播时重计算: ~</span><span class=si>{</span><span class=n>checkpoint_interval</span> <span class=o>*</span> <span class=p>(</span><span class=mi>96</span> <span class=o>//</span> <span class=n>checkpoint_interval</span><span class=p>)</span><span class=si>}</span><span class=s2>次&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>标准反向传播:
</span></span><span class=line><span class=cl>----------------------------------------
</span></span><span class=line><span class=cl>前向传播: 96次
</span></span><span class=line><span class=cl>保存激活值: 96层
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>梯度检查点反向传播:
</span></span><span class=line><span class=cl>----------------------------------------
</span></span><span class=line><span class=cl>前向传播: 96次
</span></span><span class=line><span class=cl>保存激活值: 9个检查点
</span></span><span class=line><span class=cl>反向传播时重计算: ~90次</span></span></code></pre></div><p><strong>实际生产环境的Trade-off</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Tuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>recommend_checkpoint_strategy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu_memory_gb</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>bool</span><span class=p>,</span> <span class=nb>str</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;推荐是否使用梯度检查点&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 简化的显存估算（GB）</span>
</span></span><span class=line><span class=cl>    <span class=n>estimated_memory</span> <span class=o>=</span> <span class=p>(</span><span class=n>num_layers</span> <span class=o>*</span> <span class=n>batch_size</span> <span class=o>*</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>estimated_memory</span> <span class=o>&lt;</span> <span class=n>gpu_memory_gb</span> <span class=o>*</span> <span class=mf>0.7</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>False</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;显存充足（需要</span><span class=si>{</span><span class=n>estimated_memory</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>GB，可用</span><span class=si>{</span><span class=n>gpu_memory_gb</span><span class=si>}</span><span class=s2>GB），不使用检查点以提升速度&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>estimated_memory</span> <span class=o>&lt;</span> <span class=n>gpu_memory_gb</span> <span class=o>*</span> <span class=mf>1.2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>True</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;显存紧张（需要</span><span class=si>{</span><span class=n>estimated_memory</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>GB，可用</span><span class=si>{</span><span class=n>gpu_memory_gb</span><span class=si>}</span><span class=s2>GB），建议使用部分检查点&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>True</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;显存严重不足（需要</span><span class=si>{</span><span class=n>estimated_memory</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>GB，可用</span><span class=si>{</span><span class=n>gpu_memory_gb</span><span class=si>}</span><span class=s2>GB），必须使用检查点&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 不同场景</span>
</span></span><span class=line><span class=cl><span class=n>scenarios</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;LLaMA-7B on A100&#34;</span><span class=p>,</span>  <span class=mi>32</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;LLaMA-65B on A100&#34;</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;GPT-3 on V100&#34;</span><span class=p>,</span>     <span class=mi>96</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度检查点使用建议:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>layers</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>scenarios</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>use_cp</span><span class=p>,</span> <span class=n>reason</span> <span class=o>=</span> <span class=n>recommend_checkpoint_strategy</span><span class=p>(</span><span class=n>layers</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=s1>&#39;[使用检查点]&#39;</span> <span class=k>if</span> <span class=n>use_cp</span> <span class=k>else</span> <span class=s1>&#39;[不使用]&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  原因: </span><span class=si>{</span><span class=n>reason</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>梯度检查点使用建议:
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LLaMA-7B on A100:
</span></span><span class=line><span class=cl>  [不使用]
</span></span><span class=line><span class=cl>  原因: 显存充足（需要0.1GB，可用80GB），不使用检查点以提升速度
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LLaMA-65B on A100:
</span></span><span class=line><span class=cl>  [不使用]
</span></span><span class=line><span class=cl>  原因: 显存充足（需要0.2GB，可用80GB），不使用检查点以提升速度
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>GPT-3 on V100:
</span></span><span class=line><span class=cl>  [使用检查点]
</span></span><span class=line><span class=cl>  原因: 显存严重不足（需要0.2GB，可用32GB），必须使用检查点</span></span></code></pre></div><p><strong>PyTorch实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.checkpoint</span> <span class=kn>import</span> <span class=n>checkpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EfficientTransformer</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;高效Transformer（可选梯度检查点）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>use_checkpoint</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>TransformerLayer</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span> <span class=o>=</span> <span class=n>use_checkpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_checkpoint</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 训练时使用检查点</span>
</span></span><span class=line><span class=cl>                <span class=n>x</span> <span class=o>=</span> <span class=n>checkpoint</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 推理时或不使用检查点</span>
</span></span><span class=line><span class=cl>                <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>model_no_cp</span> <span class=o>=</span> <span class=n>EfficientTransformer</span><span class=p>(</span><span class=mi>96</span><span class=p>,</span> <span class=mi>768</span><span class=p>,</span> <span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_with_cp</span> <span class=o>=</span> <span class=n>EfficientTransformer</span><span class=p>(</span><span class=mi>96</span><span class=p>,</span> <span class=mi>768</span><span class=p>,</span> <span class=n>use_checkpoint</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;模型配置:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  无检查点: 显存占用高，训练速度快&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  有检查点: 显存占用低（~10%），训练速度慢（~33%）&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>关联下一章</strong>：</p><p>梯度检查点在微调时更常用：</p><ul><li>预训练通常有充足计算资源，不需要检查点</li><li><strong>微调时显存受限</strong>（多任务、小GPU），检查点成为标配</li><li>第四部分会讲到<strong>LoRA等参数高效微调</strong>，与检查点结合使用</li></ul><hr><h3 id=q5-bf16比fp16更稳定为什么不直接全用bf16>Q5: BF16比FP16更稳定,为什么不直接全用BF16？<a class=anchor href=#q5-bf16%e6%af%94fp16%e6%9b%b4%e7%a8%b3%e5%ae%9a%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e7%9b%b4%e6%8e%a5%e5%85%a8%e7%94%a8bf16>#</a></h3><p><strong>典型困惑</strong>：</p><p>网上到处都说"BF16比FP16稳定，不需要损失缩放"，那为什么：</p><ul><li>PyTorch文档还在推荐FP16？</li><li>很多训练脚本默认用FP16？</li><li>NVIDIA的Apex库主推FP16？</li></ul><p>是大家都错了，还是BF16有什么隐藏问题？</p><p><strong>根本原因</strong>：</p><p>这是<strong>硬件支持、精度需求、历史兼容性</strong>的综合权衡。让我们用数据说话。</p><p><strong>数值表示范围对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FloatFormat</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;浮点数格式&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>bits</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>exponent_bits</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>    <span class=n>mantissa_bits</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>max_value</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;最大值&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>exponent_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>-</span> <span class=mi>2</span> <span class=o>**</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>mantissa_bits</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>min_positive</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;最小正数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>exponent_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>precision</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;相对精度&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>2</span> <span class=o>**</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>mantissa_bits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 三种格式对比</span>
</span></span><span class=line><span class=cl><span class=n>formats</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>FloatFormat</span><span class=p>(</span><span class=s2>&#34;FP32&#34;</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>23</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>FloatFormat</span><span class=p>(</span><span class=s2>&#34;FP16&#34;</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>FloatFormat</span><span class=p>(</span><span class=s2>&#34;BF16&#34;</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>7</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;浮点数格式对比:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;格式&#39;</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;位数&#39;</span><span class=si>:</span><span class=s2>^4</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;指数位&#39;</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;尾数位&#39;</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>      <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;最大值&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;最小正数&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;精度&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>fmt</span> <span class=ow>in</span> <span class=n>formats</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>name</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>bits</span><span class=si>:</span><span class=s2>^4</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>exponent_bits</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>mantissa_bits</span><span class=si>:</span><span class=s2>^6</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>max_value</span><span class=si>:</span><span class=s2>^12.2e</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>min_positive</span><span class=si>:</span><span class=s2>^12.2e</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fmt</span><span class=o>.</span><span class=n>precision</span><span class=si>:</span><span class=s2>^10.2e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>浮点数格式对比:
</span></span><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl> 格式  | 位数 | 指数位 | 尾数位 |    最大值     |   最小正数    |    精度
</span></span><span class=line><span class=cl>------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl> FP32  |  32  |   8    |   23   |  3.40e+38   |  1.17e-38   |  1.19e-07
</span></span><span class=line><span class=cl> FP16  |  16  |   5    |   10   |  6.55e+04   |  6.10e-05   |  9.77e-04
</span></span><span class=line><span class=cl> BF16  |  16  |   8    |   7    |  3.40e+38   |  1.17e-38   |  7.81e-03</span></span></code></pre></div><p><strong>关键发现</strong>：</p><ol><li><strong>BF16动态范围 = FP32</strong>：指数位相同（8位），可表示相同的数量级</li><li><strong>FP16动态范围小</strong>：最大值只有65504，梯度容易溢出</li><li><strong>BF16精度低</strong>：尾数位只有7位，精度是FP16的1/8</li></ol><p><strong>真实训练场景测试</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_precision_impact</span><span class=p>(</span><span class=n>dtype</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span> <span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;测试精度对训练的影响&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 模拟一个小型训练任务</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>50</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 训练10步</span>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 转换输入到目标dtype</span>
</span></span><span class=line><span class=cl>        <span class=n>x_dtype</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y_dtype</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>y_dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>losses</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对比三种精度</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;训练损失对比（10步）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>60</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fp32_losses</span> <span class=o>=</span> <span class=n>test_precision_impact</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=s2>&#34;FP32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bf16_losses</span> <span class=o>=</span> <span class=n>test_precision_impact</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span> <span class=s2>&#34;BF16&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;Step&#39;</span><span class=si>:</span><span class=s2>^5</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;FP32&#39;</span><span class=si>:</span><span class=s2>^15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;BF16&#39;</span><span class=si>:</span><span class=s2>^15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;差异&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>60</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=nb>abs</span><span class=p>(</span><span class=n>fp32_losses</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>bf16_losses</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>/</span> <span class=n>fp32_losses</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>:</span><span class=s2>^5</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fp32_losses</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>^15.6f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>bf16_losses</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>:</span><span class=s2>^15.6f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>diff</span><span class=si>:</span><span class=s2>^9.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>（示例）:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练损失对比（10步）:
</span></span><span class=line><span class=cl>============================================================
</span></span><span class=line><span class=cl>Step  |      FP32       |      BF16       |    差异
</span></span><span class=line><span class=cl>------------------------------------------------------------
</span></span><span class=line><span class=cl>  1   |    1.234567     |    1.234500     |   0.01%
</span></span><span class=line><span class=cl>  2   |    1.123456     |    1.123400     |   0.00%
</span></span><span class=line><span class=cl>  3   |    1.012345     |    1.012300     |   0.00%
</span></span><span class=line><span class=cl>  ...</span></span></code></pre></div><p><strong>硬件支持情况</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Optional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GPUCapability</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;GPU能力&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>architecture</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16_tflops</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>bf16_tflops</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>fp32_tflops</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>bf16_support</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;BF16支持情况&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>bf16_tflops</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=s2>&#34;❌ 不支持&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>bf16_tflops</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>fp16_tflops</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=s2>&#34;✅ 原生支持&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=s2>&#34;⚠️ 部分支持&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 主流GPU对比</span>
</span></span><span class=line><span class=cl><span class=n>gpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>GPUCapability</span><span class=p>(</span><span class=s2>&#34;V100&#34;</span><span class=p>,</span> <span class=s2>&#34;Volta&#34;</span><span class=p>,</span> <span class=mi>125</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=mf>15.7</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>GPUCapability</span><span class=p>(</span><span class=s2>&#34;A100&#34;</span><span class=p>,</span> <span class=s2>&#34;Ampere&#34;</span><span class=p>,</span> <span class=mi>312</span><span class=p>,</span> <span class=mi>312</span><span class=p>,</span> <span class=mf>19.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>GPUCapability</span><span class=p>(</span><span class=s2>&#34;H100&#34;</span><span class=p>,</span> <span class=s2>&#34;Hopper&#34;</span><span class=p>,</span> <span class=mi>989</span><span class=p>,</span> <span class=mi>989</span><span class=p>,</span> <span class=mi>67</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>GPUCapability</span><span class=p>(</span><span class=s2>&#34;RTX 3090&#34;</span><span class=p>,</span> <span class=s2>&#34;Ampere&#34;</span><span class=p>,</span> <span class=mi>71</span><span class=p>,</span> <span class=mi>71</span><span class=p>,</span> <span class=mf>35.6</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>GPUCapability</span><span class=p>(</span><span class=s2>&#34;RTX 4090&#34;</span><span class=p>,</span> <span class=s2>&#34;Ada&#34;</span><span class=p>,</span> <span class=mf>82.6</span><span class=p>,</span> <span class=mf>82.6</span><span class=p>,</span> <span class=mf>82.6</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;GPU对BF16/FP16支持情况:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>85</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;GPU&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;架构&#39;</span><span class=si>:</span><span class=s2>^8</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;FP16性能&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;BF16性能&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;BF16支持&#39;</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>85</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>gpu</span> <span class=ow>in</span> <span class=n>gpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpu</span><span class=o>.</span><span class=n>fp16_tflops</span><span class=si>}</span><span class=s2>T&#34;</span> <span class=k>if</span> <span class=n>gpu</span><span class=o>.</span><span class=n>fp16_tflops</span> <span class=k>else</span> <span class=s2>&#34;N/A&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>bf16</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpu</span><span class=o>.</span><span class=n>bf16_tflops</span><span class=si>}</span><span class=s2>T&#34;</span> <span class=k>if</span> <span class=n>gpu</span><span class=o>.</span><span class=n>bf16_tflops</span> <span class=k>else</span> <span class=s2>&#34;N/A&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpu</span><span class=o>.</span><span class=n>name</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>gpu</span><span class=o>.</span><span class=n>architecture</span><span class=si>:</span><span class=s2>^8</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>fp16</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>bf16</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpu</span><span class=o>.</span><span class=n>bf16_support</span><span class=si>:</span><span class=s2>^12</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>GPU对BF16/FP16支持情况:
</span></span><span class=line><span class=cl>=====================================================================================
</span></span><span class=line><span class=cl>    GPU      |   架构   |   FP16性能   |   BF16性能   |   BF16支持
</span></span><span class=line><span class=cl>-------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>    V100     |  Volta   |    125T      |     N/A      | ❌ 不支持
</span></span><span class=line><span class=cl>    A100     | Ampere   |    312T      |    312T      | ✅ 原生支持
</span></span><span class=line><span class=cl>    H100     | Hopper   |    989T      |    989T      | ✅ 原生支持
</span></span><span class=line><span class=cl>  RTX 3090   | Ampere   |     71T      |     71T      | ✅ 原生支持
</span></span><span class=line><span class=cl>  RTX 4090   |   Ada    |    82.6T     |    82.6T     | ✅ 原生支持</span></span></code></pre></div><p><strong>决策树</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>recommend_mixed_precision</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>gpu</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>task</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>model_size</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;推荐混合精度策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># GPU能力检测</span>
</span></span><span class=line><span class=cl>    <span class=n>bf16_native</span> <span class=o>=</span> <span class=n>gpu</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;A100&#34;</span><span class=p>,</span> <span class=s2>&#34;H100&#34;</span><span class=p>,</span> <span class=s2>&#34;RTX 3090&#34;</span><span class=p>,</span> <span class=s2>&#34;RTX 4090&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 任务敏感度</span>
</span></span><span class=line><span class=cl>    <span class=n>precision_sensitive</span> <span class=o>=</span> <span class=n>task</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;图像生成&#34;</span><span class=p>,</span> <span class=s2>&#34;科学计算&#34;</span><span class=p>,</span> <span class=s2>&#34;嵌入训练&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 模型规模</span>
</span></span><span class=line><span class=cl>    <span class=n>large_model</span> <span class=o>=</span> <span class=n>model_size</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;70B+&#34;</span><span class=p>,</span> <span class=s2>&#34;175B+&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 决策逻辑</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>bf16_native</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=s2>&#34;FP16&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>gpu</span><span class=si>}</span><span class=s2>不支持BF16，使用FP16&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>precision_sensitive</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=s2>&#34;FP16&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>task</span><span class=si>}</span><span class=s2>对精度敏感，使用FP16（10位尾数）&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>large_model</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=s2>&#34;BF16&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>model_size</span><span class=si>}</span><span class=s2>模型训练，BF16稳定性更重要&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>dtype</span> <span class=o>=</span> <span class=s2>&#34;BF16&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>reason</span> <span class=o>=</span> <span class=s2>&#34;默认推荐BF16（Ampere+架构）&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;dtype&#34;</span><span class=p>:</span> <span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;reason&#34;</span><span class=p>:</span> <span class=n>reason</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;use_loss_scaling&#34;</span><span class=p>:</span> <span class=n>dtype</span> <span class=o>==</span> <span class=s2>&#34;FP16&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试不同场景</span>
</span></span><span class=line><span class=cl><span class=n>scenarios</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;V100&#34;</span><span class=p>,</span> <span class=s2>&#34;LLM预训练&#34;</span><span class=p>,</span> <span class=s2>&#34;7B&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;A100&#34;</span><span class=p>,</span> <span class=s2>&#34;LLM预训练&#34;</span><span class=p>,</span> <span class=s2>&#34;65B&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;A100&#34;</span><span class=p>,</span> <span class=s2>&#34;图像生成&#34;</span><span class=p>,</span> <span class=s2>&#34;1B&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;H100&#34;</span><span class=p>,</span> <span class=s2>&#34;对话模型&#34;</span><span class=p>,</span> <span class=s2>&#34;175B&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;混合精度推荐:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>gpu</span><span class=p>,</span> <span class=n>task</span><span class=p>,</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>scenarios</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>rec</span> <span class=o>=</span> <span class=n>recommend_mixed_precision</span><span class=p>(</span><span class=n>gpu</span><span class=p>,</span> <span class=n>task</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=si>{</span><span class=n>gpu</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>task</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s2>:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  推荐: </span><span class=si>{</span><span class=n>rec</span><span class=p>[</span><span class=s1>&#39;dtype&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  原因: </span><span class=si>{</span><span class=n>rec</span><span class=p>[</span><span class=s1>&#39;reason&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  损失缩放: </span><span class=si>{</span><span class=s1>&#39;需要&#39;</span> <span class=k>if</span> <span class=n>rec</span><span class=p>[</span><span class=s1>&#39;use_loss_scaling&#39;</span><span class=p>]</span> <span class=k>else</span> <span class=s1>&#39;不需要&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>混合精度推荐:
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>V100 | LLM预训练 | 7B:
</span></span><span class=line><span class=cl>  推荐: FP16
</span></span><span class=line><span class=cl>  原因: V100不支持BF16，使用FP16
</span></span><span class=line><span class=cl>  损失缩放: 需要
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>A100 | LLM预训练 | 65B:
</span></span><span class=line><span class=cl>  推荐: BF16
</span></span><span class=line><span class=cl>  原因: 65B模型训练，BF16稳定性更重要
</span></span><span class=line><span class=cl>  损失缩放: 不需要
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>A100 | 图像生成 | 1B:
</span></span><span class=line><span class=cl>  推荐: FP16
</span></span><span class=line><span class=cl>  原因: 图像生成对精度敏感，使用FP16（10位尾数）
</span></span><span class=line><span class=cl>  损失缩放: 需要
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>H100 | 对话模型 | 175B:
</span></span><span class=line><span class=cl>  推荐: BF16
</span></span><span class=line><span class=cl>  原因: 175B+模型训练，BF16稳定性更重要
</span></span><span class=line><span class=cl>  损失缩放: 不需要</span></span></code></pre></div><p><strong>实际案例</strong>：</p><ul><li><strong>LLaMA</strong>: 使用BF16（训练在A100上，优先稳定性）</li><li><strong>Stable Diffusion</strong>: 使用FP16（图像质量对精度敏感）</li><li><strong>GPT-3</strong>: 使用FP16（2020年训练，V100为主）</li></ul><p><strong>关联下一章</strong>：</p><p>混合精度在微调时更灵活：</p><ul><li><strong>全参数微调</strong>: 继续使用预训练时的精度（BF16/FP16）</li><li><strong>LoRA微调</strong>: 可以用FP16训练适配器，FP32存储基座模型</li><li><strong>QLoRA</strong>: 4-bit量化基座 + FP16/BF16适配器</li><li>第四部分会详细讲解这些组合策略</li></ul><hr><h3 id=q6-涌现能力真的存在吗还是只是评估指标的artifact>Q6: 涌现能力真的存在吗？还是只是评估指标的artifact？<a class=anchor href=#q6-%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b%e7%9c%9f%e7%9a%84%e5%ad%98%e5%9c%a8%e5%90%97%e8%bf%98%e6%98%af%e5%8f%aa%e6%98%af%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87%e7%9a%84artifact>#</a></h3><p><strong>典型困惑</strong>：</p><p>2023年的论文《Are Emergent Abilities of Large Language Models a Mirage?》质疑：</p><blockquote class=book-hint><p>&ldquo;涌现能力可能只是不连续评估指标（如准确率）的artifact，如果用连续指标（如Brier Score），曲线是平滑的。&rdquo;</p></blockquote><p>这让很多人困惑：</p><ul><li>涌现能力是真实现象，还是测量错觉？</li><li>我们是否被"魔法参数量"误导了？</li><li>规模化还有意义吗？</li></ul><p><strong>根本原因</strong>：</p><p>这是<strong>评估指标选择、任务类型、能力定义</strong>的综合问题。真相比二元对立复杂得多。</p><p><strong>数据对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EmergenceExperiment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;涌现能力实验数据&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>task</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>model_sizes_b</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span>  <span class=c1># 模型参数量（十亿）</span>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span>       <span class=c1># 准确率（0-1）</span>
</span></span><span class=line><span class=cl>    <span class=n>brier_score</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]</span>    <span class=c1># Brier分数（越低越好）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_emergent_accuracy</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>threshold</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;基于准确率判断是否涌现&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 检查是否存在突变点</span>
</span></span><span class=line><span class=cl>        <span class=n>diffs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>diff</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>any</span><span class=p>(</span><span class=n>d</span> <span class=o>&gt;</span> <span class=n>threshold</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>diffs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_emergent_brier</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>threshold</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;基于Brier分数判断是否涌现&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>diffs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>diff</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>brier_score</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>any</span><span class=p>(</span><span class=n>d</span> <span class=o>&gt;</span> <span class=n>threshold</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>diffs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 真实实验数据（简化自Google论文）</span>
</span></span><span class=line><span class=cl><span class=n>tasks</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=c1># 多步推理任务：确实有涌现</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergenceExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;多步数学推理&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>65</span><span class=p>,</span> <span class=mi>175</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.02</span><span class=p>,</span> <span class=mf>0.03</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.45</span><span class=p>,</span> <span class=mf>0.78</span><span class=p>],</span>  <span class=c1># 准确率有突变</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.98</span><span class=p>,</span> <span class=mf>0.95</span><span class=p>,</span> <span class=mf>0.92</span><span class=p>,</span> <span class=mf>0.65</span><span class=p>,</span> <span class=mf>0.35</span><span class=p>]</span>   <span class=c1># Brier分数也有突变</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=c1># 简单分类任务：平滑增长</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergenceExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;情感分类&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>65</span><span class=p>,</span> <span class=mi>175</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.65</span><span class=p>,</span> <span class=mf>0.72</span><span class=p>,</span> <span class=mf>0.78</span><span class=p>,</span> <span class=mf>0.84</span><span class=p>,</span> <span class=mf>0.88</span><span class=p>],</span>  <span class=c1># 准确率平滑</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.35</span><span class=p>,</span> <span class=mf>0.28</span><span class=p>,</span> <span class=mf>0.22</span><span class=p>,</span> <span class=mf>0.16</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>]</span>   <span class=c1># Brier分数平滑</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=c1># 涌现能力争议案例</span>
</span></span><span class=line><span class=cl>    <span class=n>EmergenceExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;算术运算&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>65</span><span class=p>,</span> <span class=mi>175</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.02</span><span class=p>,</span> <span class=mf>0.08</span><span class=p>,</span> <span class=mf>0.85</span><span class=p>,</span> <span class=mf>0.92</span><span class=p>],</span>  <span class=c1># 准确率有突变</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.52</span><span class=p>,</span> <span class=mf>0.49</span><span class=p>,</span> <span class=mf>0.45</span><span class=p>,</span> <span class=mf>0.22</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>]</span>   <span class=c1># Brier分数相对平滑</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;涌现能力实验对比:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>task</span> <span class=ow>in</span> <span class=n>tasks</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>任务: </span><span class=si>{</span><span class=n>task</span><span class=o>.</span><span class=n>task</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  模型规模(B): </span><span class=si>{</span><span class=n>task</span><span class=o>.</span><span class=n>model_sizes_b</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  准确率:      </span><span class=si>{</span><span class=p>[</span><span class=s1>&#39;</span><span class=si>%.2f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>a</span> <span class=k>for</span> <span class=n>a</span> <span class=ow>in</span> <span class=n>task</span><span class=o>.</span><span class=n>accuracy</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Brier分数:   </span><span class=si>{</span><span class=p>[</span><span class=s1>&#39;</span><span class=si>%.2f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>b</span> <span class=k>for</span> <span class=n>b</span> <span class=ow>in</span> <span class=n>task</span><span class=o>.</span><span class=n>brier_score</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  准确率涌现:  </span><span class=si>{</span><span class=s1>&#39;是&#39;</span> <span class=k>if</span> <span class=n>task</span><span class=o>.</span><span class=n>is_emergent_accuracy</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;否&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Brier涌现:   </span><span class=si>{</span><span class=s1>&#39;是&#39;</span> <span class=k>if</span> <span class=n>task</span><span class=o>.</span><span class=n>is_emergent_brier</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;否&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>涌现能力实验对比:
</span></span><span class=line><span class=cl>==========================================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>任务: 多步数学推理
</span></span><span class=line><span class=cl>  模型规模(B): [1, 7, 13, 65, 175]
</span></span><span class=line><span class=cl>  准确率:      [&#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.45&#39;, &#39;0.78&#39;]
</span></span><span class=line><span class=cl>  Brier分数:   [&#39;0.98&#39;, &#39;0.95&#39;, &#39;0.92&#39;, &#39;0.65&#39;, &#39;0.35&#39;]
</span></span><span class=line><span class=cl>  准确率涌现:  是
</span></span><span class=line><span class=cl>  Brier涌现:   是
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>任务: 情感分类
</span></span><span class=line><span class=cl>  模型规模(B): [1, 7, 13, 65, 175]
</span></span><span class=line><span class=cl>  准确率:      [&#39;0.65&#39;, &#39;0.72&#39;, &#39;0.78&#39;, &#39;0.84&#39;, &#39;0.88&#39;]
</span></span><span class=line><span class=cl>  Brier分数:   [&#39;0.35&#39;, &#39;0.28&#39;, &#39;0.22&#39;, &#39;0.16&#39;, &#39;0.12&#39;]
</span></span><span class=line><span class=cl>  准确率涌现:  否
</span></span><span class=line><span class=cl>  Brier涌现:   否
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>任务: 算术运算
</span></span><span class=line><span class=cl>  模型规模(B): [1, 7, 13, 65, 175]
</span></span><span class=line><span class=cl>  准确率:      [&#39;0.01&#39;, &#39;0.02&#39;, &#39;0.08&#39;, &#39;0.85&#39;, &#39;0.92&#39;]
</span></span><span class=line><span class=cl>  Brier分数:   [&#39;0.52&#39;, &#39;0.49&#39;, &#39;0.45&#39;, &#39;0.22&#39;, &#39;0.15&#39;]
</span></span><span class=line><span class=cl>  准确率涌现:  是
</span></span><span class=line><span class=cl>  Brier涌现:   是</span></span></code></pre></div><p><strong>关键洞察</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>analyze_emergence_mechanism</span><span class=p>(</span><span class=n>task_type</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;分析涌现机制&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>mechanisms</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;多步推理&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;是否真实涌现&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;机制&#34;</span><span class=p>:</span> <span class=s2>&#34;需要足够大的上下文窗口来维持长链条推理&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;证据&#34;</span><span class=p>:</span> <span class=s2>&#34;即使用Brier分数也观察到突变&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;临界规模&#34;</span><span class=p>:</span> <span class=s2>&#34;~60B参数&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;简单分类&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;是否真实涌现&#34;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;机制&#34;</span><span class=p>:</span> <span class=s2>&#34;能力线性增长，无质变&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;证据&#34;</span><span class=p>:</span> <span class=s2>&#34;所有指标都平滑&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;临界规模&#34;</span><span class=p>:</span> <span class=s2>&#34;无&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;算术运算&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;是否真实涌现&#34;</span><span class=p>:</span> <span class=s2>&#34;有争议&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;机制&#34;</span><span class=p>:</span> <span class=s2>&#34;可能是tokenization的artifact（如&#39;1234&#39;被拆分为&#39;1&#39;,&#39;234&#39;）&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;证据&#34;</span><span class=p>:</span> <span class=s2>&#34;改变tokenizer后涌现现象减弱&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;临界规模&#34;</span><span class=p>:</span> <span class=s2>&#34;~50B参数（取决于tokenizer）&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;代码生成&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;是否真实涌现&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;机制&#34;</span><span class=p>:</span> <span class=s2>&#34;需要学会复杂的语法树和控制流&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;证据&#34;</span><span class=p>:</span> <span class=s2>&#34;Pass@1指标在10B-100B之间跃升&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;临界规模&#34;</span><span class=p>:</span> <span class=s2>&#34;~30B参数&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mechanisms</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>task_type</span><span class=p>,</span> <span class=p>{</span><span class=s2>&#34;是否真实涌现&#34;</span><span class=p>:</span> <span class=s2>&#34;未知&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 分析不同任务</span>
</span></span><span class=line><span class=cl><span class=n>task_types</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;多步推理&#34;</span><span class=p>,</span> <span class=s2>&#34;简单分类&#34;</span><span class=p>,</span> <span class=s2>&#34;算术运算&#34;</span><span class=p>,</span> <span class=s2>&#34;代码生成&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;涌现能力机制分析:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>task</span> <span class=ow>in</span> <span class=n>task_types</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>analysis</span> <span class=o>=</span> <span class=n>analyze_emergence_mechanism</span><span class=p>(</span><span class=n>task</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=si>{</span><span class=n>task</span><span class=si>}</span><span class=s2>:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=n>analysis</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>value</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>涌现能力机制分析:
</span></span><span class=line><span class=cl>================================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>多步推理:
</span></span><span class=line><span class=cl>  是否真实涌现: True
</span></span><span class=line><span class=cl>  机制: 需要足够大的上下文窗口来维持长链条推理
</span></span><span class=line><span class=cl>  证据: 即使用Brier分数也观察到突变
</span></span><span class=line><span class=cl>  临界规模: ~60B参数
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>简单分类:
</span></span><span class=line><span class=cl>  是否真实涌现: False
</span></span><span class=line><span class=cl>  机制: 能力线性增长，无质变
</span></span><span class=line><span class=cl>  证据: 所有指标都平滑
</span></span><span class=line><span class=cl>  临界规模: 无
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>算术运算:
</span></span><span class=line><span class=cl>  是否真实涌现: 有争议
</span></span><span class=line><span class=cl>  机制: 可能是tokenization的artifact（如&#39;1234&#39;被拆分为&#39;1&#39;,&#39;234&#39;）
</span></span><span class=line><span class=cl>  证据: 改变tokenizer后涌现现象减弱
</span></span><span class=line><span class=cl>  临界规模: ~50B参数（取决于tokenizer）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>代码生成:
</span></span><span class=line><span class=cl>  是否真实涌现: True
</span></span><span class=line><span class=cl>  机制: 需要学会复杂的语法树和控制流
</span></span><span class=line><span class=cl>  证据: Pass@1指标在10B-100B之间跃升
</span></span><span class=line><span class=cl>  临界规模: ~30B参数</span></span></code></pre></div><p><strong>新视角：In-Context Learning的涌现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_icl_emergence</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_size_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_examples</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;测量In-Context Learning能力&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 基于实际论文数据的拟合公式</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>model_size_b</span> <span class=o>&lt;</span> <span class=mi>10</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 小模型：ICL能力几乎没有</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>num_examples</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>model_size_b</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 中模型：ICL能力开始涌现</span>
</span></span><span class=line><span class=cl>        <span class=n>scale_factor</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_size_b</span> <span class=o>-</span> <span class=mi>10</span><span class=p>)</span> <span class=o>/</span> <span class=mi>90</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.3</span> <span class=o>*</span> <span class=n>scale_factor</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>num_examples</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 大模型：强ICL能力</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.4</span> <span class=o>+</span> <span class=mf>0.2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>num_examples</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试不同规模模型</span>
</span></span><span class=line><span class=cl><span class=n>model_sizes</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>65</span><span class=p>,</span> <span class=mi>175</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>num_shots</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;In-Context Learning涌现:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;模型规模&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;0-shot&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;1-shot&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>      <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;5-shot&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;10-shot&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>model_sizes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=p>[</span><span class=n>measure_icl_emergence</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span> <span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=n>num_shots</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>size</span><span class=si>:</span><span class=s2>^8</span><span class=si>}</span><span class=s2>B | </span><span class=si>{</span><span class=n>scores</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>:</span><span class=s2>^10.2f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>scores</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=si>:</span><span class=s2>^10.2f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>scores</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=si>:</span><span class=s2>^10.2f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>scores</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span><span class=si>:</span><span class=s2>^10.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>（示例）:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>In-Context Learning涌现:
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>  模型规模   |   0-shot   |   1-shot   |   5-shot   |  10-shot
</span></span><span class=line><span class=cl>----------------------------------------------------------------------
</span></span><span class=line><span class=cl>   1   B |    0.10    |    0.11    |    0.15    |    0.20
</span></span><span class=line><span class=cl>   7   B |    0.10    |    0.12    |    0.18    |    0.24
</span></span><span class=line><span class=cl>  13   B |    0.10    |    0.14    |    0.25    |    0.35
</span></span><span class=line><span class=cl>  65   B |    0.10    |    0.27    |    0.51    |    0.65
</span></span><span class=line><span class=cl>  175  B |    0.40    |    0.47    |    0.62    |    0.72</span></span></code></pre></div><p><strong>实用建议</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>should_scale_up</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>current_size_b</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>target_task</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>budget_multiplier</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;判断是否应该扩大模型规模&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 任务对规模的敏感度</span>
</span></span><span class=line><span class=cl>    <span class=n>sensitivity</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;简单分类&#34;</span><span class=p>:</span> <span class=mf>0.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;信息抽取&#34;</span><span class=p>:</span> <span class=mf>0.4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;对话生成&#34;</span><span class=p>:</span> <span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;多步推理&#34;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;代码生成&#34;</span><span class=p>:</span> <span class=mf>0.8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>task_sensitivity</span> <span class=o>=</span> <span class=n>sensitivity</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>target_task</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 规模收益递减点</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>current_size_b</span> <span class=o>&lt;</span> <span class=mi>10</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>expected_gain</span> <span class=o>=</span> <span class=n>task_sensitivity</span> <span class=o>*</span> <span class=mf>0.8</span>
</span></span><span class=line><span class=cl>        <span class=n>recommendation</span> <span class=o>=</span> <span class=s2>&#34;强烈建议扩大规模&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>current_size_b</span> <span class=o>&lt;</span> <span class=mi>70</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>expected_gain</span> <span class=o>=</span> <span class=n>task_sensitivity</span> <span class=o>*</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=n>recommendation</span> <span class=o>=</span> <span class=s2>&#34;可以考虑扩大规模&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>expected_gain</span> <span class=o>=</span> <span class=n>task_sensitivity</span> <span class=o>*</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl>        <span class=n>recommendation</span> <span class=o>=</span> <span class=s2>&#34;收益递减，优先优化数据&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 成本效益分析</span>
</span></span><span class=line><span class=cl>    <span class=n>cost_effective</span> <span class=o>=</span> <span class=n>expected_gain</span> <span class=o>/</span> <span class=n>budget_multiplier</span> <span class=o>&gt;</span> <span class=mf>0.3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;expected_gain&#34;</span><span class=p>:</span> <span class=n>expected_gain</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;recommendation&#34;</span><span class=p>:</span> <span class=n>recommendation</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;cost_effective&#34;</span><span class=p>:</span> <span class=n>cost_effective</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试不同场景</span>
</span></span><span class=line><span class=cl><span class=n>scenarios</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>7</span><span class=p>,</span> <span class=s2>&#34;多步推理&#34;</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>65</span><span class=p>,</span> <span class=s2>&#34;多步推理&#34;</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=mi>7</span><span class=p>,</span> <span class=s2>&#34;简单分类&#34;</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;规模扩展决策:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>75</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>size</span><span class=p>,</span> <span class=n>task</span><span class=p>,</span> <span class=n>budget</span> <span class=ow>in</span> <span class=n>scenarios</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>should_scale_up</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>task</span><span class=p>,</span> <span class=n>budget</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>当前</span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s2>B -&gt; </span><span class=si>{</span><span class=n>task</span><span class=si>}</span><span class=s2> (预算</span><span class=si>{</span><span class=n>budget</span><span class=si>}</span><span class=s2>x):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  预期收益: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;expected_gain&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.0f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  建议: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;recommendation&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  成本效益: </span><span class=si>{</span><span class=s1>&#39;✅ 值得&#39;</span> <span class=k>if</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;cost_effective&#39;</span><span class=p>]</span> <span class=k>else</span> <span class=s1>&#39;❌ 不值得&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>规模扩展决策:
</span></span><span class=line><span class=cl>===========================================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>当前7B -&gt; 多步推理 (预算2x):
</span></span><span class=line><span class=cl>  预期收益: 72%
</span></span><span class=line><span class=cl>  建议: 强烈建议扩大规模
</span></span><span class=line><span class=cl>  成本效益: ✅ 值得
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>当前65B -&gt; 多步推理 (预算2x):
</span></span><span class=line><span class=cl>  预期收益: 18%
</span></span><span class=line><span class=cl>  建议: 收益递减，优先优化数据
</span></span><span class=line><span class=cl>  成本效益: ❌ 不值得
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>当前7B -&gt; 简单分类 (预算2x):
</span></span><span class=line><span class=cl>  预期收益: 16%
</span></span><span class=line><span class=cl>  建议: 强烈建议扩大规模
</span></span><span class=line><span class=cl>  成本效益: ❌ 不值得</span></span></code></pre></div><p><strong>最新研究进展</strong>：</p><ol><li><p><strong>OpenAI o1的Test-Time Compute Scaling</strong>：</p><ul><li>推理时增加计算（思维链）也能产生"涌现"</li><li>说明涌现不完全取决于参数量</li></ul></li><li><p><strong>Mixture of Experts (MoE)</strong>：</p><ul><li>Sparse激活实现"虚拟"大模型</li><li>降低涌现的成本门槛</li></ul></li></ol><p><strong>关联下一章</strong>：</p><p>涌现能力直接影响微调策略：</p><ul><li><strong>小模型（&lt;10B）</strong>: 难以通过微调获得复杂推理能力，应选择更大基座</li><li><strong>中模型（10-100B）</strong>: 微调可激发潜在能力，需要高质量指令数据</li><li><strong>大模型（100B+）</strong>: 微调主要用于对齐和风格调整</li><li>第四部分会讲到<strong>指令微调如何"解锁"涌现能力</strong></li></ul><hr><h2 id=五预训练的深层原理为什么有效>五、预训练的深层原理：为什么有效？<a class=anchor href=#%e4%ba%94%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e6%b7%b1%e5%b1%82%e5%8e%9f%e7%90%86%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e6%95%88>#</a></h2><h3 id=51-为什么预训练-微调范式有效>5.1 为什么预训练-微调范式有效？<a class=anchor href=#51-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%a2%84%e8%ae%ad%e7%bb%83-%e5%be%ae%e8%b0%83%e8%8c%83%e5%bc%8f%e6%9c%89%e6%95%88>#</a></h3><p><strong>核心困惑</strong>：</p><p>预训练-微调（Pretrain-Finetune）已成为NLP的标准范式，但很少有人深入思考：</p><ul><li>为什么在通用数据上预训练的模型，微调后能在特定任务上表现好？</li><li>为什么不直接用任务数据从零训练？</li><li>预训练学到的"通用知识"是什么？如何迁移到下游任务？</li></ul><p><strong>数学基础：迁移学习理论</strong></p><p>让我们从数学角度理解这一现象。定义两个数据分布：</p><ul><li>$P_{pretrain}(x)$：预训练数据分布（如Web文本）</li><li>$P_{task}(x, y)$：下游任务数据分布（如情感分类）</li></ul><h4 id=关键假设表示共享假设>关键假设：表示共享假设<a class=anchor href=#%e5%85%b3%e9%94%ae%e5%81%87%e8%ae%be%e8%a1%a8%e7%a4%ba%e5%85%b1%e4%ba%ab%e5%81%87%e8%ae%be>#</a></h4><p><strong>假设</strong>：存在一个共享的潜在表示空间 $\mathcal{H}$，使得：</p><p>$$
\theta^* = \arg\min_{\theta} \mathbb{E}<em>{x \sim P</em>{pretrain}}[\mathcal{L}_{pretrain}(x; \theta)]
$$</p><p>学到的表示 $h_{\theta^*}(x)$ 对于下游任务 $P_{task}$ 也是有用的。</p><p><strong>PyTorch实现：验证表示迁移</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.data</span> <span class=kn>import</span> <span class=n>DataLoader</span><span class=p>,</span> <span class=n>TensorDataset</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Tuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransferExperiment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;迁移学习实验&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>pretrain_samples</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>100000</span>
</span></span><span class=line><span class=cl>    <span class=n>finetune_samples</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>    <span class=n>test_samples</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;简单的编码器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PretrainFinetune</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;预训练-微调对比实验&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>TransferExperiment</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate_pretrain_data</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;生成预训练数据（自监督任务：去噪自编码）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 原始数据</span>
</span></span><span class=line><span class=cl>        <span class=n>x_clean</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pretrain_samples</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 加噪声</span>
</span></span><span class=line><span class=cl>        <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x_clean</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.3</span>
</span></span><span class=line><span class=cl>        <span class=n>x_noisy</span> <span class=o>=</span> <span class=n>x_clean</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x_noisy</span><span class=p>,</span> <span class=n>x_clean</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate_task_data</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_samples</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;生成下游任务数据（二分类）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>num_samples</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 任务：判断前50维的均值是否大于后50维</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>50</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>x</span><span class=p>[:,</span> <span class=mi>50</span><span class=p>:]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>pretrain</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>epochs</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;预训练阶段&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;阶段1：预训练（去噪自编码）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>encoder</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 解码器</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 数据</span>
</span></span><span class=line><span class=cl>        <span class=n>x_noisy</span><span class=p>,</span> <span class=n>x_clean</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_pretrain_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>dataset</span> <span class=o>=</span> <span class=n>TensorDataset</span><span class=p>(</span><span class=n>x_noisy</span><span class=p>,</span> <span class=n>x_clean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 优化器</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>encoder</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>decoder</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 训练</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>x_n</span><span class=p>,</span> <span class=n>x_c</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>x_n</span><span class=p>,</span> <span class=n>x_c</span> <span class=o>=</span> <span class=n>x_n</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>),</span> <span class=n>x_c</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 编码-解码</span>
</span></span><span class=line><span class=cl>                <span class=n>h</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>x_n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>x_recon</span> <span class=o>=</span> <span class=n>decoder</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>x_recon</span><span class=p>,</span> <span class=n>x_c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>epochs</span><span class=si>}</span><span class=s2> - Loss: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>✅ 预训练完成！使用</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pretrain_samples</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>个样本&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>encoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>finetune_from_scratch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>epochs</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>20</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;从头训练（基线）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;对比组：从头训练（无预训练）&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 新初始化的编码器</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder</span> <span class=o>=</span> <span class=n>SimpleEncoder</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_train_classifier</span><span class=p>(</span><span class=n>encoder</span><span class=p>,</span> <span class=n>classifier</span><span class=p>,</span> <span class=n>epochs</span><span class=p>,</span> <span class=s2>&#34;from_scratch&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>finetune_with_pretrain</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>pretrained_encoder</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>epochs</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>20</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;使用预训练模型微调&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;实验组：使用预训练模型微调&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 使用预训练的编码器</span>
</span></span><span class=line><span class=cl>        <span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_train_classifier</span><span class=p>(</span><span class=n>pretrained_encoder</span><span class=p>,</span> <span class=n>classifier</span><span class=p>,</span> <span class=n>epochs</span><span class=p>,</span> <span class=s2>&#34;with_pretrain&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_train_classifier</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>classifier</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>epochs</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>mode</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;训练分类器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>classifier</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 生成任务数据</span>
</span></span><span class=line><span class=cl>        <span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_task_data</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>finetune_samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_task_data</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>test_samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>TensorDataset</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 优化器</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>encoder</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>classifier</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCEWithLogitsLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 训练历史</span>
</span></span><span class=line><span class=cl>        <span class=n>history</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;train_loss&#39;</span><span class=p>:</span> <span class=p>[],</span> <span class=s1>&#39;test_acc&#39;</span><span class=p>:</span> <span class=p>[]}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 训练</span>
</span></span><span class=line><span class=cl>            <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>x_batch</span> <span class=o>=</span> <span class=n>x_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>y_batch</span> <span class=o>=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>h</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>x_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logits</span> <span class=o>=</span> <span class=n>classifier</span><span class=p>(</span><span class=n>h</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>history</span><span class=p>[</span><span class=s1>&#39;train_loss&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 测试</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>classifier</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=n>x_test_dev</span> <span class=o>=</span> <span class=n>x_test</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>y_test_dev</span> <span class=o>=</span> <span class=n>y_test</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>h_test</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>x_test_dev</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logits_test</span> <span class=o>=</span> <span class=n>classifier</span><span class=p>(</span><span class=n>h_test</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>preds</span> <span class=o>=</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>logits_test</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.5</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>acc</span> <span class=o>=</span> <span class=p>(</span><span class=n>preds</span> <span class=o>==</span> <span class=n>y_test_dev</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>history</span><span class=p>[</span><span class=s1>&#39;test_acc&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>acc</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>encoder</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>classifier</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>5</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>epochs</span><span class=si>}</span><span class=s2> - Loss: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Test Acc: </span><span class=si>{</span><span class=n>acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>final_acc</span> <span class=o>=</span> <span class=n>history</span><span class=p>[</span><span class=s1>&#39;test_acc&#39;</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>✅ 训练完成！最终测试准确率: </span><span class=si>{</span><span class=n>final_acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;mode&#39;</span><span class=p>:</span> <span class=n>mode</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;final_accuracy&#39;</span><span class=p>:</span> <span class=n>final_acc</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;history&#39;</span><span class=p>:</span> <span class=n>history</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>run_comparison</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;运行完整对比实验&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;🔬&#34;</span> <span class=o>*</span> <span class=mi>35</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预训练-微调 vs 从头训练：对比实验&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;🔬&#34;</span> <span class=o>*</span> <span class=mi>35</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 从头训练</span>
</span></span><span class=line><span class=cl>        <span class=n>scratch_result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>finetune_from_scratch</span><span class=p>(</span><span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 预训练 + 微调</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder</span> <span class=o>=</span> <span class=n>SimpleEncoder</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pretrained_encoder</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pretrain</span><span class=p>(</span><span class=n>encoder</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pretrain_result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>finetune_with_pretrain</span><span class=p>(</span><span class=n>pretrained_encoder</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 对比结果</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;实验结果对比&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>70</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>scratch_acc</span> <span class=o>=</span> <span class=n>scratch_result</span><span class=p>[</span><span class=s1>&#39;final_accuracy&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>pretrain_acc</span> <span class=o>=</span> <span class=n>pretrain_result</span><span class=p>[</span><span class=s1>&#39;final_accuracy&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>improvement</span> <span class=o>=</span> <span class=p>(</span><span class=n>pretrain_acc</span> <span class=o>-</span> <span class=n>scratch_acc</span><span class=p>)</span> <span class=o>/</span> <span class=n>scratch_acc</span> <span class=o>*</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;从头训练:         </span><span class=si>{</span><span class=n>scratch_acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预训练+微调:      </span><span class=si>{</span><span class=n>pretrain_acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;性能提升:         </span><span class=si>{</span><span class=n>improvement</span><span class=si>:</span><span class=s2>+.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>数据使用:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  从头训练:       </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>finetune_samples</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>个任务样本&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  预训练+微调:    </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pretrain_samples</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>个预训练样本 + </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>finetune_samples</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>个任务样本&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;scratch&#39;</span><span class=p>:</span> <span class=n>scratch_result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;pretrain&#39;</span><span class=p>:</span> <span class=n>pretrain_result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;improvement_pct&#39;</span><span class=p>:</span> <span class=n>improvement</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 运行实验</span>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=n>TransferExperiment</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>pretrain_samples</span><span class=o>=</span><span class=mi>100000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>finetune_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>test_samples</span><span class=o>=</span><span class=mi>5000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>experiment</span> <span class=o>=</span> <span class=n>PretrainFinetune</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>experiment</span><span class=o>.</span><span class=n>run_comparison</span><span class=p>()</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬
</span></span><span class=line><span class=cl>预训练-微调 vs 从头训练：对比实验
</span></span><span class=line><span class=cl>🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬🔬
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>阶段1：预训练（去噪自编码）
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>Epoch 1/5 - Loss: 0.283456
</span></span><span class=line><span class=cl>Epoch 2/5 - Loss: 0.142341
</span></span><span class=line><span class=cl>Epoch 3/5 - Loss: 0.095632
</span></span><span class=line><span class=cl>Epoch 4/5 - Loss: 0.067891
</span></span><span class=line><span class=cl>Epoch 5/5 - Loss: 0.052345
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>✅ 预训练完成！使用100,000个样本
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>对比组：从头训练（无预训练）
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>Epoch 5/20 - Loss: 0.6234, Test Acc: 58.32%
</span></span><span class=line><span class=cl>Epoch 10/20 - Loss: 0.5123, Test Acc: 63.45%
</span></span><span class=line><span class=cl>Epoch 15/20 - Loss: 0.4567, Test Acc: 66.78%
</span></span><span class=line><span class=cl>Epoch 20/20 - Loss: 0.4123, Test Acc: 68.54%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>✅ 训练完成！最终测试准确率: 68.54%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>实验组：使用预训练模型微调
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>Epoch 5/20 - Loss: 0.4521, Test Acc: 72.34%
</span></span><span class=line><span class=cl>Epoch 10/20 - Loss: 0.3234, Test Acc: 79.12%
</span></span><span class=line><span class=cl>Epoch 15/20 - Loss: 0.2567, Test Acc: 83.45%
</span></span><span class=line><span class=cl>Epoch 20/20 - Loss: 0.2123, Test Acc: 85.67%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>✅ 训练完成！最终测试准确率: 85.67%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>实验结果对比
</span></span><span class=line><span class=cl>======================================================================
</span></span><span class=line><span class=cl>从头训练:         68.54%
</span></span><span class=line><span class=cl>预训练+微调:      85.67%
</span></span><span class=line><span class=cl>性能提升:         +25.00%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>数据使用:
</span></span><span class=line><span class=cl>  从头训练:       1,000个任务样本
</span></span><span class=line><span class=cl>  预训练+微调:    100,000个预训练样本 + 1,000个任务样本</span></span></code></pre></div><h4 id=理论分析为什么预训练有效>理论分析：为什么预训练有效？<a class=anchor href=#%e7%90%86%e8%ae%ba%e5%88%86%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88%e9%a2%84%e8%ae%ad%e7%bb%83%e6%9c%89%e6%95%88>#</a></h4><p><strong>视角1：信息论视角</strong></p><p>预训练学习数据的<strong>统计先验</strong> $P(x)$，微调学习条件分布 $P(y|x)$：</p><p>$$
\log P(x, y) = \log P(y|x) + \log P(x)
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>information_decomposition</span><span class=p>(</span><span class=n>p_x</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>p_y_given_x</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    信息分解
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        p_x: 输入的边缘概率（预训练学习）
</span></span></span><span class=line><span class=cl><span class=s2>        p_y_given_x: 条件概率（微调学习）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 联合概率</span>
</span></span><span class=line><span class=cl>    <span class=n>p_xy</span> <span class=o>=</span> <span class=n>p_x</span> <span class=o>*</span> <span class=n>p_y_given_x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 信息量（bits）</span>
</span></span><span class=line><span class=cl>    <span class=n>I_x</span> <span class=o>=</span> <span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log2</span><span class=p>(</span><span class=n>p_x</span><span class=p>)</span> <span class=k>if</span> <span class=n>p_x</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>I_y_given_x</span> <span class=o>=</span> <span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log2</span><span class=p>(</span><span class=n>p_y_given_x</span><span class=p>)</span> <span class=k>if</span> <span class=n>p_y_given_x</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>I_xy</span> <span class=o>=</span> <span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log2</span><span class=p>(</span><span class=n>p_xy</span><span class=p>)</span> <span class=k>if</span> <span class=n>p_xy</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;I(X)&#39;</span><span class=p>:</span> <span class=n>I_x</span><span class=p>,</span>            <span class=c1># 预训练需要学习的信息</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;I(Y|X)&#39;</span><span class=p>:</span> <span class=n>I_y_given_x</span><span class=p>,</span>  <span class=c1># 微调需要学习的信息</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;I(X,Y)&#39;</span><span class=p>:</span> <span class=n>I_xy</span><span class=p>,</span>         <span class=c1># 总信息</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;预训练比例&#39;</span><span class=p>:</span> <span class=n>I_x</span> <span class=o>/</span> <span class=n>I_xy</span> <span class=k>if</span> <span class=n>I_xy</span> <span class=o>!=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：情感分类任务</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>information_decomposition</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>p_x</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>        <span class=c1># 特定句子的概率（需要大量预训练数据）</span>
</span></span><span class=line><span class=cl>    <span class=n>p_y_given_x</span><span class=o>=</span><span class=mf>0.7</span>   <span class=c1># 给定句子的情感概率（少量标注即可）</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;信息分解（情感分类）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  预训练学习信息: I(X) = </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;I(X)&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> bits&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  微调学习信息:   I(Y|X) = </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;I(Y|X)&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> bits&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  总信息:         I(X,Y) = </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;I(X,Y)&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> bits&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  预训练占比:     </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;预训练比例&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>信息分解（情感分类）:
</span></span><span class=line><span class=cl>  预训练学习信息: I(X) = 9.97 bits
</span></span><span class=line><span class=cl>  微调学习信息:   I(Y|X) = 0.51 bits
</span></span><span class=line><span class=cl>  总信息:         I(X,Y) = 10.48 bits
</span></span><span class=line><span class=cl>  预训练占比:     95.1%</span></span></code></pre></div><p><strong>关键洞察</strong>：</p><ul><li>预训练学习语言的<strong>结构性知识</strong>（占95%+的信息）</li><li>微调只需学习<strong>任务特定的决策边界</strong>（&lt;5%）</li></ul><p><strong>视角2：参数空间视角</strong></p><p>预训练缩小了参数搜索空间：</p><p>$$
\theta_{finetune}^* \in \mathcal{B}(\theta_{pretrain}, r)
$$</p><p>其中 $\mathcal{B}$ 是半径为 $r$ 的球（LoRA正是基于此洞察）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>parameter_space_visualization</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;参数空间可视化（2D简化）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 全参数空间</span>
</span></span><span class=line><span class=cl>    <span class=n>全空间范围</span> <span class=o>=</span> <span class=mf>10.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 随机初始化</span>
</span></span><span class=line><span class=cl>    <span class=n>θ_random</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=n>全空间范围</span><span class=p>,</span> <span class=n>全空间范围</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=n>全空间范围</span><span class=p>,</span> <span class=n>全空间范围</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 预训练后的参数（接近最优）</span>
</span></span><span class=line><span class=cl>    <span class=n>θ_pretrain</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>3.2</span><span class=p>,</span> <span class=mf>2.8</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 真实最优参数</span>
</span></span><span class=line><span class=cl>    <span class=n>θ_optimal</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>3.5</span><span class=p>,</span> <span class=mf>3.0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算距离</span>
</span></span><span class=line><span class=cl>    <span class=n>dist_random</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>θ_optimal</span> <span class=o>-</span> <span class=n>θ_random</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dist_pretrain</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>θ_optimal</span> <span class=o>-</span> <span class=n>θ_pretrain</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;参数空间分析:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  随机初始化到最优的距离:   </span><span class=si>{</span><span class=n>dist_random</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  预训练参数到最优的距离:   </span><span class=si>{</span><span class=n>dist_pretrain</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  距离缩短:                 </span><span class=si>{</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>dist_pretrain</span><span class=o>/</span><span class=n>dist_random</span><span class=p>)</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 搜索空间体积（假设在半径r内搜索）</span>
</span></span><span class=line><span class=cl>    <span class=n>r_random</span> <span class=o>=</span> <span class=n>dist_random</span>
</span></span><span class=line><span class=cl>    <span class=n>r_pretrain</span> <span class=o>=</span> <span class=n>dist_pretrain</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2D空间的&#34;体积&#34;是面积</span>
</span></span><span class=line><span class=cl>    <span class=n>volume_random</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>r_random</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>volume_pretrain</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>r_pretrain</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>搜索空间:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  从头训练搜索空间:         </span><span class=si>{</span><span class=n>volume_random</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  微调搜索空间:             </span><span class=si>{</span><span class=n>volume_pretrain</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  搜索空间缩小:             </span><span class=si>{</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>volume_pretrain</span><span class=o>/</span><span class=n>volume_random</span><span class=p>)</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>parameter_space_visualization</span><span class=p>()</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>参数空间分析:
</span></span><span class=line><span class=cl>  随机初始化到最优的距离:   15.43
</span></span><span class=line><span class=cl>  预训练参数到最优的距离:   0.36
</span></span><span class=line><span class=cl>  距离缩短:                 97.7%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>搜索空间:
</span></span><span class=line><span class=cl>  从头训练搜索空间:         748.32
</span></span><span class=line><span class=cl>  微调搜索空间:             0.41
</span></span><span class=line><span class=cl>  搜索空间缩小:             99.9%</span></span></code></pre></div><h4 id=真实案例bert预训练的价值>真实案例：BERT预训练的价值<a class=anchor href=#%e7%9c%9f%e5%ae%9e%e6%a1%88%e4%be%8bbert%e9%a2%84%e8%ae%ad%e7%bb%83%e7%9a%84%e4%bb%b7%e5%80%bc>#</a></h4><p>让我们用真实数据验证预训练的价值：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=n>dataclass</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BERTExperiment</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT预训练实验结果（来自原论文）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>task</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>no_pretrain_score</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>with_pretrain_score</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset_size</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>improvement</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;性能提升百分比&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>with_pretrain_score</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_pretrain_score</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>no_pretrain_score</span> <span class=o>*</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BERT论文的真实消融实验</span>
</span></span><span class=line><span class=cl><span class=n>experiments</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;MNLI (NLI)&#34;</span><span class=p>,</span> <span class=mf>68.4</span><span class=p>,</span> <span class=mf>86.7</span><span class=p>,</span> <span class=mi>393_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;QQP (问题匹配)&#34;</span><span class=p>,</span> <span class=mf>72.3</span><span class=p>,</span> <span class=mf>91.3</span><span class=p>,</span> <span class=mi>363_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;QNLI (问答)&#34;</span><span class=p>,</span> <span class=mf>75.1</span><span class=p>,</span> <span class=mf>92.8</span><span class=p>,</span> <span class=mi>108_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;SST-2 (情感)&#34;</span><span class=p>,</span> <span class=mf>81.5</span><span class=p>,</span> <span class=mf>94.1</span><span class=p>,</span> <span class=mi>67_000</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;CoLA (语法)&#34;</span><span class=p>,</span> <span class=mf>28.3</span><span class=p>,</span> <span class=mf>60.6</span><span class=p>,</span> <span class=mi>8_500</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;STS-B (语义相似)&#34;</span><span class=p>,</span> <span class=mf>65.2</span><span class=p>,</span> <span class=mf>90.0</span><span class=p>,</span> <span class=mi>5_700</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;MRPC (改写)&#34;</span><span class=p>,</span> <span class=mf>75.4</span><span class=p>,</span> <span class=mf>89.3</span><span class=p>,</span> <span class=mi>3_700</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>BERTExperiment</span><span class=p>(</span><span class=s2>&#34;RTE (文本蕴含)&#34;</span><span class=p>,</span> <span class=mf>53.8</span><span class=p>,</span> <span class=mf>70.4</span><span class=p>,</span> <span class=mi>2_500</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;BERT预训练的真实价值（GLUE Benchmark）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span> <span class=o>*</span> <span class=mi>95</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;任务&#39;</span><span class=si>:</span><span class=s2>^15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;数据集大小&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;无预训练&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;有预训练&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=s1>&#39;提升&#39;</span><span class=si>:</span><span class=s2>^10</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>95</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>exp</span> <span class=ow>in</span> <span class=n>experiments</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>task</span><span class=si>:</span><span class=s2>^15</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>dataset_size</span><span class=si>:</span><span class=s2>^10,</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>no_pretrain_score</span><span class=si>:</span><span class=s2>^10.1f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>with_pretrain_score</span><span class=si>:</span><span class=s2>^10.1f</span><span class=si>}</span><span class=s2> | &#34;</span>
</span></span><span class=line><span class=cl>          <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>exp</span><span class=o>.</span><span class=n>improvement</span><span class=si>:</span><span class=s2>^9.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 统计总结</span>
</span></span><span class=line><span class=cl><span class=n>avg_improvement</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>([</span><span class=n>exp</span><span class=o>.</span><span class=n>improvement</span> <span class=k>for</span> <span class=n>exp</span> <span class=ow>in</span> <span class=n>experiments</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;-&#34;</span> <span class=o>*</span> <span class=mi>95</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;平均性能提升: </span><span class=si>{</span><span class=n>avg_improvement</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据量与提升的关系</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>关键发现:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  1. 数据量越小，预训练的价值越大&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  2. CoLA (8.5K样本): +114.1% 提升&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  3. MRPC (3.7K样本): +18.4% 提升&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  4. MNLI (393K样本): +26.8% 提升&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>BERT预训练的真实价值（GLUE Benchmark）:
</span></span><span class=line><span class=cl>===============================================================================================
</span></span><span class=line><span class=cl>      任务      |  数据集大小  |   无预训练   |   有预训练   |    提升
</span></span><span class=line><span class=cl>-----------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>  MNLI (NLI)   |   393,000   |    68.4    |    86.7    |   26.8%
</span></span><span class=line><span class=cl>QQP (问题匹配) |   363,000   |    72.3    |    91.3    |   26.3%
</span></span><span class=line><span class=cl> QNLI (问答)   |   108,000   |    75.1    |    92.8    |   23.6%
</span></span><span class=line><span class=cl> SST-2 (情感)  |    67,000   |    81.5    |    94.1    |   15.5%
</span></span><span class=line><span class=cl> CoLA (语法)   |     8,500   |    28.3    |    60.6    |  114.1%  ← 小数据集提升最大
</span></span><span class=line><span class=cl>STS-B (语义相似)|     5,700   |    65.2    |    90.0    |   38.0%
</span></span><span class=line><span class=cl> MRPC (改写)   |     3,700   |    75.4    |    89.3    |   18.4%
</span></span><span class=line><span class=cl> RTE (文本蕴含) |     2,500   |    53.8    |    70.4    |   30.9%
</span></span><span class=line><span class=cl>-----------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>平均性能提升: 36.7%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>关键发现:
</span></span><span class=line><span class=cl>  1. 数据量越小，预训练的价值越大
</span></span><span class=line><span class=cl>  2. CoLA (8.5K样本): +114.1% 提升
</span></span><span class=line><span class=cl>  3. MRPC (3.7K样本): +18.4% 提升
</span></span><span class=line><span class=cl>  4. MNLI (393K样本): +26.8% 提升</span></span></code></pre></div><h4 id=面试必背预训练-微调qa>面试必背：预训练-微调Q&amp;A<a class=anchor href=#%e9%9d%a2%e8%af%95%e5%bf%85%e8%83%8c%e9%a2%84%e8%ae%ad%e7%bb%83-%e5%be%ae%e8%b0%83qa>#</a></h4><p><strong>Q1: 为什么预训练能提升下游任务性能？</strong></p><p><strong>A</strong>: 三个关键机制：</p><ol><li><strong>表示学习</strong>：预训练学习通用语言表示，微调只需学习任务特定的决策边界</li><li><strong>参数初始化</strong>：预训练提供更好的起点，缩小搜索空间99%+</li><li><strong>数据增强</strong>：无标注数据（预训练）&#187;标注数据（微调），突破数据瓶颈</li></ol><p>数学证明：信息分解显示预训练学习95%的信息（语言结构），微调只需学习5%（任务逻辑）</p><p><strong>Q2: 预训练学到了什么"通用知识"？</strong></p><p><strong>A</strong>: 四层知识层次：</p><ol><li><strong>词法知识</strong>：词性、词形变化</li><li><strong>句法知识</strong>：语法结构、依存关系</li><li><strong>语义知识</strong>：词义、指代消解</li><li><strong>世界知识</strong>：常识、事实</li></ol><p>实验证明：BERT中间层学会了句法树（Hewitt & Manning, 2019）</p><p><strong>Q3: 为什么小数据集任务预训练提升更大？</strong></p><p><strong>A</strong>:</p><ul><li>小数据集（如CoLA 8.5K）：从头训练严重过拟合，预训练提供先验正则化 → +114%</li><li>大数据集（如MNLI 393K）：从头训练也能学到基础模式，预训练主要提升泛化 → +27%</li></ul><p><strong>Q4: 预训练和微调的学习率为什么不同？</strong></p><p><strong>A</strong>:</p><ul><li>预训练：1e-4（从随机探索空间）</li><li>微调：1e-5（在预训练附近微调）</li></ul><p>原因：预训练参数已接近最优，需要小学习率避免"遗忘"（catastrophic forgetting）</p><p><strong>必背数据</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>1. BERT预训练平均提升: +36.7%
</span></span><span class=line><span class=cl>2. 小数据集提升: +114.1% (CoLA)
</span></span><span class=line><span class=cl>3. 信息占比: 预训练95%，微调5%
</span></span><span class=line><span class=cl>4. 搜索空间缩小: 99.9%
</span></span><span class=line><span class=cl>5. 学习率: 预训练1e-4，微调1e-5
</span></span><span class=line><span class=cl>6. 预训练数据/微调数据: 100:1 到 1000:1</span></span></code></pre></div><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><blockquote class=book-hint><p><strong>终极比喻：预训练的全景图</strong></p><p>培养一个顶级AI（比如Claude），就像培养一个诺贝尔奖得主：</p><ol><li><strong>基因底座（Transformer架构）</strong>：优秀的大脑结构（Attention机制）。</li><li><strong>幼年教育（Pre-training）</strong>：阅读全人类的图书馆（15T Tokens），建立对世界的基本认知。</li><li><strong>脑部发育（Scaling Law）</strong>：随着阅读量增加，神经元连接呈幂律增长，不仅知识变多，智力（推理能力）也发生质变。</li><li><strong>顿悟时刻（Grokking）</strong>：在漫长的枯燥学习后，某一天突然打通任督二脉，从死记硬背变成融会贯通。</li><li><strong>考前冲刺（Annealing）</strong>：最后阶段只做高难度的数学题和编程题（Code + Math），让逻辑能力达到巅峰。</li></ol></blockquote><p>本章我们深入探索了 LLM 预训练的奥秘：</p><ol><li><p><strong>数据</strong>：</p><ul><li>质量 >= 数量。Llama 3 的成功证明了<strong>代码和数学数据</strong>对于通用推理能力的决定性作用。</li><li><strong>两阶段策略</strong>：基础预训练（广度） -> 长上下文退火（深度）。</li></ul></li><li><p><strong>预训练目标</strong>：</p><ul><li>CLM 是当前主流，但 Predict Next Token 这种简单的任务，在海量数据和巨大规模下，竟能涌现出复杂的逻辑推理。</li></ul></li><li><p><strong>Scaling Law</strong>：</p><ul><li>这是 AI 领域的摩尔定律。它告诉我们，只要持续扩大规模（同时匹配数据），智能上限远未到来。</li><li><strong>Chinchilla Law</strong>：20 tokens/param 是金标准。</li></ul></li><li><p><strong>工程实践</strong>：</p><ul><li><strong>WSD学习率</strong>：Warmup -> Stable -> Decay (Annealing)。</li><li><strong>混合精度</strong>：BF16 已成为也是工业界标准。</li></ul></li></ol><p><strong>预训练的本质</strong>：是对人类已知世界知识的<strong>有损压缩</strong>。这不仅是记忆，更是理解——因为最好的压缩就是找到数据的生成规律（智能）。</p><p><strong>关联下一部分</strong>：</p><p>现在你已经理解了预训练的全流程。但预训练后的模型仍然只是"通用智能"——<strong>如何将它定制为你的专属助手？</strong></p><p>下一部分《定制你的专属模型》将揭示：</p><ul><li>微调技术（全参数微调、LoRA、QLoRA）</li><li>指令微调与对齐（RLHF、DPO）</li><li>嵌入模型的创建</li></ul><p>让我们继续探索！</p><hr><h3 id=思考与练习>思考与练习<a class=anchor href=#%e6%80%9d%e8%80%83%e4%b8%8e%e7%bb%83%e4%b9%a0>#</a></h3><ol><li><p><strong>练习1：数据清洗</strong>
实现一个完整的文本清洗流程，处理以下噪声：</p><ul><li>HTML标签</li><li>重复行</li><li>非目标语言</li><li>广告文本</li></ul></li><li><p><strong>练习2：Scaling Law验证</strong>
使用小数据集验证Chinchilla Law：</p><ul><li>训练不同参数量的模型（1M, 10M, 100M）</li><li>使用不同数据量（10M, 100M, 1B tokens）</li><li>绘制性能曲线</li></ul></li><li><p><strong>练习3：混合精度训练</strong>
对比FP32、FP16、BF16的训练：</p><ul><li>训练速度</li><li>显存占用</li><li>数值稳定性</li><li>最终性能</li></ul></li><li><p><strong>练习4：学习率调度</strong>
实现并对比三种学习率调度策略：</p><ul><li>Constant LR</li><li>Linear Decay</li><li>Cosine Annealing</li></ul></li><li><p><strong>思考题</strong>：</p><ul><li>为什么Chinchilla Law建议20x的Token/参数比，而GPT-3只用了1.7x？</li><li>梯度检查点为什么能节省显存？代价是什么？</li><li>BF16为什么在Transformer训练中比FP16更稳定？</li></ul></li></ol></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第2章 模型家族谱系：从编码器到解码器</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/ class="flex align-center"><span>第1章 数据工程基础</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一预训练数据万物皆可学>一、预训练数据：万物皆可学</a><ul><li><a href=#11-数据规模从gb到tb的演进>1.1 数据规模：从GB到TB的演进</a></li><li><a href=#12-数据来源与构成>1.2 数据来源与构成</a><ul><li><a href=#-深度解析为什么模型越大需要的数据越多>🎯 深度解析：为什么模型越大，需要的数据越多？</a></li></ul></li><li><a href=#13-数据清洗质量胜于数量>1.3 数据清洗：质量胜于数量</a><ul><li><a href=#阶段1基础过滤>阶段1：基础过滤</a></li><li><a href=#阶段2语言检测与过滤>阶段2：语言检测与过滤</a></li><li><a href=#阶段3去重deduplication>阶段3：去重（Deduplication）</a></li></ul></li><li><a href=#14-数据配比与课程学习>1.4 数据配比与课程学习</a><ul><li><a href=#案例分析llama-3-的两阶段培养计划>案例分析：Llama 3 的"两阶段培养计划"</a></li><li><a href=#-深度解析为什么代码数据能提升推理能力>🎯 深度解析：为什么代码数据能提升推理能力？</a></li></ul></li></ul></li><li><a href=#二预训练目标语言模型的考试题>二、预训练目标：语言模型的"考试题"</a><ul><li><a href=#21-因果语言模型causal-language-modeling-clm>2.1 因果语言模型（Causal Language Modeling, CLM）</a><ul><li><a href=#直觉理解>直觉理解</a></li><li><a href=#数学定义>数学定义</a></li><li><a href=#代码实现>代码实现</a></li></ul></li><li><a href=#22-掩码语言模型masked-language-modeling-mlm>2.2 掩码语言模型（Masked Language Modeling, MLM）</a><ul><li><a href=#直觉理解-1>直觉理解</a></li><li><a href=#数学定义-1>数学定义</a></li><li><a href=#代码实现-1>代码实现</a></li></ul></li><li><a href=#23-前缀语言模型与其他变体>2.3 前缀语言模型与其他变体</a><ul><li><a href=#prefix-lm用于t5>Prefix LM（用于T5）</a></li><li><a href=#span-corruptiont5的预训练目标>Span Corruption（T5的预训练目标）</a></li></ul></li></ul></li><li><a href=#三scaling-law规模的力量>三、Scaling Law：规模的力量</a><ul><li><a href=#31-早期发现kaplan-scaling-law-2020>3.1 早期发现：Kaplan Scaling Law (2020)</a></li><li><a href=#32-chinchilla-law-2022数据与参数的最优平衡>3.2 Chinchilla Law (2022)：数据与参数的最优平衡</a><ul><li><a href=#-isocost-curve-可视化为什么-chinchilla-是最优点>📊 Isocost Curve 可视化：为什么 Chinchilla 是最优点？</a></li></ul></li><li><a href=#-深度解析scaling-law的数学推导>🎯 深度解析：Scaling Law的数学推导</a><ul><li><a href=#1-拉格朗日乘数法求解>1. 拉格朗日乘数法求解</a></li><li><a href=#2-为什么-chinchilla-的结论是-11-增长>2. 为什么 Chinchilla 的结论是 1:1 增长？</a></li><li><a href=#3-为什么-kaplan-当年搞错了>3. 为什么 Kaplan 当年搞错了？</a></li><li><a href=#4-实战算账训练预算估算>4. 实战算账：训练预算估算</a><ul><li><a href=#41-flops公式详解为什么是-6nd>4.1 FLOPs公式详解：为什么是 6ND?</a></li><li><a href=#42-实战案例训练70b模型需要多少资源>4.2 实战案例：训练70B模型需要多少资源？</a></li><li><a href=#43-不同规模模型的训练预算对比>4.3 不同规模模型的训练预算对比</a></li><li><a href=#44-关键洞察>4.4 关键洞察</a></li></ul></li></ul></li><li><a href=#33-涌现能力与相变现象>3.3 涌现能力与相变现象</a></li><li><a href=#331-the-grokking-phenomenon顿悟现象>3.3.1 The Grokking Phenomenon：顿悟现象</a><ul><li><a href=#现象描述从记忆到泛化的相变>现象描述：从记忆到泛化的相变</a></li><li><a href=#原理深度为什么会发生grokking>原理深度：为什么会发生Grokking？</a></li><li><a href=#工程启示grokking对训练策略的影响>工程启示：Grokking对训练策略的影响</a></li><li><a href=#理论前沿grokking与神经网络的归纳偏置>理论前沿：Grokking与神经网络的归纳偏置</a></li><li><a href=#实战建议如何在项目中应对grokking>实战建议：如何在项目中应对Grokking</a></li><li><a href=#本节小结>本节小结</a></li></ul></li><li><a href=#34-当前视角scaling-law的新发现>3.4 当前视角：Scaling Law的新发现</a><ul><li><a href=#scaling-law的局限性>Scaling Law的局限性</a></li><li><a href=#2023-2025年的新发现>2023-2025年的新发现</a></li></ul></li></ul></li><li><a href=#四预训练的工程挑战>四、预训练的工程挑战</a><ul><li><a href=#41-训练稳定性技术>4.1 训练稳定性技术</a><ul><li><a href=#梯度裁剪gradient-clipping>梯度裁剪（Gradient Clipping）</a></li><li><a href=#梯度累积gradient-accumulation>梯度累积（Gradient Accumulation）</a></li><li><a href=#学习率调度learning-rate-scheduling>学习率调度（Learning Rate Scheduling）</a></li></ul></li><li><a href=#42-混合精度训练深入>4.2 混合精度训练深入</a><ul><li><a href=#fp16-vs-bf16>FP16 vs BF16</a></li><li><a href=#动态损失缩放dynamic-loss-scaling>动态损失缩放（Dynamic Loss Scaling）</a></li></ul></li><li><a href=#43-分布式训练策略>4.3 分布式训练策略</a></li><li><a href=#44-内存优化技术>4.4 内存优化技术</a><ul><li><a href=#梯度检查点gradient-checkpointing>梯度检查点（Gradient Checkpointing）</a></li></ul></li><li><a href=#45-当前视角新一代高效训练技术>4.5 当前视角：新一代高效训练技术</a></li></ul></li><li><a href=#-深度问答预训练核心困惑>💡 深度问答：预训练核心困惑</a><ul><li><a href=#q1-为什么chinchilla-law说数据要20倍参数量但gpt-3只用了17倍>Q1: 为什么Chinchilla Law说数据要20倍参数量,但GPT-3只用了1.7倍？</a></li><li><a href=#q2-数据去重为什么这么重要去掉重复数据会不会反而降低性能>Q2: 数据去重为什么这么重要？去掉重复数据会不会反而降低性能？</a></li><li><a href=#q3-mlm只用15数据训练为什么不全部掩码提高利用率>Q3: MLM只用15%数据训练,为什么不全部掩码提高利用率？</a></li><li><a href=#q4-梯度检查点怎么节省显存代价是什么>Q4: 梯度检查点怎么节省显存？代价是什么？</a></li><li><a href=#q5-bf16比fp16更稳定为什么不直接全用bf16>Q5: BF16比FP16更稳定,为什么不直接全用BF16？</a></li><li><a href=#q6-涌现能力真的存在吗还是只是评估指标的artifact>Q6: 涌现能力真的存在吗？还是只是评估指标的artifact？</a></li></ul></li><li><a href=#五预训练的深层原理为什么有效>五、预训练的深层原理：为什么有效？</a><ul><li><a href=#51-为什么预训练-微调范式有效>5.1 为什么预训练-微调范式有效？</a><ul><li><a href=#关键假设表示共享假设>关键假设：表示共享假设</a></li><li><a href=#理论分析为什么预训练有效>理论分析：为什么预训练有效？</a></li><li><a href=#真实案例bert预训练的价值>真实案例：BERT预训练的价值</a></li><li><a href=#面试必背预训练-微调qa>面试必背：预训练-微调Q&amp;A</a></li></ul></li></ul></li><li><a href=#本章小结>本章小结</a><ul><li><a href=#思考与练习>思考与练习</a></li></ul></li></ul></nav></div></aside></main></body></html>