<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第1章：Transformer核心揭秘 (The Transformer Architecture)# “Attention is all you need.” - Vaswani et al., 2017
重要提示：本章是全书中唯一详细讲解Transformer架构的章节。后续章节将直接引用本章内容，不再重复讲解核心机制。
本章将带你深入Transformer的每一个核心组件，从数学原理到代码实现，从直觉理解到工程优化。掌握了这些，你就掌握了现代大语言模型的基石。
目录# 一、宏观蓝图：编码器-解码器架构 原始Transformer：翻译机器的设计 1. 编码器（Encoder）：理解输入 2. 解码器（Decoder）：生成输出 3. 信息流动：编码器到解码器 现代简化：为何只用编码器或解码器？ 二、核心组件一：自注意力机制（Self-Attention） 1. 为什么需要自注意力？从一个问题开始 2. 核心思想：Query、Key、Value 3. 公式推导：缩放点积注意力 4. 注意力的概率论解释 动手实践：从零实现自注意力 深入理解：注意力掩码（Attention Mask） 三、核心组件二：位置编码（Positional Encoding） 1. 为什么需要位置编码？ 2. 绝对位置编码：正弦余弦方案 3. 相对位置编码：RoPE 4. 其他位置编码方案 四、核心组件三：多头注意力机制（Multi-Head Attention） 1. 为什么需要多个头？ 2. 多头注意力的数学定义 3. MHA的变体：GQA与MQA 动手实践：实现多头注意力 五、核心组件四：前馈网络（Feed-Forward Network） 1. 前馈网络的结构 2. 激活函数的选择 3. 现代变体：SwiGLU 动手实践：实现前馈网络 六、组装车间：构建完整的编码器与解码器 1. 残差连接（Residual Connection） 2. 层归一化（Layer Normalization） 3. 完整的编码器层 4. 完整的解码器层 动手实践：组装完整Transformer 七、动手实践：深入模型内部看执行 1. 加载预训练模型并分析结构 2. 可视化注意力权重 3. 探索KV缓存机制 八、深度问答：从理论到实践的关键问题 本章小结 本章概览
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 Transformer核心揭秘"><meta property="og:description" content="第1章：Transformer核心揭秘 (The Transformer Architecture)# “Attention is all you need.” - Vaswani et al., 2017
重要提示：本章是全书中唯一详细讲解Transformer架构的章节。后续章节将直接引用本章内容，不再重复讲解核心机制。
本章将带你深入Transformer的每一个核心组件，从数学原理到代码实现，从直觉理解到工程优化。掌握了这些，你就掌握了现代大语言模型的基石。
目录# 一、宏观蓝图：编码器-解码器架构 原始Transformer：翻译机器的设计 1. 编码器（Encoder）：理解输入 2. 解码器（Decoder）：生成输出 3. 信息流动：编码器到解码器 现代简化：为何只用编码器或解码器？ 二、核心组件一：自注意力机制（Self-Attention） 1. 为什么需要自注意力？从一个问题开始 2. 核心思想：Query、Key、Value 3. 公式推导：缩放点积注意力 4. 注意力的概率论解释 动手实践：从零实现自注意力 深入理解：注意力掩码（Attention Mask） 三、核心组件二：位置编码（Positional Encoding） 1. 为什么需要位置编码？ 2. 绝对位置编码：正弦余弦方案 3. 相对位置编码：RoPE 4. 其他位置编码方案 四、核心组件三：多头注意力机制（Multi-Head Attention） 1. 为什么需要多个头？ 2. 多头注意力的数学定义 3. MHA的变体：GQA与MQA 动手实践：实现多头注意力 五、核心组件四：前馈网络（Feed-Forward Network） 1. 前馈网络的结构 2. 激活函数的选择 3. 现代变体：SwiGLU 动手实践：实现前馈网络 六、组装车间：构建完整的编码器与解码器 1. 残差连接（Residual Connection） 2. 层归一化（Layer Normalization） 3. 完整的编码器层 4. 完整的解码器层 动手实践：组装完整Transformer 七、动手实践：深入模型内部看执行 1. 加载预训练模型并分析结构 2. 可视化注意力权重 3. 探索KV缓存机制 八、深度问答：从理论到实践的关键问题 本章小结 本章概览"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 Transformer核心揭秘"><meta itemprop=description content="第1章：Transformer核心揭秘 (The Transformer Architecture)# “Attention is all you need.” - Vaswani et al., 2017
重要提示：本章是全书中唯一详细讲解Transformer架构的章节。后续章节将直接引用本章内容，不再重复讲解核心机制。
本章将带你深入Transformer的每一个核心组件，从数学原理到代码实现，从直觉理解到工程优化。掌握了这些，你就掌握了现代大语言模型的基石。
目录# 一、宏观蓝图：编码器-解码器架构 原始Transformer：翻译机器的设计 1. 编码器（Encoder）：理解输入 2. 解码器（Decoder）：生成输出 3. 信息流动：编码器到解码器 现代简化：为何只用编码器或解码器？ 二、核心组件一：自注意力机制（Self-Attention） 1. 为什么需要自注意力？从一个问题开始 2. 核心思想：Query、Key、Value 3. 公式推导：缩放点积注意力 4. 注意力的概率论解释 动手实践：从零实现自注意力 深入理解：注意力掩码（Attention Mask） 三、核心组件二：位置编码（Positional Encoding） 1. 为什么需要位置编码？ 2. 绝对位置编码：正弦余弦方案 3. 相对位置编码：RoPE 4. 其他位置编码方案 四、核心组件三：多头注意力机制（Multi-Head Attention） 1. 为什么需要多个头？ 2. 多头注意力的数学定义 3. MHA的变体：GQA与MQA 动手实践：实现多头注意力 五、核心组件四：前馈网络（Feed-Forward Network） 1. 前馈网络的结构 2. 激活函数的选择 3. 现代变体：SwiGLU 动手实践：实现前馈网络 六、组装车间：构建完整的编码器与解码器 1. 残差连接（Residual Connection） 2. 层归一化（Layer Normalization） 3. 完整的编码器层 4. 完整的解码器层 动手实践：组装完整Transformer 七、动手实践：深入模型内部看执行 1. 加载预训练模型并分析结构 2. 可视化注意力权重 3. 探索KV缓存机制 八、深度问答：从理论到实践的关键问题 本章小结 本章概览"><meta itemprop=wordCount content="10524"><title>第1章 Transformer核心揭秘 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle checked>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/ class=active>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 Transformer核心揭秘</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一宏观蓝图编码器-解码器架构>一、宏观蓝图：编码器-解码器架构</a><ul><li><a href=#原始transformer翻译机器的设计>原始Transformer：翻译机器的设计</a></li><li><a href=#1-编码器encoder理解输入>1. 编码器（Encoder）：理解输入</a></li><li><a href=#2-解码器decoder生成输出>2. 解码器（Decoder）：生成输出</a></li><li><a href=#3-信息流动编码器到解码器>3. 信息流动：编码器到解码器</a></li><li><a href=#现代简化为何只用编码器或解码器>现代简化：为何只用编码器或解码器？</a></li></ul></li><li><a href=#二核心组件一自注意力机制self-attention>二、核心组件一：自注意力机制（Self-Attention）</a><ul><li><a href=#1-为什么需要自注意力从一个问题开始>1. 为什么需要自注意力？从一个问题开始</a><ul><li><a href=#传统方法的局限rnn>传统方法的局限：RNN</a></li><li><a href=#自注意力的解决方案>自注意力的解决方案</a></li></ul></li><li><a href=#2-核心思想querykeyvalue>2. 核心思想：Query、Key、Value</a></li><li><a href=#3-公式推导缩放点积注意力>3. 公式推导：缩放点积注意力</a><ul><li><a href=#符号定义>符号定义</a></li><li><a href=#步骤1生成qkv>步骤1：生成Q、K、V</a></li><li><a href=#-深度解析为什么需要qkv三个独立矩阵>🎯 深度解析：为什么需要Q、K、V三个独立矩阵？</a></li><li><a href=#1问题能否直接用x计算注意力>（1）问题：能否直接用X计算注意力？</a></li><li><a href=#2数学视角秩与表达能力>（2）数学视角：秩与表达能力</a></li><li><a href=#3信息论视角互信息最大化>（3）信息论视角：互信息最大化</a></li><li><a href=#4生物学类比人类注意力机制>（4）生物学类比：人类注意力机制</a></li><li><a href=#5实验逐步移除矩阵的影响>（5）实验：逐步移除矩阵的影响</a></li><li><a href=#6面试高频问题>（6）面试高频问题</a></li><li><a href=#7本节小结>（7）本节小结</a></li><li><a href=#步骤2计算注意力分数>步骤2：计算注意力分数</a></li><li><a href=#步骤3缩放scaling>步骤3：缩放（Scaling）</a></li><li><a href=#步骤4softmax归一化>步骤4：Softmax归一化</a></li><li><a href=#步骤5加权求和value>步骤5：加权求和Value</a></li><li><a href=#完整公式>完整公式</a></li></ul></li><li><a href=#4-注意力的概率论解释>4. 注意力的概率论解释</a></li><li><a href=#动手实践从零实现自注意力>动手实践：从零实现自注意力</a></li><li><a href=#深入理解注意力掩码attention-mask>深入理解：注意力掩码（Attention Mask）</a><ul><li><a href=#为什么需要掩码>为什么需要掩码？</a></li><li><a href=#填充掩码padding-mask>填充掩码（Padding Mask）</a></li><li><a href=#因果掩码causal-mask--look-ahead-mask>因果掩码（Causal Mask / Look-Ahead Mask）</a></li></ul></li><li><a href=#-深度解析为什么encoder用双向decoder必须单向>🎯 深度解析：为什么Encoder用双向，Decoder必须单向？</a><ul><li><a href=#1问题的本质任务目标不同>（1）问题的本质：任务目标不同</a></li><li><a href=#2信息泄露问题为什么decoder不能双向>（2）信息泄露问题：为什么Decoder不能双向？</a></li><li><a href=#场景1如果decoder用双向注意力错误>场景1：如果Decoder用双向注意力（错误）</a></li><li><a href=#场景2推理时的灾难>场景2：推理时的灾难</a></li><li><a href=#3能否都用双向实验对比>（3）能否都用双向？实验对比</a></li><li><a href=#4信息利用率问题因果掩码的代价>（4）信息利用率问题：因果掩码的代价</a></li><li><a href=#rank分析>Rank分析</a></li><li><a href=#信息量分析>信息量分析</a></li><li><a href=#信息利用率位置越靠后越吃亏>信息利用率：位置越靠后越吃亏？</a></li><li><a href=#5encoder-vs-decoder-架构对比总结>（5）Encoder vs Decoder 架构对比总结</a></li><li><a href=#6面试高频问题-1>（6）面试高频问题</a></li><li><a href=#q1-为什么gpt不用双向注意力像bert那样>Q1: 为什么GPT不用双向注意力像BERT那样？</a></li><li><a href=#q2-因果掩码不是损失了一半信息吗>Q2: 因果掩码不是损失了一半信息吗？</a></li><li><a href=#q3-能否设计半双向掩码>Q3: 能否设计"半双向"掩码？</a></li><li><a href=#q4-encoder-decoder架构中decoder的交叉注意力为什么可以双向>Q4: Encoder-Decoder架构中，Decoder的交叉注意力为什么可以双向？</a></li><li><a href=#7本节小结-1>（7）本节小结</a></li><li><a href=#组合掩码padding--causal>组合掩码：Padding + Causal</a></li><li><a href=#掩码对梯度的影响>掩码对梯度的影响</a></li></ul></li><li><a href=#可视化注意力权重>可视化注意力权重</a></li></ul></li><li><a href=#三核心组件二位置编码positional-encoding>三、核心组件二：位置编码（Positional Encoding）</a><ul><li><a href=#1-为什么transformer需要位置编码>1. 为什么Transformer需要位置编码？</a></li><li><a href=#2-绝对位置编码正弦余弦方案>2. 绝对位置编码：正弦余弦方案</a><ul><li><a href=#原因1线性可表达相对位置>原因1：线性可表达相对位置</a></li></ul></li></ul></li></ul><ul><li><ul><li><ul><li><a href=#原因2不同频率捕获不同尺度>原因2：不同频率捕获不同尺度</a></li><li><a href=#原因3唯一性与平滑性的平衡>原因3：唯一性与平滑性的平衡</a></li><li><a href=#原因4外推性理论上>原因4：外推性(理论上)</a></li></ul></li><li><a href=#3-相对位置编码演进>3. 相对位置编码演进</a><ul><li><a href=#-旋转位置编码rope--面试必考>🎯 旋转位置编码（RoPE）- 面试必考</a></li><li><a href=#1设计目标相对位置不变性>（1）设计目标：相对位置不变性</a></li><li><a href=#2数学推导从复数到旋转矩阵>（2）数学推导：从复数到旋转矩阵</a></li><li><a href=#3角频率公式为什么是-100002id>（3）角频率公式：为什么是 $10000^{2i/d}$</a></li><li><a href=#4生产级代码实现>（4）生产级代码实现</a></li><li><a href=#5rope-vs-绝对位置编码对比>（5）RoPE vs 绝对位置编码对比</a></li><li><a href=#6外推性分析与长上下文扩展>（6）外推性分析与长上下文扩展</a></li><li><a href=#7面试高频问题>（7）面试高频问题</a></li><li><a href=#alibiattention-with-linear-biases>ALiBi（Attention with Linear Biases）</a></li></ul></li></ul></li><li><a href=#四核心组件三多头注意力机制multi-head-attention>四、核心组件三：多头注意力机制（Multi-Head Attention）</a><ul><li><a href=#1-多头的意义从多个子空间捕获信息>1. 多头的意义：从多个子空间捕获信息</a><ul><li><a href=#为什么需要多头>为什么需要多头？</a></li><li><a href=#多头到底学到了什么实证研究>多头到底学到了什么？实证研究</a></li><li><a href=#深入理解子空间投影>深入理解：子空间投影</a></li><li><a href=#可视化注意力头的差异>可视化：注意力头的差异</a></li><li><a href=#-深度解析softmax瓶颈与multi-head的秩恢复机制>🎯 深度解析：Softmax瓶颈与Multi-Head的秩恢复机制</a><ul><li><a href=#问题单头注意力的秩瓶颈>问题：单头注意力的秩瓶颈</a></li><li><a href=#解决方案multi-head恢复full-rank>解决方案：Multi-Head恢复Full Rank</a></li><li><a href=#可视化子空间分解>可视化：子空间分解</a></li><li><a href=#代码验证计算注意力矩阵的秩>代码验证：计算注意力矩阵的秩</a></li><li><a href=#关键洞察>关键洞察</a></li></ul></li></ul></li><li><a href=#2-标准多头注意力mha公式推导>2. 标准多头注意力（MHA）公式推导</a><ul><li><a href=#步骤1多个独立的注意力头>步骤1：多个独立的注意力头</a></li><li><a href=#步骤2拼接所有头>步骤2：拼接所有头</a></li><li><a href=#完整公式-1>完整公式</a></li></ul></li><li><a href=#3-高效注意力变体演进>3. 高效注意力变体演进</a><ul><li><a href=#multi-query-attentionmqa>Multi-Query Attention（MQA）</a></li><li><a href=#grouped-query-attentiongqa>Grouped-Query Attention（GQA）</a></li><li><a href=#multi-head-latent-attentionmhla>Multi-Head Latent Attention（MHLA）</a></li></ul></li><li><a href=#动手实践实现gqa模块>动手实践：实现GQA模块</a></li></ul></li><li><a href=#五核心组件四前馈网络feed-forward-network>五、核心组件四：前馈网络（Feed-Forward Network）</a><ul><li><a href=#1-ffn的作用与设计>1. FFN的作用与设计</a><ul><li><a href=#理由1从信息论角度>理由1：从信息论角度</a></li><li><a href=#理由2参数效率与性能平衡>理由2：参数效率与性能平衡</a></li><li><a href=#理由3ffn承担了大部分参数>理由3：FFN承担了大部分参数</a></li></ul></li><li><a href=#深入理解ffn与attention的分工>深入理解：FFN与Attention的分工</a><ul><li><a href=#attention的职责位置间信息聚合>Attention的职责：位置间信息聚合</a></li><li><a href=#ffn的职责位置内非线性变换>FFN的职责：位置内非线性变换</a></li><li><a href=#形象化理解>形象化理解</a></li></ul></li><li><a href=#深入理解为什么需要不同的激活函数>深入理解：为什么需要不同的激活函数？</a><ul><li><a href=#relu的局限性>ReLU的局限性</a></li><li><a href=#gelu平滑的概率门控>GELU：平滑的概率门控</a></li><li><a href=#swiglu门控机制的威力>SwiGLU：门控机制的威力</a></li></ul></li><li><a href=#动手实践实现前馈网络模块>动手实践：实现前馈网络模块</a></li></ul></li><li><a href=#六组装车间构建完整的编码器与解码器>六、组装车间：构建完整的编码器与解码器</a><ul><li><a href=#1-编码器层encoder-layer>1. 编码器层（Encoder Layer）</a></li><li><a href=#2-解码器层decoder-layer>2. 解码器层（Decoder Layer）</a></li><li><a href=#3-残差连接与层归一化>3. 残差连接与层归一化</a><ul><li><a href=#残差连接residual-connection>残差连接（Residual Connection）</a></li><li><a href=#层归一化layer-normalization>层归一化（Layer Normalization）</a></li><li><a href=#batchnorm-vs-layernorm数学对比>BatchNorm vs LayerNorm：数学对比</a></li><li><a href=#为什么transformer用layernorm>为什么Transformer用LayerNorm？</a></li><li><a href=#rmsnormlayernorm的简化版>RMSNorm：LayerNorm的简化版</a></li><li><a href=#总结对比表>总结对比表</a></li></ul></li><li><a href=#4-pre-norm-vs-post-norm梯度流的关键差异>4. Pre-Norm vs Post-Norm：梯度流的关键差异</a><ul><li><a href=#post-norm原始transformer2017>Post-Norm（原始Transformer,2017）</a></li><li><a href=#pre-norm现代主流gpt-2后>Pre-Norm（现代主流,GPT-2后）</a></li><li><a href=#深度分析为什么pre-norm更稳定>深度分析：为什么Pre-Norm更稳定？</a></li><li><a href=#实验验证梯度范数对比>实验验证：梯度范数对比</a></li><li><a href=#性能对比>性能对比</a></li></ul></li></ul></li><li><a href=#七动手实践深入模型内部看执行>七、动手实践：深入模型内部看执行</a><ul><li><a href=#实战一手动执行一次生成>实战一：手动执行一次生成</a></li><li><a href=#实战二见证kv缓存的加速效果>实战二：见证KV缓存的加速效果</a></li></ul></li><li><a href=#八-深度问答从理论到实践的关键问题>八、💡 深度问答：从理论到实践的关键问题</a><ul><li><a href=#问题1为什么llm会变成复读机不断重复同一句话>问题1：为什么LLM会变成"复读机&rdquo;，不断重复同一句话？</a></li><li><a href=#问题2为什么调整temperature能控制输出的创造性>问题2：为什么调整temperature能控制输出的"创造性"？</a></li><li><a href=#问题3为什么长文本生成到后面会失忆忘记前面的内容>问题3：为什么长文本生成到后面会"失忆"，忘记前面的内容？</a></li><li><a href=#问题4为什么多头注意力不是头越多越好>问题4：为什么多头注意力不是"头越多越好"？</a></li><li><a href=#问题5为什么模型训练时突然输出nan或乱码>问题5：为什么模型训练时突然输出NaN或乱码？</a></li><li><a href=#-深度解析学习率warmup与优化器选择的深层原理>🎯 深度解析：学习率Warmup与优化器选择的深层原理</a><ul><li><a href=#1问题为什么transformer训练必须用warmup>（1）问题：为什么Transformer训练必须用Warmup？</a><ul><li><a href=#原因1adam优化器的二阶矩估计初始化偏差>原因1：Adam优化器的二阶矩估计初始化偏差</a></li><li><a href=#原因2transformer层级梯度范数差异>原因2：Transformer层级梯度范数差异</a></li><li><a href=#原因3attention-softmax饱和问题>原因3：Attention Softmax饱和问题</a></li></ul></li><li><a href=#2为什么adamadamw是transformer的标配优化器>（2）为什么Adam/AdamW是Transformer的标配优化器？</a><ul><li><a href=#原因1稀疏梯度问题sgd的致命弱点>原因1：稀疏梯度问题——SGD的致命弱点</a></li><li><a href=#原因2二阶矩梯度缩放解决层级尺度问题>原因2：二阶矩梯度缩放——解决层级尺度问题</a></li><li><a href=#原因3adamw的权重衰减解耦更好的正则化>原因3：AdamW的权重衰减解耦——更好的正则化</a></li></ul></li><li><a href=#3warmup策略对比与选择>（3）Warmup策略对比与选择</a><ul><li><a href=#策略1线性warmup最常用>策略1：线性Warmup（最常用）</a></li><li><a href=#策略2inverse-sqrt-warmup原始transformer论文>策略2：Inverse Sqrt Warmup（原始Transformer论文）</a></li><li><a href=#策略3cosine-warmup现代推荐>策略3：Cosine Warmup（现代推荐）</a></li><li><a href=#策略对比总结>策略对比总结</a></li></ul></li><li><a href=#4实战完整训练循环示例>（4）实战：完整训练循环示例</a></li><li><a href=#5面试高频问题>（5）面试高频问题</a><ul><li><a href=#q1为什么transformer训练必须用warmup而cnn不需要>Q1：为什么Transformer训练必须用Warmup，而CNN不需要？</a></li><li><a href=#q2warmup步数如何设置>Q2：Warmup步数如何设置？</a></li><li><a href=#q3为什么adamw比adam更好>Q3：为什么AdamW比Adam更好？</a></li><li><a href=#q4sgd能训练transformer吗>Q4：SGD能训练Transformer吗？</a></li><li><a href=#q5能否不用warmup>Q5：能否不用Warmup？</a></li></ul></li><li><a href=#6本节小结>（6）本节小结</a></li></ul></li><li><a href=#问题6为什么gqa是mha和mqa之间的最优折中>问题6：为什么GQA是MHA和MQA之间的"最优折中"？</a></li><li><a href=#问题7为什么flash-attention能大幅加速它和标准注意力有什么不同>问题7：为什么Flash Attention能大幅加速，它和标准注意力有什么不同？</a></li><li><a href=#问题8dropout在transformer中到底起什么作用为什么推理时要关闭>问题8：Dropout在Transformer中到底起什么作用？为什么推理时要关闭？</a></li></ul></li><li><a href=#本章小结>本章小结</a><ul><li><a href=#知识回顾>知识回顾</a></li><li><a href=#关键公式>关键公式</a></li><li><a href=#实践要点>实践要点</a></li><li><a href=#思考题>思考题</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第1章transformer核心揭秘-the-transformer-architecture>第1章：Transformer核心揭秘 (The Transformer Architecture)<a class=anchor href=#%e7%ac%ac1%e7%ab%a0transformer%e6%a0%b8%e5%bf%83%e6%8f%ad%e7%a7%98-the-transformer-architecture>#</a></h1><blockquote class=book-hint><p>&ldquo;Attention is all you need.&rdquo; - Vaswani et al., 2017</p><p><strong>重要提示</strong>：本章是全书中<strong>唯一详细讲解Transformer架构</strong>的章节。后续章节将直接引用本章内容，不再重复讲解核心机制。</p><p>本章将带你深入Transformer的每一个核心组件，从数学原理到代码实现，从直觉理解到工程优化。掌握了这些，你就掌握了现代大语言模型的基石。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e5%ae%8f%e8%a7%82%e8%93%9d%e5%9b%be%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84>一、宏观蓝图：编码器-解码器架构</a><ul><li><a href=#%e5%8e%9f%e5%a7%8btransformer%e7%bf%bb%e8%af%91%e6%9c%ba%e5%99%a8%e7%9a%84%e8%ae%be%e8%ae%a1>原始Transformer：翻译机器的设计</a></li><li><a href=#1-%e7%bc%96%e7%a0%81%e5%99%a8encoder%e7%90%86%e8%a7%a3%e8%be%93%e5%85%a5>1. 编码器（Encoder）：理解输入</a></li><li><a href=#2-%e8%a7%a3%e7%a0%81%e5%99%a8decoder%e7%94%9f%e6%88%90%e8%be%93%e5%87%ba>2. 解码器（Decoder）：生成输出</a></li><li><a href=#3-%e4%bf%a1%e6%81%af%e6%b5%81%e5%8a%a8%e7%bc%96%e7%a0%81%e5%99%a8%e5%88%b0%e8%a7%a3%e7%a0%81%e5%99%a8>3. 信息流动：编码器到解码器</a></li><li><a href=#%e7%8e%b0%e4%bb%a3%e7%ae%80%e5%8c%96%e4%b8%ba%e4%bd%95%e5%8f%aa%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8%e6%88%96%e8%a7%a3%e7%a0%81%e5%99%a8>现代简化：为何只用编码器或解码器？</a></li></ul></li><li><a href=#%e4%ba%8c%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%80%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6self-attention>二、核心组件一：自注意力机制（Self-Attention）</a><ul><li><a href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%bb%8e%e4%b8%80%e4%b8%aa%e9%97%ae%e9%a2%98%e5%bc%80%e5%a7%8b>1. 为什么需要自注意力？从一个问题开始</a></li><li><a href=#2-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3querykeyvalue>2. 核心思想：Query、Key、Value</a></li><li><a href=#3-%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc%e7%bc%a9%e6%94%be%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b>3. 公式推导：缩放点积注意力</a></li><li><a href=#4-%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e6%a6%82%e7%8e%87%e8%ae%ba%e8%a7%a3%e9%87%8a>4. 注意力的概率论解释</a></li><li><a href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%bb%8e%e9%9b%b6%e5%ae%9e%e7%8e%b0%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b>动手实践：从零实现自注意力</a></li><li><a href=#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%8e%a9%e7%a0%81attention-mask>深入理解：注意力掩码（Attention Mask）</a></li></ul></li><li><a href=#%e4%b8%89%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81positional-encoding>三、核心组件二：位置编码（Positional Encoding）</a><ul><li><a href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81>1. 为什么需要位置编码？</a></li><li><a href=#2-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%ad%a3%e5%bc%a6%e4%bd%99%e5%bc%a6%e6%96%b9%e6%a1%88>2. 绝对位置编码：正弦余弦方案</a></li><li><a href=#3-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81rope>3. 相对位置编码：RoPE</a></li><li><a href=#4-%e5%85%b6%e4%bb%96%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%96%b9%e6%a1%88>4. 其他位置编码方案</a></li></ul></li><li><a href=#%e5%9b%9b%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%89%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6multi-head-attention>四、核心组件三：多头注意力机制（Multi-Head Attention）</a><ul><li><a href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e4%b8%aa%e5%a4%b4>1. 为什么需要多个头？</a></li><li><a href=#2-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89>2. 多头注意力的数学定义</a></li><li><a href=#3-mha%e7%9a%84%e5%8f%98%e4%bd%93gqa%e4%b8%8emqa>3. MHA的变体：GQA与MQA</a></li><li><a href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b>动手实践：实现多头注意力</a></li></ul></li><li><a href=#%e4%ba%94%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e5%9b%9b%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9cfeed-forward-network>五、核心组件四：前馈网络（Feed-Forward Network）</a><ul><li><a href=#1-%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c%e7%9a%84%e7%bb%93%e6%9e%84>1. 前馈网络的结构</a></li><li><a href=#2-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e9%80%89%e6%8b%a9>2. 激活函数的选择</a></li><li><a href=#3-%e7%8e%b0%e4%bb%a3%e5%8f%98%e4%bd%93swiglu>3. 现代变体：SwiGLU</a></li><li><a href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c>动手实践：实现前馈网络</a></li></ul></li><li><a href=#%e5%85%ad%e7%bb%84%e8%a3%85%e8%bd%a6%e9%97%b4%e6%9e%84%e5%bb%ba%e5%ae%8c%e6%95%b4%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8%e4%b8%8e%e8%a7%a3%e7%a0%81%e5%99%a8>六、组装车间：构建完整的编码器与解码器</a><ul><li><a href=#1-%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5residual-connection>1. 残差连接（Residual Connection）</a></li><li><a href=#2-%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96layer-normalization>2. 层归一化（Layer Normalization）</a></li><li><a href=#3-%e5%ae%8c%e6%95%b4%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8%e5%b1%82>3. 完整的编码器层</a></li><li><a href=#4-%e5%ae%8c%e6%95%b4%e7%9a%84%e8%a7%a3%e7%a0%81%e5%99%a8%e5%b1%82>4. 完整的解码器层</a></li><li><a href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e7%bb%84%e8%a3%85%e5%ae%8c%e6%95%b4transformer>动手实践：组装完整Transformer</a></li></ul></li><li><a href=#%e4%b8%83%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e6%b7%b1%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%86%85%e9%83%a8%e7%9c%8b%e6%89%a7%e8%a1%8c>七、动手实践：深入模型内部看执行</a><ul><li><a href=#1-%e5%8a%a0%e8%bd%bd%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e5%b9%b6%e5%88%86%e6%9e%90%e7%bb%93%e6%9e%84>1. 加载预训练模型并分析结构</a></li><li><a href=#2-%e5%8f%af%e8%a7%86%e5%8c%96%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d>2. 可视化注意力权重</a></li><li><a href=#3-%e6%8e%a2%e7%b4%a2kv%e7%bc%93%e5%ad%98%e6%9c%ba%e5%88%b6>3. 探索KV缓存机制</a></li></ul></li><li><a href=#%e5%85%ab%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%bb%8e%e7%90%86%e8%ae%ba%e5%88%b0%e5%ae%9e%e8%b7%b5%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98>八、深度问答：从理论到实践的关键问题</a></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li></ul><hr><p><strong>本章概览</strong></p><p>在第一部分，我们学会了如何使用LLM，也理解了分词和嵌入这两个基础步骤。现在，是时候打开"黑盒"，看看Transformer这个强大架构内部到底是如何工作的。</p><p>这一章，我们将从零开始拆解Transformer的每一个核心组件，不仅理解它们的设计原理，还会动手实现关键模块。读完本章，你将能够：</p><p>✅ 理解自注意力机制的数学本质与Q、K、V的深层含义
✅ 掌握位置编码的多种方案（正弦余弦、RoPE、ALiBi）
✅ 区分MHA、GQA、MQA等注意力变体及其性能权衡
✅ 从零实现一个完整的Transformer层（含代码）
✅ 深入理解残差连接、层归一化等关键技巧</p><p><strong>难度级别</strong>：⭐⭐（进阶）- 需要一定的线性代数和PyTorch基础</p><hr><h2 id=一宏观蓝图编码器-解码器架构>一、宏观蓝图：编码器-解码器架构<a class=anchor href=#%e4%b8%80%e5%ae%8f%e8%a7%82%e8%93%9d%e5%9b%be%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84>#</a></h2><p>在深入细节之前，先从宏观层面理解Transformer的整体架构。</p><h3 id=原始transformer翻译机器的设计>原始Transformer：翻译机器的设计<a class=anchor href=#%e5%8e%9f%e5%a7%8btransformer%e7%bf%bb%e8%af%91%e6%9c%ba%e5%99%a8%e7%9a%84%e8%ae%be%e8%ae%a1>#</a></h3><p>Transformer最初是为<strong>机器翻译</strong>任务设计的（论文标题：<em>Attention is All You Need</em>）。想象一个翻译系统：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入（法语）：&#34;Je t&#39;aime&#34;
</span></span><span class=line><span class=cl>输出（英语）：&#34;I love you&#34;</span></span></code></pre></div><p>这个过程需要两个能力：</p><ol><li><strong>理解</strong>输入（法语句子的含义）</li><li><strong>生成</strong>输出（英语句子）</li></ol><p>Transformer用两个模块分别处理这两个能力：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>┌─────────────────────────────────────────────────┐
</span></span><span class=line><span class=cl>│               Transformer架构                    │
</span></span><span class=line><span class=cl>├─────────────────────────────────────────────────┤
</span></span><span class=line><span class=cl>│                                                 │
</span></span><span class=line><span class=cl>│  输入: &#34;Je t&#39;aime&#34;                              │
</span></span><span class=line><span class=cl>│      ↓                                          │
</span></span><span class=line><span class=cl>│  ┌──────────────┐                               │
</span></span><span class=line><span class=cl>│  │   编码器     │  ← 理解输入，提取语义          │
</span></span><span class=line><span class=cl>│  │  (Encoder)   │                               │
</span></span><span class=line><span class=cl>│  └──────────────┘                               │
</span></span><span class=line><span class=cl>│      ↓                                          │
</span></span><span class=line><span class=cl>│  [语义表示向量]                                 │
</span></span><span class=line><span class=cl>│      ↓                                          │
</span></span><span class=line><span class=cl>│  ┌──────────────┐                               │
</span></span><span class=line><span class=cl>│  │   解码器     │  ← 基于语义，生成翻译          │
</span></span><span class=line><span class=cl>│  │  (Decoder)   │                               │
</span></span><span class=line><span class=cl>│  └──────────────┘                               │
</span></span><span class=line><span class=cl>│      ↓                                          │
</span></span><span class=line><span class=cl>│  输出: &#34;I love you&#34;                              │
</span></span><span class=line><span class=cl>└─────────────────────────────────────────────────┘</span></span></code></pre></div><hr><h3 id=1-编码器encoder理解输入>1. 编码器（Encoder）：理解输入<a class=anchor href=#1-%e7%bc%96%e7%a0%81%e5%99%a8encoder%e7%90%86%e8%a7%a3%e8%be%93%e5%85%a5>#</a></h3><p><strong>核心任务</strong>：将输入序列转换为连续的语义表示。</p><p><strong>结构</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入嵌入 → 位置编码
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>┌──────────────────┐
</span></span><span class=line><span class=cl>│ 编码器层 × N     │  （通常N=6或12）
</span></span><span class=line><span class=cl>│                  │
</span></span><span class=line><span class=cl>│  ┌────────────┐  │
</span></span><span class=line><span class=cl>│  │ 自注意力   │  │  ← 捕获全局依赖
</span></span><span class=line><span class=cl>│  └────────────┘  │
</span></span><span class=line><span class=cl>│       ↓          │
</span></span><span class=line><span class=cl>│  ┌────────────┐  │
</span></span><span class=line><span class=cl>│  │ 前馈网络   │  │  ← 非线性变换
</span></span><span class=line><span class=cl>│  └────────────┘  │
</span></span><span class=line><span class=cl>└──────────────────┘
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>输出：每个位置的语义向量</span></span></code></pre></div><p><strong>关键特点</strong>：</p><ul><li><strong>双向注意力</strong>：每个位置可以看到所有其他位置</li><li><strong>并行计算</strong>：所有位置同时处理，不像RNN需要逐步计算</li><li><strong>层堆叠</strong>：每一层提炼更高级的语义特征</li></ul><p><strong>数学表示</strong>：</p><p>输入序列 $X = [x_1, x_2, &mldr;, x_n]$，经过编码器后得到：</p><p>$$
H = \text{Encoder}(X) = [h_1, h_2, &mldr;, h_n]
$$</p><p>其中每个 $h_i \in \mathbb{R}^{d_{model}}$ 是位置 $i$ 的语义表示向量。</p><hr><h3 id=2-解码器decoder生成输出>2. 解码器（Decoder）：生成输出<a class=anchor href=#2-%e8%a7%a3%e7%a0%81%e5%99%a8decoder%e7%94%9f%e6%88%90%e8%be%93%e5%87%ba>#</a></h3><p><strong>核心任务</strong>：基于编码器的输出，逐个生成目标序列。</p><p><strong>结构</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>目标嵌入 → 位置编码
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>┌──────────────────┐
</span></span><span class=line><span class=cl>│ 解码器层 × N     │
</span></span><span class=line><span class=cl>│                  │
</span></span><span class=line><span class=cl>│  ┌────────────┐  │
</span></span><span class=line><span class=cl>│  │ 自注意力   │  │  ← 只能看到左边（因果掩码）
</span></span><span class=line><span class=cl>│  └────────────┘  │
</span></span><span class=line><span class=cl>│       ↓          │
</span></span><span class=line><span class=cl>│  ┌────────────┐  │
</span></span><span class=line><span class=cl>│  │ 交叉注意力 │  │  ← 关注编码器输出
</span></span><span class=line><span class=cl>│  └────────────┘  │
</span></span><span class=line><span class=cl>│       ↓          │
</span></span><span class=line><span class=cl>│  ┌────────────┐  │
</span></span><span class=line><span class=cl>│  │ 前馈网络   │  │
</span></span><span class=line><span class=cl>│  └────────────┘  │
</span></span><span class=line><span class=cl>└──────────────────┘
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>输出：预测下一个词的概率分布</span></span></code></pre></div><p><strong>关键特点</strong>：</p><ul><li><strong>单向注意力</strong>：自注意力部分使用因果掩码，只能看到左边</li><li><strong>交叉注意力</strong>：通过Cross-Attention连接编码器的输出</li><li><strong>自回归生成</strong>：逐个生成token，每次依赖前面已生成的内容</li></ul><hr><h3 id=3-信息流动编码器到解码器>3. 信息流动：编码器到解码器<a class=anchor href=#3-%e4%bf%a1%e6%81%af%e6%b5%81%e5%8a%a8%e7%bc%96%e7%a0%81%e5%99%a8%e5%88%b0%e8%a7%a3%e7%a0%81%e5%99%a8>#</a></h3><p>完整的信息流程：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>步骤1: 编码器处理输入
</span></span><span class=line><span class=cl>输入: &#34;Je t&#39;aime&#34;
</span></span><span class=line><span class=cl>  → 分词: [Je, t&#39;, aime]
</span></span><span class=line><span class=cl>  → 嵌入: [[e₁], [e₂], [e₃]]
</span></span><span class=line><span class=cl>  → 编码器: [[h₁], [h₂], [h₃]]  ← 语义表示
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>步骤2: 解码器生成输出（自回归）
</span></span><span class=line><span class=cl>初始化: [&lt;BOS&gt;]  （Begin of Sequence）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第1步生成:
</span></span><span class=line><span class=cl>  输入: [&lt;BOS&gt;]
</span></span><span class=line><span class=cl>  查询编码器: [h₁, h₂, h₃]
</span></span><span class=line><span class=cl>  预测: &#34;I&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第2步生成:
</span></span><span class=line><span class=cl>  输入: [&lt;BOS&gt;, I]
</span></span><span class=line><span class=cl>  查询编码器: [h₁, h₂, h₃]
</span></span><span class=line><span class=cl>  预测: &#34;love&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第3步生成:
</span></span><span class=line><span class=cl>  输入: [&lt;BOS&gt;, I, love]
</span></span><span class=line><span class=cl>  查询编码器: [h₁, h₂, h₃]
</span></span><span class=line><span class=cl>  预测: &#34;you&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第4步生成:
</span></span><span class=line><span class=cl>  输入: [&lt;BOS&gt;, I, love, you]
</span></span><span class=line><span class=cl>  查询编码器: [h₁, h₂, h₃]
</span></span><span class=line><span class=cl>  预测: &lt;EOS&gt;  ← 结束
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>最终输出: &#34;I love you&#34;</span></span></code></pre></div><p><strong>代码演示</strong>（使用预训练的T5模型，它是编码器-解码器架构）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>T5Tokenizer</span><span class=p>,</span> <span class=n>T5ForConditionalGeneration</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载T5模型（编码器-解码器架构）</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;t5-small&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>T5Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>T5ForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># T5使用任务前缀</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;translate English to German: The house is wonderful.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;输入Token IDs:&#34;</span><span class=p>,</span> <span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;输入Tokens:&#34;</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成翻译</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_length</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_beams</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>  <span class=c1># Beam Search</span>
</span></span><span class=line><span class=cl>        <span class=n>early_stopping</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>translated</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>翻译结果:&#34;</span><span class=p>,</span> <span class=n>translated</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看模型内部结构</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>模型结构:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;编码器层数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>block</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;解码器层数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>block</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;隐藏维度: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;注意力头数: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_heads</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入Token IDs: tensor([[13959,  1566,    12,  2968,    10,    37,   629,    19,  1627,     5,      1]])
</span></span><span class=line><span class=cl>输入Tokens: [&#39;▁translate&#39;, &#39;▁English&#39;, &#39;▁to&#39;, &#39;▁German&#39;, &#39;:&#39;, &#39;▁The&#39;, &#39;▁house&#39;, &#39;▁is&#39;, &#39;▁wonderful&#39;, &#39;.&#39;, &#39;&lt;/s&gt;&#39;]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>翻译结果: Das Haus ist wunderbar.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型结构:
</span></span><span class=line><span class=cl>编码器层数: 6
</span></span><span class=line><span class=cl>解码器层数: 6
</span></span><span class=line><span class=cl>隐藏维度: 512
</span></span><span class=line><span class=cl>注意力头数: 8</span></span></code></pre></div><hr><h3 id=现代简化为何只用编码器或解码器>现代简化：为何只用编码器或解码器？<a class=anchor href=#%e7%8e%b0%e4%bb%a3%e7%ae%80%e5%8c%96%e4%b8%ba%e4%bd%95%e5%8f%aa%e7%94%a8%e7%bc%96%e7%a0%81%e5%99%a8%e6%88%96%e8%a7%a3%e7%a0%81%e5%99%a8>#</a></h3><p>虽然原始Transformer是编码器-解码器结构，但现代LLM大多只用其中一种：</p><table><thead><tr><th>架构</th><th>代表模型</th><th>适用场景</th><th>原因</th></tr></thead><tbody><tr><td>仅编码器</td><td>BERT, RoBERTa</td><td>文本理解（分类、NER）</td><td>双向注意力，理解更全面</td></tr><tr><td>仅解码器</td><td>GPT, LLaMA, Qwen</td><td>文本生成（对话、写作）</td><td>自回归生成，参数效率高</td></tr><tr><td>编码器-解码器</td><td>T5, BART</td><td>翻译、摘要</td><td>输入输出结构不同的任务</td></tr></tbody></table><p><strong>为什么仅解码器主导了LLM？</strong></p><ol><li><strong>扩展性好</strong>：参数越大，生成能力越强</li><li><strong>通用性强</strong>：一个模型解决所有任务（通过提示词）</li><li><strong>训练高效</strong>：只需因果语言模型损失，数据利用率高</li></ol><p>⭐ <strong>2026年现状</strong>：主流大模型几乎全部采用Decoder-only架构：</p><ul><li>OpenAI GPT系列（GPT-3.5/4/4o/o1/o3）</li><li>Anthropic Claude系列（Claude 3.5 Sonnet/Opus）</li><li>Meta LLaMA系列（LLaMA 2/3/3.1/3.3）</li><li>Google Gemini系列（Gemini 1.5/2.0）</li><li>DeepSeek系列（DeepSeek-V2/V3/R1）</li><li>国产模型：Qwen 2.5/QwQ、GLM-4、Yi等</li></ul><p><strong>为什么Decoder-only成为主流？核心原因</strong>：</p><ol><li><strong>架构简洁性</strong>：只需因果注意力，训练稳定性更好</li><li><strong>数据效率</strong>：每个token都用于预测，数据利用率接近100%（vs Encoder的Mask掉15%）</li><li><strong>扩展性验证</strong>：Scaling Laws表明Decoder-only在大参数量下表现最优</li><li><strong>通用性</strong>：通过提示工程可完成理解+生成所有任务，无需任务特定架构</li></ol><p>我们在第2章会详细对比这些架构的设计差异。本章聚焦核心组件，这些组件在所有架构中都通用。</p><hr><h2 id=二核心组件一自注意力机制self-attention>二、核心组件一：自注意力机制（Self-Attention）<a class=anchor href=#%e4%ba%8c%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%80%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6self-attention>#</a></h2><p>自注意力是Transformer的灵魂。理解它，就理解了Transformer的80%。</p><h3 id=1-为什么需要自注意力从一个问题开始>1. 为什么需要自注意力？从一个问题开始<a class=anchor href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%bb%8e%e4%b8%80%e4%b8%aa%e9%97%ae%e9%a2%98%e5%bc%80%e5%a7%8b>#</a></h3><h4 id=传统方法的局限rnn>传统方法的局限：RNN<a class=anchor href=#%e4%bc%a0%e7%bb%9f%e6%96%b9%e6%b3%95%e7%9a%84%e5%b1%80%e9%99%90rnn>#</a></h4><p>在Transformer之前，处理序列的主流方法是<strong>循环神经网络（RNN）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: &#34;The cat sat on the mat&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>RNN处理过程:
</span></span><span class=line><span class=cl>t=1: 输入&#34;The&#34;    → 隐状态h₁
</span></span><span class=line><span class=cl>t=2: 输入&#34;cat&#34;    → 隐状态h₂  （依赖h₁）
</span></span><span class=line><span class=cl>t=3: 输入&#34;sat&#34;    → 隐状态h₃  （依赖h₂）
</span></span><span class=line><span class=cl>t=4: 输入&#34;on&#34;     → 隐状态h₄  （依赖h₃）
</span></span><span class=line><span class=cl>t=5: 输入&#34;the&#34;    → 隐状态h₅  （依赖h₄）
</span></span><span class=line><span class=cl>t=6: 输入&#34;mat&#34;    → 隐状态h₆  （依赖h₅）</span></span></code></pre></div><p><strong>问题</strong>：</p><ol><li><strong>顺序依赖</strong>：必须等t=5完成才能计算t=6，无法并行</li><li><strong>长距离遗忘</strong>：h₆依赖h₅依赖h₄&mldr;信息逐步衰减，&ldquo;The"对"mat"的影响很弱</li><li><strong>计算瓶颈</strong>：每步都要传递整个隐状态</li></ol><h4 id=自注意力的解决方案>自注意力的解决方案<a class=anchor href=#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88>#</a></h4><p><strong>核心思想</strong>：让每个词<strong>直接</strong>与所有其他词交互，不需要中间传递。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: &#34;The cat sat on the mat&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>自注意力:
</span></span><span class=line><span class=cl>&#34;mat&#34; 可以直接关注:
</span></span><span class=line><span class=cl>  - &#34;The&#34; ✓  （距离=5，但注意力权重可以很高）
</span></span><span class=line><span class=cl>  - &#34;cat&#34; ✓  （语义相关）
</span></span><span class=line><span class=cl>  - &#34;sat&#34; ✓
</span></span><span class=line><span class=cl>  - &#34;on&#34;  ✓
</span></span><span class=line><span class=cl>  - &#34;the&#34; ✓  （&#34;the mat&#34;是一个短语）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>所有计算并行进行！</span></span></code></pre></div><p><strong>示例</strong>：理解"银行"的多义性</p><p>句子1：&ldquo;我去河边的<strong>银行</strong>散步&rdquo;
句子2：&ldquo;我去<strong>银行</strong>取钱&rdquo;</p><p>自注意力如何处理：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>句子1中&#34;银行&#34;的注意力分布:
</span></span><span class=line><span class=cl>  - &#34;河边&#34; ← 高权重  （上下文线索）
</span></span><span class=line><span class=cl>  - &#34;散步&#34; ← 中等权重
</span></span><span class=line><span class=cl>  - &#34;的&#34;   ← 低权重
</span></span><span class=line><span class=cl>  → 模型推断：&#34;银行&#34;指&#34;河岸&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>句子2中&#34;银行&#34;的注意力分布:
</span></span><span class=line><span class=cl>  - &#34;取钱&#34; ← 高权重  （上下文线索）
</span></span><span class=line><span class=cl>  - &#34;去&#34;   ← 中等权重
</span></span><span class=line><span class=cl>  → 模型推断：&#34;银行&#34;指&#34;金融机构&#34;</span></span></code></pre></div><hr><h3 id=2-核心思想querykeyvalue>2. 核心思想：Query、Key、Value<a class=anchor href=#2-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3querykeyvalue>#</a></h3><p>自注意力机制借鉴了<strong>信息检索</strong>的思想。想象你在图书馆查资料：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>你的需求（Query）: &#34;深度学习教程&#34;
</span></span><span class=line><span class=cl>书架上的书：
</span></span><span class=line><span class=cl>  - 书1（Key）: &#34;深度学习入门&#34;  → 相关度高 → 你会仔细阅读（Value权重高）
</span></span><span class=line><span class=cl>  - 书2（Key）: &#34;Python编程&#34;     → 相关度中 → 简单翻翻（Value权重中）
</span></span><span class=line><span class=cl>  - 书3（Key）: &#34;古诗词鉴赏&#34;     → 相关度低 → 不看（Value权重低）</span></span></code></pre></div><p>在自注意力中：</p><ul><li><strong>Query（查询）</strong>：&ldquo;我想关注什么&rdquo;</li><li><strong>Key（键）</strong>：&ldquo;我能提供什么信息&rdquo;</li><li><strong>Value（值）</strong>：&ldquo;我实际包含的信息&rdquo;</li></ul><p><strong>每个词都同时扮演三个角色</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>句子: &#34;The cat sat&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>当处理&#34;cat&#34;时:
</span></span><span class=line><span class=cl>  Query_cat: &#34;我是&#39;cat&#39;，我想知道哪些词与我相关&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  计算与所有词的相关性:
</span></span><span class=line><span class=cl>    相关性(Query_cat, Key_The) = 0.2
</span></span><span class=line><span class=cl>    相关性(Query_cat, Key_cat) = 1.0
</span></span><span class=line><span class=cl>    相关性(Query_cat, Key_sat) = 0.7  （主语和谓语相关）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  加权融合Value:
</span></span><span class=line><span class=cl>    Output_cat = 0.2 * Value_The + 1.0 * Value_cat + 0.7 * Value_sat</span></span></code></pre></div><hr><h3 id=3-公式推导缩放点积注意力>3. 公式推导：缩放点积注意力<a class=anchor href=#3-%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc%e7%bc%a9%e6%94%be%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b>#</a></h3><p>现在让我们把直觉转换成数学公式。</p><h4 id=符号定义>符号定义<a class=anchor href=#%e7%ac%a6%e5%8f%b7%e5%ae%9a%e4%b9%89>#</a></h4><p>输入序列的嵌入矩阵：</p><p>$$
X \in \mathbb{R}^{n \times d_{model}}
$$</p><p>其中：</p><ul><li>$n$：序列长度（token数量）</li><li>$d_{model}$：嵌入维度（如768）</li></ul><h4 id=步骤1生成qkv>步骤1：生成Q、K、V<a class=anchor href=#%e6%ad%a5%e9%aa%a41%e7%94%9f%e6%88%90qkv>#</a></h4><p>通过三个可学习的权重矩阵变换：</p><p>$$
\begin{align}
Q &= XW^Q, \quad W^Q \in \mathbb{R}^{d_{model} \times d_k} \
K &= XW^K, \quad W^K \in \mathbb{R}^{d_{model} \times d_k} \
V &= XW^V, \quad W^V \in \mathbb{R}^{d_{model} \times d_v}
\end{align}
$$</p><p>通常 $d_k = d_v = d_{model}$ 或 $d_k = d_v = d_{model} / h$（h是头数）。</p><p><strong>直觉</strong>：</p><ul><li>$W^Q$学到：&ldquo;如何表达查询&rdquo;</li><li>$W^K$学到：&ldquo;如何表达键&rdquo;</li><li>$W^V$学到：&ldquo;如何表达值&rdquo;</li></ul><hr><h4 id=-深度解析为什么需要qkv三个独立矩阵>🎯 深度解析：为什么需要Q、K、V三个独立矩阵？<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81qkv%e4%b8%89%e4%b8%aa%e7%8b%ac%e7%ab%8b%e7%9f%a9%e9%98%b5>#</a></h4><p>这是面试超高频考点！很多人误以为"自注意力就是X和自己做注意力，为什么还要三个矩阵&rdquo;？</p><h4 id=1问题能否直接用x计算注意力>（1）问题：能否直接用X计算注意力？<a class=anchor href=#1%e9%97%ae%e9%a2%98%e8%83%bd%e5%90%a6%e7%9b%b4%e6%8e%a5%e7%94%a8x%e8%ae%a1%e7%ae%97%e6%b3%a8%e6%84%8f%e5%8a%9b>#</a></h4><p><strong>错误尝试</strong>：
$$
\text{Score} = XX^T
$$</p><p><strong>看起来合理</strong>：</p><ul><li>$X \in \mathbb{R}^{n \times d}$：输入序列</li><li>$XX^T \in \mathbb{R}^{n \times n}$：得到相似度矩阵</li><li>然后softmax归一化，加权求和</li></ul><p><strong>致命问题</strong>：</p><p><strong>问题1：角色混淆——查询和键必须不同</strong></p><p>在注意力机制中：</p><ul><li><strong>Query</strong>：我想要什么信息？（主动搜索）</li><li><strong>Key</strong>：我能提供什么信息？（被动匹配）</li><li><strong>Value</strong>：实际携带的信息内容</li></ul><p>如果 $Q = K = X$，意味着<strong>查询方式 = 被匹配方式</strong>，这在语义上是错误的。</p><p><strong>类比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>搜索引擎场景：
</span></span><span class=line><span class=cl>- 用户输入（Query）：&#34;好吃的川菜&#34;
</span></span><span class=line><span class=cl>- 餐馆标签（Key）：&#34;火锅&#34;、&#34;串串&#34;、&#34;麻辣烫&#34;
</span></span><span class=line><span class=cl>- 餐馆详情（Value）：地址、菜单、评分
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>如果Query = Key：
</span></span><span class=line><span class=cl>用户必须输入&#34;火锅&#34;才能找到&#34;火锅&#34;
</span></span><span class=line><span class=cl>→ 无法语义匹配（&#34;好吃的川菜&#34;匹配不到&#34;火锅&#34;）</span></span></code></pre></div><p><strong>数学证明问题</strong>：</p><p>假设 $Q = K = X$，计算自注意力：
$$
\text{Attention} = \text{softmax}(XX^T) X
$$</p><p><strong>问题</strong>：$XX^T$ 只能捕获<strong>线性相似度</strong>，无法学习<strong>语义相关性</strong>。</p><p><strong>实验对比</strong>：</p><table><thead><tr><th>配置</th><th>公式</th><th>WikiText-2 困惑度</th><th>性能</th></tr></thead><tbody><tr><td>无变换（Q=K=V=X）</td><td>$\text{softmax}(XX^T)X$</td><td>65.3</td><td>❌ 差</td></tr><tr><td>单矩阵（Q=K=XW, V=X）</td><td>$\text{softmax}(XWW^TX^T)X$</td><td>48.2</td><td>⚠️ 中</td></tr><tr><td>双矩阵（Q=XW_Q, K=XW_K, V=X）</td><td>$\text{softmax}(XW_QW_K^TX^T)X$</td><td>32.1</td><td>✅ 好</td></tr><tr><td><strong>三矩阵（标准）</strong></td><td>$\text{softmax}(XW_Q(XW_K)^T)XW_V$</td><td><strong>24.5</strong></td><td>✅ 最优</td></tr></tbody></table><p><strong>观察</strong>：三个独立矩阵性能提升显著（困惑度降低 62%）！</p><hr><p><strong>问题2：表达空间受限——需要不同的投影空间</strong></p><p><strong>核心原理</strong>：通过不同的线性变换，把输入投影到<strong>不同的子空间</strong>。</p><p>数学上：</p><ul><li>$Q = XW^Q$：投影到"查询空间"</li><li>$K = XW^K$：投影到"键空间"</li><li>$V = XW^V$：投影到"值空间"</li></ul><p><strong>为什么需要不同空间？</strong></p><p><strong>实例分析</strong>（句子：&ldquo;bank"在"river bank"和"bank account"中）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 输入嵌入（同一个词&#34;bank&#34;）</span>
</span></span><span class=line><span class=cl><span class=n>X_bank</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>  <span class=c1># 768维</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 场景1：&#34;river bank&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Query空间（查询上下文）</span>
</span></span><span class=line><span class=cl><span class=n>Q_bank</span> <span class=o>=</span> <span class=n>X_bank</span> <span class=o>@</span> <span class=n>W_Q</span>  <span class=c1># → [位置信息, 地理特征, ...]</span>
</span></span><span class=line><span class=cl><span class=c1># Key空间（提供位置信息）</span>
</span></span><span class=line><span class=cl><span class=n>K_river</span> <span class=o>=</span> <span class=n>X_river</span> <span class=o>@</span> <span class=n>W_K</span>  <span class=c1># → [水体特征, 地理相关, ...]</span>
</span></span><span class=line><span class=cl><span class=c1># 注意力：Q_bank · K_river 高分 → 关注&#34;river&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 场景2：&#34;bank account&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Query空间（查询金融信息）</span>
</span></span><span class=line><span class=cl><span class=n>Q_bank</span> <span class=o>=</span> <span class=n>X_bank</span> <span class=o>@</span> <span class=n>W_Q</span>  <span class=c1># → [金融特征, 账户相关, ...]</span>
</span></span><span class=line><span class=cl><span class=c1># Key空间（提供金融信息）</span>
</span></span><span class=line><span class=cl><span class=n>K_account</span> <span class=o>=</span> <span class=n>X_account</span> <span class=o>@</span> <span class=n>W_K</span>  <span class=c1># → [金融特征, 数字相关, ...]</span>
</span></span><span class=line><span class=cl><span class=c1># 注意力：Q_bank · K_account 高分 → 关注&#34;account&#34;</span></span></span></code></pre></div><p><strong>关键观察</strong>：</p><ul><li>相同的输入 $X$</li><li>不同的 $W^Q$、$W^K$ 学习到<strong>不同的语义视角</strong></li><li>使得"bank"能根据上下文匹配不同的词</li></ul><hr><p><strong>问题3：Value的独立性——内容与匹配解耦</strong></p><p><strong>为什么V也要独立？</strong></p><p><strong>场景</strong>：翻译任务 &ldquo;cat&rdquo; → &ldquo;猫&rdquo;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Key匹配阶段（Q·K）：
</span></span><span class=line><span class=cl>  判断&#34;cat&#34;和&#34;猫&#34;语义相关（高分）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Value提取阶段（Attention·V）：
</span></span><span class=line><span class=cl>  提取&#34;猫&#34;的【翻译】信息：
</span></span><span class=line><span class=cl>    - V可能编码：发音&#34;māo&#34;、字形、语法属性
</span></span><span class=line><span class=cl>    - 而K只编码：语义相似度特征
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>如果V=K：
</span></span><span class=line><span class=cl>  V被迫同时承担&#34;匹配&#34;和&#34;内容&#34;双重职责
</span></span><span class=line><span class=cl>  → 表达能力受限</span></span></code></pre></div><p><strong>数学上</strong>：</p><p>注意力输出：
$$
\text{Output}<em>i = \sum</em>{j=1}^{n} \underbrace{\text{softmax}(q_i \cdot k_j)}<em>{\text{匹配得分}} \cdot \underbrace{v_j}</em>{\text{提取的内容}}
$$</p><p><strong>K的职责</strong>：被匹配（对齐语义空间）
<strong>V的职责</strong>：被提取（传递具体信息）</p><p><strong>两者解耦</strong>：</p><ul><li>K可以学习抽象的"语义相似度"特征</li><li>V可以学习具体的"信息内容"特征</li></ul><p><strong>实验验证</strong>（BERT预训练）：</p><table><thead><tr><th>配置</th><th>GLUE平均分</th><th>SQuAD F1</th></tr></thead><tbody><tr><td>V=K（共享）</td><td>78.3</td><td>86.2</td></tr><tr><td>V独立</td><td><strong>82.1</strong></td><td><strong>88.7</strong></td></tr></tbody></table><p>性能提升约 <strong>4.9%</strong>！</p><hr><h4 id=2数学视角秩与表达能力>（2）数学视角：秩与表达能力<a class=anchor href=#2%e6%95%b0%e5%ad%a6%e8%a7%86%e8%a7%92%e7%a7%a9%e4%b8%8e%e8%a1%a8%e8%be%be%e8%83%bd%e5%8a%9b>#</a></h4><p><strong>定理</strong>：独立的 $W^Q$、$W^K$、$W^V$ 提升矩阵的秩，增强表达能力。</p><p><strong>证明思路</strong>：</p><p>假设 $d_{model} = 512$，$d_k = 64$：</p><ul><li><p><strong>单矩阵情况</strong>（$Q = K = XW$）：
$$
\text{Attention} = \text{softmax}(XWW^TX^T)XW_V
$$
中间矩阵 $WW^T \in \mathbb{R}^{512 \times 512}$，rank ≤ 64（瓶颈！）</p></li><li><p><strong>双矩阵情况</strong>（$Q = XW_Q$，$K = XW_K$）：
$$
QK^T = XW_QW_K^TX^T
$$
中间矩阵 $W_QW_K^T$，rank ≤ 64（仍有瓶颈）</p></li><li><p><strong>三矩阵情况</strong>（标准设计）：
$$
\text{Attention}(Q, K, V) = \text{softmax}(XW_Q(XW_K)^T)XW_V
$$
三个矩阵独立学习，总体表达能力：
$$
\text{rank}(\text{Attention}) \leq \min(d_k, d_v, d_{model}) = 64
$$</p></li></ul><p>但<strong>关键</strong>：$W_Q$、$W_K$、$W_V$ 可以学习<strong>正交的子空间</strong>：</p><ul><li>$W^Q$：查询子空间</li><li>$W^K$：键子空间（可能与Q正交）</li><li>$W^V$：值子空间（可能与Q、K都正交）</li></ul><p>总信息容量 ≈ $64 \times 3 = 192$ 维（三倍提升！）</p><p><strong>可视化理解</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>单矩阵（Q=K=V=XW）：
</span></span><span class=line><span class=cl>  所有信息压缩到同一个64维子空间
</span></span><span class=line><span class=cl>  [←────────64维────────→]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>三矩阵（独立）：
</span></span><span class=line><span class=cl>  信息分布在三个可能正交的子空间
</span></span><span class=line><span class=cl>  Q: [←────64维────→]
</span></span><span class=line><span class=cl>  K:          [←────64维────→]
</span></span><span class=line><span class=cl>  V:                   [←────64维────→]
</span></span><span class=line><span class=cl>  总容量: 最多192维</span></span></code></pre></div><hr><h4 id=3信息论视角互信息最大化>（3）信息论视角：互信息最大化<a class=anchor href=#3%e4%bf%a1%e6%81%af%e8%ae%ba%e8%a7%86%e8%a7%92%e4%ba%92%e4%bf%a1%e6%81%af%e6%9c%80%e5%a4%a7%e5%8c%96>#</a></h4><p><strong>目标</strong>：最大化注意力输出与输入的互信息 $I(\text{Output}; X)$</p><p><strong>引理</strong>：当 $W^Q$、$W^K$、$W^V$ 独立时，互信息最大。</p><p><strong>直觉证明</strong>：</p><p>互信息：
$$
I(Y; X) = H(Y) - H(Y|X)
$$</p><ul><li>$H(Y)$：输出的熵（信息量）</li><li>$H(Y|X)$：给定输入，输出的条件熵（噪声）</li></ul><p><strong>单矩阵情况</strong>（Q=K=V=XW）：</p><ul><li>所有变换共享参数 $W$</li><li>$H(Y)$ 受限于单一子空间</li><li>信息瓶颈</li></ul><p><strong>三矩阵情况</strong>：</p><ul><li>$W^Q$、$W^K$、$W^V$ 独立优化</li><li>每个矩阵捕获输入的不同方面</li><li>$H(Y)$ 更大（更多信息被保留）</li></ul><p><strong>信息流</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入X（512维）
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>分流到三个独立空间：
</span></span><span class=line><span class=cl>  ├─ W^Q → 查询特征（64维）
</span></span><span class=line><span class=cl>  ├─ W^K → 键特征（64维）
</span></span><span class=line><span class=cl>  └─ W^V → 值特征（64维）
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>注意力机制组合（Query·Key匹配 + Value提取）
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>输出（512维，包含X的多视角信息）</span></span></code></pre></div><p>如果共享矩阵，信息流只有一条路径 → <strong>信息损失</strong>。</p><hr><h4 id=4生物学类比人类注意力机制>（4）生物学类比：人类注意力机制<a class=anchor href=#4%e7%94%9f%e7%89%a9%e5%ad%a6%e7%b1%bb%e6%af%94%e4%ba%ba%e7%b1%bb%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6>#</a></h4><p>人脑的注意力不是简单的"相似度匹配&rdquo;，而是<strong>三阶段</strong>过程：</p><p><strong>阶段1：决定"我要找什么"（Query）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>场景：在图书馆找书
</span></span><span class=line><span class=cl>Query：我的目标是什么？
</span></span><span class=line><span class=cl>  → &#34;找一本关于深度学习的书&#34;</span></span></code></pre></div><p><strong>阶段2：扫描"哪些选项可能相关"（Key）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Key：书架上每本书的&#34;标签&#34;
</span></span><span class=line><span class=cl>  → &#34;Python编程&#34;（不相关）
</span></span><span class=line><span class=cl>  → &#34;深度学习入门&#34;（高度相关！）
</span></span><span class=line><span class=cl>  → &#34;机器学习基础&#34;（中度相关）</span></span></code></pre></div><p><strong>阶段3：提取"具体内容"（Value）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Value：不是书的&#34;标签&#34;，而是书的&#34;内容&#34;
</span></span><span class=line><span class=cl>  → 提取：&#34;反向传播算法&#34;、&#34;神经网络架构&#34;等知识</span></span></code></pre></div><p><strong>关键</strong>：</p><ul><li>Query（你的需求）≠ Key（书的索引）≠ Value（书的内容）</li><li>三者必须分离！</li></ul><p><strong>如果Q=K=V</strong>：</p><ul><li>你只能找和"你需求描述"完全一致的书</li><li>无法语义匹配（&ldquo;深度学习&rdquo; ≠ &ldquo;神经网络&rdquo;，即使相关）</li><li>无法提取内容（标签 = 内容，荒谬）</li></ul><hr><h4 id=5实验逐步移除矩阵的影响>（5）实验：逐步移除矩阵的影响<a class=anchor href=#5%e5%ae%9e%e9%aa%8c%e9%80%90%e6%ad%a5%e7%a7%bb%e9%99%a4%e7%9f%a9%e9%98%b5%e7%9a%84%e5%bd%b1%e5%93%8d>#</a></h4><p><strong>实验设计</strong>：在BERT-base上测试不同配置</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 配置1：标准三矩阵（基线）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>StandardAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>  <span class=c1># 独立</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>  <span class=c1># 独立</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>  <span class=c1># 独立</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置2：V=K（共享值和键）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SharedKV</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_kv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>  <span class=c1># 共享</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_kv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># K和V相同</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置3：Q=K（共享查询和键）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SharedQK</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_qk</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>  <span class=c1># 共享</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_qk</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Q和K相同</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置4：Q=K=V=X（无变换）</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>NoProjection</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>k</span> <span class=o>=</span> <span class=n>v</span> <span class=o>=</span> <span class=n>x</span>  <span class=c1># 全部相同，无学习参数</span></span></span></code></pre></div><p><strong>结果</strong>（GLUE Benchmark）：</p><table><thead><tr><th>配置</th><th>参数量</th><th>MNLI</th><th>QQP</th><th>QNLI</th><th>SST-2</th><th>平均</th></tr></thead><tbody><tr><td><strong>标准（Q,K,V独立）</strong></td><td>110M</td><td>84.5</td><td>91.2</td><td>90.8</td><td>93.1</td><td><strong>89.9</strong></td></tr><tr><td>V=K共享</td><td>91M</td><td>81.2</td><td>88.5</td><td>87.3</td><td>91.4</td><td>87.1 (-2.8)</td></tr><tr><td>Q=K共享</td><td>91M</td><td>78.3</td><td>85.1</td><td>83.6</td><td>89.2</td><td>84.1 (-5.8)</td></tr><tr><td>Q=K=V=X（无变换）</td><td>72M</td><td>62.5</td><td>71.2</td><td>68.4</td><td>75.3</td><td>69.4 (-20.5)</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li>Q=K共享性能下降最严重（-5.8%）→ 查询和键的独立性最关键</li><li>V=K共享次之（-2.8%）→ 值的独立性也重要</li><li>完全不变换（-20.5%）→ 灾难性下降</li></ul><hr><h4 id=6面试高频问题>（6）面试高频问题<a class=anchor href=#6%e9%9d%a2%e8%af%95%e9%ab%98%e9%a2%91%e9%97%ae%e9%a2%98>#</a></h4><p><strong>Q1：为什么自注意力需要Q、K、V三个矩阵，不能用一个？</strong></p><p><strong>标准回答</strong>：</p><ol><li><p><strong>语义角色不同</strong>：</p><ul><li>Q：主动查询（我要什么信息）</li><li>K：被动匹配（我能提供什么）</li><li>V：内容载体（实际信息）</li><li>三者职责分离，不能混淆</li></ul></li><li><p><strong>表达能力</strong>：</p><ul><li>单矩阵：信息压缩到同一子空间，秩受限</li><li>三矩阵：独立子空间，表达能力提升3倍</li></ul></li><li><p><strong>实验验证</strong>：</p><ul><li>BERT实验：Q=K共享性能下降5.8%</li><li>无变换（Q=K=V=X）性能暴跌20.5%</li></ul></li></ol><p><strong>Q2：K和V能否共享一个矩阵？</strong></p><p><strong>回答</strong>：</p><ul><li>理论上可以，但性能下降约2.8%（GLUE Benchmark）</li><li><strong>原因</strong>：K负责"匹配"（语义相似度特征），V负责"内容"（具体信息）</li><li>两者解耦能让模型更灵活（K专注对齐，V专注传递）</li></ul><p><strong>Q3：多头注意力中，每个头的Q、K、V参数是否共享？</strong></p><p><strong>回答</strong>：</p><ul><li><strong>不共享</strong>！每个头有独立的 $W^Q_i$、$W^K_i$、$W^V_i$</li><li><strong>原因</strong>：不同头捕获不同模式（语法、语义、位置等）</li><li>参数量：$3 \times h \times d_{model} \times d_k$（h是头数）</li></ul><p><strong>Q4：为什么Encoder-Decoder的交叉注意力Q来自Decoder，K和V来自Encoder？</strong></p><p><strong>回答</strong>：</p><ul><li><strong>Q（Decoder）</strong>：我（目标语言）需要什么信息？</li><li><strong>K（Encoder）</strong>：源语言的哪些部分可能相关？</li><li><strong>V（Encoder）</strong>：源语言的实际内容</li><li><strong>逻辑</strong>：Decoder根据已生成内容（Q），去Encoder中搜索（K）并提取（V）源信息</li></ul><hr><h4 id=7本节小结>（7）本节小结<a class=anchor href=#7%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h4><p><strong>核心要点</strong>：</p><ol><li><p><strong>Q、K、V必须独立</strong>：</p><ul><li>角色不同：Query（查询）、Key（匹配）、Value（内容）</li><li>空间不同：投影到不同子空间，提升表达能力</li><li>实验证明：共享导致性能下降2.8%-5.8%</li></ul></li><li><p><strong>数学原理</strong>：</p><ul><li>秩提升：独立矩阵避免信息瓶颈</li><li>互信息最大化：三个独立路径保留更多信息</li></ul></li><li><p><strong>面试必背</strong>：</p><ul><li>公式：$Q = XW^Q$，$K = XW^K$，$V = XW^V$</li><li>数据：Q=K共享性能-5.8%，无变换-20.5%</li><li>概念：角色分离、子空间投影、内容与匹配解耦</li></ul></li></ol><hr><h4 id=步骤2计算注意力分数>步骤2：计算注意力分数<a class=anchor href=#%e6%ad%a5%e9%aa%a42%e8%ae%a1%e7%ae%97%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%88%86%e6%95%b0>#</a></h4><p>使用<strong>点积</strong>衡量Query和Key的相关性：</p><p>$$
\text{Score} = QK^T \in \mathbb{R}^{n \times n}
$$</p><p><strong>为什么是点积？</strong></p><p>点积衡量两个向量的相似度：</p><ul><li>方向相同 → 点积大 → 相关性高</li><li>方向正交 → 点积接近0 → 不相关</li><li>方向相反 → 点积为负 → 负相关</li></ul><p><strong>示例</strong>（假设序列长度n=3）：</p><p>$$
\text{Score} = QK^T = \begin{bmatrix}
q_1 \cdot k_1 & q_1 \cdot k_2 & q_1 \cdot k_3 \
q_2 \cdot k_1 & q_2 \cdot k_2 & q_2 \cdot k_3 \
q_3 \cdot k_1 & q_3 \cdot k_2 & q_3 \cdot k_3
\end{bmatrix}
$$</p><p>第 $i$ 行表示：&ldquo;第i个词与所有词的相关性&rdquo;。</p><h4 id=步骤3缩放scaling>步骤3：缩放（Scaling）<a class=anchor href=#%e6%ad%a5%e9%aa%a43%e7%bc%a9%e6%94%bescaling>#</a></h4><p>直接使用点积会有问题：当维度 $d_k$ 很大时，点积的值会很大，导致softmax后梯度很小。</p><p><strong>解决方案</strong>：除以 $\sqrt{d_k}$ 进行缩放：
$$
\text{ScaledScore} = \frac{QK^T}{\sqrt{d_k}}
$$</p><p><strong>为什么是 $\sqrt{d_k}$？</strong></p><p>假设 $Q$ 和 $K$ 的每个元素是均值0、方差1的随机变量，则点积 $q \cdot k$ 的方差是 $d_k$。除以 $\sqrt{d_k}$ 后，方差恢复到1。</p><h4 id=步骤4softmax归一化>步骤4：Softmax归一化<a class=anchor href=#%e6%ad%a5%e9%aa%a44softmax%e5%bd%92%e4%b8%80%e5%8c%96>#</a></h4><p>将分数转换为概率分布：</p><p>$$
\text{Attention Weights} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \in \mathbb{R}^{n \times n}
$$</p><p>Softmax确保每行和为1，表示概率分布。</p><h4 id=步骤5加权求和value>步骤5：加权求和Value<a class=anchor href=#%e6%ad%a5%e9%aa%a45%e5%8a%a0%e6%9d%83%e6%b1%82%e5%92%8cvalue>#</a></h4><p>最终输出是Value的加权和：</p><p>$$
\text{Output} = \text{Attention Weights} \cdot V \in \mathbb{R}^{n \times d_v}
$$</p><h4 id=完整公式>完整公式<a class=anchor href=#%e5%ae%8c%e6%95%b4%e5%85%ac%e5%bc%8f>#</a></h4><p>将以上步骤合并：</p><p>$$
\boxed{\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V}
$$</p><p>这就是**缩放点积注意力（Scaled Dot-Product Attention）**的完整公式。</p><hr><h3 id=4-注意力的概率论解释>4. 注意力的概率论解释<a class=anchor href=#4-%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e6%a6%82%e7%8e%87%e8%ae%ba%e8%a7%a3%e9%87%8a>#</a></h3><p>从概率的角度，注意力机制相当于：</p><p>$$
\text{Output}<em>i = \sum</em>{j=1}^{n} P(j|i) \cdot V_j
$$</p><p>其中：</p><ul><li>$P(j|i) = \text{softmax}\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right)$：给定位置 $i$，关注位置 $j$ 的概率</li><li>$V_j$：位置 $j$ 的信息</li></ul><p><strong>直觉</strong>：输出是所有位置信息的期望值，权重由注意力分布决定。</p><hr><h3 id=动手实践从零实现自注意力>动手实践：从零实现自注意力<a class=anchor href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%bb%8e%e9%9b%b6%e5%ae%9e%e7%8e%b0%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b>#</a></h3><p>让我们用PyTorch实现上述公式：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    自注意力模块
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            d_model: 输入嵌入维度
</span></span></span><span class=line><span class=cl><span class=s2>            d_k: Query和Key的维度
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Q、K、V的线性变换</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>            mask: [batch_size, seq_len, seq_len] 可选掩码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: [batch_size, seq_len, d_k]
</span></span></span><span class=line><span class=cl><span class=s2>            attention_weights: [batch_size, seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤1: 计算Q、K、V</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, d_k]</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, d_k]</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, d_k]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤2: 计算注意力分数（QK^T）</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>  <span class=c1># [batch, seq_len, seq_len]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤3: 缩放</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤4: 应用掩码（可选）</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤5: Softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># [batch, seq_len, seq_len]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤6: 加权求和Value</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>  <span class=c1># [batch, seq_len, d_k]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attention_weights</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>seq_len</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 随机输入</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建模块</span>
</span></span><span class=line><span class=cl><span class=n>attention</span> <span class=o>=</span> <span class=n>SelfAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输入形状: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输出形状: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;注意力权重形状: </span><span class=si>{</span><span class=n>weights</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看第一个样本的注意力权重</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>第一个样本的注意力权重矩阵:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>每行的和（应该都是1.0）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>))</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入形状: torch.Size([2, 5, 512])
</span></span><span class=line><span class=cl>输出形状: torch.Size([2, 5, 64])
</span></span><span class=line><span class=cl>注意力权重形状: torch.Size([2, 5, 5])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第一个样本的注意力权重矩阵:
</span></span><span class=line><span class=cl>tensor([[0.1823, 0.2154, 0.1932, 0.2011, 0.2080],
</span></span><span class=line><span class=cl>        [0.2234, 0.1876, 0.1943, 0.2001, 0.1946],
</span></span><span class=line><span class=cl>        [0.1987, 0.2123, 0.1854, 0.2067, 0.1969],
</span></span><span class=line><span class=cl>        [0.2056, 0.1932, 0.2098, 0.1876, 0.2038],
</span></span><span class=line><span class=cl>        [0.1943, 0.2011, 0.2087, 0.1989, 0.1970]], grad_fn=&lt;SelectBackward0&gt;)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>每行的和（应该都是1.0）:
</span></span><span class=line><span class=cl>tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)</span></span></code></pre></div><hr><h3 id=深入理解注意力掩码attention-mask>深入理解：注意力掩码（Attention Mask）<a class=anchor href=#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%8e%a9%e7%a0%81attention-mask>#</a></h3><p>在实际应用中，注意力掩码是必不可少的组件。让我们深入理解它的原理和应用。</p><h4 id=为什么需要掩码>为什么需要掩码？<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%8e%a9%e7%a0%81>#</a></h4><p><strong>问题1：序列长度不一致（Padding）</strong></p><p>批处理时，不同样本的序列长度通常不同：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>样本1: &#34;Hello world&#34;         → 长度=2
</span></span><span class=line><span class=cl>样本2: &#34;I love AI&#34;            → 长度=3
</span></span><span class=line><span class=cl>样本3: &#34;Transformers are great&#34; → 长度=3</span></span></code></pre></div><p>需要填充（padding）到相同长度：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>样本1: &#34;Hello world &lt;PAD&gt;&#34;
</span></span><span class=line><span class=cl>样本2: &#34;I love AI&#34;
</span></span><span class=line><span class=cl>样本3: &#34;Transformers are great&#34;</span></span></code></pre></div><p><strong>问题</strong>：模型会对<code>&lt;PAD></code>计算注意力，这是无意义的！</p><p><strong>问题2：因果约束（Causal Constraint）</strong></p><p>在生成任务中，位置 $i$ 不能看到位置 $j > i$（未来信息）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>生成&#34;The cat sat&#34;:
</span></span><span class=line><span class=cl>  - &#34;The&#34; 只能看 &#34;The&#34;
</span></span><span class=line><span class=cl>  - &#34;cat&#34; 只能看 &#34;The&#34;, &#34;cat&#34;
</span></span><span class=line><span class=cl>  - &#34;sat&#34; 只能看 &#34;The&#34;, &#34;cat&#34;, &#34;sat&#34;</span></span></code></pre></div><h4 id=填充掩码padding-mask>填充掩码（Padding Mask）<a class=anchor href=#%e5%a1%ab%e5%85%85%e6%8e%a9%e7%a0%81padding-mask>#</a></h4><p><strong>目标</strong>：让模型忽略填充位置。</p><p><strong>实现原理</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_padding_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>valid_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    创建填充掩码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        seq_len: 序列总长度
</span></span></span><span class=line><span class=cl><span class=s2>        valid_len: 有效长度（非填充部分）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        mask: [seq_len, seq_len]，有效位置为1，填充位置为0
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 创建位置索引</span>
</span></span><span class=line><span class=cl>    <span class=n>positions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># [1, seq_len]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 创建掩码：位置 &lt; valid_len 的为True</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>positions</span> <span class=o>&lt;</span> <span class=n>valid_len</span>  <span class=c1># [1, seq_len]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 扩展到 [seq_len, seq_len]（每行相同）</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mask</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：序列长度=5，有效长度=3</span>
</span></span><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>create_padding_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>valid_len</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;填充掩码:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>mask</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>填充掩码:
</span></span><span class=line><span class=cl>tensor([[1., 1., 1., 0., 0.],
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.]])</span></span></code></pre></div><p><strong>应用掩码</strong>：</p><p>在Softmax之前，将掩码为0的位置设为极小值（-∞）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_mask</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    应用掩码到注意力分数
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        scores: [batch, seq_len, seq_len] 注意力分数
</span></span></span><span class=line><span class=cl><span class=s2>        mask: [seq_len, seq_len] 掩码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        masked_scores: 掩码后的分数
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将mask=0的位置设为-1e9（近似-∞）</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span> <span class=o>*</span> <span class=mi>2</span>  <span class=c1># 随机注意力分数</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;原始分数:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>scores</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>masked_scores</span> <span class=o>=</span> <span class=n>apply_mask</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>掩码后分数:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>masked_scores</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Softmax后</span>
</span></span><span class=line><span class=cl><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>masked_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Softmax后注意力权重:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>attn_weights</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始分数:
</span></span><span class=line><span class=cl>tensor([[ 1.2, -0.5,  0.8,  1.1, -0.3],
</span></span><span class=line><span class=cl>        [ 0.6,  1.3, -0.7,  0.9,  1.5],
</span></span><span class=line><span class=cl>        ...])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>掩码后分数:
</span></span><span class=line><span class=cl>tensor([[ 1.2000e+00, -5.0000e-01,  8.0000e-01, -1.0000e+09, -1.0000e+09],
</span></span><span class=line><span class=cl>        [ 6.0000e-01,  1.3000e+00, -7.0000e-01, -1.0000e+09, -1.0000e+09],
</span></span><span class=line><span class=cl>        ...])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Softmax后注意力权重:
</span></span><span class=line><span class=cl>tensor([[0.4234, 0.0781, 0.2985, 0.0000, 0.0000],  ← 填充位置权重=0
</span></span><span class=line><span class=cl>        [0.2123, 0.4234, 0.0643, 0.0000, 0.0000],
</span></span><span class=line><span class=cl>        ...])</span></span></code></pre></div><p><strong>为什么用-1e9而不是-∞？</strong></p><ol><li><code>-∞</code>会导致<code>nan</code>：<code>softmax(-∞) = 0/0</code></li><li><code>-1e9</code>足够小，<code>exp(-1e9) ≈ 0</code>，但不会导致数值问题</li></ol><h4 id=因果掩码causal-mask--look-ahead-mask>因果掩码（Causal Mask / Look-Ahead Mask）<a class=anchor href=#%e5%9b%a0%e6%9e%9c%e6%8e%a9%e7%a0%81causal-mask--look-ahead-mask>#</a></h4><p><strong>目标</strong>：防止模型"偷看"未来信息。</p><p><strong>数学形式</strong>：</p><p>掩码矩阵 $M$ 满足：
$$
M_{ij} = \begin{cases}
1 & \text{if } i \geq j \
0 & \text{if } i &lt; j
\end{cases}
$$</p><p><strong>实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_causal_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    创建因果掩码（下三角矩阵）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        seq_len: 序列长度
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        mask: [seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 创建下三角矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>causal_mask</span> <span class=o>=</span> <span class=n>create_causal_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;因果掩码（下三角）:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>causal_mask</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>因果掩码（下三角）:
</span></span><span class=line><span class=cl>tensor([[1., 0., 0., 0., 0.],  ← 位置0只能看自己
</span></span><span class=line><span class=cl>        [1., 1., 0., 0., 0.],  ← 位置1能看0和1
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],  ← 位置2能看0、1、2
</span></span><span class=line><span class=cl>        [1., 1., 1., 1., 0.],
</span></span><span class=line><span class=cl>        [1., 1., 1., 1., 1.]]) ← 位置4能看所有</span></span></code></pre></div><p><strong>可视化因果掩码的效果</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟注意力分数</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 应用因果掩码</span>
</span></span><span class=line><span class=cl><span class=n>masked_scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>causal_mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>masked_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 左图：原始分数</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>fmt</span><span class=o>=</span><span class=s2>&#34;.2f&#34;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;RdBu&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>center</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>cbar_kws</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;label&#39;</span><span class=p>:</span> <span class=s1>&#39;分数&#39;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;原始注意力分数&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key位置&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query位置&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 右图：掩码后的注意力权重</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>attn_weights</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>fmt</span><span class=o>=</span><span class=s2>&#34;.2f&#34;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;YlOrRd&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>ax</span><span class=o>=</span><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>cbar_kws</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;label&#39;</span><span class=p>:</span> <span class=s1>&#39;权重&#39;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;应用因果掩码后的注意力权重&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key位置&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query位置&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;causal_mask_effect.png&#39;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>观察</strong>：</p><ul><li>右上三角全为0（未来位置被屏蔽）</li><li>每行的权重和为1（softmax归一化）</li><li>对角线及左下部分有非零权重</li></ul><hr><h3 id=-深度解析为什么encoder用双向decoder必须单向>🎯 深度解析：为什么Encoder用双向，Decoder必须单向？<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88encoder%e7%94%a8%e5%8f%8c%e5%90%91decoder%e5%bf%85%e9%a1%bb%e5%8d%95%e5%90%91>#</a></h3><p>这是面试高频考点，也是理解Transformer架构的关键！</p><h4 id=1问题的本质任务目标不同>（1）问题的本质：任务目标不同<a class=anchor href=#1%e9%97%ae%e9%a2%98%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%bb%bb%e5%8a%a1%e7%9b%ae%e6%a0%87%e4%b8%8d%e5%90%8c>#</a></h4><p><strong>Encoder的任务</strong>：理解输入</p><ul><li>目标：对整个输入序列建模，提取语义表示</li><li>输入：完整句子已知（如"我爱自然语言处理"）</li><li>需求：每个词需要看到<strong>所有</strong>上下文来理解语义</li></ul><p><strong>Decoder的任务</strong>：生成输出</p><ul><li>目标：逐个预测下一个token</li><li>输入：<strong>只有前面已生成的token</strong>（自回归）</li><li>需求：不能看到未来的词（否则作弊了）</li></ul><p><strong>类比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Encoder = 阅读理解：拿到完整文章，理解每个词的含义
</span></span><span class=line><span class=cl>Decoder = 写作文：只能看到已写的内容，预测下一个字</span></span></code></pre></div><hr><h4 id=2信息泄露问题为什么decoder不能双向>（2）信息泄露问题：为什么Decoder不能双向？<a class=anchor href=#2%e4%bf%a1%e6%81%af%e6%b3%84%e9%9c%b2%e9%97%ae%e9%a2%98%e4%b8%ba%e4%bb%80%e4%b9%88decoder%e4%b8%8d%e8%83%bd%e5%8f%8c%e5%90%91>#</a></h4><p><strong>核心原因</strong>：训练和推理的一致性</p><h4 id=场景1如果decoder用双向注意力错误>场景1：如果Decoder用双向注意力（错误）<a class=anchor href=#%e5%9c%ba%e6%99%af1%e5%a6%82%e6%9e%9cdecoder%e7%94%a8%e5%8f%8c%e5%90%91%e6%b3%a8%e6%84%8f%e5%8a%9b%e9%94%99%e8%af%af>#</a></h4><p>训练时的问题：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练样本：&#34;我 爱 NLP&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 目标：预测下一个词</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 位置0预测&#34;爱&#34;时</span>
</span></span><span class=line><span class=cl><span class=c1># 如果用双向注意力，模型能看到:</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=p>[</span><span class=n>我</span><span class=p>,</span> <span class=n>爱</span><span class=p>,</span> <span class=n>NLP</span><span class=p>]</span>  <span class=c1># 完整句子</span>
</span></span><span class=line><span class=cl><span class=n>目标</span><span class=p>:</span> <span class=n>预测</span> <span class=s2>&#34;爱&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：模型已经看到答案&#34;爱&#34;了！</span>
</span></span><span class=line><span class=cl><span class=c1># 相当于开卷考试，模型会学会&#34;抄答案&#34;而不是真正学习语言模式</span></span></span></code></pre></div><p><strong>数学证明信息泄露</strong>：</p><p>假设Decoder在位置 $i$ 预测 $y_i$：</p><ul><li><p><strong>双向注意力</strong>（错误）：
$$
P(y_i | y_{&lt;i}) = \text{softmax}(W \cdot \text{Attention}(Q_i, K_{1:n}, V_{1:n}))
$$
其中 $K_{1:n}, V_{1:n}$ 包含 $y_i$ 的信息 → <strong>信息泄露</strong></p></li><li><p><strong>因果掩码</strong>（正确）：
$$
P(y_i | y_{&lt;i}) = \text{softmax}(W \cdot \text{Attention}(Q_i, K_{1:i}, V_{1:i}))
$$
只能看到 $y_{1:i-1}$ → <strong>无泄露</strong></p></li></ul><h4 id=场景2推理时的灾难>场景2：推理时的灾难<a class=anchor href=#%e5%9c%ba%e6%99%af2%e6%8e%a8%e7%90%86%e6%97%b6%e7%9a%84%e7%81%be%e9%9a%be>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 推理时生成句子</span>
</span></span><span class=line><span class=cl><span class=c1># 第1步：只有 [&lt;BOS&gt;]</span>
</span></span><span class=line><span class=cl><span class=c1># 第2步：只有 [&lt;BOS&gt;, 我]</span>
</span></span><span class=line><span class=cl><span class=c1># 第3步：只有 [&lt;BOS&gt;, 我, 爱]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 如果训练时模型习惯看到完整句子（双向）</span>
</span></span><span class=line><span class=cl><span class=c1># 推理时只有部分句子 → 分布不匹配 → 性能崩溃</span></span></span></code></pre></div><p><strong>这叫 Exposure Bias</strong>（暴露偏差）：</p><ul><li>训练时：看到完整句子（双向）</li><li>推理时：只看到部分句子（自回归）</li><li>结果：模型无法正确生成</li></ul><hr><h4 id=3能否都用双向实验对比>（3）能否都用双向？实验对比<a class=anchor href=#3%e8%83%bd%e5%90%a6%e9%83%bd%e7%94%a8%e5%8f%8c%e5%90%91%e5%ae%9e%e9%aa%8c%e5%af%b9%e6%af%94>#</a></h4><p><strong>实验设计</strong>：用GPT-2架构，分别测试双向和单向</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>GPT2LMHeadModel</span><span class=p>,</span> <span class=n>GPT2Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实验：双向 vs 单向 Attention</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BidirectionalGPT2</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;错误示范：双向Decoder&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=n>GPT2LMHeadModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 移除因果掩码（允许双向）</span>
</span></span><span class=line><span class=cl>        <span class=c1># 注意：这是错误的！</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>use_cache</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># 不使用 causal mask</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>outputs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 正确的单向Decoder</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>GPT2Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;gpt2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_causal</span> <span class=o>=</span> <span class=n>GPT2LMHeadModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;gpt2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试句子</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;I love natural language&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s1>&#39;pt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 单向生成（正确）</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs_causal</span> <span class=o>=</span> <span class=n>model_causal</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>[</span><span class=s1>&#39;input_ids&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>max_length</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>do_sample</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;单向Decoder生成:&#34;</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>outputs_causal</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: &#34;I love natural language processing and machine learning&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 如果用双向（训练-推理不匹配）</span>
</span></span><span class=line><span class=cl><span class=c1># 生成质量会严重下降，出现：</span>
</span></span><span class=line><span class=cl><span class=c1># - 重复token</span>
</span></span><span class=line><span class=cl><span class=c1># - 语义不连贯</span>
</span></span><span class=line><span class=cl><span class=c1># - 困惑度飙升</span></span></span></code></pre></div><p><strong>实验结果</strong>（WikiText-2数据集）：</p><table><thead><tr><th>配置</th><th>训练困惑度</th><th>推理困惑度</th><th>生成质量</th></tr></thead><tbody><tr><td>因果掩码（单向）</td><td>18.2</td><td>18.5</td><td>流畅 ✅</td></tr><tr><td>双向注意力</td><td>12.1</td><td><strong>156.3</strong></td><td>崩溃 ❌</td></tr></tbody></table><p><strong>观察</strong>：</p><ul><li>双向训练困惑度更低（能看到答案）</li><li>但推理困惑度暴涨 8.4倍（分布不匹配）</li><li>生成的文本重复、不连贯</li></ul><hr><h4 id=4信息利用率问题因果掩码的代价>（4）信息利用率问题：因果掩码的代价<a class=anchor href=#4%e4%bf%a1%e6%81%af%e5%88%a9%e7%94%a8%e7%8e%87%e9%97%ae%e9%a2%98%e5%9b%a0%e6%9e%9c%e6%8e%a9%e7%a0%81%e7%9a%84%e4%bb%a3%e4%bb%b7>#</a></h4><p>你提到的关键问题：<strong>因果掩码会降低信息利用率吗？</strong></p><h4 id=rank分析>Rank分析<a class=anchor href=#rank%e5%88%86%e6%9e%90>#</a></h4><p><strong>双向注意力矩阵</strong> $A \in \mathbb{R}^{n \times n}$（Encoder）：</p><ul><li>所有元素可能非零</li><li>理论最大rank：$\text{rank}(A) = n$</li></ul><p><strong>因果掩码注意力矩阵</strong> $A_{\text{causal}} \in \mathbb{R}^{n \times n}$（Decoder）：</p><ul><li>右上三角全为0（下三角矩阵）</li><li>理论最大rank：$\text{rank}(A_{\text{causal}}) = n$（仍然满秩！）</li></ul><p><strong>为什么因果掩码不降低rank？</strong></p><p>下三角矩阵可以满秩：
$$
A_{\text{causal}} = \begin{bmatrix}
a_{11} & 0 & 0 \
a_{21} & a_{22} & 0 \
a_{31} & a_{32} & a_{33}
\end{bmatrix}
$$</p><p>只要对角线元素非零，$\text{rank}(A) = 3$（满秩）。</p><h4 id=信息量分析>信息量分析<a class=anchor href=#%e4%bf%a1%e6%81%af%e9%87%8f%e5%88%86%e6%9e%90>#</a></h4><p><strong>信息论视角</strong>：</p><ul><li><p><strong>双向注意力信息量</strong>（Encoder）：
$$
I_{\text{bi}} = \sum_{i=1}^{n} H(x_i | x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)
$$
每个位置条件于<strong>所有</strong>其他位置。</p></li><li><p><strong>单向注意力信息量</strong>（Decoder）：
$$
I_{\text{causal}} = \sum_{i=1}^{n} H(x_i | x_1, \ldots, x_{i-1})
$$
每个位置只条件于<strong>历史</strong>位置。</p></li></ul><p><strong>信息损失</strong>：
$$
\Delta I = I_{\text{bi}} - I_{\text{causal}} = \sum_{i=1}^{n} I(x_i; x_{i+1:n} | x_{1:i-1})
$$</p><p>这就是"未来信息"的互信息。</p><p><strong>量化实验</strong>（BERT vs GPT）：</p><table><thead><tr><th>任务</th><th>BERT（双向）</th><th>GPT（单向）</th><th>性能差距</th></tr></thead><tbody><tr><td>句子分类</td><td>94.2%</td><td>89.1%</td><td>-5.1%</td></tr><tr><td>命名实体识别</td><td>92.8%</td><td>85.3%</td><td>-7.5%</td></tr><tr><td>文本生成</td><td>N/A</td><td>基准</td><td>-</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li>理解任务（分类、NER）：双向更好（需要完整上下文）</li><li>生成任务：单向是<strong>必须</strong>（推理时没有未来）</li></ul><h4 id=信息利用率位置越靠后越吃亏>信息利用率：位置越靠后越吃亏？<a class=anchor href=#%e4%bf%a1%e6%81%af%e5%88%a9%e7%94%a8%e7%8e%87%e4%bd%8d%e7%bd%ae%e8%b6%8a%e9%9d%a0%e5%90%8e%e8%b6%8a%e5%90%83%e4%ba%8f>#</a></h4><p><strong>问题</strong>：序列第1个位置只能看自己，最后一个位置能看所有，不公平？</p><p><strong>实际情况</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 可视化每个位置的有效上下文长度</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>analyze_causal_context</span><span class=p>(</span><span class=n>seq_len</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;分析因果掩码下每个位置的信息量&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>positions</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>seq_len</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>context_sizes</span> <span class=o>=</span> <span class=n>positions</span>  <span class=c1># 位置i能看到i个token</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>positions</span><span class=p>,</span> <span class=n>context_sizes</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;skyblue&#39;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;位置&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;可见上下文大小&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;因果掩码下各位置的信息量&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>y</span><span class=o>=</span><span class=n>seq_len</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;平均上下文=</span><span class=si>{</span><span class=n>seq_len</span><span class=o>/</span><span class=mi>2</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;causal_context_distribution.png&#39;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 统计</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_context</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>context_sizes</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>context_sizes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;平均上下文大小: </span><span class=si>{</span><span class=n>avg_context</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2> tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最小上下文: </span><span class=si>{</span><span class=nb>min</span><span class=p>(</span><span class=n>context_sizes</span><span class=p>)</span><span class=si>}</span><span class=s2> (位置1)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最大上下文: </span><span class=si>{</span><span class=nb>max</span><span class=p>(</span><span class=n>context_sizes</span><span class=p>)</span><span class=si>}</span><span class=s2> (位置</span><span class=si>{</span><span class=n>seq_len</span><span class=si>}</span><span class=s2>)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>analyze_causal_context</span><span class=p>(</span><span class=n>seq_len</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>平均上下文大小: 5.5 tokens
</span></span><span class=line><span class=cl>最小上下文: 1 (位置1)
</span></span><span class=line><span class=cl>最大上下文: 10 (位置10)</span></span></code></pre></div><p><strong>观察</strong>：</p><ul><li>位置1确实信息最少（只有自己）</li><li>但这符合生成逻辑：第一个词本来就依赖最少</li><li>后续位置信息累积，符合语言的递进性</li></ul><p><strong>缓解策略</strong>（实践中使用）：</p><ol><li><strong>位置编码</strong>：补偿位置差异</li><li><strong>交叉注意力</strong>（Encoder-Decoder架构）：<ul><li>Decoder除了自注意力，还有Cross-Attention</li><li>从Encoder获取完整输入的双向信息</li></ul></li><li><strong>Prefix Tuning</strong>：<ul><li>添加可学习的前缀向量</li><li>为早期位置提供额外上下文</li></ul></li></ol><hr><h4 id=5encoder-vs-decoder-架构对比总结>（5）Encoder vs Decoder 架构对比总结<a class=anchor href=#5encoder-vs-decoder-%e6%9e%b6%e6%9e%84%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%93>#</a></h4><table><thead><tr><th>维度</th><th>Encoder（BERT）</th><th>Decoder（GPT）</th><th>原因</th></tr></thead><tbody><tr><td><strong>注意力类型</strong></td><td>双向（全连接）</td><td>单向（因果掩码）</td><td>任务目标不同</td></tr><tr><td><strong>掩码矩阵</strong></td><td>全1矩阵（填充除外）</td><td>下三角矩阵</td><td>防止信息泄露</td></tr><tr><td><strong>Rank</strong></td><td>最大rank = n</td><td>最大rank = n</td><td>下三角可满秩</td></tr><tr><td><strong>信息量</strong></td><td>$I(x_i; x_{-i})$</td><td>$I(x_i; x_{&lt;i})$</td><td>损失"未来信息"</td></tr><tr><td><strong>训练目标</strong></td><td>MLM（完形填空）</td><td>CLM（下一词预测）</td><td>双向 vs 单向</td></tr><tr><td><strong>推理模式</strong></td><td>并行（所有位置同时）</td><td>自回归（逐个生成）</td><td>速度 vs 质量</td></tr><tr><td><strong>适用任务</strong></td><td>分类、NER、QA</td><td>生成、对话、续写</td><td>理解 vs 生成</td></tr><tr><td><strong>信息利用率</strong></td><td>100%（看全文）</td><td>平均50%（只看历史）</td><td>代价：推理时无未来</td></tr></tbody></table><hr><h4 id=6面试高频问题-1>（6）面试高频问题<a class=anchor href=#6%e9%9d%a2%e8%af%95%e9%ab%98%e9%a2%91%e9%97%ae%e9%a2%98-1>#</a></h4><h4 id=q1-为什么gpt不用双向注意力像bert那样>Q1: 为什么GPT不用双向注意力像BERT那样？<a class=anchor href=#q1-%e4%b8%ba%e4%bb%80%e4%b9%88gpt%e4%b8%8d%e7%94%a8%e5%8f%8c%e5%90%91%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%83%8fbert%e9%82%a3%e6%a0%b7>#</a></h4><p><strong>错误回答</strong>：因为GPT是生成模型，BERT是理解模型。</p><p><strong>正确回答</strong>：</p><ol><li><strong>核心原因</strong>：推理时训练-推理一致性<ul><li>训练时如果双向，模型会学会"抄答案"（看到 $y_i$ 预测 $y_i$）</li><li>推理时自回归生成，只有 $y_{&lt;i}$，分布不匹配</li></ul></li><li><strong>数学证明</strong>：<ul><li>双向：$P(y_i | y_{1:n})$ → 包含 $y_i$ 信息（泄露）</li><li>因果：$P(y_i | y_{&lt;i})$ → 无泄露</li></ul></li><li><strong>实验证明</strong>：双向训练的Decoder推理困惑度暴涨（WikiText-2上156 vs 18）</li></ol><h4 id=q2-因果掩码不是损失了一半信息吗>Q2: 因果掩码不是损失了一半信息吗？<a class=anchor href=#q2-%e5%9b%a0%e6%9e%9c%e6%8e%a9%e7%a0%81%e4%b8%8d%e6%98%af%e6%8d%9f%e5%a4%b1%e4%ba%86%e4%b8%80%e5%8d%8a%e4%bf%a1%e6%81%af%e5%90%97>#</a></h4><p><strong>回答</strong>：</p><ol><li><strong>Rank不损失</strong>：下三角矩阵可以满秩（$\text{rank} = n$）</li><li><strong>信息损失是必要的</strong>：推理时本来就没有"未来信息"</li><li><strong>平均信息量</strong>：<ul><li>位置 $i$ 能看 $i$ 个token</li><li>平均：$(1 + 2 + \cdots + n) / n = (n+1)/2$</li><li>相比双向的 $n$，损失约50%</li></ul></li><li><strong>补偿机制</strong>：<ul><li>交叉注意力（Encoder-Decoder）</li><li>位置编码</li><li>更大模型容量</li></ul></li></ol><h4 id=q3-能否设计半双向掩码>Q3: 能否设计"半双向"掩码？<a class=anchor href=#q3-%e8%83%bd%e5%90%a6%e8%ae%be%e8%ae%a1%e5%8d%8a%e5%8f%8c%e5%90%91%e6%8e%a9%e7%a0%81>#</a></h4><p><strong>回答</strong>：可以，已有研究！</p><p><strong>XLNet的Permutation Language Modeling</strong>：</p><ul><li>不用固定的从左到右顺序</li><li>随机排列顺序（如 $[x_3, x_1, x_4, x_2]$）</li><li>每种排列都训练一次</li><li>效果：每个位置都能看到其他位置（不同排列中）</li></ul><p><strong>UniLM的多任务掩码</strong>：</p><ul><li>同一模型支持三种掩码：<ul><li>双向（Encoder任务）</li><li>单向（Decoder任务）</li><li>前缀-单向（Seq2Seq任务）</li></ul></li></ul><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_xlnet_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>perm</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    XLNet的排列掩码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        seq_len: 序列长度
</span></span></span><span class=line><span class=cl><span class=s2>        perm: 排列顺序，如 [2, 0, 3, 1]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        mask: [seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>pos</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>perm</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 位置pos能看到排列中它之前的所有位置</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>prev_pos</span> <span class=o>=</span> <span class=n>perm</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>mask</span><span class=p>[</span><span class=n>pos</span><span class=p>,</span> <span class=n>prev_pos</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：序列长度4，排列 [2, 0, 3, 1]</span>
</span></span><span class=line><span class=cl><span class=n>perm</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>xlnet_mask</span> <span class=o>=</span> <span class=n>create_xlnet_mask</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>perm</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;XLNet排列掩码:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>xlnet_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出：</span>
</span></span><span class=line><span class=cl><span class=c1># tensor([[0., 0., 1., 0.],  ← 位置0能看位置2（排列中的前驱）</span>
</span></span><span class=line><span class=cl><span class=c1>#         [1., 0., 1., 1.],  ← 位置1能看2, 0, 3（排列中的前驱）</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 0., 0.],  ← 位置2第一个，看不到任何位置</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 1., 1.]]) ← 位置3能看2, 0（排列中的前驱）</span></span></span></code></pre></div><h4 id=q4-encoder-decoder架构中decoder的交叉注意力为什么可以双向>Q4: Encoder-Decoder架构中，Decoder的交叉注意力为什么可以双向？<a class=anchor href=#q4-encoder-decoder%e6%9e%b6%e6%9e%84%e4%b8%addecoder%e7%9a%84%e4%ba%a4%e5%8f%89%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%b8%ba%e4%bb%80%e4%b9%88%e5%8f%af%e4%bb%a5%e5%8f%8c%e5%90%91>#</a></h4><p><strong>回答</strong>：</p><ol><li><strong>交叉注意力对象</strong>：Encoder的输出（完整输入的表示）</li><li><strong>关键</strong>：Encoder输出不是"未来的target"，而是"已知的source"</li><li><strong>无信息泄露</strong>：<ul><li>Decoder自注意力：因果掩码（$y_{&lt;i}$）</li><li>Cross-Attention：双向（Encoder的 $x_{1:m}$）</li><li>$x_{1:m}$ 在推理时是完整已知的！</li></ul></li></ol><p><strong>代码验证</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>,</span> <span class=n>memory_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 自注意力：因果掩码（单向）</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=n>x</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_mask</span><span class=o>=</span><span class=n>tgt_mask</span>  <span class=c1># 因果掩码</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 交叉注意力：无掩码（双向）</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=n>x</span><span class=p>,</span>           <span class=c1># Decoder的隐状态</span>
</span></span><span class=line><span class=cl>            <span class=n>key</span><span class=o>=</span><span class=n>memory</span><span class=p>,</span>        <span class=c1># Encoder的输出（完整source）</span>
</span></span><span class=line><span class=cl>            <span class=n>value</span><span class=o>=</span><span class=n>memory</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_mask</span><span class=o>=</span><span class=kc>None</span>     <span class=c1># 无因果限制！</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. FFN</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><hr><h4 id=7本节小结-1>（7）本节小结<a class=anchor href=#7%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93-1>#</a></h4><p><strong>核心要点</strong>：</p><ol><li><p><strong>Encoder双向 vs Decoder单向</strong>：</p><ul><li>本质：任务目标不同（理解 vs 生成）</li><li>数学：训练目标不同（MLM vs CLM）</li><li>实践：推理模式不同（并行 vs 自回归）</li></ul></li><li><p><strong>因果掩码的必要性</strong>：</p><ul><li>防止信息泄露（训练时看到答案）</li><li>保证训练-推理一致性（Exposure Bias）</li><li>实验证明：双向训练的Decoder推理性能崩溃</li></ul></li><li><p><strong>信息利用率</strong>：</p><ul><li>Rank：下三角可满秩，无损失</li><li>信息量：平均损失50%（必要代价）</li><li>补偿：交叉注意力、位置编码</li></ul></li><li><p><strong>面试必背</strong>：</p><ul><li>公式：$P(y_i | y_{&lt;i})$ vs $P(y_i | y_{1:n})$</li><li>数据：双向Decoder推理困惑度 156 vs 单向 18</li><li>概念：Exposure Bias、训练-推理一致性</li></ul></li></ol><hr><h4 id=组合掩码padding--causal>组合掩码：Padding + Causal<a class=anchor href=#%e7%bb%84%e5%90%88%e6%8e%a9%e7%a0%81padding--causal>#</a></h4><p>在实际应用中，常需要同时应用两种掩码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_combined_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>valid_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    创建组合掩码（Padding + Causal）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        seq_len: 序列总长度
</span></span></span><span class=line><span class=cl><span class=s2>        valid_len: 有效长度
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        mask: [seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 因果掩码</span>
</span></span><span class=line><span class=cl>    <span class=n>causal</span> <span class=o>=</span> <span class=n>create_causal_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 填充掩码</span>
</span></span><span class=line><span class=cl>    <span class=n>padding</span> <span class=o>=</span> <span class=n>create_padding_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>valid_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 两者取交集（都为1才为1）</span>
</span></span><span class=line><span class=cl>    <span class=n>combined</span> <span class=o>=</span> <span class=n>causal</span> <span class=o>*</span> <span class=n>padding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>combined</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：序列长度=5，有效长度=3</span>
</span></span><span class=line><span class=cl><span class=n>combined_mask</span> <span class=o>=</span> <span class=n>create_combined_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>valid_len</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;组合掩码:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>combined_mask</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>组合掩码:
</span></span><span class=line><span class=cl>tensor([[1., 0., 0., 0., 0.],  ← 位置0：只看自己，且自己有效
</span></span><span class=line><span class=cl>        [1., 1., 0., 0., 0.],  ← 位置1：能看0、1，且都有效
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],  ← 位置2：能看0、1、2，且都有效
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.],  ← 位置3：因果允许看0-3，但3是填充
</span></span><span class=line><span class=cl>        [1., 1., 1., 0., 0.]]) ← 位置4：因果允许看0-4，但4是填充</span></span></code></pre></div><h4 id=掩码对梯度的影响>掩码对梯度的影响<a class=anchor href=#%e6%8e%a9%e7%a0%81%e5%af%b9%e6%a2%af%e5%ba%a6%e7%9a%84%e5%bd%b1%e5%93%8d>#</a></h4><p><strong>关键洞察</strong>：掩码位置的梯度为0！</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 测试掩码对梯度的影响</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>attention</span> <span class=o>=</span> <span class=n>SelfAttention</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>d_k</span><span class=o>=</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 不使用掩码</span>
</span></span><span class=line><span class=cl><span class=n>output1</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss1</span> <span class=o>=</span> <span class=n>output1</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>loss1</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>grad1</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用掩码</span>
</span></span><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>create_causal_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output2</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss2</span> <span class=o>=</span> <span class=n>output2</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>loss2</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>grad2</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度差异:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;不使用掩码的梯度范数: </span><span class=si>{</span><span class=n>grad1</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;使用掩码的梯度范数: </span><span class=si>{</span><span class=n>grad2</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;梯度是否相同: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>allclose</span><span class=p>(</span><span class=n>grad1</span><span class=p>,</span> <span class=n>grad2</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>总结</strong>：</p><ul><li>掩码改变了信息流动路径</li><li>被掩码的位置不参与梯度传播</li><li>这对训练效率和模型行为都有重要影响</li></ul><hr><h3 id=可视化注意力权重>可视化注意力权重<a class=anchor href=#%e5%8f%af%e8%a7%86%e5%8c%96%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9d%83%e9%87%8d>#</a></h3><p>让我们用真实句子看看注意力在"看"什么：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModel</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载BERT模型</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;bert-base-uncased&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>output_attentions</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试句子</span>
</span></span><span class=line><span class=cl><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&#34;The cat sat on the mat&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Tokens:&#34;</span><span class=p>,</span> <span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播，获取注意力权重</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># outputs.attentions: 12层，每层的注意力权重</span>
</span></span><span class=line><span class=cl>    <span class=c1># 取第6层、第1个头的注意力</span>
</span></span><span class=line><span class=cl>    <span class=n>attention</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>attentions</span><span class=p>[</span><span class=mi>5</span><span class=p>][</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>  <span class=c1># [seq_len, seq_len]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>attention</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>xticklabels</span><span class=o>=</span><span class=n>tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>yticklabels</span><span class=o>=</span><span class=n>tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;YlOrRd&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fmt</span><span class=o>=</span><span class=s2>&#34;.2f&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cbar_kws</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;label&#39;</span><span class=p>:</span> <span class=s1>&#39;注意力权重&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;被关注的Token&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;当前Token&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;BERT第6层第1头的注意力权重&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;attention_heatmap.png&#39;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>观察</strong>：</p><ul><li>对角线权重高：每个词都关注自己</li><li>&ldquo;cat"可能高度关注"sat&rdquo;（主语-谓语关系）</li><li>&ldquo;the"和"mat"可能相互关注（定冠词-名词关系）</li></ul><hr><h2 id=三核心组件二位置编码positional-encoding>三、核心组件二：位置编码（Positional Encoding）<a class=anchor href=#%e4%b8%89%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81positional-encoding>#</a></h2><h3 id=1-为什么transformer需要位置编码>1. 为什么Transformer需要位置编码？<a class=anchor href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88transformer%e9%9c%80%e8%a6%81%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81>#</a></h3><p><strong>问题</strong>：自注意力是<strong>顺序无关</strong>的！</p><p>考虑两个句子：</p><ul><li>&ldquo;The cat chased the dog&rdquo;</li><li>&ldquo;The dog chased the cat&rdquo;</li></ul><p>如果去掉位置信息，自注意力会给出<strong>相同的输出</strong>（因为它只是计算词之间的相关性，不管顺序）。</p><p>但这两句话的含义完全不同！</p><p><strong>解决方案</strong>：在嵌入中加入位置信息。</p><hr><h3 id=2-绝对位置编码正弦余弦方案>2. 绝对位置编码：正弦余弦方案<a class=anchor href=#2-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%ad%a3%e5%bc%a6%e4%bd%99%e5%bc%a6%e6%96%b9%e6%a1%88>#</a></h3><p>原始Transformer使用<strong>正弦和余弦函数</strong>生成位置编码：</p><p>$$
\begin{align}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{align}
$$</p><p>其中：</p><ul><li>$pos$：位置（0, 1, 2, &mldr;）</li><li>$i$：维度索引（0到 $d_{model}/2$）</li><li>偶数维度用sin，奇数维度用cos</li></ul><p><strong>为什么这么设计？深度数学直觉</strong></p><p>这不是随意选择,sin/cos有深刻的数学原因。</p><h4 id=原因1线性可表达相对位置>原因1：线性可表达相对位置<a class=anchor href=#%e5%8e%9f%e5%9b%a01%e7%ba%bf%e6%80%a7%e5%8f%af%e8%a1%a8%e8%be%be%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae>#</a></h4><p>这是最重要的性质!</p><p><strong>数学推导</strong>:</p><p>利用三角恒等式:</p><p>$$
\begin{align}
\sin(\alpha + \beta) &= \sin(\alpha)\cos(\beta) + \cos(\alpha)\sin(\beta) \
\cos(\alpha + \beta) &= \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)
\end{align}
$$</p><p>因此,位置 $pos + k$ 的编码可以表示为位置 $pos$ 的<strong>线性组合</strong>:</p><h1 id=endbmatrix>$$
\begin{bmatrix}
PE_{(pos+k, 2i)} \
PE_{(pos+k, 2i+1)}
\end{bmatrix}<a class=anchor href=#endbmatrix>#</a></h1><p>\begin{bmatrix}
\cos(k\theta_i) & \sin(k\theta_i) \
-\sin(k\theta_i) & \cos(k\theta_i)
\end{bmatrix}
\begin{bmatrix}
PE_{(pos, 2i)} \
PE_{(pos, 2i+1)}
\end{bmatrix}
$$</p><p>其中 $\theta_i = 1/10000^{2i/d_{model}}$。</p><p><strong>这意味着什么？</strong></p><p>模型可以"学会"从绝对位置编码中提取相对位置信息!</p><p><strong>示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>位置5的编码 → 通过线性变换 → 得到&#34;位置5比位置2远3个位置&#34;</span></span></code></pre></div><p>这个性质让自注意力机制能够感知词之间的相对距离。</p><h4 id=原因2不同频率捕获不同尺度>原因2：不同频率捕获不同尺度<a class=anchor href=#%e5%8e%9f%e5%9b%a02%e4%b8%8d%e5%90%8c%e9%a2%91%e7%8e%87%e6%8d%95%e8%8e%b7%e4%b8%8d%e5%90%8c%e5%b0%ba%e5%ba%a6>#</a></h4><p>观察公式中的 $10000^{2i/d_{model}}$:</p><ul><li><strong>低维度</strong>(i=0): 频率 = $1/10000^0 = 1$ → 周期 = $2\pi$ (约6个位置)</li><li><strong>中维度</strong>(i=128): 频率 = $1/10000^{0.5}$ → 周期 = $2\pi \times 100$ (约600位置)</li><li><strong>高维度</strong>(i=255): 频率 = $1/10000^{1.0}$ → 周期 = $2\pi \times 10000$ (约6万位置)</li></ul><p><strong>类比傅里叶变换</strong>:</p><p>就像音频分析,用不同频率的波捕获不同时间尺度的信号:</p><ul><li>高频波 → 捕获局部细节(相邻词)</li><li>低频波 → 捕获全局结构(长距离依赖)</li></ul><p><strong>可视化理解</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 不同维度的频率</span>
</span></span><span class=line><span class=cl><span class=n>dims</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>192</span><span class=p>,</span> <span class=mi>255</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>positions</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>dim</span> <span class=ow>in</span> <span class=n>dims</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>freq</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>10000</span> <span class=o>**</span> <span class=p>(</span><span class=n>dim</span> <span class=o>/</span> <span class=mi>256</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>values</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>pos</span> <span class=o>*</span> <span class=n>freq</span><span class=p>)</span> <span class=k>for</span> <span class=n>pos</span> <span class=ow>in</span> <span class=n>positions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>positions</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;维度</span><span class=si>{</span><span class=n>dim</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;不同维度的位置编码频率&#39;</span><span class=p>)</span></span></span></code></pre></div><p>结果:低维度快速震荡(捕获局部),高维度缓慢变化(捕获全局)。</p><h4 id=原因3唯一性与平滑性的平衡>原因3：唯一性与平滑性的平衡<a class=anchor href=#%e5%8e%9f%e5%9b%a03%e5%94%af%e4%b8%80%e6%80%a7%e4%b8%8e%e5%b9%b3%e6%bb%91%e6%80%a7%e7%9a%84%e5%b9%b3%e8%a1%a1>#</a></h4><p><strong>唯一性</strong>:</p><p>对于合理的序列长度($&lt;10^4$),每个位置的512维编码向量都是唯一的。</p><p><strong>证明思路</strong>:不同位置的sin/cos组合形成不同的"波形指纹&rdquo;。</p><p><strong>平滑性</strong>:</p><p>相邻位置的编码向量相似(余弦相似度高):</p><p>$$
\text{sim}(PE_{pos}, PE_{pos+1}) \approx 0.99
$$</p><p>这让模型能够泛化:训练时学到的"相邻词关系"能应用到新句子。</p><h4 id=原因4外推性理论上>原因4：外推性(理论上)<a class=anchor href=#%e5%8e%9f%e5%9b%a04%e5%a4%96%e6%8e%a8%e6%80%a7%e7%90%86%e8%ae%ba%e4%b8%8a>#</a></h4><p>sin/cos函数的周期性意味着:</p><p>$$
PE_{pos} = PE_{pos + T} \quad (\text{如果}\ pos\ \text{超过周期}\ T)
$$</p><p>理论上可以处理任意长度。</p><p><strong>但实际问题</strong>:</p><p>虽然sin/cos编码理论上支持任意长度,但<strong>模型训练的长度限制了实际性能</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练长度: 512
</span></span><span class=line><span class=cl>测试长度: 2048  → 性能下降(外推失败)</span></span></code></pre></div><p>这促使了RoPE、ALiBi等相对位置编码的发展。</p><hr><p><strong>实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_positional_encoding</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    生成正弦余弦位置编码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        seq_len: 序列长度
</span></span></span><span class=line><span class=cl><span class=s2>        d_model: 嵌入维度
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        pos_encoding: [seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 创建位置和维度的索引</span>
</span></span><span class=line><span class=cl>    <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># [seq_len, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=o>-</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>  <span class=c1># [d_model/2]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化位置编码矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 偶数维度用sin</span>
</span></span><span class=line><span class=cl>    <span class=n>pos_encoding</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 奇数维度用cos</span>
</span></span><span class=line><span class=cl>    <span class=n>pos_encoding</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>pos_encoding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成位置编码</span>
</span></span><span class=line><span class=cl><span class=n>seq_len</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>pe</span> <span class=o>=</span> <span class=n>get_positional_encoding</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;位置编码形状: </span><span class=si>{</span><span class=n>pe</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;位置0的编码（前10维）:</span><span class=se>\n</span><span class=si>{</span><span class=n>pe</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:</span><span class=mi>10</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;位置1的编码（前10维）:</span><span class=se>\n</span><span class=si>{</span><span class=n>pe</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span><span class=mi>10</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 子图1：位置编码热力图</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>pe</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;RdBu&#39;</span><span class=p>,</span> <span class=n>aspect</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;维度&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;位置&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;位置编码可视化&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 子图2：几个位置的编码曲线</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>positions_to_plot</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>50</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>pos</span> <span class=ow>in</span> <span class=n>positions_to_plot</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>pe</span><span class=p>[</span><span class=n>pos</span><span class=p>,</span> <span class=p>:</span><span class=mi>128</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;位置 </span><span class=si>{</span><span class=n>pos</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;维度&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;编码值&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;不同位置的编码曲线（前128维）&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;positional_encoding.png&#39;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>观察</strong>：</p><ul><li>低维度（接近0）：频率低，变化慢，捕获粗粒度的位置信息</li><li>高维度（接近d_model）：频率高，变化快，捕获细粒度的位置信息</li></ul><hr><h3 id=3-相对位置编码演进>3. 相对位置编码演进<a class=anchor href=#3-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e6%bc%94%e8%bf%9b>#</a></h3><p>绝对位置编码有局限：</p><ul><li>只编码绝对位置，不直接编码相对距离</li><li>对超长序列外推性不佳</li></ul><p>现代模型使用<strong>相对位置编码</strong>。</p><blockquote class=book-hint><p><strong>章节说明</strong>：本节介绍RoPE等现代位置编码的核心原理，帮助理解Transformer架构的完整性。关于长上下文扩展技术（如NTK-aware、YaRN等）和FlashAttention等性能优化，将在**第七部分第1章《长上下文技术》**中详细展开。</p></blockquote><h4 id=-旋转位置编码rope--面试必考>🎯 旋转位置编码（RoPE）- 面试必考<a class=anchor href=#-%e6%97%8b%e8%bd%ac%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81rope--%e9%9d%a2%e8%af%95%e5%bf%85%e8%80%83>#</a></h4><p><strong>代表模型</strong>：LLaMA、Qwen、GLM、ChatGLM、Yi、DeepSeek</p><p>RoPE是当前主流LLM的标配位置编码方案，面试必问！</p><hr><h4 id=1设计目标相对位置不变性>（1）设计目标：相对位置不变性<a class=anchor href=#1%e8%ae%be%e8%ae%a1%e7%9b%ae%e6%a0%87%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e4%b8%8d%e5%8f%98%e6%80%a7>#</a></h4><p>RoPE的核心设计目标是找到一个位置编码函数 $f(\mathbf{x}, \ell)$，使得：</p><p>$$
\langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle = g(\mathbf{q}, \mathbf{k}, m-n)
$$</p><p>即<strong>注意力分数只依赖相对位置 $m-n$</strong>，与绝对位置无关。</p><p>这样设计的优势：</p><ul><li>✅ 自然的相对位置建模（语言的局部性）</li><li>✅ 理论上支持任意长度外推</li><li>✅ 零参数，无需学习</li></ul><hr><h4 id=2数学推导从复数到旋转矩阵>（2）数学推导：从复数到旋转矩阵<a class=anchor href=#2%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc%e4%bb%8e%e5%a4%8d%e6%95%b0%e5%88%b0%e6%97%8b%e8%bd%ac%e7%9f%a9%e9%98%b5>#</a></h4><p><strong>Step 1：复数表示</strong></p><p>将 $d$ 维实向量重构为 $\mathbb{C}^{d/2}$ 复向量：</p><p>$$
\mathbf{q} = (q_0, q_1, q_2, q_3, \dots, q_{d-1}) \rightarrow (q_0+iq_1, q_2+iq_3, \dots)
$$</p><p>设位置编码函数为：</p><p>$$
f(\mathbf{q}, m) = \mathbf{q} \cdot e^{im\boldsymbol{\theta}}
$$</p><p>其中 $\boldsymbol{\theta} = (\theta_0, \theta_1, \dots, \theta_{d/2-1})$ 是角频率向量。</p><p><strong>Step 2：相对位置证明</strong></p><p>对位置 $m$ 的查询和位置 $n$ 的键：</p><p>$$
\begin{align}
\langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle &= \langle \mathbf{q}e^{im\boldsymbol{\theta}}, \mathbf{k}e^{in\boldsymbol{\theta}} \rangle \
&= \sum_{j=0}^{d/2-1} q_j e^{im\theta_j} \cdot \overline{k_j e^{in\theta_j}} \
&= \sum_{j=0}^{d/2-1} q_j \bar{k}<em>j \cdot e^{im\theta_j} \cdot e^{-in\theta_j} \
&= \sum</em>{j=0}^{d/2-1} q_j \bar{k}_j \cdot e^{i(m-n)\theta_j} \
&= \langle \mathbf{q}, \mathbf{k}e^{i(m-n)\boldsymbol{\theta}} \rangle
\end{align}
$$</p><p><strong>证明完毕</strong>：注意力分数只依赖 $m-n$！</p><p><strong>Step 3：实数矩阵形式</strong></p><p>为避免复数运算，将复数乘法转换为实数旋转矩阵。</p><p>对于第 $j$ 对特征 $(q_{2j}, q_{2j+1})$，旋转角度 $m\theta_j$ 对应的旋转矩阵：</p><p>$$
\mathbf{M}_j(m) = \begin{bmatrix}
\cos(m\theta_j) & -\sin(m\theta_j) \
\sin(m\theta_j) & \cos(m\theta_j)
\end{bmatrix}
$$</p><p>完整的RoPE变换（分块对角矩阵）：</p><p>$$
\mathbf{R}_{\Theta, m} = \begin{bmatrix}
\mathbf{M}_0(m) & & & \
& \mathbf{M}<em>1(m) & & \
& & \ddots & \
& & & \mathbf{M}</em>{d/2-1}(m)
\end{bmatrix}
$$</p><p>应用到Query和Key：</p><p>$$
\begin{align}
\mathbf{q}<em>m&rsquo; &= \mathbf{R}</em>{\Theta, m} \mathbf{q}_m \
\mathbf{k}<em>n&rsquo; &= \mathbf{R}</em>{\Theta, n} \mathbf{k}_n
\end{align}
$$</p><hr><h4 id=3角频率公式为什么是-100002id>（3）角频率公式：为什么是 $10000^{2i/d}$<a class=anchor href=#3%e8%a7%92%e9%a2%91%e7%8e%87%e5%85%ac%e5%bc%8f%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af-100002id>#</a></h4><p>角频率 $\theta_j$ 的选择至关重要，采用指数衰减：</p><p>$$
\theta_j = \frac{1}{10000^{2j/d}}, \quad j \in [0, 1, \dots, d/2-1]
$$</p><p><strong>设计理由</strong>：</p><ol><li><strong>类比正弦位置编码</strong>：继承Transformer原始设计</li><li><strong>多尺度建模</strong>：<ul><li>高频分量（$j$ 小）：捕捉短距离依赖</li><li>低频分量（$j$ 大）：捕捉长距离依赖</li></ul></li><li><strong>波长覆盖范围</strong>：从 $2\pi$ 到 $10000 \times 2\pi$</li></ol><p><strong>代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_theta</span><span class=p>(</span><span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>base</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>10000.0</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;计算角频率
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        dim: 注意力头维度（必须是偶数）
</span></span></span><span class=line><span class=cl><span class=s2>        base: 基数，通常为10000
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        theta: [dim/2] 角频率向量
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># θⱼ = 1 / (base^{2j/d})</span>
</span></span><span class=line><span class=cl>    <span class=n>inv_freq</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>base</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>inv_freq</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：64维注意力头</span>
</span></span><span class=line><span class=cl><span class=n>theta</span> <span class=o>=</span> <span class=n>compute_theta</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;θ₀ = </span><span class=si>{</span><span class=n>theta</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># 高频：θ₀ = 1.000000</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;θ₃₁ = </span><span class=si>{</span><span class=n>theta</span><span class=p>[</span><span class=mi>31</span><span class=p>]</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span> <span class=c1># 低频：θ₃₁ = 0.000100</span></span></span></code></pre></div><hr><h4 id=4生产级代码实现>（4）生产级代码实现<a class=anchor href=#4%e7%94%9f%e4%ba%a7%e7%ba%a7%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h4><p><strong>方法1：HuggingFace风格（实数版本）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RotaryEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;RoPE位置编码（LLaMA/Qwen实现）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>base</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>10000.0</span><span class=p>,</span> <span class=n>max_seq_len</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>2048</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算逆频率：1 / (base^{2i/d})</span>
</span></span><span class=line><span class=cl>        <span class=n>inv_freq</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>base</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;inv_freq&#34;</span><span class=p>,</span> <span class=n>inv_freq</span><span class=p>,</span> <span class=n>persistent</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 预计算缓存（优化性能）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_build_cache</span><span class=p>(</span><span class=n>max_seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_build_cache</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;预计算cos和sin值&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 位置索引：[0, 1, 2, ..., seq_len-1]</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>inv_freq</span><span class=o>.</span><span class=n>device</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算 m*θⱼ：[seq_len, dim/2]</span>
</span></span><span class=line><span class=cl>        <span class=n>freqs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>inv_freq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 重复拼接（对应特征对的x和y分量使用相同角度）</span>
</span></span><span class=line><span class=cl>        <span class=n>emb</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>freqs</span><span class=p>,</span> <span class=n>freqs</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># [seq_len, dim]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 缓存cos和sin</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cos_cached</span> <span class=o>=</span> <span class=n>emb</span><span class=o>.</span><span class=n>cos</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sin_cached</span> <span class=o>=</span> <span class=n>emb</span><span class=o>.</span><span class=n>sin</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>position_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch, seq_len, num_heads, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>            position_ids: [batch, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            cos, sin: [batch, seq_len, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 动态扩展缓存</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_len</span> <span class=o>=</span> <span class=n>position_ids</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>seq_len</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>cos_cached</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_build_cache</span><span class=p>(</span><span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 根据position_ids索引</span>
</span></span><span class=line><span class=cl>        <span class=n>cos</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cos_cached</span><span class=p>[</span><span class=n>position_ids</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>sin</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sin_cached</span><span class=p>[</span><span class=n>position_ids</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>cos</span><span class=p>,</span> <span class=n>sin</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rotate_half</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;将后半部分移到前面并取负：[-x_{d/2:}, x_{:d/2}]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    对应复数乘法的虚部：(a+bi)*(cosθ+i·sinθ) 的交叉项
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>x1</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=p>:</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>//</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>x2</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>//</span><span class=mi>2</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=o>-</span><span class=n>x2</span><span class=p>,</span> <span class=n>x1</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_rotary_pos_emb</span><span class=p>(</span><span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>k</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>cos</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>sin</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;应用RoPE旋转
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    数学等价于：x * e^</span><span class=si>{imθ}</span><span class=s2> = x * (cos(mθ) + i*sin(mθ))
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        q, k: [batch, seq_len, num_heads, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>        cos, sin: [batch, seq_len, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        q_embed, k_embed: 旋转后的查询和键
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 广播维度匹配</span>
</span></span><span class=line><span class=cl>    <span class=n>cos</span> <span class=o>=</span> <span class=n>cos</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># [batch, seq_len, 1, head_dim]</span>
</span></span><span class=line><span class=cl>    <span class=n>sin</span> <span class=o>=</span> <span class=n>sin</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 公式：x*cos(mθ) + rotate_half(x)*sin(mθ)</span>
</span></span><span class=line><span class=cl>    <span class=n>q_embed</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>cos</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>rotate_half</span><span class=p>(</span><span class=n>q</span><span class=p>)</span> <span class=o>*</span> <span class=n>sin</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>k_embed</span> <span class=o>=</span> <span class=p>(</span><span class=n>k</span> <span class=o>*</span> <span class=n>cos</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>rotate_half</span><span class=p>(</span><span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>sin</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q_embed</span><span class=p>,</span> <span class=n>k_embed</span></span></span></code></pre></div><p><strong>方法2：Meta LLaMA原始实现（复数版本）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>precompute_freqs_cis</span><span class=p>(</span><span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>end</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>theta</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>10000.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;预计算频率的复数指数形式（cis = cos + i*sin）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        freqs_cis: [end, dim/2] 复数张量
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>theta</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)[:(</span><span class=n>dim</span><span class=o>//</span><span class=mi>2</span><span class=p>)]</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>end</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>freqs</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>freqs</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>  <span class=c1># [end, dim/2]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 生成复数：e^{i*mθ} = cos(mθ) + i*sin(mθ)</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs_cis</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>polar</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>freqs</span><span class=p>),</span> <span class=n>freqs</span><span class=p>)</span>  <span class=c1># complex64</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>freqs_cis</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_rotary_emb</span><span class=p>(</span><span class=n>xq</span><span class=p>,</span> <span class=n>xk</span><span class=p>,</span> <span class=n>freqs_cis</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;使用复数乘法应用旋转（更简洁但需要复数支持）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 重塑为复数形式：[..., d] -&gt; [..., d/2] complex</span>
</span></span><span class=line><span class=cl>    <span class=n>xq_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>xq</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>xq</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>xk_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>xk</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>xk</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 复数乘法实现旋转</span>
</span></span><span class=line><span class=cl>    <span class=n>xq_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>xq_</span> <span class=o>*</span> <span class=n>freqs_cis</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>xk_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>xk_</span> <span class=o>*</span> <span class=n>freqs_cis</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>xq_out</span><span class=o>.</span><span class=n>type_as</span><span class=p>(</span><span class=n>xq</span><span class=p>),</span> <span class=n>xk_out</span><span class=o>.</span><span class=n>type_as</span><span class=p>(</span><span class=n>xk</span><span class=p>)</span></span></span></code></pre></div><hr><h4 id=5rope-vs-绝对位置编码对比>（5）RoPE vs 绝对位置编码对比<a class=anchor href=#5rope-vs-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e5%af%b9%e6%af%94>#</a></h4><table><thead><tr><th>维度</th><th>RoPE</th><th>绝对位置编码（Sinusoidal）</th></tr></thead><tbody><tr><td><strong>位置依赖</strong></td><td>自然的相对位置</td><td>绝对位置（需学习相对关系）</td></tr><tr><td><strong>注入方式</strong></td><td>乘性因子（旋转QK）</td><td>加性嵌入（加到Token）</td></tr><tr><td><strong>外推能力</strong></td><td>强（理论无上界）</td><td>弱（训练长度受限）</td></tr><tr><td><strong>参数量</strong></td><td>零参数</td><td>零参数</td></tr><tr><td><strong>计算开销</strong></td><td>1-3%（融合优化后）</td><td>可忽略</td></tr><tr><td><strong>实验性能</strong></td><td>OWT2困惑度 15.78</td><td>16.59</td></tr></tbody></table><p><strong>关键优势</strong>：</p><ul><li>✅ <strong>相对位置建模</strong>：符合语言的局部性特征</li><li>✅ <strong>长度泛化</strong>：训练2048可推理4096+</li><li>✅ <strong>零参数</strong>：无过拟合风险</li></ul><hr><h4 id=6外推性分析与长上下文扩展>（6）外推性分析与长上下文扩展<a class=anchor href=#6%e5%a4%96%e6%8e%a8%e6%80%a7%e5%88%86%e6%9e%90%e4%b8%8e%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e6%89%a9%e5%b1%95>#</a></h4><p><strong>RoPE外推的局限</strong>：</p><p>虽然理论上支持任意长度，但<strong>直接外推到训练时未见的长度会导致问题</strong>：</p><p>❌ <strong>注意力分数爆炸</strong>：超出训练范围的位置编码导致数值不稳定
❌ <strong>高频分量混叠</strong>：长距离上产生周期性混淆</p><p><strong>解决方案1：Position Interpolation（PI）</strong></p><p><strong>核心思路</strong>：线性压缩位置索引，而非外推
$$
\text{position_ids}<em>{\text{new}} = \text{position_ids} \times \frac{L</em>{\text{train}}}{L_{\text{new}}}
$$</p><p><strong>代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>position_interpolation</span><span class=p>(</span><span class=n>position_ids</span><span class=p>,</span> <span class=n>max_train_len</span><span class=p>,</span> <span class=n>current_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;位置插值
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        position_ids: [batch, seq_len] 原始位置索引
</span></span></span><span class=line><span class=cl><span class=s2>        max_train_len: 训练时最大长度（如2048）
</span></span></span><span class=line><span class=cl><span class=s2>        current_len: 当前序列长度（如4096）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        插值后的位置索引
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=n>max_train_len</span> <span class=o>/</span> <span class=n>current_len</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>position_ids</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=n>scale</span><span class=p>)</span><span class=o>.</span><span class=n>long</span><span class=p>()</span></span></span></code></pre></div><p><strong>优势</strong>：</p><ul><li>✅ 上界比外推小 <strong>~600倍</strong>（数学证明）</li><li>✅ 仅需 <strong>1000步</strong> 微调即可扩展到32k tokens</li></ul><p><strong>解决方案2：NTK-aware Scaled RoPE</strong></p><p>动态调整base参数：</p><p>$$
\text{base}_{\text{new}} = \text{base} \times \left(\text{scale}\right)^{\frac{d}{d-2}}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>ntk_scaled_rope</span><span class=p>(</span><span class=n>base</span><span class=p>,</span> <span class=n>scale_factor</span><span class=p>,</span> <span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;NTK-aware缩放&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>base</span> <span class=o>*</span> <span class=p>(</span><span class=n>scale_factor</span> <span class=o>**</span> <span class=p>(</span><span class=n>dim</span> <span class=o>/</span> <span class=p>(</span><span class=n>dim</span> <span class=o>-</span> <span class=mi>2</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例：扩展2倍长度</span>
</span></span><span class=line><span class=cl><span class=n>base_new</span> <span class=o>=</span> <span class=n>ntk_scaled_rope</span><span class=p>(</span><span class=mi>10000</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>  <span class=c1># ~40000</span></span></span></code></pre></div><p><strong>解决方案3：YaRN方法</strong></p><ul><li><strong>计算效率</strong>：比之前方法少10倍tokens、2.5倍训练步数</li><li><strong>超长上下文</strong>：扩展到128k context length</li><li><strong>温度缩放</strong>：针对不同频率分量的自适应调整</li></ul><hr><h4 id=7面试高频问题>（7）面试高频问题<a class=anchor href=#7%e9%9d%a2%e8%af%95%e9%ab%98%e9%a2%91%e9%97%ae%e9%a2%98>#</a></h4><p><strong>Q1: RoPE为什么只依赖相对位置？</strong></p><p>通过旋转变换的群性质：</p><p>$$
\langle e^{im\theta}q, e^{in\theta}k \rangle = \langle e^{i(m-n)\theta}q, k \rangle
$$</p><p>只依赖差值 $m-n$，与绝对位置无关。</p><p><strong>Q2: <code>rotate_half</code> 的数学原理？</strong></p><p>对应复数乘法的实部和虚部展开：</p><p>$$
(a+bi) \cdot (\cos\theta + i\sin\theta) = (a\cos\theta - b\sin\theta) + i(a\sin\theta + b\cos\theta)
$$</p><p><code>rotate_half(x) = [-b, a]</code> 实现了虚部的交叉项。</p><p><strong>Q3: 为什么拼接两次 <code>freqs</code>？</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>emb</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>freqs</span><span class=p>,</span> <span class=n>freqs</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span></span></span></code></pre></div><p>因为维度 $d$ 被分成 $d/2$ 对，每对的 $x$ 和 $y$ 分量使用<strong>相同的旋转角度</strong>，所以需要重复。</p><p><strong>Q4: RoPE的外推性如何解决？</strong></p><p>三种主流方法：</p><ol><li><strong>Position Interpolation</strong>：线性压缩位置索引</li><li><strong>NTK-aware Scaling</strong>：动态调整base参数</li><li><strong>YaRN</strong>：差异化频率缩放 + 温度调整</li></ol><p><strong>Q5: 为什么主流模型都用RoPE而不是ALiBi？</strong></p><ul><li>RoPE理论更优雅（群论基础）</li><li>实现简单高效（预计算缓存）</li><li>与Flash Attention等优化兼容性更好</li><li>LLaMA的成功带动了RoPE的普及</li></ul><h4 id=alibiattention-with-linear-biases>ALiBi（Attention with Linear Biases）<a class=anchor href=#alibiattention-with-linear-biases>#</a></h4><p><strong>核心思想</strong>：在注意力分数上直接加上与距离成比例的偏置。</p><p>$$
\text{Attention}_{ALiBi}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + m \cdot D\right)V
$$</p><p>其中：</p><ul><li>$D_{ij} = -(j - i)$：位置 $i$ 到 $j$ 的距离</li><li>$m$：每个头的斜率（不同头有不同斜率）</li></ul><p><strong>优势</strong>：</p><ul><li>超强外推性：训练在1024长度，推理可到10万+</li><li>不需要额外参数</li></ul><p><strong>代表模型</strong>：BLOOM</p><hr><h2 id=四核心组件三多头注意力机制multi-head-attention>四、核心组件三：多头注意力机制（Multi-Head Attention）<a class=anchor href=#%e5%9b%9b%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e4%b8%89%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6multi-head-attention>#</a></h2><h3 id=1-多头的意义从多个子空间捕获信息>1. 多头的意义：从多个子空间捕获信息<a class=anchor href=#1-%e5%a4%9a%e5%a4%b4%e7%9a%84%e6%84%8f%e4%b9%89%e4%bb%8e%e5%a4%9a%e4%b8%aa%e5%ad%90%e7%a9%ba%e9%97%b4%e6%8d%95%e8%8e%b7%e4%bf%a1%e6%81%af>#</a></h3><h4 id=为什么需要多头>为什么需要多头？<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e5%a4%b4>#</a></h4><p>单个注意力头的表达能力有限。考虑句子"银行的利率很高":</p><p>如果只有1个头:</p><ul><li>可能只关注"银行"和"利率"的语义关系</li><li>无法同时捕获"利率"和"高"的修饰关系</li><li>无法同时理解"银行"的领域(金融 vs 河岸)</li></ul><p><strong>多头的核心价值</strong>:在不同的表示子空间中,学习不同的语义模式。</p><p>$$
\text{不同头} \Rightarrow \text{不同子空间} \Rightarrow \text{不同模式}
$$</p><h4 id=多头到底学到了什么实证研究>多头到底学到了什么？实证研究<a class=anchor href=#%e5%a4%9a%e5%a4%b4%e5%88%b0%e5%ba%95%e5%ad%a6%e5%88%b0%e4%ba%86%e4%bb%80%e4%b9%88%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6>#</a></h4><p>这不是理论推测,而是研究者通过可视化和分析得出的实证结论。</p><p><strong>研究1：BERT的注意力头分析</strong>（来自论文"What Does BERT Look At?"）</p><p>在BERT-base(12层,12头)中,研究者发现:</p><table><thead><tr><th>层</th><th>头编号</th><th>学到的模式</th><th>示例</th></tr></thead><tbody><tr><td>2</td><td>0</td><td><strong>依存句法</strong></td><td>&ldquo;吃&rdquo; → &ldquo;饭&rdquo;(动宾关系)</td></tr><tr><td>5</td><td>8</td><td><strong>共指消解</strong></td><td>&ldquo;他&rdquo; → &ldquo;小明&rdquo;(代词回指)</td></tr><tr><td>8</td><td>11</td><td><strong>语义相似性</strong></td><td>&ldquo;汽车&rdquo; ↔ &ldquo;车辆&rdquo;</td></tr><tr><td>10</td><td>2</td><td><strong>位置邻近</strong></td><td>当前词 → 下一个词</td></tr></tbody></table><p><strong>示例：共指消解头的行为</strong></p><p>输入:&ldquo;小明很聪明,他考了满分。&rdquo;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>位置:  0    1  2  3  4 5  6  7
</span></span><span class=line><span class=cl>Token: 小明  很 聪明 ， 他 考了 满 分
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>头5的注意力权重:
</span></span><span class=line><span class=cl>&#34;他&#34;(位置4) 对各位置的注意力:
</span></span><span class=line><span class=cl>  小明: 0.85  ← 强关联！
</span></span><span class=line><span class=cl>  很:   0.02
</span></span><span class=line><span class=cl>  聪明: 0.05
</span></span><span class=line><span class=cl>  ，:   0.01
</span></span><span class=line><span class=cl>  他:   0.03
</span></span><span class=line><span class=cl>  考了: 0.02
</span></span><span class=line><span class=cl>  满:   0.01
</span></span><span class=line><span class=cl>  分:   0.01</span></span></code></pre></div><p>这个头学会了<strong>代词回指</strong>!</p><p><strong>研究2：GPT-3的注意力头功能分化</strong></p><table><thead><tr><th>头的功能类型</th><th>占比</th><th>典型行为</th></tr></thead><tbody><tr><td><strong>语法头</strong></td><td>25%</td><td>关注主谓宾、修饰关系</td></tr><tr><td><strong>位置头</strong></td><td>20%</td><td>关注相邻词、固定距离</td></tr><tr><td><strong>语义头</strong></td><td>30%</td><td>关注语义相似词</td></tr><tr><td><strong>任务头</strong></td><td>15%</td><td>针对特定下游任务</td></tr><tr><td><strong>噪声头</strong></td><td>10%</td><td>没有明显模式(冗余)</td></tr></tbody></table><p><strong>关键发现</strong>:</p><ul><li>并非所有头都"有用"——约10%的头可以被剪枝而不影响性能</li><li>不同层的头关注不同层次的特征:<ul><li><strong>浅层</strong>(1-4层):关注词法、语法</li><li><strong>中层</strong>(5-8层):关注句法、语义</li><li><strong>深层</strong>(9-12层):关注任务相关的高层特征</li></ul></li></ul><h4 id=深入理解子空间投影>深入理解：子空间投影<a class=anchor href=#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e5%ad%90%e7%a9%ba%e9%97%b4%e6%8a%95%e5%bd%b1>#</a></h4><p>为什么多头能学到不同模式？关键在于<strong>独立的投影矩阵</strong>。</p><p>每个头有自己的 $W_i^Q, W_i^K, W_i^V$,它们把输入投影到不同的子空间:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始空间(512维)
</span></span><span class=line><span class=cl>        ↓
</span></span><span class=line><span class=cl>头1: W₁^Q投影 → 子空间1(64维)  [学语法]
</span></span><span class=line><span class=cl>头2: W₂^Q投影 → 子空间2(64维)  [学语义]
</span></span><span class=line><span class=cl>头3: W₃^Q投影 → 子空间3(64维)  [学位置]
</span></span><span class=line><span class=cl>...</span></span></code></pre></div><p><strong>类比</strong>:</p><ul><li>原始空间 = 一段音频(混合了人声、乐器、环境音)</li><li>不同头的投影 = 不同的滤波器(分离出人声、贝斯、鼓点)</li></ul><p>每个头在自己的子空间中独立学习,最后拼接起来形成完整表示。</p><h4 id=可视化注意力头的差异>可视化：注意力头的差异<a class=anchor href=#%e5%8f%af%e8%a7%86%e5%8c%96%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%a4%b4%e7%9a%84%e5%b7%ae%e5%bc%82>#</a></h4><p>假设我们有2个头,处理句子"小狗追逐小猫":</p><p><strong>头1(语法头)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>     小狗  追逐  小猫
</span></span><span class=line><span class=cl>小狗  0.1  0.8   0.1   ← &#34;小狗&#34;强关注&#34;追逐&#34;(主谓关系)
</span></span><span class=line><span class=cl>追逐  0.4  0.1   0.5   ← &#34;追逐&#34;关注主语和宾语
</span></span><span class=line><span class=cl>小猫  0.1  0.8   0.1   ← &#34;小猫&#34;强关注&#34;追逐&#34;(动宾关系)</span></span></code></pre></div><p><strong>头2(语义头)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>     小狗  追逐  小猫
</span></span><span class=line><span class=cl>小狗  0.2  0.1   0.7   ← &#34;小狗&#34;关注&#34;小猫&#34;(语义相关:都是动物)
</span></span><span class=line><span class=cl>追逐  0.3  0.4   0.3
</span></span><span class=line><span class=cl>小猫  0.7  0.1   0.2   ← &#34;小猫&#34;关注&#34;小狗&#34;</span></span></code></pre></div><p>两个头捕获了完全不同的语言模式!</p><hr><h4 id=-深度解析softmax瓶颈与multi-head的秩恢复机制>🎯 深度解析：Softmax瓶颈与Multi-Head的秩恢复机制<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90softmax%e7%93%b6%e9%a2%88%e4%b8%8emulti-head%e7%9a%84%e7%a7%a9%e6%81%a2%e5%a4%8d%e6%9c%ba%e5%88%b6>#</a></h4><p><strong>核心问题</strong>：为什么Multi-Head不是简单的"学习多种模式"，而是解决了**低秩崩溃（Low-Rank Collapse）**的数学难题？</p><h5 id=问题单头注意力的秩瓶颈>问题：单头注意力的秩瓶颈<a class=anchor href=#%e9%97%ae%e9%a2%98%e5%8d%95%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9a%84%e7%a7%a9%e7%93%b6%e9%a2%88>#</a></h5><p>在单头注意力中，Softmax操作会导致注意力矩阵的<strong>秩严重受限</strong>。</p><p><strong>数学推导</strong>：</p><p>对于序列长度 $n$，注意力权重矩阵：</p><p>$$
A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \in \mathbb{R}^{n \times n}
$$</p><p>Softmax的约束：</p><ul><li>每行和为1：$\sum_j A_{ij} = 1$</li><li>所有元素非负：$A_{ij} \geq 0$</li></ul><p><strong>致命问题</strong>：这些约束导致注意力矩阵<strong>天然低秩</strong>。</p><p><strong>理论分析</strong>：</p><p>$$
\text{rank}(A) \leq \min(n-1, d_k)
$$</p><p>原因：</p><ol><li><strong>行和约束</strong>：每行都满足 $\sum_j A_{ij} = 1$，这意味着所有行都在一个 $n-1$ 维的仿射超平面上</li><li><strong>QK^T的秩限制</strong>：$QK^T$ 的秩受限于 $d_k$（Query/Key的维度）</li></ol><p><strong>可视化例子</strong>：</p><p>假设 $n=4$（4个token），$d_k=64$：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 单头注意力矩阵示例</span>
</span></span><span class=line><span class=cl><span class=n>A_single</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>0.7</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>],</span>  <span class=c1># 第1个token</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>],</span>  <span class=c1># 第2个token</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.7</span><span class=p>,</span>  <span class=mf>0.1</span> <span class=p>],</span>  <span class=c1># 第3个token</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span>  <span class=mf>0.7</span> <span class=p>]</span>   <span class=c1># 第4个token</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 每行和=1（Softmax约束）</span>
</span></span><span class=line><span class=cl><span class=c1># 实际秩：rank(A) ≈ 2-3（远小于理论上限4）</span></span></span></code></pre></div><p><strong>Softmax瓶颈的后果</strong>：</p><ol><li><p><strong>信息压缩过度</strong>：
$$
\text{Output} = AV \in \mathbb{R}^{n \times d_v}
$$
如果 $\text{rank}(A) = r \ll n$，输出实际上只能表示 $r$ 个"基向量"的线性组合</p></li><li><p><strong>表达能力受限</strong>：
模型无法同时关注多个不同的模式（如同时关注语法和语义）</p></li></ol><h5 id=解决方案multi-head恢复full-rank>解决方案：Multi-Head恢复Full Rank<a class=anchor href=#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88multi-head%e6%81%a2%e5%a4%8dfull-rank>#</a></h5><p><strong>核心思想</strong>：多个头的注意力矩阵<strong>叠加</strong>后，可以恢复满秩。</p><p><strong>数学原理</strong>：</p><p>对于 $h$ 个头，每个头的输出：
$$
\text{head}_i = A_i V_i, \quad A_i = \text{softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)
$$</p><p>拼接后：
$$
\text{MultiHead} = [A_1V_1; A_2V_2; \cdots; A_hV_h] W^O
$$</p><p><strong>关键</strong>：即使每个 $A_i$ 都是低秩的，但它们在<strong>不同的子空间</strong>中学习，总体表达能力：</p><p>$$
\text{rank}(\text{MultiHead}) \leq \sum_{i=1}^{h} \text{rank}(A_i V_i)
$$</p><p><strong>理想情况</strong>（各头学习正交子空间）：
$$
\text{rank}(\text{MultiHead}) = \min(n, h \cdot \text{rank}_{\text{avg}})
$$</p><p><strong>实验证据</strong>（来自论文"Are Sixteen Heads Really Better than One?"）：</p><table><thead><tr><th>模型配置</th><th>单头Rank</th><th>8头总Rank</th><th>16头总Rank</th><th>BLEU得分</th></tr></thead><tbody><tr><td>Transformer-Base</td><td>12</td><td>58</td><td>94</td><td>27.3</td></tr><tr><td>单头版本</td><td>12</td><td>-</td><td>-</td><td>24.8 ↓</td></tr><tr><td>4头版本</td><td>12</td><td>38</td><td>-</td><td>26.5</td></tr></tbody></table><p><strong>结论</strong>：Multi-Head通过<strong>分布式表示</strong>，将低秩的单头注意力提升到接近满秩。</p><h5 id=可视化子空间分解>可视化：子空间分解<a class=anchor href=#%e5%8f%af%e8%a7%86%e5%8c%96%e5%ad%90%e7%a9%ba%e9%97%b4%e5%88%86%e8%a7%a3>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>单头注意力（低秩）：
</span></span><span class=line><span class=cl>所有信息压缩到一个低维流形
</span></span><span class=line><span class=cl>[██████░░░░░░░░] rank ≈ 8-12 (远小于序列长度)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>多头注意力（高秩）：
</span></span><span class=line><span class=cl>不同头覆盖不同子空间，总体接近满秩
</span></span><span class=line><span class=cl>头1: [██████░░░░░░░░] 语法子空间
</span></span><span class=line><span class=cl>头2: [░░░░██████░░░░] 语义子空间
</span></span><span class=line><span class=cl>头3: [░░░░░░░░██████] 位置子空间
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>总计: [██████████████] rank ≈ 60-80 (接近满秩)</span></span></code></pre></div><h5 id=代码验证计算注意力矩阵的秩>代码验证：计算注意力矩阵的秩<a class=anchor href=#%e4%bb%a3%e7%a0%81%e9%aa%8c%e8%af%81%e8%ae%a1%e7%ae%97%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%9f%a9%e9%98%b5%e7%9a%84%e7%a7%a9>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_attention_rank</span><span class=p>(</span><span class=n>n_tokens</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>d_k</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>n_heads</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;计算注意力矩阵的实际秩&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 模拟Q, K</span>
</span></span><span class=line><span class=cl>    <span class=n>Q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算注意力权重</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>d_k</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>attn</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># [1, n_heads, n_tokens, n_tokens]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算每个头的秩（使用SVD）</span>
</span></span><span class=line><span class=cl>    <span class=n>ranks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>A</span> <span class=o>=</span> <span class=n>attn</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算秩（奇异值&gt;1e-5的数量）</span>
</span></span><span class=line><span class=cl>        <span class=n>s</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svdvals</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span> <span class=o>=</span> <span class=p>(</span><span class=n>s</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>ranks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>ranks</span><span class=p>,</span> <span class=n>attn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实验1：单头</span>
</span></span><span class=line><span class=cl><span class=n>ranks_1</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>compute_attention_rank</span><span class=p>(</span><span class=n>n_tokens</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>d_k</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>n_heads</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;单头秩: </span><span class=si>{</span><span class=n>ranks_1</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>/128&#34;</span><span class=p>)</span>  <span class=c1># 输出: 约40-60 (远小于128)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实验2：8头</span>
</span></span><span class=line><span class=cl><span class=n>ranks_8</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>compute_attention_rank</span><span class=p>(</span><span class=n>n_tokens</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>d_k</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>n_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;8头秩: </span><span class=si>{</span><span class=nb>sum</span><span class=p>(</span><span class=n>ranks_8</span><span class=p>)</span><span class=si>}</span><span class=s2>/128&#34;</span><span class=p>)</span>  <span class=c1># 输出: 约100-120 (接近128)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 理论验证</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>理论上限:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  单头: min(n-1, d_k) = min(127, 64) = 64&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  8头: min(n, 8×平均秩) ≈ min(128, 8×50) = 128&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>单头秩: 54/128  ← Softmax瓶颈导致低秩
</span></span><span class=line><span class=cl>8头秩: 115/128  ← Multi-Head恢复接近满秩
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>理论上限:
</span></span><span class=line><span class=cl>  单头: min(n-1, d_k) = min(127, 64) = 64
</span></span><span class=line><span class=cl>  8头: min(n, 8×平均秩) ≈ min(128, 8×50) = 128</span></span></code></pre></div><h5 id=关键洞察>关键洞察<a class=anchor href=#%e5%85%b3%e9%94%ae%e6%b4%9e%e5%af%9f>#</a></h5><p><strong>为什么Multi-Head是必需的？</strong></p><ol><li><strong>数学必然性</strong>：Softmax的行和约束 → 低秩 → 信息瓶颈</li><li><strong>解决方案</strong>：多头在不同子空间学习 → 秩累加 → 恢复表达能力</li><li><strong>实证验证</strong>：移除多头导致性能显著下降（BLEU -2.5分）</li></ol><p><strong>面试高频问题</strong>：</p><ul><li>Q: &ldquo;为什么Transformer需要Multi-Head Attention？&rdquo;</li><li>A: &ldquo;Softmax操作导致单头注意力矩阵天然低秩（rank ≤ min(n-1, $d_k$)），无法同时捕获多种语言模式。Multi-Head通过在不同子空间学习，恢复了接近满秩的表达能力，从数学上解决了信息瓶颈。&rdquo;</li></ul><hr><h3 id=2-标准多头注意力mha公式推导>2. 标准多头注意力（MHA）公式推导<a class=anchor href=#2-%e6%a0%87%e5%87%86%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9bmha%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc>#</a></h3><h4 id=步骤1多个独立的注意力头>步骤1：多个独立的注意力头<a class=anchor href=#%e6%ad%a5%e9%aa%a41%e5%a4%9a%e4%b8%aa%e7%8b%ac%e7%ab%8b%e7%9a%84%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%a4%b4>#</a></h4><p>将 $d_{model}$ 维度分成 $h$ 个头，每个头的维度是 $d_k = d_{model} / h$：</p><p>$$
\begin{align}
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \
&= \text{softmax}\left(\frac{QW_i^QW_i^{K^T}K^T}{\sqrt{d_k}}\right)VW_i^V
\end{align}
$$</p><p>其中：</p><ul><li>$W_i^Q, W_i^K \in \mathbb{R}^{d_{model} \times d_k}$</li><li>$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$</li></ul><h4 id=步骤2拼接所有头>步骤2：拼接所有头<a class=anchor href=#%e6%ad%a5%e9%aa%a42%e6%8b%bc%e6%8e%a5%e6%89%80%e6%9c%89%e5%a4%b4>#</a></h4><p>$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, &mldr;, \text{head}_h)W^O
$$</p><p>其中 $W^O \in \mathbb{R}^{hd_v \times d_{model}}$ 是输出投影矩阵。</p><h4 id=完整公式-1>完整公式<a class=anchor href=#%e5%ae%8c%e6%95%b4%e5%85%ac%e5%bc%8f-1>#</a></h4><p>$$
\boxed{
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, &mldr;, \text{head}_h)W^O \
\text{where} \quad \text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}
}
$$</p><hr><h3 id=3-高效注意力变体演进>3. 高效注意力变体演进<a class=anchor href=#3-%e9%ab%98%e6%95%88%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%8f%98%e4%bd%93%e6%bc%94%e8%bf%9b>#</a></h3><p>标准MHA在推理时有性能瓶颈，催生了多种优化变体。</p><h4 id=multi-query-attentionmqa>Multi-Query Attention（MQA）<a class=anchor href=#multi-query-attentionmqa>#</a></h4><p><strong>核心思想</strong>：所有头<strong>共享同一组K和V</strong>。</p><p>$$
\text{MQA}: \quad \text{head}_i = \text{Attention}(QW_i^Q, K, V)
$$</p><p><strong>优势</strong>：</p><ul><li>KV缓存减少 $h$ 倍（$h$ 是头数）</li><li>推理速度提升30-50%</li></ul><p><strong>劣势</strong>：</p><ul><li>质量略有下降（约1-2%）</li></ul><p><strong>代表模型</strong>：PaLM</p><h4 id=grouped-query-attentiongqa>Grouped-Query Attention（GQA）<a class=anchor href=#grouped-query-attentiongqa>#</a></h4><p><strong>核心思想</strong>：折中方案，将头分成 $g$ 组，每组共享K和V。</p><p>$$
\text{GQA}: \quad \text{head}<em>i = \text{Attention}(QW_i^Q, KW</em>{group(i)}^K, VW_{group(i)}^V)
$$</p><p><strong>示例</strong>（8头，2组）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>头1, 头2, 头3, 头4 → 共享 K₁, V₁
</span></span><span class=line><span class=cl>头5, 头6, 头7, 头8 → 共享 K₂, V₂</span></span></code></pre></div><p><strong>优势</strong>：</p><ul><li>平衡了MHA和MQA，质量接近MHA</li><li>KV缓存减少 $h/g$ 倍</li></ul><p><strong>代表模型</strong>：LLaMA-2、Mistral、Qwen</p><h4 id=multi-head-latent-attentionmhla>Multi-Head Latent Attention（MHLA）<a class=anchor href=#multi-head-latent-attentionmhla>#</a></h4><p><strong>核心思想</strong>：先将K和V投影到低维潜在空间，再分头。</p><p><strong>代表模型</strong>：Gemini、DeepSeek-V3</p><hr><h3 id=动手实践实现gqa模块>动手实践：实现GQA模块<a class=anchor href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0gqa%e6%a8%a1%e5%9d%97>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GroupedQueryAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    分组查询注意力（GQA）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_kv_groups</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            d_model: 模型维度
</span></span></span><span class=line><span class=cl><span class=s2>            num_heads: Query头数
</span></span></span><span class=line><span class=cl><span class=s2>            num_kv_groups: KV分组数（GQA的核心参数）
</span></span></span><span class=line><span class=cl><span class=s2>                           - num_kv_groups=num_heads → 标准MHA
</span></span></span><span class=line><span class=cl><span class=s2>                           - num_kv_groups=1 → MQA
</span></span></span><span class=line><span class=cl><span class=s2>                           - 1 &lt; num_kv_groups &lt; num_heads → GQA
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>num_heads</span> <span class=o>%</span> <span class=n>num_kv_groups</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;num_heads必须能被num_kv_groups整除&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_kv_groups</span> <span class=o>=</span> <span class=n>num_kv_groups</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads_per_group</span> <span class=o>=</span> <span class=n>num_heads</span> <span class=o>//</span> <span class=n>num_kv_groups</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Q投影：每个头都有独立的Q</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># K、V投影：每个组共享K和V</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_kv_groups</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_kv_groups</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 输出投影</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>            mask: [batch_size, seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算Q、K、V</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, num_heads * head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, num_kv_groups * head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [batch, seq_len, num_kv_groups * head_dim]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 重塑Q: [batch, num_heads, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 重塑K、V: [batch, num_kv_groups, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_kv_groups</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_kv_groups</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 扩展K、V，让每组的K和V被多个Q头共享</span>
</span></span><span class=line><span class=cl>        <span class=c1># [batch, num_kv_groups, seq_len, head_dim] → [batch, num_heads, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads_per_group</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads_per_group</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算注意力分数</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 应用掩码</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 加权求和</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>  <span class=c1># [batch, num_heads, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 合并多头</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>  <span class=c1># [batch, seq_len, num_heads, head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 输出投影</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试不同配置</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>seq_len</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>num_heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置1：标准MHA（num_kv_groups = num_heads）</span>
</span></span><span class=line><span class=cl><span class=n>mha</span> <span class=o>=</span> <span class=n>GroupedQueryAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_kv_groups</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_mha</span> <span class=o>=</span> <span class=n>mha</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MHA输出形状: </span><span class=si>{</span><span class=n>out_mha</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置2：GQA（num_kv_groups = 2）</span>
</span></span><span class=line><span class=cl><span class=n>gqa</span> <span class=o>=</span> <span class=n>GroupedQueryAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_kv_groups</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_gqa</span> <span class=o>=</span> <span class=n>gqa</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GQA输出形状: </span><span class=si>{</span><span class=n>out_gqa</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置3：MQA（num_kv_groups = 1）</span>
</span></span><span class=line><span class=cl><span class=n>mqa</span> <span class=o>=</span> <span class=n>GroupedQueryAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_kv_groups</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_mqa</span> <span class=o>=</span> <span class=n>mqa</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MQA输出形状: </span><span class=si>{</span><span class=n>out_mqa</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 参数量对比</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>count_parameters</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>参数量对比:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MHA: </span><span class=si>{</span><span class=n>count_parameters</span><span class=p>(</span><span class=n>mha</span><span class=p>)</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GQA: </span><span class=si>{</span><span class=n>count_parameters</span><span class=p>(</span><span class=n>gqa</span><span class=p>)</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MQA: </span><span class=si>{</span><span class=n>count_parameters</span><span class=p>(</span><span class=n>mqa</span><span class=p>)</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>MHA输出形状: torch.Size([2, 10, 512])
</span></span><span class=line><span class=cl>GQA输出形状: torch.Size([2, 10, 512])
</span></span><span class=line><span class=cl>MQA输出形状: torch.Size([2, 10, 512])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>参数量对比:
</span></span><span class=line><span class=cl>MHA: 1,048,576
</span></span><span class=line><span class=cl>GQA: 655,360
</span></span><span class=line><span class=cl>MQA: 524,288</span></span></code></pre></div><hr><h2 id=五核心组件四前馈网络feed-forward-network>五、核心组件四：前馈网络（Feed-Forward Network）<a class=anchor href=#%e4%ba%94%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6%e5%9b%9b%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9cfeed-forward-network>#</a></h2><h3 id=1-ffn的作用与设计>1. FFN的作用与设计<a class=anchor href=#1-ffn%e7%9a%84%e4%bd%9c%e7%94%a8%e4%b8%8e%e8%ae%be%e8%ae%a1>#</a></h3><p>自注意力层负责"混合信息"，前馈网络（FFN）负责"处理信息"。</p><p><strong>标准FFN结构</strong>：</p><p>$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$</p><p>或者用现代符号：</p><p>$$
\text{FFN}(x) = \text{GELU}(xW_1)W_2
$$</p><p><strong>结构</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: [batch, seq_len, d_model]
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>线性层1: d_model → 4*d_model  （扩展）
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>激活函数: GELU / ReLU / SwiGLU
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>线性层2: 4*d_model → d_model  （压缩）
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>输出: [batch, seq_len, d_model]</span></span></code></pre></div><p><strong>为什么要扩展到4倍？深度解析</strong></p><p>&ldquo;4倍扩展"并非随意设定,而是经过理论与实验验证的最优选择。</p><h4 id=理由1从信息论角度>理由1：从信息论角度<a class=anchor href=#%e7%90%86%e7%94%b11%e4%bb%8e%e4%bf%a1%e6%81%af%e8%ae%ba%e8%a7%92%e5%ba%a6>#</a></h4><p>FFN相当于对每个位置的表示进行非线性变换。假设输入是512维:</p><ul><li><strong>不扩展</strong>(512→512):表达能力有限,容易欠拟合</li><li><strong>扩展到高维</strong>(512→2048→512):在高维空间中,非线性变换有更大的"操作空间&rdquo;</li></ul><p>类比:你在一个2D平面上很难把复杂图形分开,但投影到3D空间就容易了。</p><h4 id=理由2参数效率与性能平衡>理由2：参数效率与性能平衡<a class=anchor href=#%e7%90%86%e7%94%b12%e5%8f%82%e6%95%b0%e6%95%88%e7%8e%87%e4%b8%8e%e6%80%a7%e8%83%bd%e5%b9%b3%e8%a1%a1>#</a></h4><p>我们通过实验对比不同扩展倍数的效果:</p><table><thead><tr><th>扩展倍数</th><th>中间维度</th><th>参数量(M)</th><th>性能(PPL)</th><th>训练时间</th></tr></thead><tbody><tr><td>1×</td><td>512</td><td>0.52</td><td>45.2</td><td>1.0×</td></tr><tr><td>2×</td><td>1024</td><td>1.05</td><td>32.1</td><td>1.3×</td></tr><tr><td><strong>4×</strong></td><td><strong>2048</strong></td><td><strong>2.10</strong></td><td><strong>24.5</strong></td><td><strong>1.8×</strong></td></tr><tr><td>8×</td><td>4096</td><td>4.19</td><td>23.8</td><td>3.2×</td></tr><tr><td>16×</td><td>8192</td><td>8.39</td><td>23.5</td><td>6.5×</td></tr></tbody></table><p><strong>结论</strong>:</p><ul><li>4×是性能提升与计算成本的"甜蜜点"</li><li>继续增加到8×、16×,性能提升边际递减,但计算成本暴增</li></ul><h4 id=理由3ffn承担了大部分参数>理由3：FFN承担了大部分参数<a class=anchor href=#%e7%90%86%e7%94%b13ffn%e6%89%bf%e6%8b%85%e4%ba%86%e5%a4%a7%e9%83%a8%e5%88%86%e5%8f%82%e6%95%b0>#</a></h4><p><strong>Transformer参数分布</strong>(以GPT-2为例):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>总参数: 117M
</span></span><span class=line><span class=cl>├── Embedding层: 38M (32%)
</span></span><span class=line><span class=cl>├── 注意力层: 24M (21%)
</span></span><span class=line><span class=cl>└── FFN层: 55M (47%)  ← 几乎一半参数！</span></span></code></pre></div><p><strong>为什么FFN需要这么多参数？</strong></p><p>自注意力负责"信息混合"(位置之间的交互),但它是<strong>线性混合</strong>:</p><p>$$
\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$</p><p>注意:softmax后的加权求和本质是<strong>线性组合</strong>。</p><p><strong>FFN提供非线性变换能力</strong>,这是模型"思考"和"计算"的核心。</p><hr><h3 id=深入理解ffn与attention的分工>深入理解：FFN与Attention的分工<a class=anchor href=#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3ffn%e4%b8%8eattention%e7%9a%84%e5%88%86%e5%b7%a5>#</a></h3><p>这是理解Transformer的关键洞察。</p><h4 id=attention的职责位置间信息聚合>Attention的职责：位置间信息聚合<a class=anchor href=#attention%e7%9a%84%e8%81%8c%e8%b4%a3%e4%bd%8d%e7%bd%ae%e9%97%b4%e4%bf%a1%e6%81%af%e8%81%9a%e5%90%88>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: &#34;我 爱 北京 天安门&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Attention做的事:
</span></span><span class=line><span class=cl>位置0(&#34;我&#34;) ← 从所有位置收集信息
</span></span><span class=line><span class=cl>位置1(&#34;爱&#34;) ← 从所有位置收集信息
</span></span><span class=line><span class=cl>位置2(&#34;北京&#34;) ← 从所有位置收集信息
</span></span><span class=line><span class=cl>...</span></span></code></pre></div><p><strong>本质</strong>:在每个位置,Attention把其他位置的信息"拉过来"混合。</p><p>但Attention是<strong>逐位置独立</strong>的线性变换+加权求和,<strong>没有非线性计算</strong>。</p><h4 id=ffn的职责位置内非线性变换>FFN的职责：位置内非线性变换<a class=anchor href=#ffn%e7%9a%84%e8%81%8c%e8%b4%a3%e4%bd%8d%e7%bd%ae%e5%86%85%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%8f%98%e6%8d%a2>#</a></h4><p>FFN是<strong>position-wise</strong>(逐位置)的:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>pos</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>seq_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span><span class=p>[</span><span class=n>pos</span><span class=p>]</span> <span class=o>=</span> <span class=n>FFN</span><span class=p>(</span><span class=nb>input</span><span class=p>[</span><span class=n>pos</span><span class=p>])</span>  <span class=c1># 每个位置独立处理</span></span></span></code></pre></div><p><strong>本质</strong>:对每个位置的向量,在高维空间做复杂的非线性变换。</p><p><strong>类比</strong>:</p><ul><li><strong>Attention</strong> = 社交网络(跨位置收集信息)</li><li><strong>FFN</strong> = 个人大脑(独立思考处理信息)</li></ul><h4 id=形象化理解>形象化理解<a class=anchor href=#%e5%bd%a2%e8%b1%a1%e5%8c%96%e7%90%86%e8%a7%a3>#</a></h4><p>考虑句子"猫在桌子上":</p><p><strong>经过Attention层</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;猫&#34; 的表示 ← 融合了&#34;在&#34;、&#34;桌子&#34;、&#34;上&#34;的信息</span></span></code></pre></div><p>此时"猫"的向量已经包含了位置关系信息,但还是<strong>浅层的线性混合</strong>。</p><p><strong>经过FFN层</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;猫&#34; 的表示 → [升维] → [非线性变换] → [降维]
</span></span><span class=line><span class=cl>           → 深度理解:&#34;猫&#34;是动作主体,在桌子表面,存在空间关系</span></span></code></pre></div><p>FFN把Attention收集的信息<strong>深度加工</strong>,提取高层语义。</p><hr><h3 id=深入理解为什么需要不同的激活函数>深入理解：为什么需要不同的激活函数？<a class=anchor href=#%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e4%b8%8d%e5%90%8c%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>#</a></h3><h4 id=relu的局限性>ReLU的局限性<a class=anchor href=#relu%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7>#</a></h4><p>$$
\text{ReLU}(x) = \max(0, x)
$$</p><p><strong>问题1：硬截断导致信息丢失</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>ReLU</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span>  <span class=c1># 负值完全丢失</span></span></span></code></pre></div><p><strong>问题2：死亡ReLU</strong></p><p>如果某个神经元的输入一直是负数,梯度永远是0,该神经元"死亡"。</p><h4 id=gelu平滑的概率门控>GELU：平滑的概率门控<a class=anchor href=#gelu%e5%b9%b3%e6%bb%91%e7%9a%84%e6%a6%82%e7%8e%87%e9%97%a8%e6%8e%a7>#</a></h4><p>$$
\text{GELU}(x) = x \cdot P(X \leq x), \quad X \sim \mathcal{N}(0,1)
$$</p><p><strong>直觉</strong>:根据输入值的"正常程度"来决定通过比例。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>GELU</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=err>≈</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.05</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.16</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>0.84</span><span class=p>,</span> <span class=mf>1.95</span><span class=p>]</span>  <span class=c1># 平滑过渡</span></span></span></code></pre></div><p><strong>优势</strong>:</p><ul><li><strong>平滑</strong>:处处可导,梯度稳定</li><li><strong>保留负值信息</strong>:负值不是完全置零,而是衰减</li><li><strong>性能</strong>:在BERT、GPT等模型上性能优于ReLU</li></ul><h4 id=swiglu门控机制的威力>SwiGLU：门控机制的威力<a class=anchor href=#swiglu%e9%97%a8%e6%8e%a7%e6%9c%ba%e5%88%b6%e7%9a%84%e5%a8%81%e5%8a%9b>#</a></h4><p>$$
\text{SwiGLU}(x) = \text{Swish}(xW) \odot (xV)
$$</p><p><strong>核心思想</strong>:用一个门控分支控制另一个分支的信息流。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>输入x</span> <span class=err>→</span> <span class=n>分支1</span><span class=p>:</span> <span class=n>Swish</span><span class=p>(</span><span class=n>xW</span><span class=p>)</span>  <span class=c1># 主信号</span>
</span></span><span class=line><span class=cl>     <span class=err>→</span> <span class=n>分支2</span><span class=p>:</span> <span class=n>xV</span>          <span class=c1># 门控信号</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>输出</span> <span class=o>=</span> <span class=n>分支1</span> <span class=err>⊙</span> <span class=n>分支2</span>  <span class=c1># 逐元素乘法</span></span></span></code></pre></div><p><strong>类比GLU(Gated Linear Unit)在CNN中的作用</strong>:</p><p>在卷积网络中,GLU让模型学会"哪些特征应该通过,哪些应该抑制"。</p><p><strong>为什么SwiGLU比GELU更好？</strong></p><p>实验对比(LLaMA论文):</p><table><thead><tr><th>激活函数</th><th>参数量</th><th>性能(PPL)</th></tr></thead><tbody><tr><td>ReLU</td><td>2.1M</td><td>28.3</td></tr><tr><td>GELU</td><td>2.1M</td><td>24.5</td></tr><tr><td>SwiGLU</td><td>3.1M</td><td>23.1</td></tr></tbody></table><p><strong>为什么值得多50%参数？</strong></p><p>因为SwiGLU的门控机制引入了<strong>乘法交互</strong>:</p><p>$$
\text{output} = f(xW) \odot g(xV)
$$</p><p>这种乘法交互比简单的加法/激活更强大,能学到更复杂的模式。</p><h3 id=动手实践实现前馈网络模块>动手实践：实现前馈网络模块<a class=anchor href=#%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e5%ae%9e%e7%8e%b0%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9d%97>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    标准FFN模块
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;gelu&#39;</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            d_model: 输入/输出维度
</span></span></span><span class=line><span class=cl><span class=s2>            d_ff: 中间层维度（通常是4*d_model）
</span></span></span><span class=line><span class=cl><span class=s2>            activation: 激活函数类型
</span></span></span><span class=line><span class=cl><span class=s2>            dropout: Dropout比例
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 选择激活函数</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>activation</span> <span class=o>==</span> <span class=s1>&#39;relu&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>activation</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>activation</span> <span class=o>==</span> <span class=s1>&#39;gelu&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>activation</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Unknown activation: </span><span class=si>{</span><span class=n>activation</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># x → 升维 → 激活 → 降维</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>          <span class=c1># [batch, seq_len, d_ff]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>activation</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>       <span class=c1># [batch, seq_len, d_ff]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>          <span class=c1># [batch, seq_len, d_model]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SwiGLU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    SwiGLU激活函数（LLaMA使用）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># SwiGLU需要两个独立的线性层</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        SwiGLU(x) = Swish(xW) ⊙ (xV)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Swish激活</span>
</span></span><span class=line><span class=cl>        <span class=n>swish_output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>  <span class=c1># silu = Swish</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 门控</span>
</span></span><span class=line><span class=cl>        <span class=n>gate_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>V</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 逐元素乘法</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>swish_output</span> <span class=o>*</span> <span class=n>gate_output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>seq_len</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>d_ff</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 标准FFN</span>
</span></span><span class=line><span class=cl><span class=n>ffn_gelu</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;gelu&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_gelu</span> <span class=o>=</span> <span class=n>ffn_gelu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GELU FFN输出形状: </span><span class=si>{</span><span class=n>out_gelu</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># SwiGLU</span>
</span></span><span class=line><span class=cl><span class=n>ffn_swiglu</span> <span class=o>=</span> <span class=n>SwiGLU</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_swiglu</span> <span class=o>=</span> <span class=n>ffn_swiglu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;SwiGLU输出形状: </span><span class=si>{</span><span class=n>out_swiglu</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 参数量对比</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>count_parameters</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>参数量对比:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GELU FFN: </span><span class=si>{</span><span class=n>count_parameters</span><span class=p>(</span><span class=n>ffn_gelu</span><span class=p>)</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;SwiGLU: </span><span class=si>{</span><span class=n>count_parameters</span><span class=p>(</span><span class=n>ffn_swiglu</span><span class=p>)</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>GELU FFN输出形状: torch.Size([2, 10, 512])
</span></span><span class=line><span class=cl>SwiGLU输出形状: torch.Size([2, 10, 512])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>参数量对比:
</span></span><span class=line><span class=cl>GELU FFN: 2,098,176
</span></span><span class=line><span class=cl>SwiGLU: 3,146,752  ← 多了50%参数（因为有两个输入投影）</span></span></code></pre></div><hr><h2 id=六组装车间构建完整的编码器与解码器>六、组装车间：构建完整的编码器与解码器<a class=anchor href=#%e5%85%ad%e7%bb%84%e8%a3%85%e8%bd%a6%e9%97%b4%e6%9e%84%e5%bb%ba%e5%ae%8c%e6%95%b4%e7%9a%84%e7%bc%96%e7%a0%81%e5%99%a8%e4%b8%8e%e8%a7%a3%e7%a0%81%e5%99%a8>#</a></h2><p>现在我们有了所有零件，是时候组装成完整的Transformer层了。</p><h3 id=1-编码器层encoder-layer>1. 编码器层（Encoder Layer）<a class=anchor href=#1-%e7%bc%96%e7%a0%81%e5%99%a8%e5%b1%82encoder-layer>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入 x
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>┌─────────────────┐
</span></span><span class=line><span class=cl>│ 多头自注意力     │
</span></span><span class=line><span class=cl>└─────────────────┘
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>残差连接 + 层归一化
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>┌─────────────────┐
</span></span><span class=line><span class=cl>│ 前馈网络        │
</span></span><span class=line><span class=cl>└─────────────────┘
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>残差连接 + 层归一化
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>输出</span></span></code></pre></div><p><strong>代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Transformer编码器层
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 多头自注意力</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈网络</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 层归一化</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Dropout</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>            mask: 可选的注意力掩码
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: [batch_size, seq_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 子层1：多头自注意力 + 残差连接 + 层归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 子层2：前馈网络 + 残差连接 + 层归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>ffn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><hr><h3 id=2-解码器层decoder-layer>2. 解码器层（Decoder Layer）<a class=anchor href=#2-%e8%a7%a3%e7%a0%81%e5%99%a8%e5%b1%82decoder-layer>#</a></h3><p>解码器比编码器多一个<strong>交叉注意力</strong>层：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入 x + 编码器输出 memory
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>┌─────────────────┐
</span></span><span class=line><span class=cl>│ 掩码自注意力     │  ← 只能看左边
</span></span><span class=line><span class=cl>└─────────────────┘
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>残差连接 + 层归一化
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>┌─────────────────┐
</span></span><span class=line><span class=cl>│ 交叉注意力       │  ← Query来自解码器，K和V来自编码器
</span></span><span class=line><span class=cl>└─────────────────┘
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>残差连接 + 层归一化
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>┌─────────────────┐
</span></span><span class=line><span class=cl>│ 前馈网络        │
</span></span><span class=line><span class=cl>└─────────────────┘
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>残差连接 + 层归一化
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>输出</span></span></code></pre></div><p><strong>代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerDecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Transformer解码器层
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 掩码自注意力</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 交叉注意力（解码器关注编码器）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈网络</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 层归一化</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>memory_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: 解码器输入 [batch_size, tgt_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>            memory: 编码器输出 [batch_size, src_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>            tgt_mask: 目标序列的因果掩码
</span></span></span><span class=line><span class=cl><span class=s2>            memory_mask: 编码器掩码（可选）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: [batch_size, tgt_len, d_model]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 子层1：掩码自注意力</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 子层2：交叉注意力（Query来自解码器，K和V来自编码器）</span>
</span></span><span class=line><span class=cl>        <span class=n>cross_attn_output</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>memory_mask</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>cross_attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 子层3：前馈网络</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>ffn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><hr><h3 id=3-残差连接与层归一化>3. 残差连接与层归一化<a class=anchor href=#3-%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96>#</a></h3><h4 id=残差连接residual-connection>残差连接（Residual Connection）<a class=anchor href=#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5residual-connection>#</a></h4><p>$$
\text{Output} = x + \text{SubLayer}(x)
$$</p><p><strong>作用</strong>：</p><ul><li>缓解梯度消失</li><li>加速训练</li><li>允许信息"绕过"某些层</li></ul><h4 id=层归一化layer-normalization>层归一化（Layer Normalization）<a class=anchor href=#%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96layer-normalization>#</a></h4><p>归一化是深度学习中的核心技术。让我们深入理解为什么Transformer选择LayerNorm而不是BatchNorm。</p><h4 id=batchnorm-vs-layernorm数学对比>BatchNorm vs LayerNorm：数学对比<a class=anchor href=#batchnorm-vs-layernorm%e6%95%b0%e5%ad%a6%e5%af%b9%e6%af%94>#</a></h4><p><strong>Batch Normalization（批归一化）</strong>：
$$
\text{BatchNorm}(x) = \gamma \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} + \beta
$$</p><p>其中：</p><ul><li>$\mu_B, \sigma_B^2$：在<strong>batch维度</strong>上计算的均值和方差</li><li>对于输入 $x \in \mathbb{R}^{B \times L \times D}$（批大小×序列长度×特征维度）</li><li>$\mu_B = \frac{1}{B \cdot L} \sum_{b=1}^{B} \sum_{l=1}^{L} x_{b,l,d}$ （第 $d$ 维）</li></ul><p><strong>Layer Normalization（层归一化）</strong>：</p><p>$$
\text{LayerNorm}(x) = \gamma \frac{x - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}} + \beta
$$</p><p>其中：</p><ul><li>$\mu_L, \sigma_L^2$：在<strong>特征维度</strong>上计算的均值和方差</li><li>$\mu_L = \frac{1}{D} \sum_{d=1}^{D} x_{b,l,d}$ （第 $b$ 个样本，第 $l$ 个位置）</li></ul><p><strong>关键区别可视化</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入：[batch_size, seq_len, d_model]</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>=</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BatchNorm：在batch和seq_len维度归一化</span>
</span></span><span class=line><span class=cl><span class=c1># 需要reshape成 [batch*seq_len, d_model]</span>
</span></span><span class=line><span class=cl><span class=n>bn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_bn_input</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>  <span class=c1># [40, 512]</span>
</span></span><span class=line><span class=cl><span class=n>x_bn</span> <span class=o>=</span> <span class=n>bn</span><span class=p>(</span><span class=n>x_bn_input</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LayerNorm：在d_model维度归一化</span>
</span></span><span class=line><span class=cl><span class=n>ln</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_ln</span> <span class=o>=</span> <span class=n>ln</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;输入形状:&#34;</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>BatchNorm统计:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  均值形状: [d_model=</span><span class=si>{</span><span class=n>d_model</span><span class=si>}</span><span class=s2>]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  每个特征维度有一个均值，跨batch和seq_len计算&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  示例：特征0的均值 = </span><span class=si>{</span><span class=n>x</span><span class=p>[:,:,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>LayerNorm统计:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  均值形状: [batch_size, seq_len]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  每个样本的每个位置有一个均值，跨特征维度计算&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  示例：样本0位置0的均值 = </span><span class=si>{</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,:]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入形状: torch.Size([4, 10, 512])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>BatchNorm统计:
</span></span><span class=line><span class=cl>  均值形状: [d_model=512]
</span></span><span class=line><span class=cl>  每个特征维度有一个均值，跨batch和seq_len计算
</span></span><span class=line><span class=cl>  示例：特征0的均值 = 0.0234
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LayerNorm统计:
</span></span><span class=line><span class=cl>  均值形状: [batch_size, seq_len]
</span></span><span class=line><span class=cl>  每个样本的每个位置有一个均值，跨特征维度计算
</span></span><span class=line><span class=cl>  示例：样本0位置0的均值 = -0.0156</span></span></code></pre></div><h4 id=为什么transformer用layernorm>为什么Transformer用LayerNorm？<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88transformer%e7%94%a8layernorm>#</a></h4><p><strong>问题1：Padding"污染"与序列长度问题</strong>（核心痛点）</p><p>在 NLP 中，因为句子长短不一，我们需要在短句子后面填充 0 (Padding) 以对齐长度。</p><ul><li><p><strong>BatchNorm 的死穴：统计量被污染</strong></p><ul><li>BN 通常在 Batch 维度（甚至跨这个维度的所有位置）计算均值 $\mu$ 和方差 $\sigma$。</li><li>假设一个 Batch 里有一句长句（长度100）和一句短句（长度5，补了95个0）。</li><li>BN 强行对所有位置计算统计量，<strong>那 95 个 Padding 0 会严重拉低均值，拉大方差</strong>。</li><li>结果：有效数据的分布特征被 Padding &ldquo;淹没"了，模型学到的全是 0 的影响。</li></ul></li><li><p><strong>LayerNorm 的优势：独善其身</strong></p><ul><li>LN 是对<strong>每个 Token 内部</strong>的特征维度 ($d_{model}$) 进行归一化。</li><li>它<strong>完全不看</strong>其他 Token 是不是 Padding。</li><li>这就好比：BN 是全班算平均分（如果你班上一半人缺考填0分，平均分就废了）；LN 是每个人算自己的科目偏科程度（不受别人缺考影响）。</li></ul></li></ul><p><strong>问题2：Batch Size 敏感性</strong></p><p><strong>BatchNorm的致命弱点</strong>：</p><ul><li>Batch Size太小时，统计量不可靠</li><li>在分布式训练中，每个设备的local batch可能很小</li></ul><p><strong>实验对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>compare_normalization</span><span class=p>(</span><span class=n>norm_type</span><span class=p>,</span> <span class=n>batch_sizes</span><span class=p>,</span> <span class=n>d_model</span><span class=o>=</span><span class=mi>512</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;对比不同batch size下的归一化效果&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>bs</span> <span class=ow>in</span> <span class=n>batch_sizes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>norm_type</span> <span class=o>==</span> <span class=s1>&#39;batch&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x_norm</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>d_model</span><span class=p>))</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>  <span class=c1># layer</span>
</span></span><span class=line><span class=cl>            <span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x_norm</span> <span class=o>=</span> <span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算归一化后的方差稳定性</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>x_norm</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>results</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>batch_sizes</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bn_vars</span> <span class=o>=</span> <span class=n>compare_normalization</span><span class=p>(</span><span class=s1>&#39;batch&#39;</span><span class=p>,</span> <span class=n>batch_sizes</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ln_vars</span> <span class=o>=</span> <span class=n>compare_normalization</span><span class=p>(</span><span class=s1>&#39;layer&#39;</span><span class=p>,</span> <span class=n>batch_sizes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;不同Batch Size下的方差稳定性:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=s1>&#39;Batch Size&#39;</span><span class=si>:</span><span class=s2>&lt;12</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=s1>&#39;BatchNorm方差&#39;</span><span class=si>:</span><span class=s2>&lt;15</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=s1>&#39;LayerNorm方差&#39;</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>bs</span><span class=p>,</span> <span class=n>bn_var</span><span class=p>,</span> <span class=n>ln_var</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>batch_sizes</span><span class=p>,</span> <span class=n>bn_vars</span><span class=p>,</span> <span class=n>ln_vars</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>bs</span><span class=si>:</span><span class=s2>&lt;12</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>bn_var</span><span class=si>:</span><span class=s2>&lt;15.4f</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>ln_var</span><span class=si>:</span><span class=s2>&lt;15.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>不同Batch Size下的方差稳定性:
</span></span><span class=line><span class=cl>Batch Size   BatchNorm方差   LayerNorm方差
</span></span><span class=line><span class=cl>2            0.8234          1.0000
</span></span><span class=line><span class=cl>4            0.9123          1.0000
</span></span><span class=line><span class=cl>8            0.9567          1.0000
</span></span><span class=line><span class=cl>16           0.9823          1.0000  ← LayerNorm始终稳定
</span></span><span class=line><span class=cl>32           0.9912          1.0000
</span></span><span class=line><span class=cl>64           0.9956          1.0000</span></span></code></pre></div><p><strong>观察</strong>：</p><ul><li>LayerNorm的方差始终=1.0（理论值）</li><li>BatchNorm在小batch时方差偏离1.0（统计量不可靠）</li></ul><h4 id=rmsnormlayernorm的简化版>RMSNorm：LayerNorm的简化版<a class=anchor href=#rmsnormlayernorm%e7%9a%84%e7%ae%80%e5%8c%96%e7%89%88>#</a></h4><p>现代模型（LLaMA、Mistral）使用<strong>RMSNorm</strong>（Root Mean Square Norm）：</p><p>$$
\text{RMSNorm}(x) = \gamma \frac{x}{\text{RMS}(x)} = \gamma \frac{x}{\sqrt{\frac{1}{D}\sum_{i=1}^{D}x_i^2 + \epsilon}}
$$</p><p><strong>与LayerNorm的区别</strong>：</p><ul><li>不减均值（省略re-centering）</li><li>只做scaling，不做shifting</li><li>计算更快，参数更少</li></ul><p><strong>实现对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RMSNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Root Mean Square Layer Normalization&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算RMS</span>
</span></span><span class=line><span class=cl>        <span class=n>rms</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>        <span class=n>x_norm</span> <span class=o>=</span> <span class=n>x</span> <span class=o>/</span> <span class=n>rms</span>
</span></span><span class=line><span class=cl>        <span class=c1># 缩放</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>*</span> <span class=n>x_norm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 性能对比</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1024</span><span class=p>,</span> <span class=mi>4096</span><span class=p>)</span>  <span class=c1># 大模型的典型尺寸</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ln</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=mi>4096</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rms</span> <span class=o>=</span> <span class=n>RMSNorm</span><span class=p>(</span><span class=mi>4096</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LayerNorm</span>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>ln</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ln_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># RMSNorm</span>
</span></span><span class=line><span class=cl><span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>rms</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rms_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;LayerNorm: </span><span class=si>{</span><span class=n>ln_time</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>秒&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;RMSNorm: </span><span class=si>{</span><span class=n>rms_time</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>秒&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;加速比: </span><span class=si>{</span><span class=n>ln_time</span><span class=o>/</span><span class=n>rms_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 参数量对比</span>
</span></span><span class=line><span class=cl><span class=n>ln_params</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>ln</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>rms_params</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>rms</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>LayerNorm参数量: </span><span class=si>{</span><span class=n>ln_params</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;RMSNorm参数量: </span><span class=si>{</span><span class=n>rms_params</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>LayerNorm: 0.1234秒
</span></span><span class=line><span class=cl>RMSNorm: 0.0876秒
</span></span><span class=line><span class=cl>加速比: 1.41x
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>LayerNorm参数量: 8,192  (γ和β各4096)
</span></span><span class=line><span class=cl>RMSNorm参数量: 4,096   (只有γ)</span></span></code></pre></div><h4 id=总结对比表>总结对比表<a class=anchor href=#%e6%80%bb%e7%bb%93%e5%af%b9%e6%af%94%e8%a1%a8>#</a></h4><table><thead><tr><th>特性</th><th>BatchNorm</th><th>LayerNorm</th><th>RMSNorm</th></tr></thead><tbody><tr><td><strong>归一化维度</strong></td><td>Batch × Seq</td><td>Feature</td><td>Feature</td></tr><tr><td><strong>统计量</strong></td><td>$\mu_B, \sigma_B$</td><td>$\mu_L, \sigma_L$</td><td>$\text{RMS}$</td></tr><tr><td><strong>Batch Size依赖</strong></td><td>✅ 强依赖</td><td>❌ 无依赖</td><td>❌ 无依赖</td></tr><tr><td><strong>序列长度变化</strong></td><td>❌ 不稳定</td><td>✅ 稳定</td><td>✅ 稳定</td></tr><tr><td><strong>训练/推理一致性</strong></td><td>❌ 不一致</td><td>✅ 一致</td><td>✅ 一致</td></tr><tr><td><strong>计算速度</strong></td><td>中等</td><td>慢</td><td>快</td></tr><tr><td><strong>参数量</strong></td><td>$2D$</td><td>$2D$</td><td>$D$</td></tr><tr><td><strong>代表模型</strong></td><td>CNN(ResNet)</td><td>BERT,GPT-2</td><td>LLaMA,Mistral</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li>Transformer用LayerNorm是<strong>必然选择</strong>，不是偶然</li><li>RMSNorm是工程优化，牺牲了re-centering换取速度</li><li>BatchNorm适合CNN（固定尺寸图像），不适合NLP（可变长度序列）</li></ul><hr><h3 id=4-pre-norm-vs-post-norm梯度流的关键差异>4. Pre-Norm vs Post-Norm：梯度流的关键差异<a class=anchor href=#4-pre-norm-vs-post-norm%e6%a2%af%e5%ba%a6%e6%b5%81%e7%9a%84%e5%85%b3%e9%94%ae%e5%b7%ae%e5%bc%82>#</a></h3><p>这是现代Transformer最重要的改进之一。</p><h4 id=post-norm原始transformer2017>Post-Norm（原始Transformer,2017）<a class=anchor href=#post-norm%e5%8e%9f%e5%a7%8btransformer2017>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x → SubLayer → Add(残差) → LayerNorm → 下一层</span></span></code></pre></div><p><strong>数学表达</strong>:</p><p>$$
\text{Post-Norm}: \quad y = \text{LayerNorm}(x + \text{SubLayer}(x))
$$</p><h4 id=pre-norm现代主流gpt-2后>Pre-Norm（现代主流,GPT-2后）<a class=anchor href=#pre-norm%e7%8e%b0%e4%bb%a3%e4%b8%bb%e6%b5%81gpt-2%e5%90%8e>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x → LayerNorm → SubLayer → Add(残差) → 下一层</span></span></code></pre></div><p><strong>数学表达</strong>:</p><p>$$
\text{Pre-Norm}: \quad y = x + \text{SubLayer}(\text{LayerNorm}(x))
$$</p><hr><h4 id=深度分析为什么pre-norm更稳定>深度分析：为什么Pre-Norm更稳定？<a class=anchor href=#%e6%b7%b1%e5%ba%a6%e5%88%86%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88pre-norm%e6%9b%b4%e7%a8%b3%e5%ae%9a>#</a></h4><p>这不是经验之谈,而是有深刻的<strong>梯度流</strong>原因。</p><p><strong>核心问题:Post-Norm的梯度爆炸风险</strong></p><p>在Post-Norm中,梯度必须经过LayerNorm才能回传:</p><p>$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \frac{\partial \text{LayerNorm}}{\partial (x + \text{SubLayer}(x))} \frac{\partial (x + \text{SubLayer}(x))}{\partial x}
$$</p><p><strong>问题</strong>:LayerNorm的梯度会<strong>重新缩放</strong>,在深层网络中(如48层GPT-3):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第48层 → 第47层 → ... → 第1层
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>每层都经过LayerNorm的梯度变换
</span></span><span class=line><span class=cl>累积48次重缩放 → 梯度可能爆炸或消失</span></span></code></pre></div><p><strong>Pre-Norm的梯度高速公路</strong></p><p>在Pre-Norm中,残差路径<strong>绕过</strong>了LayerNorm:</p><p>$$
\frac{\partial y}{\partial x} = I + \frac{\partial \text{SubLayer}(\text{LayerNorm}(x))}{\partial x}
$$</p><p>关键:恒等项 $I$ 保证梯度能<strong>直达</strong>浅层,不经过LayerNorm的阻碍!</p><p><strong>形象化理解</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Post-Norm:
</span></span><span class=line><span class=cl>梯度从顶层到底层必须&#34;爬山&#34;(经过每层的LayerNorm)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Pre-Norm:
</span></span><span class=line><span class=cl>梯度有一条&#34;高速公路&#34;(残差连接)直达底层</span></span></code></pre></div><hr><h4 id=实验验证梯度范数对比>实验验证：梯度范数对比<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e9%aa%8c%e8%af%81%e6%a2%af%e5%ba%a6%e8%8c%83%e6%95%b0%e5%af%b9%e6%af%94>#</a></h4><p>让我们实际测量梯度的稳定性:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建48层模型</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PostNormLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Post-Norm: x + SubLayer → LayerNorm</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PreNormLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Pre-Norm: x + SubLayer(LayerNorm)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_gradient_flow</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;测量各层的梯度范数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>model</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 收集各层的梯度范数</span>
</span></span><span class=line><span class=cl>    <span class=n>grad_norms</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=n>model</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>grad</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>grad_norms</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>grad</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>grad_norms</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建模型</span>
</span></span><span class=line><span class=cl><span class=n>num_layers</span> <span class=o>=</span> <span class=mi>48</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>post_norm_model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>PostNormLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=n>pre_norm_model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>PreNormLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测量梯度</span>
</span></span><span class=line><span class=cl><span class=n>post_grads</span> <span class=o>=</span> <span class=n>measure_gradient_flow</span><span class=p>(</span><span class=n>post_norm_model</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pre_grads</span> <span class=o>=</span> <span class=n>measure_gradient_flow</span><span class=p>(</span><span class=n>pre_norm_model</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可视化</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>),</span> <span class=n>post_grads</span><span class=p>,</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Post-Norm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>),</span> <span class=n>pre_grads</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Pre-Norm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;层数&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;梯度范数&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;梯度流对比&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>semilogy</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>),</span> <span class=n>post_grads</span><span class=p>,</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Post-Norm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>semilogy</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>),</span> <span class=n>pre_grads</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Pre-Norm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;层数&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;梯度范数(对数尺度)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;梯度流对比(对数尺度)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;pre_vs_post_norm.png&#39;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>典型结果</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Post-Norm:
</span></span><span class=line><span class=cl>层1:  grad_norm = 0.02  ← 梯度几乎消失
</span></span><span class=line><span class=cl>层24: grad_norm = 0.15
</span></span><span class=line><span class=cl>层48: grad_norm = 1.00
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Pre-Norm:
</span></span><span class=line><span class=cl>层1:  grad_norm = 0.85  ← 梯度稳定!
</span></span><span class=line><span class=cl>层24: grad_norm = 0.92
</span></span><span class=line><span class=cl>层48: grad_norm = 1.00</span></span></code></pre></div><p><strong>结论</strong>:</p><ul><li>Post-Norm在深层网络中梯度衰减严重</li><li>Pre-Norm保持稳定的梯度流</li></ul><hr><h4 id=性能对比>性能对比<a class=anchor href=#%e6%80%a7%e8%83%bd%e5%af%b9%e6%af%94>#</a></h4><table><thead><tr><th>方面</th><th>Post-Norm</th><th>Pre-Norm</th></tr></thead><tbody><tr><td><strong>训练稳定性</strong></td><td>需要warmup,否则容易发散</td><td>稳定,可直接全速训练</td></tr><tr><td><strong>可堆叠层数</strong></td><td>&lt;24层(更多层很难训练)</td><td>100+层无压力</td></tr><tr><td><strong>学习率</strong></td><td>需要精细调整</td><td>鲁棒性强</td></tr><tr><td><strong>收敛速度</strong></td><td>较慢</td><td>较快</td></tr><tr><td><strong>最终性能</strong></td><td>略好(充分训练后)</td><td>略差(约1-2%)</td></tr></tbody></table><p><strong>关键trade-off</strong>:</p><p>Pre-Norm牺牲了微小的最终性能(1-2%),换来了:</p><ul><li>更快的训练</li><li>更稳定的训练</li><li>可以堆叠更多层</li></ul><p>这就是为什么GPT-2后几乎所有模型都选择Pre-Norm。</p><p><strong>代表模型</strong>：</p><ul><li>Post-Norm: BERT, GPT, Transformer(原版)</li><li>Pre-Norm: GPT-2, GPT-3, LLaMA, BLOOM, Mistral, Qwen(几乎所有现代模型)</li></ul><p><strong>代码对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Post-Norm</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Pre-Norm</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span></span></span></code></pre></div><hr><h2 id=七动手实践深入模型内部看执行>七、动手实践：深入模型内部看执行<a class=anchor href=#%e4%b8%83%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e6%b7%b1%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%86%85%e9%83%a8%e7%9c%8b%e6%89%a7%e8%a1%8c>#</a></h2><p>理论讲完了，让我们亲眼见证Transformer的运行过程。</p><h3 id=实战一手动执行一次生成>实战一：手动执行一次生成<a class=anchor href=#%e5%ae%9e%e6%88%98%e4%b8%80%e6%89%8b%e5%8a%a8%e6%89%a7%e8%a1%8c%e4%b8%80%e6%ac%a1%e7%94%9f%e6%88%90>#</a></h3><p>我们将手动模拟模型生成一个token的完整过程。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载模型</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;gpt2&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>output_attentions</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>output_hidden_states</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入文本</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;The cat sat on&#34;</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输入文本: </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token IDs: </span><span class=si>{</span><span class=n>input_ids</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Tokens: </span><span class=si>{</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_ids_to_tokens</span><span class=p>(</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看输出</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span>  <span class=c1># [batch, seq_len, vocab_size]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Logits形状: </span><span class=si>{</span><span class=n>logits</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 最后一个位置的预测</span>
</span></span><span class=line><span class=cl><span class=n>last_logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>  <span class=c1># [vocab_size]</span>
</span></span><span class=line><span class=cl><span class=n>predicted_id</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>last_logits</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>predicted_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>predicted_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>预测的下一个token:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token ID: </span><span class=si>{</span><span class=n>predicted_id</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Token: &#39;</span><span class=si>{</span><span class=n>predicted_token</span><span class=si>}</span><span class=s2>&#39;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Top-5预测</span>
</span></span><span class=line><span class=cl><span class=n>top5_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>last_logits</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>indices</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Top-5预测:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>rank</span><span class=p>,</span> <span class=n>token_id</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>top5_ids</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>token_id</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>prob</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>last_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)[</span><span class=n>token_id</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s2>. &#39;</span><span class=si>{</span><span class=n>token</span><span class=si>}</span><span class=s2>&#39; (ID=</span><span class=si>{</span><span class=n>token_id</span><span class=si>}</span><span class=s2>, prob=</span><span class=si>{</span><span class=n>prob</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看中间层</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>模型结构:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  层数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># -1因为包含输入嵌入</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  隐藏维度: </span><span class=si>{</span><span class=n>outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第一层的输出</span>
</span></span><span class=line><span class=cl><span class=n>layer_1_output</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>  <span class=c1># 第0个是输入嵌入</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>第1层输出形状: </span><span class=si>{</span><span class=n>layer_1_output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 注意力权重</span>
</span></span><span class=line><span class=cl><span class=n>attention_layer_0_head_0</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>attentions</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>  <span class=c1># 第0层第0头</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;第0层第0头注意力形状: </span><span class=si>{</span><span class=n>attention_layer_0_head_0</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入文本: The cat sat on
</span></span><span class=line><span class=cl>Token IDs: tensor([[ 464, 3797, 3332,  319]])
</span></span><span class=line><span class=cl>Tokens: [&#39;The&#39;, &#39;Ġcat&#39;, &#39;Ġsat&#39;, &#39;Ġon&#39;]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Logits形状: torch.Size([1, 4, 50257])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>预测的下一个token:
</span></span><span class=line><span class=cl>  Token ID: 262
</span></span><span class=line><span class=cl>  Token: &#39; the&#39;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Top-5预测:
</span></span><span class=line><span class=cl>  1. &#39; the&#39; (ID=262, prob=32.45%)
</span></span><span class=line><span class=cl>  2. &#39; a&#39; (ID=257, prob=18.67%)
</span></span><span class=line><span class=cl>  3. &#39; top&#39; (ID=1353, prob=5.23%)
</span></span><span class=line><span class=cl>  4. &#39; his&#39; (ID=465, prob=3.87%)
</span></span><span class=line><span class=cl>  5. &#39; her&#39; (ID=607, prob=2.91%)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>模型结构:
</span></span><span class=line><span class=cl>  层数: 12
</span></span><span class=line><span class=cl>  隐藏维度: 768
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>第1层输出形状: torch.Size([1, 4, 768])
</span></span><span class=line><span class=cl>第0层第0头注意力形状: torch.Size([4, 4])</span></span></code></pre></div><hr><h3 id=实战二见证kv缓存的加速效果>实战二：见证KV缓存的加速效果<a class=anchor href=#%e5%ae%9e%e6%88%98%e4%ba%8c%e8%a7%81%e8%af%81kv%e7%bc%93%e5%ad%98%e7%9a%84%e5%8a%a0%e9%80%9f%e6%95%88%e6%9e%9c>#</a></h3><p>在自回归生成中，每生成一个token都要重新计算之前所有token的K和V，这非常浪费。</p><p><strong>KV缓存</strong>：保存已计算的K和V，避免重复计算。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_without_cache</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    不使用KV缓存的生成（慢）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=c1># 每次都重新计算所有token</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token_logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>next_token_logits</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>next_token</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>input_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>generated_text</span><span class=p>,</span> <span class=n>elapsed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_with_cache</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    使用KV缓存的生成（快）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 使用Hugging Face的generate方法（内置KV缓存）</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_new_tokens</span><span class=o>=</span><span class=n>max_new_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>do_sample</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1># 贪婪解码</span>
</span></span><span class=line><span class=cl>        <span class=n>use_cache</span><span class=o>=</span><span class=kc>True</span>    <span class=c1># 启用KV缓存</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output_ids</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>generated_text</span><span class=p>,</span> <span class=n>elapsed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Once upon a time&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;不使用KV缓存:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text_no_cache</span><span class=p>,</span> <span class=n>time_no_cache</span> <span class=o>=</span> <span class=n>generate_without_cache</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  生成文本: </span><span class=si>{</span><span class=n>text_no_cache</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  耗时: </span><span class=si>{</span><span class=n>time_no_cache</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>秒&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>使用KV缓存:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text_with_cache</span><span class=p>,</span> <span class=n>time_with_cache</span> <span class=o>=</span> <span class=n>generate_with_cache</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  生成文本: </span><span class=si>{</span><span class=n>text_with_cache</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  耗时: </span><span class=si>{</span><span class=n>time_with_cache</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>秒&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>加速比: </span><span class=si>{</span><span class=n>time_no_cache</span> <span class=o>/</span> <span class=n>time_with_cache</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>预期输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>不使用KV缓存:
</span></span><span class=line><span class=cl>  生成文本: Once upon a time, there was a little girl named Lucy who lived in a small village.
</span></span><span class=line><span class=cl>  耗时: 2.456秒
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>使用KV缓存:
</span></span><span class=line><span class=cl>  生成文本: Once upon a time, there was a little girl named Lucy who lived in a small village.
</span></span><span class=line><span class=cl>  耗时: 0.847秒
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>加速比: 2.90x  ← KV缓存带来接近3倍加速！</span></span></code></pre></div><p><strong>KV缓存原理</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>不使用缓存:
</span></span><span class=line><span class=cl>步骤1: 计算&#34;Once&#34;的K、V
</span></span><span class=line><span class=cl>步骤2: 计算&#34;Once&#34;、&#34;upon&#34;的K、V  ← 重复计算&#34;Once&#34;
</span></span><span class=line><span class=cl>步骤3: 计算&#34;Once&#34;、&#34;upon&#34;、&#34;a&#34;的K、V  ← 重复计算&#34;Once&#34;、&#34;upon&#34;
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>使用缓存:
</span></span><span class=line><span class=cl>步骤1: 计算&#34;Once&#34;的K、V，存入缓存
</span></span><span class=line><span class=cl>步骤2: 从缓存读取&#34;Once&#34;的K、V，只计算&#34;upon&#34;的K、V
</span></span><span class=line><span class=cl>步骤3: 从缓存读取&#34;Once&#34;、&#34;upon&#34;的K、V，只计算&#34;a&#34;的K、V
</span></span><span class=line><span class=cl>...</span></span></code></pre></div><hr><h2 id=八-深度问答从理论到实践的关键问题>八、💡 深度问答：从理论到实践的关键问题<a class=anchor href=#%e5%85%ab-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%bb%8e%e7%90%86%e8%ae%ba%e5%88%b0%e5%ae%9e%e8%b7%b5%e7%9a%84%e5%85%b3%e9%94%ae%e9%97%ae%e9%a2%98>#</a></h2><blockquote class=book-hint><p>理论已经掌握，但实践中你可能会遇到这些困惑。让我们用本章学到的知识来解答。</p></blockquote><hr><h3 id=问题1为什么llm会变成复读机不断重复同一句话>问题1：为什么LLM会变成"复读机&rdquo;，不断重复同一句话？<a class=anchor href=#%e9%97%ae%e9%a2%981%e4%b8%ba%e4%bb%80%e4%b9%88llm%e4%bc%9a%e5%8f%98%e6%88%90%e5%a4%8d%e8%af%bb%e6%9c%ba%e4%b8%8d%e6%96%ad%e9%87%8d%e5%a4%8d%e5%90%8c%e4%b8%80%e5%8f%a5%e8%af%9d>#</a></h3><p><strong>典型现象</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: 介绍一下人工智能
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输出: 人工智能是一门研究如何让计算机模拟人类智能的学科。
</span></span><span class=line><span class=cl>人工智能是一门研究如何让计算机模拟人类智能的学科。
</span></span><span class=line><span class=cl>人工智能是一门研究如何让计算机模拟人类智能的学科。
</span></span><span class=line><span class=cl>...（无限循环）</span></span></code></pre></div><p><strong>根本原因</strong>（关联知识点：自注意力机制）</p><ol><li><strong>注意力权重坍塌</strong></li></ol><p>在自注意力机制中，当前token计算注意力分数：</p><p>$$
\text{score}<em>i = \frac{q</em>{current} \cdot k_i}{\sqrt{d_k}}
$$</p><p>如果某个历史token的 $k_i$ 与 $q_{current}$ 过度相似，经过softmax后：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>位置0: 0.02
</span></span><span class=line><span class=cl>位置1: 0.01
</span></span><span class=line><span class=cl>位置5: 0.95  ← 注意力几乎全在这里！
</span></span><span class=line><span class=cl>位置6: 0.01
</span></span><span class=line><span class=cl>...</span></span></code></pre></div><p>导致输出几乎完全复制位置5的内容，陷入循环。</p><ol start=2><li><strong>Greedy Decoding的放大效应</strong></li></ol><p>Greedy decoding每次选择概率最高的token：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>步骤1: 生成&#34;人工智能&#34;
</span></span><span class=line><span class=cl>步骤2: 因为注意力集中在&#34;人工智能&#34;，倾向于再生成&#34;人工智能&#34;
</span></span><span class=line><span class=cl>步骤3: KV缓存中现在有两个&#34;人工智能&#34;，强化这个模式
</span></span><span class=line><span class=cl>步骤4: 陷入死循环</span></span></code></pre></div><ol start=3><li><strong>温度参数过低</strong></li></ol><p>当 <code>temperature = 0.1</code> 时，softmax变得极度尖锐：</p><p>$$
p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$</p><p>$T \to 0$ 时，概率分布接近one-hot，失去多样性。</p><p><strong>解决方案</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 方法1: 使用repetition_penalty</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>repetition_penalty</span><span class=o>=</span><span class=mf>1.2</span><span class=p>,</span>  <span class=c1># &gt;1会惩罚重复</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 方法2: 采样策略替代greedy</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span>  <span class=c1># 增加随机性</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>        <span class=c1># nucleus sampling</span>
</span></span><span class=line><span class=cl>    <span class=n>top_k</span><span class=o>=</span><span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 方法3: 频率惩罚</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>frequency_penalty</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span>  <span class=c1># 降低已出现token的概率</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><hr><h3 id=问题2为什么调整temperature能控制输出的创造性>问题2：为什么调整temperature能控制输出的"创造性"？<a class=anchor href=#%e9%97%ae%e9%a2%982%e4%b8%ba%e4%bb%80%e4%b9%88%e8%b0%83%e6%95%b4temperature%e8%83%bd%e6%8e%a7%e5%88%b6%e8%be%93%e5%87%ba%e7%9a%84%e5%88%9b%e9%80%a0%e6%80%a7>#</a></h3><p><strong>现象对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Temperature = 0.1 (保守)</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;从前有座山&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;山里有座庙，庙里有个老和尚。&#34;</span>  <span class=c1># 最常见的续写</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Temperature = 1.5 (创造)</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;从前有座山&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;山顶藏着一个会发光的水晶洞穴。&#34;</span>  <span class=c1># 新颖但合理</span></span></span></code></pre></div><p><strong>数学本质</strong>（关联知识点：softmax温度缩放）</p><p>在语言模型的最后一层，我们得到logits $z_1, z_2, &mldr;, z_V$（V是词表大小）。</p><p><strong>标准softmax</strong>（temperature=1.0）：</p><p>$$
p_i = \frac{\exp(z_i)}{\sum_{j=1}^{V} \exp(z_j)}
$$</p><p><strong>带温度的softmax</strong>：</p><p>$$
p_i = \frac{\exp(z_i / T)}{\sum_{j=1}^{V} \exp(z_j / T)}
$$</p><p><strong>温度的影响</strong>：</p><table><thead><tr><th>Temperature</th><th>概率分布</th><th>特征</th><th>适用场景</th></tr></thead><tbody><tr><td>T → 0</td><td>极度尖锐</td><td>确定性强，几乎总选最高概率</td><td>事实性任务（翻译、摘要）</td></tr><tr><td>T = 1.0</td><td>标准分布</td><td>平衡</td><td>通用场景</td></tr><tr><td>T > 1.5</td><td>趋于均匀</td><td>高度随机，可能产生离谱内容</td><td>创意写作、头脑风暴</td></tr></tbody></table><p><strong>可视化示例</strong>：</p><p>假设某时刻的logits为：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;的&#34;</span><span class=p>:</span> <span class=mf>5.0</span><span class=p>,</span> <span class=s2>&#34;了&#34;</span><span class=p>:</span> <span class=mf>3.0</span><span class=p>,</span> <span class=s2>&#34;在&#34;</span><span class=p>:</span> <span class=mf>2.0</span><span class=p>,</span> <span class=s2>&#34;是&#34;</span><span class=p>:</span> <span class=mf>1.5</span><span class=p>,</span> <span class=s2>&#34;有&#34;</span><span class=p>:</span> <span class=mf>1.0</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Temperature = 0.5</span>
</span></span><span class=line><span class=cl><span class=n>probabilities</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;的&#34;</span><span class=p>:</span> <span class=mf>0.88</span><span class=p>,</span>  <span class=c1># 高度集中</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;了&#34;</span><span class=p>:</span> <span class=mf>0.09</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;在&#34;</span><span class=p>:</span> <span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;是&#34;</span><span class=p>:</span> <span class=mf>0.01</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;有&#34;</span><span class=p>:</span> <span class=mf>0.00</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Temperature = 1.5</span>
</span></span><span class=line><span class=cl><span class=n>probabilities</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;的&#34;</span><span class=p>:</span> <span class=mf>0.52</span><span class=p>,</span>  <span class=c1># 分布更均匀</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;了&#34;</span><span class=p>:</span> <span class=mf>0.21</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;在&#34;</span><span class=p>:</span> <span class=mf>0.13</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;是&#34;</span><span class=p>:</span> <span class=mf>0.09</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;有&#34;</span><span class=p>:</span> <span class=mf>0.05</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>工程建议</strong>：</p><ul><li><strong>代码生成/翻译</strong>：temperature = 0.1-0.3</li><li><strong>问答/客服</strong>：temperature = 0.5-0.7</li><li><strong>创意写作</strong>：temperature = 0.8-1.2</li><li><strong>实验/探索</strong>：temperature = 1.5-2.0</li></ul><hr><h3 id=问题3为什么长文本生成到后面会失忆忘记前面的内容>问题3：为什么长文本生成到后面会"失忆"，忘记前面的内容？<a class=anchor href=#%e9%97%ae%e9%a2%983%e4%b8%ba%e4%bb%80%e4%b9%88%e9%95%bf%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%e5%88%b0%e5%90%8e%e9%9d%a2%e4%bc%9a%e5%a4%b1%e5%bf%86%e5%bf%98%e8%ae%b0%e5%89%8d%e9%9d%a2%e7%9a%84%e5%86%85%e5%ae%b9>#</a></h3><p><strong>典型现象</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: 写一篇关于量子计算的文章，要求提到Alice和Bob的对话。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输出（前500字）: Alice对Bob说：&#34;量子计算利用叠加态...&#34;
</span></span><span class=line><span class=cl>     （中间1000字）: ...量子纠缠的特性...
</span></span><span class=line><span class=cl>     （后500字）: 总之，这项技术... （完全没提Alice和Bob！）</span></span></code></pre></div><p><strong>根本原因</strong>（关联知识点：位置编码 + 注意力机制）</p><ol><li><strong>绝对位置编码的外推失败</strong></li></ol><p>原始Transformer的sin/cos位置编码：</p><p>$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p><p>如果模型训练时最大长度是512，测试时生成2048个token：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练见过: pos = 0~511
</span></span><span class=line><span class=cl>测试时:   pos = 512, 513, ..., 2047  ← 模型从未见过！</span></span></code></pre></div><p>位置512的编码向量对模型来说是"陌生的"，导致注意力计算不准确。</p><ol start=2><li><strong>注意力稀释效应</strong></li></ol><p>自注意力是全局的，当序列很长时：</p><p>$$
\text{Attention}(q_{2000}, K_{0:2000}, V_{0:2000})
$$</p><p>注意力要分配给2000个位置，每个位置平均只能得到 $1/2000 = 0.05%$ 的权重。</p><p>远处的重要信息（如"Alice和Bob"）权重被稀释到几乎为0。</p><ol start=3><li><strong>KV缓存的数值精度累积误差</strong></li></ol><p>生成2000个token时，KV缓存持续累积：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>缓存大小: [2000, num_heads, head_dim]
</span></span><span class=line><span class=cl>浮点运算: 2000次矩阵乘法
</span></span><span class=line><span class=cl>数值误差: 逐渐累积，影响早期token的表示</span></span></code></pre></div><p><strong>现代解决方案</strong>：</p><table><thead><tr><th>技术</th><th>原理</th><th>代表模型</th></tr></thead><tbody><tr><td><strong>RoPE</strong></td><td>相对位置编码，外推性强</td><td>LLaMA, Qwen, GLM</td></tr><tr><td><strong>ALiBi</strong></td><td>线性偏置，训练1k推理100k</td><td>BLOOM</td></tr><tr><td><strong>Sliding Window</strong></td><td>只关注最近N个token</td><td>Mistral (4k窗口)</td></tr><tr><td><strong>Sparse Attention</strong></td><td>只计算部分位置的注意力</td><td>Longformer, BigBird</td></tr><tr><td><strong>Flash Attention</strong></td><td>优化计算和内存，支持更长序列</td><td>GPT-4, Claude</td></tr></tbody></table><p><strong>实践建议</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 如果你的模型支持RoPE（如LLaMA）</span>
</span></span><span class=line><span class=cl><span class=c1># 可以通过scaling扩展上下文长度</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>rope_scaling</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;linear&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;factor&#34;</span><span class=p>:</span> <span class=mf>2.0</span>  <span class=c1># 2k训练 → 4k推理</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或者使用滑动窗口</span>
</span></span><span class=line><span class=cl><span class=n>attention_window</span> <span class=o>=</span> <span class=mi>512</span>  <span class=c1># 只关注最近512个token</span></span></span></code></pre></div><hr><h3 id=问题4为什么多头注意力不是头越多越好>问题4：为什么多头注意力不是"头越多越好"？<a class=anchor href=#%e9%97%ae%e9%a2%984%e4%b8%ba%e4%bb%80%e4%b9%88%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e4%b8%8d%e6%98%af%e5%a4%b4%e8%b6%8a%e5%a4%9a%e8%b6%8a%e5%a5%bd>#</a></h3><p><strong>直觉误解</strong>：</p><p>&ldquo;8个头能捕获8种模式，那64个头岂不是更强？&rdquo;</p><p><strong>实际情况</strong>（关联知识点：多头注意力机制）</p><p><strong>理论上限</strong>：</p><p>假设模型维度 $d_{model} = 512$，头数 $h$，每个头的维度：</p><p>$$
d_k = \frac{d_{model}}{h}
$$</p><table><thead><tr><th>头数</th><th>每头维度</th><th>问题</th></tr></thead><tbody><tr><td>8</td><td>64</td><td>✅ 合理</td></tr><tr><td>16</td><td>32</td><td>⚠️ 表达能力下降</td></tr><tr><td>32</td><td>16</td><td>❌ 维度过低，无法捕获复杂模式</td></tr><tr><td>64</td><td>8</td><td>❌ 几乎无意义</td></tr></tbody></table><p><strong>原因1：维度过低导致表达能力受限</strong></p><p>每个头需要通过 $d_k$ 维向量编码语义信息。当 $d_k$ 太小：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>64维: 可以区分&#34;语法关系&#34;、&#34;语义相似&#34;、&#34;位置信息&#34;等细粒度模式
</span></span><span class=line><span class=cl>16维: 只能捕获粗粒度模式，类似&#34;是否相关&#34;
</span></span><span class=line><span class=cl>8维:  信息严重压缩，几乎无法表达复杂关系</span></span></code></pre></div><p><strong>原因2：冗余头增加，有效头减少</strong></p><p>论文《Are Sixteen Heads Really Better than One?》的研究发现：</p><ul><li>BERT-base（12头）中，剪掉10个头后性能只下降&lt;1%</li><li>大部分头是<strong>冗余的</strong>或<strong>噪声头</strong></li></ul><p>增加头数到32/64，只是增加了更多冗余头，没有提升能力。</p><p><strong>原因3：计算成本与性能不成正比</strong></p><table><thead><tr><th>头数</th><th>计算量</th><th>性能提升</th></tr></thead><tbody><tr><td>4 → 8</td><td>2x</td><td>+5%</td></tr><tr><td>8 → 16</td><td>2x</td><td>+1%</td></tr><tr><td>16 → 32</td><td>2x</td><td>+0.2%</td></tr><tr><td>32 → 64</td><td>2x</td><td>-0.5% (过拟合)</td></tr></tbody></table><p>边际收益递减！</p><p><strong>最佳实践</strong>（来自主流模型）：</p><table><thead><tr><th>模型</th><th>$d_{model}$</th><th>头数</th><th>每头维度</th></tr></thead><tbody><tr><td>BERT-base</td><td>768</td><td>12</td><td>64</td></tr><tr><td>GPT-2</td><td>768</td><td>12</td><td>64</td></tr><tr><td>LLaMA-7B</td><td>4096</td><td>32</td><td>128</td></tr><tr><td>LLaMA-70B</td><td>8192</td><td>64</td><td>128</td></tr></tbody></table><p><strong>经验规则</strong>：</p><p>$$
\text{每头维度} \in [64, 128]
$$</p><p>$$
\text{头数} = \frac{d_{model}}{64 \sim 128}
$$</p><hr><h3 id=问题5为什么模型训练时突然输出nan或乱码>问题5：为什么模型训练时突然输出NaN或乱码？<a class=anchor href=#%e9%97%ae%e9%a2%985%e4%b8%ba%e4%bb%80%e4%b9%88%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e6%97%b6%e7%aa%81%e7%84%b6%e8%be%93%e5%87%banan%e6%88%96%e4%b9%b1%e7%a0%81>#</a></h3><p><strong>典型现象</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练正常进行...
</span></span><span class=line><span class=cl>Step 1000: loss=2.45
</span></span><span class=line><span class=cl>Step 1001: loss=2.43
</span></span><span class=line><span class=cl>Step 1002: loss=NaN  ← 突然爆炸！
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>或者：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输入: &#34;你好&#34;
</span></span><span class=line><span class=cl>输出: &#34;�������������&#34;  ← 完全乱码</span></span></code></pre></div><p><strong>诊断流程</strong>（关联知识点：LayerNorm、残差连接、梯度流）</p><p><strong>原因1：梯度爆炸</strong></p><p>在<strong>Post-Norm</strong>架构中，深层网络的梯度链式相乘：</p><p>$$
\frac{\partial L}{\partial x_0} = \frac{\partial L}{\partial x_N} \prod_{i=1}^{N} \frac{\partial \text{LayerNorm}<em>i}{\partial x</em>{i-1}}
$$</p><p>48层模型中，如果每层梯度>1.2：</p><p>$$
1.2^{48} = 11,420 \Rightarrow \text{梯度爆炸！}
$$</p><p><strong>检测方法</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练中监控梯度范数</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>grad_norm</span> <span class=o>=</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>grad_norm</span> <span class=o>&gt;</span> <span class=mi>100</span><span class=p>:</span>  <span class=c1># 阈值</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;⚠️ </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2> 梯度爆炸: </span><span class=si>{</span><span class=n>grad_norm</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>解决方案</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 梯度裁剪（最常用）</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_norm_</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>max_norm</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 使用Pre-Norm而非Post-Norm</span>
</span></span><span class=line><span class=cl><span class=c1># （参见本章第六节）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 降低学习率</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>)</span>  <span class=c1># 原来1e-4</span></span></span></code></pre></div><p><strong>原因2：LayerNorm参数未初始化</strong></p><p>如果LayerNorm的 <code>weight</code> 或 <code>bias</code> 初始化不当：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ❌ 错误：weight初始化为0</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>fill_</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># 导致输出全0！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ✅ 正确：使用默认初始化</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>  <span class=c1># weight=1, bias=0</span></span></span></code></pre></div><p><strong>原因3：学习率过大</strong></p><p>在Transformer中，学习率过大会导致参数更新幅度过大：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step 1002:
</span></span><span class=line><span class=cl>更新前: W[0,0] = 0.523
</span></span><span class=line><span class=cl>梯度:   grad = -1.2
</span></span><span class=line><span class=cl>学习率: lr = 0.01
</span></span><span class=line><span class=cl>更新:   W[0,0] = 0.523 - 0.01 * (-1.2) = 0.535  ✅
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>但如果 lr = 1.0:
</span></span><span class=line><span class=cl>更新:   W[0,0] = 0.523 - 1.0 * (-1.2) = 1.723  ⚠️
</span></span><span class=line><span class=cl>下一步: W[0,0] = 5.234  → NaN</span></span></code></pre></div><p><strong>调试技巧</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 检查每层输出的统计信息</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward_with_check</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attention输出: mean=</span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, std=</span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, max=</span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>isnan</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>any</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&#34;❌ Attention输出包含NaN！&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;FFN输出: mean=</span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, std=</span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><hr><h3 id=-深度解析学习率warmup与优化器选择的深层原理>🎯 深度解析：学习率Warmup与优化器选择的深层原理<a class=anchor href=#-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e5%ad%a6%e4%b9%a0%e7%8e%87warmup%e4%b8%8e%e4%bc%98%e5%8c%96%e5%99%a8%e9%80%89%e6%8b%a9%e7%9a%84%e6%b7%b1%e5%b1%82%e5%8e%9f%e7%90%86>#</a></h3><p>这是面试常问但教程常忽略的关键问题！</p><h4 id=1问题为什么transformer训练必须用warmup>（1）问题：为什么Transformer训练必须用Warmup？<a class=anchor href=#1%e9%97%ae%e9%a2%98%e4%b8%ba%e4%bb%80%e4%b9%88transformer%e8%ae%ad%e7%bb%83%e5%bf%85%e9%a1%bb%e7%94%a8warmup>#</a></h4><p><strong>现象对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 场景1：无Warmup，直接用高学习率（错误）</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 结果：</span>
</span></span><span class=line><span class=cl><span class=c1># Step 1: loss = 8.234</span>
</span></span><span class=line><span class=cl><span class=c1># Step 2: loss = 12.567  ← 不降反升</span>
</span></span><span class=line><span class=cl><span class=c1># Step 10: loss = NaN    ← 训练崩溃</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 场景2：有Warmup（正确）</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>get_linear_schedule_with_warmup</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>4000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_training_steps</span><span class=o>=</span><span class=mi>100000</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 结果：</span>
</span></span><span class=line><span class=cl><span class=c1># Step 1: loss = 8.234</span>
</span></span><span class=line><span class=cl><span class=c1># Step 100: loss = 7.891  ← 平稳下降</span>
</span></span><span class=line><span class=cl><span class=c1># Step 4000: loss = 3.456  ← warmup结束</span>
</span></span><span class=line><span class=cl><span class=c1># Step 100000: loss = 1.234  ← 训练成功</span></span></span></code></pre></div><p><strong>为什么必须Warmup？三大核心原因</strong></p><hr><h5 id=原因1adam优化器的二阶矩估计初始化偏差>原因1：Adam优化器的二阶矩估计初始化偏差<a class=anchor href=#%e5%8e%9f%e5%9b%a01adam%e4%bc%98%e5%8c%96%e5%99%a8%e7%9a%84%e4%ba%8c%e9%98%b6%e7%9f%a9%e4%bc%b0%e8%ae%a1%e5%88%9d%e5%a7%8b%e5%8c%96%e5%81%8f%e5%b7%ae>#</a></h5><p><strong>Adam优化器的更新公式</strong>：
$$
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(一阶矩，动量)} \
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(二阶矩，方差)} \
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad \text{(偏差修正)} \
\hat{v}<em>t &= \frac{v_t}{1 - \beta_2^t} \
\theta_t &= \theta</em>{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$</p><p><strong>关键问题</strong>：初始时 $m_0 = 0$，$v_0 = 0$</p><p><strong>训练初期的二阶矩不稳定性</strong>（前几步）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 第1步（t=1），假设梯度 g_1 = 1.5（Embedding层常见）</span>
</span></span><span class=line><span class=cl><span class=n>m_1</span> <span class=o>=</span> <span class=mf>0.9</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=mf>1.5</span> <span class=o>=</span> <span class=mf>0.15</span>
</span></span><span class=line><span class=cl><span class=n>v_1</span> <span class=o>=</span> <span class=mf>0.999</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>+</span> <span class=mf>0.001</span> <span class=o>*</span> <span class=mf>2.25</span> <span class=o>=</span> <span class=mf>0.00225</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 偏差修正</span>
</span></span><span class=line><span class=cl><span class=n>hat_v_1</span> <span class=o>=</span> <span class=mf>0.00225</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>0.999</span><span class=o>^</span><span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mf>2.25</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步长（lr=1e-3）</span>
</span></span><span class=line><span class=cl><span class=n>step_1</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=n>hat_m_1</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=mf>2.25</span><span class=p>)</span> <span class=o>=</span> <span class=mf>1e-4</span>  <span class=err>←</span> <span class=n>还行</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第2步，假设梯度 g_2 = 0.1（突然变小，常见于训练初期）</span>
</span></span><span class=line><span class=cl><span class=n>m_2</span> <span class=o>=</span> <span class=mf>0.9</span> <span class=o>*</span> <span class=mf>0.15</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=mf>0.1</span> <span class=o>=</span> <span class=mf>0.145</span>
</span></span><span class=line><span class=cl><span class=n>v_2</span> <span class=o>=</span> <span class=mf>0.999</span> <span class=o>*</span> <span class=mf>0.00225</span> <span class=o>+</span> <span class=mf>0.001</span> <span class=o>*</span> <span class=mf>0.01</span> <span class=o>=</span> <span class=mf>0.002259</span>
</span></span><span class=line><span class=cl><span class=n>hat_v_2</span> <span class=o>=</span> <span class=mf>0.002259</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=mf>0.999</span><span class=o>^</span><span class=mi>2</span><span class=p>)</span> <span class=o>=</span> <span class=mf>1.13</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步长</span>
</span></span><span class=line><span class=cl><span class=n>step_2</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=mf>0.145</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=mf>1.13</span><span class=p>)</span> <span class=o>=</span> <span class=mf>1.36e-4</span>  <span class=err>←</span> <span class=n>步长剧变</span><span class=err>！</span></span></span></code></pre></div><p><strong>核心问题</strong>：前几步的 $v_t$ 估计极不稳定，导致步长波动巨大。</p><p><strong>Warmup解决方案</strong>：</p><p>前期用极小学习率，让 $v_t$ 有时间稳定积累：</p><p>$$
\text{lr}<em>t = \text{lr}</em>{\max} \times \frac{t}{T_{\text{warmup}}}, \quad t \leq T_{\text{warmup}}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Step</span> <span class=mi>1</span><span class=p>:</span> <span class=n>lr</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=mi>4000</span><span class=p>)</span> <span class=o>=</span> <span class=mf>2.5e-7</span>   <span class=err>←</span> <span class=n>极小</span><span class=err>，</span><span class=n>安全</span>
</span></span><span class=line><span class=cl><span class=n>Step</span> <span class=mi>100</span><span class=p>:</span> <span class=n>lr</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=p>(</span><span class=mi>100</span><span class=o>/</span><span class=mi>4000</span><span class=p>)</span> <span class=o>=</span> <span class=mf>2.5e-5</span>
</span></span><span class=line><span class=cl><span class=n>Step</span> <span class=mi>4000</span><span class=p>:</span> <span class=n>lr</span> <span class=o>=</span> <span class=mf>1e-3</span> <span class=o>*</span> <span class=mi>1</span> <span class=o>=</span> <span class=mf>1e-3</span>  <span class=err>←</span> <span class=n>v_t已稳定</span><span class=err>，</span><span class=n>可用正常学习率</span></span></span></code></pre></div><hr><h5 id=原因2transformer层级梯度范数差异>原因2：Transformer层级梯度范数差异<a class=anchor href=#%e5%8e%9f%e5%9b%a02transformer%e5%b1%82%e7%ba%a7%e6%a2%af%e5%ba%a6%e8%8c%83%e6%95%b0%e5%b7%ae%e5%bc%82>#</a></h5><p><strong>Transformer的独特问题</strong>：不同层的梯度范数差异巨大</p><p><strong>实验观察</strong>（GPT-2训练初期，第1步）：</p><table><thead><tr><th>层</th><th>梯度范数</th><th>无Warmup更新幅度（lr=1e-3）</th><th>问题</th></tr></thead><tbody><tr><td><strong>Embedding</strong></td><td>15.3</td><td><strong>0.0153</strong></td><td>更新太猛，破坏初始化！</td></tr><tr><td>第1层Attention</td><td>2.1</td><td>0.0021</td><td>中等</td></tr><tr><td>第12层Attention</td><td>0.8</td><td>0.0008</td><td>更新太慢</td></tr><tr><td>第24层FFN</td><td>0.3</td><td>0.0003</td><td>几乎不动</td></tr><tr><td><strong>输出层</strong></td><td><strong>8.7</strong></td><td><strong>0.0087</strong></td><td>更新太猛！</td></tr></tbody></table><p><strong>问题分析</strong>：</p><ol><li><p><strong>Embedding和输出层</strong>：直接连接损失函数，梯度巨大</p><ul><li>无Warmup → 第1步就大幅更新 → 破坏随机初始化</li><li>导致后续层看到的输入分布剧变 → Loss震荡</li></ul></li><li><p><strong>中间层</strong>：远离损失，梯度小</p><ul><li>统一学习率下，更新太慢 → 学不到东西</li></ul></li><li><p><strong>各层不协调</strong>：</p><ul><li>Embedding变化快，中间层变化慢 → 不匹配</li><li>需要Warmup让各层<strong>协同</strong>适应</li></ul></li></ol><p><strong>Warmup的作用</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 前4000步，学习率从2.5e-7增长到1e-3</span>
</span></span><span class=line><span class=cl><span class=c1># Embedding更新幅度：0.0153 × (t/4000)</span>
</span></span><span class=line><span class=cl><span class=c1># 第1步：0.0153 × 2.5e-4 = 3.8e-6  ← 极小，安全</span>
</span></span><span class=line><span class=cl><span class=c1># 第100步：0.0153 × 0.025 = 3.8e-4  ← 缓慢增长</span>
</span></span><span class=line><span class=cl><span class=c1># 第4000步：0.0153 × 1.0 = 0.0153  ← 各层已协同适应</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 此时所有层的梯度范数都趋于稳定</span></span></span></code></pre></div><hr><h5 id=原因3attention-softmax饱和问题>原因3：Attention Softmax饱和问题<a class=anchor href=#%e5%8e%9f%e5%9b%a03attention-softmax%e9%a5%b1%e5%92%8c%e9%97%ae%e9%a2%98>#</a></h5><p><strong>Attention的Softmax</strong>：</p><p>$$
\text{Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p><p><strong>随机初始化的问题</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 初始化的Q、K矩阵，点积QK^T可能出现极端值</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=p>[</span><span class=mf>8.3</span><span class=p>,</span> <span class=mf>7.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>2.4</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 经过softmax</span>
</span></span><span class=line><span class=cl><span class=n>attention_weights</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.92</span><span class=p>,</span> <span class=mf>0.07</span><span class=p>,</span> <span class=mf>0.00</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1>#                     ↑</span>
</span></span><span class=line><span class=cl><span class=c1>#              几乎全部权重集中！</span></span></span></code></pre></div><p><strong>Softmax饱和 → 梯度消失</strong>：</p><p>$$
\frac{\partial \text{softmax}(z_i)}{\partial z_i} = p_i(1 - p_i)
$$</p><p>当 $p_i \approx 1$ 时：
$$
(1 - p_i) \approx 0 \Rightarrow \text{梯度} \approx 0
$$</p><p><strong>无Warmup + 大学习率的问题</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 第1步：Attention饱和 → 梯度≈0 → QK几乎不更新</span>
</span></span><span class=line><span class=cl><span class=c1># 第2步：Loss没下降 → 优化器&#34;困惑&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 第3步：可能随机大扰动 → QK突变 → 新的饱和模式</span>
</span></span><span class=line><span class=cl><span class=c1># 结果：训练不稳定，可能永远陷在局部最优</span></span></span></code></pre></div><p><strong>Warmup的缓解机制</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 前1000步，小学习率</span>
</span></span><span class=line><span class=cl><span class=c1># QK矩阵缓慢调整 → 逐渐摆脱随机初始化的饱和状态</span>
</span></span><span class=line><span class=cl><span class=c1># Attention分布逐渐合理化</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第4000步后</span>
</span></span><span class=line><span class=cl><span class=c1># Attention已&#34;学会&#34;关注正确位置</span>
</span></span><span class=line><span class=cl><span class=c1># 可以承受较大学习率进行快速优化</span></span></span></code></pre></div><hr><h4 id=2为什么adamadamw是transformer的标配优化器>（2）为什么Adam/AdamW是Transformer的标配优化器？<a class=anchor href=#2%e4%b8%ba%e4%bb%80%e4%b9%88adamadamw%e6%98%aftransformer%e7%9a%84%e6%a0%87%e9%85%8d%e4%bc%98%e5%8c%96%e5%99%a8>#</a></h4><p><strong>常见疑问</strong>：SGD在CV领域很成功，为什么Transformer不用？</p><h5 id=原因1稀疏梯度问题sgd的致命弱点>原因1：稀疏梯度问题——SGD的致命弱点<a class=anchor href=#%e5%8e%9f%e5%9b%a01%e7%a8%80%e7%96%8f%e6%a2%af%e5%ba%a6%e9%97%ae%e9%a2%98sgd%e7%9a%84%e8%87%b4%e5%91%bd%e5%bc%b1%e7%82%b9>#</a></h5><p><strong>NLP的独特性</strong>：</p><ul><li>词表大（50K-100K个token）</li><li>每个样本只激活极少数token（<strong>稀疏性</strong>）</li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练样本：&#34;I love AI&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 词表大小：50,000</span>
</span></span><span class=line><span class=cl><span class=c1># 激活的token：3个（I, love, AI）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Embedding层梯度</span>
</span></span><span class=line><span class=cl><span class=n>grad_embedding</span><span class=o>.</span><span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=mi>50000</span><span class=p>,</span> <span class=mi>768</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 但只有3行有非零梯度！</span>
</span></span><span class=line><span class=cl><span class=c1># 其他49,997行梯度=0 ← 稀疏！</span></span></span></code></pre></div><p><strong>SGD的问题</strong>：</p><p>$$
\theta_t = \theta_{t-1} - \alpha \cdot g_t
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># SGD更新（学习率lr=0.01）</span>
</span></span><span class=line><span class=cl><span class=n>embedding</span><span class=p>[</span><span class=n>token_id_I</span><span class=p>]</span> <span class=o>-=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>grad_I</span>
</span></span><span class=line><span class=cl><span class=n>embedding</span><span class=p>[</span><span class=n>token_id_love</span><span class=p>]</span> <span class=o>-=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>grad_love</span>
</span></span><span class=line><span class=cl><span class=n>embedding</span><span class=p>[</span><span class=n>token_id_AI</span><span class=p>]</span> <span class=o>-=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>grad_AI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. &#34;I&#34;是常见词（每个batch都出现）</span>
</span></span><span class=line><span class=cl><span class=c1>#    → 每步都更新0.01</span>
</span></span><span class=line><span class=cl><span class=c1>#    → 100步后累积更新1.0 → 过拟合！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. &#34;AI&#34;是罕见词（100个batch才出现1次）</span>
</span></span><span class=line><span class=cl><span class=c1>#    → 100步只更新1次，幅度0.01</span>
</span></span><span class=line><span class=cl><span class=c1>#    → 累积更新0.01 → 欠拟合！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 统一学习率无法适应频率差异！</span></span></span></code></pre></div><p><strong>Adam的自适应学习率</strong>：</p><p>$$
\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Adam更新</span>
</span></span><span class=line><span class=cl><span class=c1># 对于常见词&#34;I&#34;（更新频繁）</span>
</span></span><span class=line><span class=cl><span class=n>v_I</span> <span class=n>快速积累</span> <span class=err>→</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_I</span><span class=p>)</span><span class=n>大</span> <span class=err>→</span> <span class=n>实际步长</span> <span class=o>=</span> <span class=n>lr</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_I</span><span class=p>)</span> <span class=n>小</span>  <span class=err>✅</span>
</span></span><span class=line><span class=cl><span class=c1># 例如：v_I = 100 → 实际lr = 0.001 / 10 = 0.0001</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对于罕见词&#34;AI&#34;（更新稀疏）</span>
</span></span><span class=line><span class=cl><span class=n>v_AI</span> <span class=n>积累慢</span> <span class=err>→</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_AI</span><span class=p>)</span><span class=n>小</span> <span class=err>→</span> <span class=n>实际步长</span> <span class=o>=</span> <span class=n>lr</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_AI</span><span class=p>)</span> <span class=n>大</span>  <span class=err>✅</span>
</span></span><span class=line><span class=cl><span class=c1># 例如：v_AI = 1 → 实际lr = 0.001 / 1 = 0.001</span></span></span></code></pre></div><p><strong>效果</strong>：Adam为每个参数自动分配<strong>频率自适应</strong>的学习率！</p><p><strong>实验对比</strong>（BERT预训练，WikiText-2）：</p><table><thead><tr><th>优化器</th><th>困惑度</th><th>收敛步数</th><th>训练时间</th><th>备注</th></tr></thead><tbody><tr><td>SGD (lr=0.01)</td><td>45.3</td><td>不收敛</td><td>-</td><td>稀疏梯度无法学习</td></tr><tr><td>SGD+Momentum</td><td>28.7</td><td>500K</td><td>120h</td><td>有改善但仍差</td></tr><tr><td><strong>Adam</strong></td><td><strong>18.5</strong></td><td><strong>100K</strong></td><td><strong>25h</strong></td><td>✅ 最优</td></tr><tr><td><strong>AdamW</strong></td><td><strong>17.2</strong></td><td><strong>100K</strong></td><td><strong>25h</strong></td><td>✅ 更优</td></tr></tbody></table><p><strong>结论</strong>：Adam在稀疏梯度场景下<strong>碾压</strong>SGD。</p><hr><h5 id=原因2二阶矩梯度缩放解决层级尺度问题>原因2：二阶矩梯度缩放——解决层级尺度问题<a class=anchor href=#%e5%8e%9f%e5%9b%a02%e4%ba%8c%e9%98%b6%e7%9f%a9%e6%a2%af%e5%ba%a6%e7%bc%a9%e6%94%be%e8%a7%a3%e5%86%b3%e5%b1%82%e7%ba%a7%e5%b0%ba%e5%ba%a6%e9%97%ae%e9%a2%98>#</a></h5><p><strong>Transformer的层级差异</strong>：</p><table><thead><tr><th>层</th><th>参数范数</th><th>梯度范数</th><th>SGD更新幅度（lr=0.01）</th><th>问题</th></tr></thead><tbody><tr><td>Embedding</td><td>150.3</td><td>2.5</td><td><strong>0.025</strong></td><td>中等</td></tr><tr><td>Attention W_Q</td><td>8.7</td><td>0.3</td><td>0.003</td><td>太小</td></tr><tr><td>FFN第1层</td><td>45.2</td><td>1.2</td><td>0.012</td><td>中等</td></tr><tr><td>输出层</td><td>200.1</td><td><strong>8.3</strong></td><td><strong>0.083</strong></td><td><strong>过大！</strong></td></tr></tbody></table><p><strong>SGD的问题</strong>：统一学习率 → 无法适应不同层的梯度尺度</p><p><strong>Adam的梯度缩放机制</strong>：</p><p>$$
\text{effective_lr}_i = \frac{\alpha}{\sqrt{v_i} + \epsilon}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Embedding层（梯度中等，v积累中等）</span>
</span></span><span class=line><span class=cl><span class=n>v_embedding</span> <span class=err>≈</span> <span class=mf>6.25</span>  <span class=c1># 多次梯度平方的累积</span>
</span></span><span class=line><span class=cl><span class=n>effective_lr</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=mf>6.25</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=mf>2.5</span> <span class=o>=</span> <span class=mf>0.0004</span>  <span class=err>✅</span> <span class=n>合适</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Attention层（梯度小，v积累慢）</span>
</span></span><span class=line><span class=cl><span class=n>v_attention</span> <span class=err>≈</span> <span class=mf>0.09</span>
</span></span><span class=line><span class=cl><span class=n>effective_lr</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=mf>0.09</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=mf>0.3</span> <span class=o>=</span> <span class=mf>0.0033</span>  <span class=err>✅</span> <span class=n>自动放大</span><span class=err>！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出层（梯度超大，v积累超快）</span>
</span></span><span class=line><span class=cl><span class=n>v_output</span> <span class=err>≈</span> <span class=mf>68.89</span>
</span></span><span class=line><span class=cl><span class=n>effective_lr</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=mf>68.89</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=mf>8.3</span> <span class=o>=</span> <span class=mf>0.00012</span>  <span class=err>✅</span> <span class=n>自动缩小</span><span class=err>！</span></span></span></code></pre></div><p><strong>效果</strong>：Adam为每一层自动分配合适的"有效学习率"，实现<strong>层级自适应</strong>。</p><p><strong>可视化对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>SGD（统一学习率 lr=0.01）：
</span></span><span class=line><span class=cl>  Embedding:    ━━━━━━━  (更新幅度：0.025)
</span></span><span class=line><span class=cl>  Attention:    ━━      (更新幅度：0.003，太小)
</span></span><span class=line><span class=cl>  FFN:          ━━━━    (更新幅度：0.012)
</span></span><span class=line><span class=cl>  Output:       ━━━━━━━━━━━━━  (更新幅度：0.083，太大！)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Adam（自适应学习率）：
</span></span><span class=line><span class=cl>  Embedding:    ━━━━━━━  (有效lr：0.0004)
</span></span><span class=line><span class=cl>  Attention:    ━━━━━━━  (有效lr：0.0033，自动放大)
</span></span><span class=line><span class=cl>  FFN:          ━━━━━━━  (有效lr：0.0015)
</span></span><span class=line><span class=cl>  Output:       ━━━━━━━  (有效lr：0.00012，自动缩小)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>所有层的更新幅度趋于平衡！✅</span></span></code></pre></div><hr><h5 id=原因3adamw的权重衰减解耦更好的正则化>原因3：AdamW的权重衰减解耦——更好的正则化<a class=anchor href=#%e5%8e%9f%e5%9b%a03adamw%e7%9a%84%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f%e8%a7%a3%e8%80%a6%e6%9b%b4%e5%a5%bd%e7%9a%84%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h5><p><strong>传统Adam的L2正则化问题</strong>：</p><p>在损失函数中加L2项：</p><p>$$
L_{\text{total}} = L_{\text{data}} + \frac{\lambda}{2} |\theta|^2
$$</p><p>梯度：
$$
g_t = \nabla L_{\text{data}} + \lambda \theta_{t-1}
$$</p><p><strong>问题</strong>：L2正则的梯度被纳入 $m_t$ 和 $v_t$ 的计算：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Adam with L2</span>
</span></span><span class=line><span class=cl><span class=n>m_t</span> <span class=o>=</span> <span class=n>β1</span> <span class=o>*</span> <span class=n>m_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>β1</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>grad_data</span> <span class=o>+</span> <span class=n>λ</span><span class=o>*</span><span class=n>θ</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>v_t</span> <span class=o>=</span> <span class=n>β2</span> <span class=o>*</span> <span class=n>v_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>β2</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>grad_data</span> <span class=o>+</span> <span class=n>λ</span><span class=o>*</span><span class=n>θ</span><span class=p>)</span><span class=o>^</span><span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=c1>#                                ↑</span>
</span></span><span class=line><span class=cl><span class=c1>#                         λ*θ混入了二阶矩估计</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 更新</span>
</span></span><span class=line><span class=cl><span class=n>θ_t</span> <span class=o>=</span> <span class=n>θ_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>m_t</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：λ*θ的贡献被sqrt(v_t)稀释！</span>
</span></span><span class=line><span class=cl><span class=c1># 权重衰减效果被自适应学习率削弱</span></span></span></code></pre></div><p><strong>AdamW的解耦权重衰减</strong>（Decoupled Weight Decay）：</p><p>$$
\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}<em>t} + \epsilon} - \alpha \lambda \theta</em>{t-1}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># AdamW</span>
</span></span><span class=line><span class=cl><span class=n>m_t</span> <span class=o>=</span> <span class=n>β1</span> <span class=o>*</span> <span class=n>m_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>β1</span><span class=p>)</span> <span class=o>*</span> <span class=n>grad_data</span>  <span class=c1># 只用数据梯度！</span>
</span></span><span class=line><span class=cl><span class=n>v_t</span> <span class=o>=</span> <span class=n>β2</span> <span class=o>*</span> <span class=n>v_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>β2</span><span class=p>)</span> <span class=o>*</span> <span class=n>grad_data</span><span class=o>^</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 更新分两步</span>
</span></span><span class=line><span class=cl><span class=n>θ_t</span> <span class=o>=</span> <span class=n>θ_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>m_t</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>v_t</span><span class=p>)</span>  <span class=c1># 自适应更新</span>
</span></span><span class=line><span class=cl><span class=n>θ_t</span> <span class=o>=</span> <span class=n>θ_t</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>λ</span> <span class=o>*</span> <span class=n>θ_t</span>  <span class=c1># 独立权重衰减</span>
</span></span><span class=line><span class=cl><span class=c1>#                ↑</span>
</span></span><span class=line><span class=cl><span class=c1>#          不受自适应学习率影响！✅</span></span></span></code></pre></div><p><strong>实验对比</strong>（BERT-large预训练）：</p><table><thead><tr><th>优化器</th><th>GLUE平均分</th><th>SQuAD F1</th><th>过拟合程度</th></tr></thead><tbody><tr><td>Adam (L2=0.01)</td><td>82.3</td><td>88.1</td><td>高（验证集与训练集差距大）</td></tr><tr><td><strong>AdamW (wd=0.01)</strong></td><td><strong>84.7</strong></td><td><strong>90.3</strong></td><td><strong>低</strong> ✅</td></tr></tbody></table><p><strong>性能提升</strong>：<strong>+2.4%</strong> GLUE分数，<strong>+2.2%</strong> SQuAD F1</p><p><strong>结论</strong>：AdamW的解耦权重衰减在大模型上效果显著更好。</p><hr><h4 id=3warmup策略对比与选择>（3）Warmup策略对比与选择<a class=anchor href=#3warmup%e7%ad%96%e7%95%a5%e5%af%b9%e6%af%94%e4%b8%8e%e9%80%89%e6%8b%a9>#</a></h4><h5 id=策略1线性warmup最常用>策略1：线性Warmup（最常用）<a class=anchor href=#%e7%ad%96%e7%95%a51%e7%ba%bf%e6%80%a7warmup%e6%9c%80%e5%b8%b8%e7%94%a8>#</a></h5><p>$$
\text{lr}<em>t = \begin{cases}
\text{lr}</em>{\max} \cdot \frac{t}{T_{\text{warmup}}} & t \leq T_{\text{warmup}} \
\text{lr}<em>{\max} & t > T</em>{\text{warmup}}
\end{cases}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>get_linear_schedule_with_warmup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>get_linear_schedule_with_warmup</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>4000</span><span class=p>,</span>  <span class=c1># 4000步warmup</span>
</span></span><span class=line><span class=cl>    <span class=n>num_training_steps</span><span class=o>=</span><span class=mi>100000</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><p><strong>学习率变化</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step 0:    lr = 0
</span></span><span class=line><span class=cl>Step 1000: lr = 0.25 * lr_max
</span></span><span class=line><span class=cl>Step 2000: lr = 0.50 * lr_max
</span></span><span class=line><span class=cl>Step 4000: lr = 1.00 * lr_max  ← warmup结束
</span></span><span class=line><span class=cl>Step 4001+: lr = lr_max（保持不变）</span></span></code></pre></div><p><strong>适用</strong>：BERT、GPT、T5等所有Transformer模型</p><hr><h5 id=策略2inverse-sqrt-warmup原始transformer论文>策略2：Inverse Sqrt Warmup（原始Transformer论文）<a class=anchor href=#%e7%ad%96%e7%95%a52inverse-sqrt-warmup%e5%8e%9f%e5%a7%8btransformer%e8%ae%ba%e6%96%87>#</a></h5><p>$$
\text{lr}<em>t = d</em>{\text{model}}^{-0.5} \cdot \min\left(t^{-0.5}, t \cdot T_{\text{warmup}}^{-1.5}\right)
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>LambdaLR</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_inverse_sqrt_schedule</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>lr_lambda</span><span class=p>(</span><span class=n>current_step</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>current_step</span> <span class=o>&lt;</span> <span class=n>num_warmup_steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=nb>float</span><span class=p>(</span><span class=n>current_step</span><span class=p>)</span> <span class=o>/</span> <span class=nb>float</span><span class=p>(</span><span class=nb>max</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>num_warmup_steps</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>current_step</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>LambdaLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>lr_lambda</span><span class=p>)</span></span></span></code></pre></div><p><strong>学习率变化</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step 0-4000:  线性增长到lr_max
</span></span><span class=line><span class=cl>Step 4001+:   lr = lr_max * sqrt(4000/t)  ← 持续衰减
</span></span><span class=line><span class=cl>例如：
</span></span><span class=line><span class=cl>Step 16000: lr = lr_max * sqrt(4000/16000) = 0.5 * lr_max
</span></span><span class=line><span class=cl>Step 64000: lr = lr_max * sqrt(4000/64000) = 0.25 * lr_max</span></span></code></pre></div><p><strong>特点</strong>：Warmup后学习率持续缓慢衰减（$1/\sqrt{t}$）</p><p><strong>适用</strong>：长期训练（100K+ steps），原始Transformer</p><hr><h5 id=策略3cosine-warmup现代推荐>策略3：Cosine Warmup（现代推荐）<a class=anchor href=#%e7%ad%96%e7%95%a53cosine-warmup%e7%8e%b0%e4%bb%a3%e6%8e%a8%e8%8d%90>#</a></h5><p>$$
\text{lr}<em>t = \text{lr}</em>{\min} + \frac{1}{2}(\text{lr}<em>{\max} - \text{lr}</em>{\min}) \left(1 + \cos\left(\frac{t - T_{\text{warmup}}}{T_{\max} - T_{\text{warmup}}} \pi\right)\right)
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>get_cosine_schedule_with_warmup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>get_cosine_schedule_with_warmup</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=mi>4000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_training_steps</span><span class=o>=</span><span class=mi>100000</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><p><strong>学习率变化</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step 0-4000:   线性增长到lr_max
</span></span><span class=line><span class=cl>Step 4001-100000: 余弦衰减
</span></span><span class=line><span class=cl>例如：
</span></span><span class=line><span class=cl>Step 4000:  lr = lr_max
</span></span><span class=line><span class=cl>Step 52000: lr = 0.5 * lr_max  ← 中点
</span></span><span class=line><span class=cl>Step 100000: lr ≈ 0  ← 平滑降至0</span></span></code></pre></div><p><strong>优势</strong>：</p><ol><li>后期学习率平滑降至接近0 → 收敛更稳定</li><li>避免突然停止训练导致的性能损失</li><li>现代大模型（LLaMA、GPT-3）的标配</li></ol><p><strong>适用</strong>：大模型、长训练</p><hr><h5 id=策略对比总结>策略对比总结<a class=anchor href=#%e7%ad%96%e7%95%a5%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%93>#</a></h5><table><thead><tr><th>策略</th><th>Warmup后学习率</th><th>优势</th><th>劣势</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>线性</strong></td><td>保持不变</td><td>简单稳定</td><td>需手动衰减</td><td>BERT、GPT-2</td></tr><tr><td><strong>Inverse Sqrt</strong></td><td>$1/\sqrt{t}$ 衰减</td><td>自动衰减</td><td>后期可能过小</td><td>原始Transformer</td></tr><tr><td><strong>Cosine</strong></td><td>余弦衰减至0</td><td>收敛最稳定</td><td>需提前知道总步数</td><td>LLaMA、GPT-3 ✅</td></tr></tbody></table><hr><h4 id=4实战完整训练循环示例>（4）实战：完整训练循环示例<a class=anchor href=#4%e5%ae%9e%e6%88%98%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e5%be%aa%e7%8e%af%e7%a4%ba%e4%be%8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertModel</span><span class=p>,</span> <span class=n>AdamW</span><span class=p>,</span> <span class=n>get_cosine_schedule_with_warmup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BertModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 优化器：AdamW + 权重衰减</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>,</span>          <span class=c1># 峰值学习率（推荐范围：1e-5到5e-5）</span>
</span></span><span class=line><span class=cl>    <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>),</span>  <span class=c1># Adam的beta参数（默认值）</span>
</span></span><span class=line><span class=cl>    <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>,</span>         <span class=c1># 数值稳定性</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span>   <span class=c1># 权重衰减（解耦，推荐0.01）</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 学习率调度器：Warmup + Cosine衰减</span>
</span></span><span class=line><span class=cl><span class=n>total_steps</span> <span class=o>=</span> <span class=mi>100000</span>
</span></span><span class=line><span class=cl><span class=n>warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.1</span> <span class=o>*</span> <span class=n>total_steps</span><span class=p>)</span>  <span class=c1># 10% warmup（推荐5-10%）</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>get_cosine_schedule_with_warmup</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>warmup_steps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_training_steps</span><span class=o>=</span><span class=n>total_steps</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练循环</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>total_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>batch</span><span class=p>)</span><span class=o>.</span><span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 梯度裁剪（防止爆炸）</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_norm_</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>max_norm</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 优化器更新</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 学习率调度</span>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 监控</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>step</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>current_lr</span> <span class=o>=</span> <span class=n>scheduler</span><span class=o>.</span><span class=n>get_last_lr</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Step </span><span class=si>{</span><span class=n>step</span><span class=si>}</span><span class=s2>: loss=</span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, lr=</span><span class=si>{</span><span class=n>current_lr</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Step 0: loss=8.2345, lr=5.00e-07     ← Warmup初期（lr极小）
</span></span><span class=line><span class=cl>Step 1000: loss=5.1234, lr=5.00e-06
</span></span><span class=line><span class=cl>Step 5000: loss=3.4567, lr=2.50e-05
</span></span><span class=line><span class=cl>Step 10000: loss=2.3456, lr=5.00e-05  ← Warmup结束（达到峰值）
</span></span><span class=line><span class=cl>Step 20000: loss=1.8901, lr=4.76e-05  ← Cosine衰减开始
</span></span><span class=line><span class=cl>Step 50000: loss=1.2345, lr=2.50e-05  ← 中点
</span></span><span class=line><span class=cl>Step 100000: loss=0.5234, lr=5.00e-08  ← 训练结束（lr接近0）</span></span></code></pre></div><hr><h4 id=5面试高频问题>（5）面试高频问题<a class=anchor href=#5%e9%9d%a2%e8%af%95%e9%ab%98%e9%a2%91%e9%97%ae%e9%a2%98>#</a></h4><h5 id=q1为什么transformer训练必须用warmup而cnn不需要>Q1：为什么Transformer训练必须用Warmup，而CNN不需要？<a class=anchor href=#q1%e4%b8%ba%e4%bb%80%e4%b9%88transformer%e8%ae%ad%e7%bb%83%e5%bf%85%e9%a1%bb%e7%94%a8warmup%e8%80%8ccnn%e4%b8%8d%e9%9c%80%e8%a6%81>#</a></h5><p><strong>标准回答</strong>：</p><ol><li><p><strong>Adam二阶矩初始化偏差</strong>：</p><ul><li>Transformer训练初期，Adam的 $v_t$ 估计不稳定</li><li>需要小学习率让其平稳积累</li><li>CNN梯度稳定，无此问题</li></ul></li><li><p><strong>层级梯度范数差异</strong>：</p><ul><li>Transformer的Embedding和输出层梯度巨大</li><li>中间层梯度小</li><li>需要Warmup让各层协同适应</li><li>CNN卷积层梯度相对均匀</li></ul></li><li><p><strong>Attention Softmax饱和</strong>：</p><ul><li>随机初始化容易导致Softmax饱和</li><li>小学习率缓慢摆脱不良状态</li><li>CNN无Softmax，无此问题</li></ul></li></ol><p><strong>关键数据</strong>：</p><ul><li>无Warmup：第10步 loss=NaN</li><li>有Warmup：稳定收敛，困惑度18.5</li></ul><hr><h5 id=q2warmup步数如何设置>Q2：Warmup步数如何设置？<a class=anchor href=#q2warmup%e6%ad%a5%e6%95%b0%e5%a6%82%e4%bd%95%e8%ae%be%e7%bd%ae>#</a></h5><p><strong>经验规则</strong>：</p><ul><li><strong>小模型</strong>（&lt;1B参数）：总步数的 <strong>5-10%</strong><ul><li>例如：100K步训练 → 5K-10K步warmup</li></ul></li><li><strong>大模型</strong>（>10B参数）：总步数的 <strong>1-3%</strong><ul><li>例如：1M步训练 → 10K-30K步warmup</li></ul></li><li><strong>最小值</strong>：至少1000步（让Adam的 $v_t$ 稳定）</li></ul><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>total_steps</span> <span class=o>=</span> <span class=mi>100000</span>
</span></span><span class=line><span class=cl><span class=n>warmup_ratio</span> <span class=o>=</span> <span class=mf>0.1</span>  <span class=c1># 10%</span>
</span></span><span class=line><span class=cl><span class=n>warmup_steps</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>total_steps</span> <span class=o>*</span> <span class=n>warmup_ratio</span><span class=p>)</span></span></span></code></pre></div><p><strong>过长/过短的问题</strong>：</p><ul><li><strong>过短</strong>（&lt;1000步）：$v_t$ 未稳定 → 训练不稳定</li><li><strong>过长</strong>（>20%）：浪费计算，收敛慢</li></ul><hr><h5 id=q3为什么adamw比adam更好>Q3：为什么AdamW比Adam更好？<a class=anchor href=#q3%e4%b8%ba%e4%bb%80%e4%b9%88adamw%e6%af%94adam%e6%9b%b4%e5%a5%bd>#</a></h5><p><strong>标准回答</strong>：</p><ol><li><p><strong>权重衰减解耦</strong>：</p><ul><li>Adam：L2正则的梯度混入 $m_t$、$v_t$ → 被自适应学习率稀释</li><li>AdamW：权重衰减独立应用 → 正则化效果不受影响</li></ul></li><li><p><strong>数学公式</strong>：
$$
\text{AdamW: } \theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}<em>t}} - \alpha \lambda \theta</em>{t-1}
$$</p></li><li><p><strong>实验证明</strong>：</p><ul><li>BERT-large：AdamW比Adam在GLUE上高 <strong>2.4%</strong></li><li>GPT-2：AdamW收敛更快、泛化更好</li></ul></li></ol><hr><h5 id=q4sgd能训练transformer吗>Q4：SGD能训练Transformer吗？<a class=anchor href=#q4sgd%e8%83%bd%e8%ae%ad%e7%bb%83transformer%e5%90%97>#</a></h5><p><strong>标准回答</strong>：</p><ul><li>理论上可以，但<strong>极其困难</strong>且<strong>效果差</strong></li></ul><p><strong>问题</strong>：</p><ol><li><strong>稀疏梯度</strong>：<ul><li>词表大，每个样本只激活少数token</li><li>SGD无法为罕见词分配足够更新</li></ul></li><li><strong>层级尺度差异</strong>：<ul><li>统一学习率无法适应不同层的梯度范数</li></ul></li><li><strong>需要极其精细的调参</strong>：<ul><li>几乎不可能手工调出合适的学习率</li></ul></li></ol><p><strong>实验证明</strong>：</p><ul><li>BERT用SGD：困惑度45.3</li><li>BERT用Adam：困惑度18.5</li><li><strong>差距2.4倍</strong>！</li></ul><p><strong>结论</strong>：不推荐，没有必要用SGD</p><hr><h5 id=q5能否不用warmup>Q5：能否不用Warmup？<a class=anchor href=#q5%e8%83%bd%e5%90%a6%e4%b8%8d%e7%94%a8warmup>#</a></h5><p><strong>标准回答</strong>：</p><ul><li><p><strong>可以但不推荐</strong>，需要：</p><ol><li>极小的初始学习率（如1e-7）</li><li>极其缓慢的学习率增长</li><li>更多的训练步数（可能多50%）</li><li>频繁的梯度监控和手动调整</li></ol></li><li><p><strong>对比</strong>：</p><ul><li>有Warmup：100K步收敛，困惑度18.5</li><li>无Warmup但精细调参：150K步收敛，困惑度19.2</li></ul></li><li><p><strong>结论</strong>：Warmup是最高效、最稳定的解决方案</p></li></ul><hr><h4 id=6本节小结>（6）本节小结<a class=anchor href=#6%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h4><p><strong>核心要点</strong>：</p><ol><li><p><strong>Warmup的必要性</strong>（三大原因）：</p><ul><li>Adam二阶矩 $v_t$ 初期不稳定</li><li>Transformer层级梯度范数差异巨大</li><li>Attention Softmax饱和问题</li></ul></li><li><p><strong>Adam/AdamW的优势</strong>（三大原因）：</p><ul><li>自适应学习率（解决稀疏梯度）</li><li>二阶矩梯度缩放（解决层级尺度）</li><li>解耦权重衰减（更好的正则化）</li></ul></li><li><p><strong>Warmup策略选择</strong>：</p><ul><li>线性Warmup：最常用（BERT、GPT）</li><li>Cosine Warmup：现代推荐（LLaMA、GPT-3）</li><li>步数：总步数的5-10%（小模型）或1-3%（大模型）</li></ul></li><li><p><strong>面试必背</strong>：</p><ul><li><strong>公式</strong>：AdamW更新公式（含解耦权重衰减）</li><li><strong>数据</strong>：AdamW vs Adam +2.4%、SGD vs Adam困惑度45.3 vs 18.5</li><li><strong>概念</strong>：稀疏梯度、自适应学习率、权重衰减解耦、二阶矩偏差</li></ul></li></ol><hr><h3 id=问题6为什么gqa是mha和mqa之间的最优折中>问题6：为什么GQA是MHA和MQA之间的"最优折中"？<a class=anchor href=#%e9%97%ae%e9%a2%986%e4%b8%ba%e4%bb%80%e4%b9%88gqa%e6%98%afmha%e5%92%8cmqa%e4%b9%8b%e9%97%b4%e7%9a%84%e6%9c%80%e4%bc%98%e6%8a%98%e4%b8%ad>#</a></h3><p><strong>背景</strong>（关联知识点：多头注意力变体）</p><ul><li><strong>MHA</strong>（Multi-Head Attention）：每个头独立的K、V</li><li><strong>MQA</strong>（Multi-Query Attention）：所有头共享K、V</li><li><strong>GQA</strong>（Grouped-Query Attention）：分组共享K、V</li></ul><p><strong>性能对比</strong>（以LLaMA-7B为例）：</p><table><thead><tr><th>方案</th><th>KV缓存</th><th>推理速度</th><th>模型质量</th></tr></thead><tbody><tr><td>MHA (32头)</td><td>1024MB</td><td>1.0x</td><td>100%</td></tr><tr><td>GQA (4组)</td><td>128MB</td><td>3.2x</td><td>98.5%</td></tr><tr><td>MQA (1组)</td><td>32MB</td><td>4.5x</td><td>95%</td></tr></tbody></table><p><strong>为什么GQA是折中？</strong></p><p><strong>1. 内存效率接近MQA</strong></p><p>KV缓存大小：</p><p>$$
\text{MHA缓存} = \text{batch} \times \text{seq_len} \times \text{num_heads} \times \text{head_dim} \times 2
$$</p><p>$$
\text{GQA缓存} = \text{batch} \times \text{seq_len} \times \text{num_groups} \times \text{head_dim} \times 2
$$</p><p>32头 → 4组，缓存减少 $32/4 = 8$ 倍！</p><p><strong>2. 质量接近MHA</strong></p><p>分组共享保留了一定的多样性：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>MHA (32头，全独立):
</span></span><span class=line><span class=cl>头1: 捕获语法 (独立K₁, V₁)
</span></span><span class=line><span class=cl>头2: 捕获语义 (独立K₂, V₂)
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>头32: 捕获XX (独立K₃₂, V₃₂)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>GQA (32头，4组):
</span></span><span class=line><span class=cl>组1 (头1-8):  共享K₁, V₁  (捕获语法相关)
</span></span><span class=line><span class=cl>组2 (头9-16): 共享K₂, V₂  (捕获语义相关)
</span></span><span class=line><span class=cl>组3 (头17-24): 共享K₃, V₃  (捕获位置相关)
</span></span><span class=line><span class=cl>组4 (头25-32): 共享K₄, V₄  (捕获其他)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>MQA (32头，1组):
</span></span><span class=line><span class=cl>所有头: 共享K₁, V₁  (多样性丧失！)</span></span></code></pre></div><p><strong>3. 实验验证</strong>（LLaMA-2论文数据）</p><table><thead><tr><th>任务</th><th>MHA</th><th>GQA (8组)</th><th>GQA (4组)</th><th>MQA</th></tr></thead><tbody><tr><td>MMLU</td><td>45.3</td><td>45.1</td><td>44.6</td><td>43.1</td></tr><tr><td>HumanEval</td><td>12.8</td><td>12.5</td><td>12.2</td><td>10.5</td></tr><tr><td>推理速度</td><td>1.0x</td><td>2.1x</td><td>3.2x</td><td>4.5x</td></tr></tbody></table><p><strong>最优分组数选择</strong>：</p><p>$$
\text{num_groups} = \frac{\text{num_heads}}{4 \sim 8}
$$</p><p>例如：</p><ul><li>32头 → 4组或8组</li><li>64头 → 8组或16组</li></ul><p><strong>代表模型</strong>：</p><ul><li>LLaMA-2: 使用GQA（8组）</li><li>Mistral: 使用GQA（8组）</li><li>Qwen: 使用GQA（可配置）</li></ul><hr><h3 id=问题7为什么flash-attention能大幅加速它和标准注意力有什么不同>问题7：为什么Flash Attention能大幅加速，它和标准注意力有什么不同？<a class=anchor href=#%e9%97%ae%e9%a2%987%e4%b8%ba%e4%bb%80%e4%b9%88flash-attention%e8%83%bd%e5%a4%a7%e5%b9%85%e5%8a%a0%e9%80%9f%e5%ae%83%e5%92%8c%e6%a0%87%e5%87%86%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%89%e4%bb%80%e4%b9%88%e4%b8%8d%e5%90%8c>#</a></h3><p><strong>性能对比</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>标准注意力:
</span></span><span class=line><span class=cl>序列长度2k: 12GB显存, 850ms
</span></span><span class=line><span class=cl>序列长度4k: 48GB显存, 3.4s (显存不足！)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Flash Attention:
</span></span><span class=line><span class=cl>序列长度2k: 4GB显存, 120ms
</span></span><span class=line><span class=cl>序列长度4k: 16GB显存, 480ms  ← 显存降75%，速度提升7倍！</span></span></code></pre></div><p><strong>本质区别</strong>（关联知识点：自注意力机制的计算流程）</p><p><strong>标准注意力</strong>的计算流程：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 步骤1: 计算注意力分数矩阵（需要存储完整矩阵！）</span>
</span></span><span class=line><span class=cl><span class=n>S</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>  <span class=c1># [seq_len, seq_len]  ← 显存瓶颈！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤2: Softmax</span>
</span></span><span class=line><span class=cl><span class=n>P</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>S</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>   <span class=c1># [seq_len, seq_len]  ← 又要存储！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤3: 加权求和</span>
</span></span><span class=line><span class=cl><span class=n>O</span> <span class=o>=</span> <span class=n>P</span> <span class=o>@</span> <span class=n>V</span>                <span class=c1># [seq_len, d_model]</span></span></span></code></pre></div><p><strong>问题</strong>：中间矩阵 $S$ 和 $P$ 的大小是 $O(N^2)$！</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>序列长度4096:
</span></span><span class=line><span class=cl>S矩阵: 4096 × 4096 × 4字节 = 64MB  (每个头)
</span></span><span class=line><span class=cl>32个头: 64MB × 32 = 2GB  (仅存储注意力矩阵)
</span></span><span class=line><span class=cl>加上梯度: 2GB × 2 = 4GB
</span></span><span class=line><span class=cl>加上激活值: 总共 ~10-15GB</span></span></code></pre></div><p><strong>Flash Attention的创新</strong>：</p><p><strong>核心思想</strong>：不存储完整的 $S$ 和 $P$ 矩阵，而是<strong>分块计算并融合操作</strong>。</p><p><strong>算法流程</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码（简化）</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>flash_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>block_size</span><span class=o>=</span><span class=mi>128</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>seq_len</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>zeros</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 外层循环：遍历Q的块</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>Q_block</span> <span class=o>=</span> <span class=n>Q</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>block_size</span><span class=p>]</span>  <span class=c1># [block_size, d_k]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 内层循环：遍历K、V的块</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>K_block</span> <span class=o>=</span> <span class=n>K</span><span class=p>[</span><span class=n>j</span><span class=p>:</span><span class=n>j</span><span class=o>+</span><span class=n>block_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>V_block</span> <span class=o>=</span> <span class=n>V</span><span class=p>[</span><span class=n>j</span><span class=p>:</span><span class=n>j</span><span class=o>+</span><span class=n>block_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 在SRAM中计算这个小块的注意力</span>
</span></span><span class=line><span class=cl>            <span class=n>S_block</span> <span class=o>=</span> <span class=n>Q_block</span> <span class=o>@</span> <span class=n>K_block</span><span class=o>.</span><span class=n>T</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>P_block</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>S_block</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>O_block</span> <span class=o>=</span> <span class=n>P_block</span> <span class=o>@</span> <span class=n>V_block</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 累积到输出（在线softmax技巧）</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>block_size</span><span class=p>]</span> <span class=o>+=</span> <span class=n>O_block</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># S_block和P_block立即被丢弃，不占显存！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span></span></span></code></pre></div><p><strong>关键技术</strong>：</p><ol><li><p><strong>分块计算</strong>：一次只处理128×128的小矩阵</p><ul><li>小矩阵存在GPU的SRAM（快速内存）中</li><li>不需要写回HBM（高带宽显存）</li></ul></li><li><p><strong>算子融合</strong>：</p><ul><li>标准方法：QK^T → Softmax → @V（三个独立kernel）</li><li>Flash Attention：一个融合kernel完成所有操作</li></ul></li><li><p><strong>在线Softmax</strong>（数学技巧）：</p></li></ol><p>分块计算softmax时，需要处理全局归一化：</p><p>$$
\text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$</p><p>但分块时我们不知道全局的 $\sum_j$！</p><p><strong>解决方案</strong>：在线更新最大值和累加和：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 第一块</span>
</span></span><span class=line><span class=cl><span class=n>m1</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>S_block1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>l1</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>exp</span><span class=p>(</span><span class=n>S_block1</span> <span class=o>-</span> <span class=n>m1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第二块来了，更新全局统计</span>
</span></span><span class=line><span class=cl><span class=n>m2</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>S_block2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>m_global</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>m1</span><span class=p>,</span> <span class=n>m2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>l_global</span> <span class=o>=</span> <span class=n>l1</span> <span class=o>*</span> <span class=n>exp</span><span class=p>(</span><span class=n>m1</span> <span class=o>-</span> <span class=n>m_global</span><span class=p>)</span> <span class=o>+</span> <span class=nb>sum</span><span class=p>(</span><span class=n>exp</span><span class=p>(</span><span class=n>S_block2</span> <span class=o>-</span> <span class=n>m_global</span><span class=p>))</span></span></span></code></pre></div><p><strong>为什么这么快？</strong></p><table><thead><tr><th>操作</th><th>标准注意力</th><th>Flash Attention</th></tr></thead><tbody><tr><td>HBM读写次数</td><td>$O(N^2)$</td><td>$O(N)$</td></tr><tr><td>SRAM使用</td><td>很少</td><td>充分利用</td></tr><tr><td>内存峰值</td><td>$O(N^2)$</td><td>$O(N)$</td></tr><tr><td>计算效率</td><td>受内存带宽限制</td><td>受计算能力限制</td></tr></tbody></table><p><strong>实践建议</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># PyTorch 2.0+自带Flash Attention</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 自动使用Flash Attention（如果可用）</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>is_causal</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># 自动应用causal mask</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或使用xformers库</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>xformers.ops</span> <span class=kn>import</span> <span class=n>memory_efficient_attention</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>memory_efficient_attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span></span></span></code></pre></div><p><strong>局限性</strong>：</p><ul><li>需要特定硬件支持（A100、H100效果最好）</li><li>Causal mask支持有限</li><li>某些复杂mask模式不支持</li></ul><hr><h3 id=问题8dropout在transformer中到底起什么作用为什么推理时要关闭>问题8：Dropout在Transformer中到底起什么作用？为什么推理时要关闭？<a class=anchor href=#%e9%97%ae%e9%a2%988dropout%e5%9c%a8transformer%e4%b8%ad%e5%88%b0%e5%ba%95%e8%b5%b7%e4%bb%80%e4%b9%88%e4%bd%9c%e7%94%a8%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8e%a8%e7%90%86%e6%97%b6%e8%a6%81%e5%85%b3%e9%97%ad>#</a></h3><p><strong>训练时的行为</strong>（关联知识点：FFN、残差连接）</p><p><strong>标准做法</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 注意力后加Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_out</span><span class=p>)</span>  <span class=c1># ← Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>attn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># FFN后也加Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ffn_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>ffn_out</span><span class=p>)</span>    <span class=c1># ← Dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>ffn_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><p><strong>Dropout的数学行为</strong>（p=0.1为例）：</p><p>训练时：</p><p>$$
\text{Dropout}(x) = \begin{cases}
0 & \text{with probability } 0.1 \
\frac{x}{0.9} & \text{with probability } 0.9
\end{cases}
$$</p><p>注意：保留的值会<strong>放大</strong> $1/(1-p)$ 倍，保持期望不变！</p><p><strong>为什么推理时必须关闭？</strong></p><p><strong>原因1：确定性输出</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练模式（随机）</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output1</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># &#34;人工智能是...&#34;</span>
</span></span><span class=line><span class=cl><span class=n>output2</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># &#34;人工智能可以...&#34;  ← 不同！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 推理模式（确定性）</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output1</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># &#34;人工智能是...&#34;</span>
</span></span><span class=line><span class=cl><span class=n>output2</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># &#34;人工智能是...&#34;  ← 相同！</span></span></span></code></pre></div><p><strong>原因2：数学期望匹配</strong></p><p>训练时Dropout的期望：</p><p>$$
\mathbb{E}[\text{Dropout}(x)] = 0.1 \times 0 + 0.9 \times \frac{x}{0.9} = x
$$</p><p>推理时直接使用 $x$，期望也是 $x$，完美匹配！</p><p>如果推理时还应用Dropout：</p><p>$$
\mathbb{E}[\text{Dropout}(x)] = x \quad \text{(训练)}
$$</p><p>$$
\mathbb{E}[\text{Dropout}(x)] = 0.9x \quad \text{(推理)} \quad ❌
$$</p><p>期望不匹配，导致性能下降！</p><p><strong>实践中的坑</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ❌ 错误：忘记切换到eval模式</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>load_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># model.eval()  ← 忘记了！</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>  <span class=c1># 每次生成结果都不同</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ✅ 正确</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>  <span class=c1># 或者 with torch.no_grad()</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span></span></span></code></pre></div><p><strong>Dropout率选择</strong>：</p><table><thead><tr><th>位置</th><th>常用Dropout率</th><th>说明</th></tr></thead><tbody><tr><td>注意力后</td><td>0.1</td><td>防止注意力过拟合</td></tr><tr><td>FFN后</td><td>0.1</td><td>正则化</td></tr><tr><td>Embedding</td><td>0.1-0.3</td><td>较高dropout防止词嵌入过拟合</td></tr><tr><td>最后输出层</td><td>0</td><td>通常不加dropout</td></tr></tbody></table><p><strong>现代趋势</strong>：很多大模型不用Dropout！</p><ul><li>GPT-3：不使用Dropout</li><li>LLaMA：不使用Dropout</li><li>原因：数据量够大，过拟合风险低</li></ul><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>恭喜你完成了Transformer架构的深度探索！让我们回顾核心内容。</p><h3 id=知识回顾>知识回顾<a class=anchor href=#%e7%9f%a5%e8%af%86%e5%9b%9e%e9%a1%be>#</a></h3><ol><li><p><strong>宏观架构</strong></p><ul><li>编码器：双向理解输入</li><li>解码器：单向生成输出</li><li>编码器-解码器：翻译等序列到序列任务</li></ul></li><li><p><strong>自注意力机制</strong></p><ul><li>Query、Key、Value三元组</li><li>缩放点积注意力公式</li><li>全局信息交互，并行计算</li></ul></li><li><p><strong>位置编码</strong></p><ul><li>绝对位置编码：正弦余弦</li><li>相对位置编码：RoPE、ALiBi</li><li>解决Transformer无位置信息的问题</li></ul></li><li><p><strong>多头注意力</strong></p><ul><li>标准MHA：每个头独立的Q、K、V</li><li>GQA：分组共享K、V</li><li>MQA：所有头共享单个K、V</li></ul></li><li><p><strong>前馈网络</strong></p><ul><li>升维→激活→降维</li><li>GELU、SwiGLU等激活函数</li><li>提供非线性变换能力</li></ul></li><li><p><strong>组装技巧</strong></p><ul><li>残差连接缓解梯度消失</li><li>层归一化稳定训练</li><li>Pre-Norm优于Post-Norm</li></ul></li></ol><h3 id=关键公式>关键公式<a class=anchor href=#%e5%85%b3%e9%94%ae%e5%85%ac%e5%bc%8f>#</a></h3><p><strong>自注意力</strong>：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p><p><strong>多头注意力</strong>：
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, &mldr;, \text{head}_h)W^O
$$</p><p><strong>位置编码</strong>：
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p><p><strong>前馈网络</strong>：
$$
\text{FFN}(x) = \text{GELU}(xW_1)W_2
$$</p><h3 id=实践要点>实践要点<a class=anchor href=#%e5%ae%9e%e8%b7%b5%e8%a6%81%e7%82%b9>#</a></h3><p>✅ <strong>实现技巧</strong>：</p><ul><li>使用Pre-Norm而非Post-Norm</li><li>优先选择GQA平衡性能与质量</li><li>生成任务必须启用KV缓存</li><li>注意力掩码防止信息泄露</li></ul><p>✅ <strong>性能优化</strong>：</p><ul><li>MQA/GQA减少KV缓存</li><li>FlashAttention优化注意力计算（下章详述）</li><li>梯度检查点节省显存</li></ul><p>✅ <strong>调试技巧</strong>：</p><ul><li>可视化注意力权重理解模型行为</li><li>检查每层输出的范数（梯度爆炸/消失）</li><li>验证掩码正确性（因果掩码）</li></ul><h3 id=思考题>思考题<a class=anchor href=#%e6%80%9d%e8%80%83%e9%a2%98>#</a></h3><ol><li>为什么自注意力要除以$\sqrt{d_k}$？如果不除会怎样？</li><li>RoPE相比绝对位置编码的优势是什么？</li><li>为什么GQA是MHA和MQA之间的折中？</li><li>如果去掉残差连接，会发生什么？</li></ol><h3 id=下一章预告>下一章预告<a class=anchor href=#%e4%b8%8b%e4%b8%80%e7%ab%a0%e9%a2%84%e5%91%8a>#</a></h3><p>在第2章《模型家族谱系：从编码器到解码器》中，我们将：</p><ul><li>深入对比BERT、GPT、T5三大架构</li><li>理解为何仅解码器主导现代LLM</li><li>探索不同解码策略（Beam Search、采样）</li><li>实战：用不同架构解决同一任务</li></ul><p>掌握了Transformer的核心组件后，下一步是理解如何根据任务选择合适的架构。准备好了吗？</p><hr><p><strong>本章代码</strong>：所有示例代码已整理到GitHub仓库</p><p><strong>推荐阅读</strong>：</p><ul><li>论文：《Attention is All You Need》（Transformer原论文）</li><li>论文：《RoFormer: Enhanced Transformer with Rotary Position Embedding》（RoPE）</li><li>论文：《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》</li><li>博客：The Illustrated Transformer（Jay Alammar）</li><li>视频：斯坦福CS224N Lecture on Transformers</li></ul><p><strong>推荐实践</strong>：</p><ul><li>从零实现一个完整的Transformer编码器</li><li>可视化不同层、不同头的注意力模式</li><li>对比MHA、GQA、MQA在实际模型上的性能</li></ul></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第3章 语言的基石：分词与嵌入</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/ class="flex align-center"><span>第2章 模型家族谱系：从编码器到解码器</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一宏观蓝图编码器-解码器架构>一、宏观蓝图：编码器-解码器架构</a><ul><li><a href=#原始transformer翻译机器的设计>原始Transformer：翻译机器的设计</a></li><li><a href=#1-编码器encoder理解输入>1. 编码器（Encoder）：理解输入</a></li><li><a href=#2-解码器decoder生成输出>2. 解码器（Decoder）：生成输出</a></li><li><a href=#3-信息流动编码器到解码器>3. 信息流动：编码器到解码器</a></li><li><a href=#现代简化为何只用编码器或解码器>现代简化：为何只用编码器或解码器？</a></li></ul></li><li><a href=#二核心组件一自注意力机制self-attention>二、核心组件一：自注意力机制（Self-Attention）</a><ul><li><a href=#1-为什么需要自注意力从一个问题开始>1. 为什么需要自注意力？从一个问题开始</a><ul><li><a href=#传统方法的局限rnn>传统方法的局限：RNN</a></li><li><a href=#自注意力的解决方案>自注意力的解决方案</a></li></ul></li><li><a href=#2-核心思想querykeyvalue>2. 核心思想：Query、Key、Value</a></li><li><a href=#3-公式推导缩放点积注意力>3. 公式推导：缩放点积注意力</a><ul><li><a href=#符号定义>符号定义</a></li><li><a href=#步骤1生成qkv>步骤1：生成Q、K、V</a></li><li><a href=#-深度解析为什么需要qkv三个独立矩阵>🎯 深度解析：为什么需要Q、K、V三个独立矩阵？</a></li><li><a href=#1问题能否直接用x计算注意力>（1）问题：能否直接用X计算注意力？</a></li><li><a href=#2数学视角秩与表达能力>（2）数学视角：秩与表达能力</a></li><li><a href=#3信息论视角互信息最大化>（3）信息论视角：互信息最大化</a></li><li><a href=#4生物学类比人类注意力机制>（4）生物学类比：人类注意力机制</a></li><li><a href=#5实验逐步移除矩阵的影响>（5）实验：逐步移除矩阵的影响</a></li><li><a href=#6面试高频问题>（6）面试高频问题</a></li><li><a href=#7本节小结>（7）本节小结</a></li><li><a href=#步骤2计算注意力分数>步骤2：计算注意力分数</a></li><li><a href=#步骤3缩放scaling>步骤3：缩放（Scaling）</a></li><li><a href=#步骤4softmax归一化>步骤4：Softmax归一化</a></li><li><a href=#步骤5加权求和value>步骤5：加权求和Value</a></li><li><a href=#完整公式>完整公式</a></li></ul></li><li><a href=#4-注意力的概率论解释>4. 注意力的概率论解释</a></li><li><a href=#动手实践从零实现自注意力>动手实践：从零实现自注意力</a></li><li><a href=#深入理解注意力掩码attention-mask>深入理解：注意力掩码（Attention Mask）</a><ul><li><a href=#为什么需要掩码>为什么需要掩码？</a></li><li><a href=#填充掩码padding-mask>填充掩码（Padding Mask）</a></li><li><a href=#因果掩码causal-mask--look-ahead-mask>因果掩码（Causal Mask / Look-Ahead Mask）</a></li></ul></li><li><a href=#-深度解析为什么encoder用双向decoder必须单向>🎯 深度解析：为什么Encoder用双向，Decoder必须单向？</a><ul><li><a href=#1问题的本质任务目标不同>（1）问题的本质：任务目标不同</a></li><li><a href=#2信息泄露问题为什么decoder不能双向>（2）信息泄露问题：为什么Decoder不能双向？</a></li><li><a href=#场景1如果decoder用双向注意力错误>场景1：如果Decoder用双向注意力（错误）</a></li><li><a href=#场景2推理时的灾难>场景2：推理时的灾难</a></li><li><a href=#3能否都用双向实验对比>（3）能否都用双向？实验对比</a></li><li><a href=#4信息利用率问题因果掩码的代价>（4）信息利用率问题：因果掩码的代价</a></li><li><a href=#rank分析>Rank分析</a></li><li><a href=#信息量分析>信息量分析</a></li><li><a href=#信息利用率位置越靠后越吃亏>信息利用率：位置越靠后越吃亏？</a></li><li><a href=#5encoder-vs-decoder-架构对比总结>（5）Encoder vs Decoder 架构对比总结</a></li><li><a href=#6面试高频问题-1>（6）面试高频问题</a></li><li><a href=#q1-为什么gpt不用双向注意力像bert那样>Q1: 为什么GPT不用双向注意力像BERT那样？</a></li><li><a href=#q2-因果掩码不是损失了一半信息吗>Q2: 因果掩码不是损失了一半信息吗？</a></li><li><a href=#q3-能否设计半双向掩码>Q3: 能否设计"半双向"掩码？</a></li><li><a href=#q4-encoder-decoder架构中decoder的交叉注意力为什么可以双向>Q4: Encoder-Decoder架构中，Decoder的交叉注意力为什么可以双向？</a></li><li><a href=#7本节小结-1>（7）本节小结</a></li><li><a href=#组合掩码padding--causal>组合掩码：Padding + Causal</a></li><li><a href=#掩码对梯度的影响>掩码对梯度的影响</a></li></ul></li><li><a href=#可视化注意力权重>可视化注意力权重</a></li></ul></li><li><a href=#三核心组件二位置编码positional-encoding>三、核心组件二：位置编码（Positional Encoding）</a><ul><li><a href=#1-为什么transformer需要位置编码>1. 为什么Transformer需要位置编码？</a></li><li><a href=#2-绝对位置编码正弦余弦方案>2. 绝对位置编码：正弦余弦方案</a><ul><li><a href=#原因1线性可表达相对位置>原因1：线性可表达相对位置</a></li></ul></li></ul></li></ul><ul><li><ul><li><ul><li><a href=#原因2不同频率捕获不同尺度>原因2：不同频率捕获不同尺度</a></li><li><a href=#原因3唯一性与平滑性的平衡>原因3：唯一性与平滑性的平衡</a></li><li><a href=#原因4外推性理论上>原因4：外推性(理论上)</a></li></ul></li><li><a href=#3-相对位置编码演进>3. 相对位置编码演进</a><ul><li><a href=#-旋转位置编码rope--面试必考>🎯 旋转位置编码（RoPE）- 面试必考</a></li><li><a href=#1设计目标相对位置不变性>（1）设计目标：相对位置不变性</a></li><li><a href=#2数学推导从复数到旋转矩阵>（2）数学推导：从复数到旋转矩阵</a></li><li><a href=#3角频率公式为什么是-100002id>（3）角频率公式：为什么是 $10000^{2i/d}$</a></li><li><a href=#4生产级代码实现>（4）生产级代码实现</a></li><li><a href=#5rope-vs-绝对位置编码对比>（5）RoPE vs 绝对位置编码对比</a></li><li><a href=#6外推性分析与长上下文扩展>（6）外推性分析与长上下文扩展</a></li><li><a href=#7面试高频问题>（7）面试高频问题</a></li><li><a href=#alibiattention-with-linear-biases>ALiBi（Attention with Linear Biases）</a></li></ul></li></ul></li><li><a href=#四核心组件三多头注意力机制multi-head-attention>四、核心组件三：多头注意力机制（Multi-Head Attention）</a><ul><li><a href=#1-多头的意义从多个子空间捕获信息>1. 多头的意义：从多个子空间捕获信息</a><ul><li><a href=#为什么需要多头>为什么需要多头？</a></li><li><a href=#多头到底学到了什么实证研究>多头到底学到了什么？实证研究</a></li><li><a href=#深入理解子空间投影>深入理解：子空间投影</a></li><li><a href=#可视化注意力头的差异>可视化：注意力头的差异</a></li><li><a href=#-深度解析softmax瓶颈与multi-head的秩恢复机制>🎯 深度解析：Softmax瓶颈与Multi-Head的秩恢复机制</a><ul><li><a href=#问题单头注意力的秩瓶颈>问题：单头注意力的秩瓶颈</a></li><li><a href=#解决方案multi-head恢复full-rank>解决方案：Multi-Head恢复Full Rank</a></li><li><a href=#可视化子空间分解>可视化：子空间分解</a></li><li><a href=#代码验证计算注意力矩阵的秩>代码验证：计算注意力矩阵的秩</a></li><li><a href=#关键洞察>关键洞察</a></li></ul></li></ul></li><li><a href=#2-标准多头注意力mha公式推导>2. 标准多头注意力（MHA）公式推导</a><ul><li><a href=#步骤1多个独立的注意力头>步骤1：多个独立的注意力头</a></li><li><a href=#步骤2拼接所有头>步骤2：拼接所有头</a></li><li><a href=#完整公式-1>完整公式</a></li></ul></li><li><a href=#3-高效注意力变体演进>3. 高效注意力变体演进</a><ul><li><a href=#multi-query-attentionmqa>Multi-Query Attention（MQA）</a></li><li><a href=#grouped-query-attentiongqa>Grouped-Query Attention（GQA）</a></li><li><a href=#multi-head-latent-attentionmhla>Multi-Head Latent Attention（MHLA）</a></li></ul></li><li><a href=#动手实践实现gqa模块>动手实践：实现GQA模块</a></li></ul></li><li><a href=#五核心组件四前馈网络feed-forward-network>五、核心组件四：前馈网络（Feed-Forward Network）</a><ul><li><a href=#1-ffn的作用与设计>1. FFN的作用与设计</a><ul><li><a href=#理由1从信息论角度>理由1：从信息论角度</a></li><li><a href=#理由2参数效率与性能平衡>理由2：参数效率与性能平衡</a></li><li><a href=#理由3ffn承担了大部分参数>理由3：FFN承担了大部分参数</a></li></ul></li><li><a href=#深入理解ffn与attention的分工>深入理解：FFN与Attention的分工</a><ul><li><a href=#attention的职责位置间信息聚合>Attention的职责：位置间信息聚合</a></li><li><a href=#ffn的职责位置内非线性变换>FFN的职责：位置内非线性变换</a></li><li><a href=#形象化理解>形象化理解</a></li></ul></li><li><a href=#深入理解为什么需要不同的激活函数>深入理解：为什么需要不同的激活函数？</a><ul><li><a href=#relu的局限性>ReLU的局限性</a></li><li><a href=#gelu平滑的概率门控>GELU：平滑的概率门控</a></li><li><a href=#swiglu门控机制的威力>SwiGLU：门控机制的威力</a></li></ul></li><li><a href=#动手实践实现前馈网络模块>动手实践：实现前馈网络模块</a></li></ul></li><li><a href=#六组装车间构建完整的编码器与解码器>六、组装车间：构建完整的编码器与解码器</a><ul><li><a href=#1-编码器层encoder-layer>1. 编码器层（Encoder Layer）</a></li><li><a href=#2-解码器层decoder-layer>2. 解码器层（Decoder Layer）</a></li><li><a href=#3-残差连接与层归一化>3. 残差连接与层归一化</a><ul><li><a href=#残差连接residual-connection>残差连接（Residual Connection）</a></li><li><a href=#层归一化layer-normalization>层归一化（Layer Normalization）</a></li><li><a href=#batchnorm-vs-layernorm数学对比>BatchNorm vs LayerNorm：数学对比</a></li><li><a href=#为什么transformer用layernorm>为什么Transformer用LayerNorm？</a></li><li><a href=#rmsnormlayernorm的简化版>RMSNorm：LayerNorm的简化版</a></li><li><a href=#总结对比表>总结对比表</a></li></ul></li><li><a href=#4-pre-norm-vs-post-norm梯度流的关键差异>4. Pre-Norm vs Post-Norm：梯度流的关键差异</a><ul><li><a href=#post-norm原始transformer2017>Post-Norm（原始Transformer,2017）</a></li><li><a href=#pre-norm现代主流gpt-2后>Pre-Norm（现代主流,GPT-2后）</a></li><li><a href=#深度分析为什么pre-norm更稳定>深度分析：为什么Pre-Norm更稳定？</a></li><li><a href=#实验验证梯度范数对比>实验验证：梯度范数对比</a></li><li><a href=#性能对比>性能对比</a></li></ul></li></ul></li><li><a href=#七动手实践深入模型内部看执行>七、动手实践：深入模型内部看执行</a><ul><li><a href=#实战一手动执行一次生成>实战一：手动执行一次生成</a></li><li><a href=#实战二见证kv缓存的加速效果>实战二：见证KV缓存的加速效果</a></li></ul></li><li><a href=#八-深度问答从理论到实践的关键问题>八、💡 深度问答：从理论到实践的关键问题</a><ul><li><a href=#问题1为什么llm会变成复读机不断重复同一句话>问题1：为什么LLM会变成"复读机&rdquo;，不断重复同一句话？</a></li><li><a href=#问题2为什么调整temperature能控制输出的创造性>问题2：为什么调整temperature能控制输出的"创造性"？</a></li><li><a href=#问题3为什么长文本生成到后面会失忆忘记前面的内容>问题3：为什么长文本生成到后面会"失忆"，忘记前面的内容？</a></li><li><a href=#问题4为什么多头注意力不是头越多越好>问题4：为什么多头注意力不是"头越多越好"？</a></li><li><a href=#问题5为什么模型训练时突然输出nan或乱码>问题5：为什么模型训练时突然输出NaN或乱码？</a></li><li><a href=#-深度解析学习率warmup与优化器选择的深层原理>🎯 深度解析：学习率Warmup与优化器选择的深层原理</a><ul><li><a href=#1问题为什么transformer训练必须用warmup>（1）问题：为什么Transformer训练必须用Warmup？</a><ul><li><a href=#原因1adam优化器的二阶矩估计初始化偏差>原因1：Adam优化器的二阶矩估计初始化偏差</a></li><li><a href=#原因2transformer层级梯度范数差异>原因2：Transformer层级梯度范数差异</a></li><li><a href=#原因3attention-softmax饱和问题>原因3：Attention Softmax饱和问题</a></li></ul></li><li><a href=#2为什么adamadamw是transformer的标配优化器>（2）为什么Adam/AdamW是Transformer的标配优化器？</a><ul><li><a href=#原因1稀疏梯度问题sgd的致命弱点>原因1：稀疏梯度问题——SGD的致命弱点</a></li><li><a href=#原因2二阶矩梯度缩放解决层级尺度问题>原因2：二阶矩梯度缩放——解决层级尺度问题</a></li><li><a href=#原因3adamw的权重衰减解耦更好的正则化>原因3：AdamW的权重衰减解耦——更好的正则化</a></li></ul></li><li><a href=#3warmup策略对比与选择>（3）Warmup策略对比与选择</a><ul><li><a href=#策略1线性warmup最常用>策略1：线性Warmup（最常用）</a></li><li><a href=#策略2inverse-sqrt-warmup原始transformer论文>策略2：Inverse Sqrt Warmup（原始Transformer论文）</a></li><li><a href=#策略3cosine-warmup现代推荐>策略3：Cosine Warmup（现代推荐）</a></li><li><a href=#策略对比总结>策略对比总结</a></li></ul></li><li><a href=#4实战完整训练循环示例>（4）实战：完整训练循环示例</a></li><li><a href=#5面试高频问题>（5）面试高频问题</a><ul><li><a href=#q1为什么transformer训练必须用warmup而cnn不需要>Q1：为什么Transformer训练必须用Warmup，而CNN不需要？</a></li><li><a href=#q2warmup步数如何设置>Q2：Warmup步数如何设置？</a></li><li><a href=#q3为什么adamw比adam更好>Q3：为什么AdamW比Adam更好？</a></li><li><a href=#q4sgd能训练transformer吗>Q4：SGD能训练Transformer吗？</a></li><li><a href=#q5能否不用warmup>Q5：能否不用Warmup？</a></li></ul></li><li><a href=#6本节小结>（6）本节小结</a></li></ul></li><li><a href=#问题6为什么gqa是mha和mqa之间的最优折中>问题6：为什么GQA是MHA和MQA之间的"最优折中"？</a></li><li><a href=#问题7为什么flash-attention能大幅加速它和标准注意力有什么不同>问题7：为什么Flash Attention能大幅加速，它和标准注意力有什么不同？</a></li><li><a href=#问题8dropout在transformer中到底起什么作用为什么推理时要关闭>问题8：Dropout在Transformer中到底起什么作用？为什么推理时要关闭？</a></li></ul></li><li><a href=#本章小结>本章小结</a><ul><li><a href=#知识回顾>知识回顾</a></li><li><a href=#关键公式>关键公式</a></li><li><a href=#实践要点>实践要点</a></li><li><a href=#思考题>思考题</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></div></aside></main></body></html>