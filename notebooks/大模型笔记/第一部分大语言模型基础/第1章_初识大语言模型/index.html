<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第1章：初识大语言模型# “The best way to predict the future is to invent it.” — Alan Kay, 计算机科学家
本章承诺：带你穿越NLP发展史，理解为什么我们需要大语言模型，以及它们如何从"词袋"进化到"大脑"。
目录# 引言：穿越NLP发展史 一、一段简史：从"词袋"到"大脑" 词袋模型的局限 为什么需要更好的表示 二、词嵌入：让计算机理解"国王-男人=女王" 分布式假设：物以类聚，词以群分 Word2Vec：神奇的词向量 词嵌入的局限性 三、Transformer革命：从"读死书"到"举一反三" RNN的困境：梯度消失 Self-Attention：理解上下文的艺术 Encoder vs Decoder：两种思维方式 四、认识两大模型家族 BERT：双向理解的大师 GPT：生成式的魔法师 两者的区别与适用场景 五、动手实践：与大模型对话 实战一：文本生成（流式输出） 实战二：文本分类（情感分析） 实战三：Token计数与成本估算 六、新手问答 七、本章小结 引言：穿越NLP发展史# 想象一下，你是一个从未接触过语言的外星人，突然被投放到地球。你看到人类用奇怪的符号（文字）交流，发出各种声音（语言）。你的任务是：理解并使用这些符号和声音。
这就是自然语言处理（NLP）面临的核心挑战：如何让计算机理解人类语言？
在过去的几十年里，人类尝试了各种方法：
1950-1990年代：基于规则的方法（专家系统） 1990-2010年代：统计方法（词袋模型、n-gram） 2013-2017年：词嵌入时代（Word2Vec、GloVe） 2017-2020年：Transformer革命（BERT、GPT） 2020年至今：大语言模型时代（GPT-3/4、Claude、ChatGPT） 让我们一起回到起点，看看这段激动人心的演化历程。
一、一段简史：从"词袋"到"大脑"# 词袋模型的局限# 在深度学习出现之前，NLP领域最常用的方法是词袋模型（Bag of Words, BoW）。
核心思想：把文本看作一个"词的袋子"，只关心词出现的频率，不关心词的顺序。
举个例子# 句子1: "我爱自然语言处理" 句子2: "自然语言处理爱我" # 词袋表示（词频统计） 句子1: {"我": 1, "爱": 1, "自然": 1, "语言": 1, "处理": 1} 句子2: {"自然": 1, "语言": 1, "处理": 1, "爱": 1, "我": 1} # 结果：两个句子完全相同！❌问题显而易见：
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 初识大语言模型"><meta property="og:description" content='第1章：初识大语言模型# “The best way to predict the future is to invent it.” — Alan Kay, 计算机科学家
本章承诺：带你穿越NLP发展史，理解为什么我们需要大语言模型，以及它们如何从"词袋"进化到"大脑"。
目录# 引言：穿越NLP发展史 一、一段简史：从"词袋"到"大脑" 词袋模型的局限 为什么需要更好的表示 二、词嵌入：让计算机理解"国王-男人=女王" 分布式假设：物以类聚，词以群分 Word2Vec：神奇的词向量 词嵌入的局限性 三、Transformer革命：从"读死书"到"举一反三" RNN的困境：梯度消失 Self-Attention：理解上下文的艺术 Encoder vs Decoder：两种思维方式 四、认识两大模型家族 BERT：双向理解的大师 GPT：生成式的魔法师 两者的区别与适用场景 五、动手实践：与大模型对话 实战一：文本生成（流式输出） 实战二：文本分类（情感分析） 实战三：Token计数与成本估算 六、新手问答 七、本章小结 引言：穿越NLP发展史# 想象一下，你是一个从未接触过语言的外星人，突然被投放到地球。你看到人类用奇怪的符号（文字）交流，发出各种声音（语言）。你的任务是：理解并使用这些符号和声音。
这就是自然语言处理（NLP）面临的核心挑战：如何让计算机理解人类语言？
在过去的几十年里，人类尝试了各种方法：
1950-1990年代：基于规则的方法（专家系统） 1990-2010年代：统计方法（词袋模型、n-gram） 2013-2017年：词嵌入时代（Word2Vec、GloVe） 2017-2020年：Transformer革命（BERT、GPT） 2020年至今：大语言模型时代（GPT-3/4、Claude、ChatGPT） 让我们一起回到起点，看看这段激动人心的演化历程。
一、一段简史：从"词袋"到"大脑"# 词袋模型的局限# 在深度学习出现之前，NLP领域最常用的方法是词袋模型（Bag of Words, BoW）。
核心思想：把文本看作一个"词的袋子"，只关心词出现的频率，不关心词的顺序。
举个例子# 句子1: "我爱自然语言处理" 句子2: "自然语言处理爱我" # 词袋表示（词频统计） 句子1: {"我": 1, "爱": 1, "自然": 1, "语言": 1, "处理": 1} 句子2: {"自然": 1, "语言": 1, "处理": 1, "爱": 1, "我": 1} # 结果：两个句子完全相同！❌问题显而易见：'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 初识大语言模型"><meta itemprop=description content='第1章：初识大语言模型# “The best way to predict the future is to invent it.” — Alan Kay, 计算机科学家
本章承诺：带你穿越NLP发展史，理解为什么我们需要大语言模型，以及它们如何从"词袋"进化到"大脑"。
目录# 引言：穿越NLP发展史 一、一段简史：从"词袋"到"大脑" 词袋模型的局限 为什么需要更好的表示 二、词嵌入：让计算机理解"国王-男人=女王" 分布式假设：物以类聚，词以群分 Word2Vec：神奇的词向量 词嵌入的局限性 三、Transformer革命：从"读死书"到"举一反三" RNN的困境：梯度消失 Self-Attention：理解上下文的艺术 Encoder vs Decoder：两种思维方式 四、认识两大模型家族 BERT：双向理解的大师 GPT：生成式的魔法师 两者的区别与适用场景 五、动手实践：与大模型对话 实战一：文本生成（流式输出） 实战二：文本分类（情感分析） 实战三：Token计数与成本估算 六、新手问答 七、本章小结 引言：穿越NLP发展史# 想象一下，你是一个从未接触过语言的外星人，突然被投放到地球。你看到人类用奇怪的符号（文字）交流，发出各种声音（语言）。你的任务是：理解并使用这些符号和声音。
这就是自然语言处理（NLP）面临的核心挑战：如何让计算机理解人类语言？
在过去的几十年里，人类尝试了各种方法：
1950-1990年代：基于规则的方法（专家系统） 1990-2010年代：统计方法（词袋模型、n-gram） 2013-2017年：词嵌入时代（Word2Vec、GloVe） 2017-2020年：Transformer革命（BERT、GPT） 2020年至今：大语言模型时代（GPT-3/4、Claude、ChatGPT） 让我们一起回到起点，看看这段激动人心的演化历程。
一、一段简史：从"词袋"到"大脑"# 词袋模型的局限# 在深度学习出现之前，NLP领域最常用的方法是词袋模型（Bag of Words, BoW）。
核心思想：把文本看作一个"词的袋子"，只关心词出现的频率，不关心词的顺序。
举个例子# 句子1: "我爱自然语言处理" 句子2: "自然语言处理爱我" # 词袋表示（词频统计） 句子1: {"我": 1, "爱": 1, "自然": 1, "语言": 1, "处理": 1} 句子2: {"自然": 1, "语言": 1, "处理": 1, "爱": 1, "我": 1} # 结果：两个句子完全相同！❌问题显而易见：'><meta itemprop=wordCount content="1596"><title>第1章 初识大语言模型 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle checked>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/ class=active>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 初识大语言模型</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#引言穿越nlp发展史>引言：穿越NLP发展史</a></li><li><a href=#一一段简史从词袋到大脑>一、一段简史：从"词袋"到"大脑"</a><ul><li><a href=#词袋模型的局限>词袋模型的局限</a><ul><li><a href=#举个例子>举个例子</a></li></ul></li><li><a href=#为什么需要更好的表示>为什么需要更好的表示</a></li></ul></li><li><a href=#二词嵌入让计算机理解国王-男人女王>二、词嵌入：让计算机理解"国王-男人=女王"</a><ul><li><a href=#分布式假设物以类聚词以群分>分布式假设：物以类聚，词以群分</a></li><li><a href=#word2vec神奇的词向量>Word2Vec：神奇的词向量</a><ul><li><a href=#核心思想>核心思想</a></li><li><a href=#skip-gram-图示>Skip-gram 图示</a></li><li><a href=#训练过程示例>训练过程示例</a></li><li><a href=#神奇的类比能力>神奇的类比能力</a></li></ul></li><li><a href=#词嵌入的局限性>词嵌入的局限性</a><ul><li><a href=#问题示例>问题示例</a></li></ul></li></ul></li><li><a href=#三transformer革命从读死书到举一反三>三、Transformer革命：从"读死书"到"举一反三&rdquo;</a><ul><li><a href=#rnn的困境梯度消失>RNN的困境：梯度消失</a><ul><li><a href=#rnn的工作方式>RNN的工作方式</a></li></ul></li><li><a href=#self-attention理解上下文的艺术>Self-Attention：理解上下文的艺术</a><ul><li><a href=#直觉理解>直觉理解</a></li><li><a href=#self-attention的优势>Self-Attention的优势</a></li></ul></li><li><a href=#encoder-vs-decoder两种思维方式>Encoder vs Decoder：两种思维方式</a><ul><li><a href=#1-encoder编码器>1. Encoder（编码器）</a></li><li><a href=#2-decoder解码器>2. Decoder（解码器）</a></li></ul></li></ul></li><li><a href=#四认识两大模型家族>四、认识两大模型家族</a><ul><li><a href=#bert双向理解的大师>BERT：双向理解的大师</a><ul><li><a href=#架构>架构</a></li><li><a href=#预训练任务>预训练任务</a></li><li><a href=#适用场景>适用场景</a></li></ul></li><li><a href=#gpt生成式的魔法师>GPT：生成式的魔法师</a><ul><li><a href=#架构-1>架构</a></li><li><a href=#预训练任务-1>预训练任务</a></li><li><a href=#演化历程>演化历程</a></li></ul></li><li><a href=#两者的区别与适用场景>两者的区别与适用场景</a></li></ul></li><li><a href=#五动手实践与大模型对话>五、动手实践：与大模型对话</a><ul><li><a href=#实战一文本生成流式输出>实战一：文本生成（流式输出）</a><ul><li><a href=#代码实现>代码实现</a></li><li><a href=#高级带打字机效果>高级：带打字机效果</a></li></ul></li><li><a href=#实战二文本分类情感分析>实战二：文本分类（情感分析）</a><ul><li><a href=#方法1零样本分类>方法1：零样本分类</a></li><li><a href=#方法2少样本学习few-shot>方法2：少样本学习（Few-shot）</a></li></ul></li><li><a href=#实战三token计数与成本估算>实战三：Token计数与成本估算</a><ul><li><a href=#token计数>Token计数</a></li><li><a href=#成本估算>成本估算</a></li><li><a href=#上下文窗口管理>上下文窗口管理</a></li></ul></li></ul></li><li><a href=#六新手问答>六、新手问答</a><ul><li><a href=#q1-bert和gpt到底有什么区别>Q1: BERT和GPT到底有什么区别？</a></li><li><a href=#q2-为什么transformer比rnn好>Q2: 为什么Transformer比RNN好？</a></li><li><a href=#q3-什么是涌现能力>Q3: 什么是"涌现能力&rdquo;？</a></li><li><a href=#q4-为什么需要这么大的模型>Q4: 为什么需要这么大的模型？</a></li><li><a href=#q5-我应该学bert还是gpt>Q5: 我应该学BERT还是GPT？</a></li></ul></li><li><a href=#七本章小结>七、本章小结</a><ul><li><a href=#核心收获>核心收获</a></li><li><a href=#思维导图>思维导图</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第1章初识大语言模型>第1章：初识大语言模型<a class=anchor href=#%e7%ac%ac1%e7%ab%a0%e5%88%9d%e8%af%86%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b>#</a></h1><blockquote class=book-hint><p>&ldquo;The best way to predict the future is to invent it.&rdquo;
— <strong>Alan Kay</strong>, 计算机科学家</p></blockquote><p><strong>本章承诺</strong>：带你穿越NLP发展史，理解为什么我们需要大语言模型，以及它们如何从"词袋"进化到"大脑"。</p><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e5%bc%95%e8%a8%80%e7%a9%bf%e8%b6%8anlp%e5%8f%91%e5%b1%95%e5%8f%b2>引言：穿越NLP发展史</a></li><li><a href=#%e4%b8%80%e4%b8%80%e6%ae%b5%e7%ae%80%e5%8f%b2%e4%bb%8e%e8%af%8d%e8%a2%8b%e5%88%b0%e5%a4%a7%e8%84%91>一、一段简史：从"词袋"到"大脑"</a><ul><li><a href=#%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b1%80%e9%99%90>词袋模型的局限</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%9b%b4%e5%a5%bd%e7%9a%84%e8%a1%a8%e7%a4%ba>为什么需要更好的表示</a></li></ul></li><li><a href=#%e4%ba%8c%e8%af%8d%e5%b5%8c%e5%85%a5%e8%ae%a9%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%90%86%e8%a7%a3%e5%9b%bd%e7%8e%8b-%e7%94%b7%e4%ba%ba%e5%a5%b3%e7%8e%8b>二、词嵌入：让计算机理解"国王-男人=女王"</a><ul><li><a href=#%e5%88%86%e5%b8%83%e5%bc%8f%e5%81%87%e8%ae%be%e7%89%a9%e4%bb%a5%e7%b1%bb%e8%81%9a%e8%af%8d%e4%bb%a5%e7%be%a4%e5%88%86>分布式假设：物以类聚，词以群分</a></li><li><a href=#word2vec%e7%a5%9e%e5%a5%87%e7%9a%84%e8%af%8d%e5%90%91%e9%87%8f>Word2Vec：神奇的词向量</a></li><li><a href=#%e8%af%8d%e5%b5%8c%e5%85%a5%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7>词嵌入的局限性</a></li></ul></li><li><a href=#%e4%b8%89transformer%e9%9d%a9%e5%91%bd%e4%bb%8e%e8%af%bb%e6%ad%bb%e4%b9%a6%e5%88%b0%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89>三、Transformer革命：从"读死书"到"举一反三"</a><ul><li><a href=#rnn%e7%9a%84%e5%9b%b0%e5%a2%83%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1>RNN的困境：梯度消失</a></li><li><a href=#self-attention%e7%90%86%e8%a7%a3%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e8%89%ba%e6%9c%af>Self-Attention：理解上下文的艺术</a></li><li><a href=#encoder-vs-decoder%e4%b8%a4%e7%a7%8d%e6%80%9d%e7%bb%b4%e6%96%b9%e5%bc%8f>Encoder vs Decoder：两种思维方式</a></li></ul></li><li><a href=#%e5%9b%9b%e8%ae%a4%e8%af%86%e4%b8%a4%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%ae%b6%e6%97%8f>四、认识两大模型家族</a><ul><li><a href=#bert%e5%8f%8c%e5%90%91%e7%90%86%e8%a7%a3%e7%9a%84%e5%a4%a7%e5%b8%88>BERT：双向理解的大师</a></li><li><a href=#gpt%e7%94%9f%e6%88%90%e5%bc%8f%e7%9a%84%e9%ad%94%e6%b3%95%e5%b8%88>GPT：生成式的魔法师</a></li><li><a href=#%e4%b8%a4%e8%80%85%e7%9a%84%e5%8c%ba%e5%88%ab%e4%b8%8e%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>两者的区别与适用场景</a></li></ul></li><li><a href=#%e4%ba%94%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%8e%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%af%b9%e8%af%9d>五、动手实践：与大模型对话</a><ul><li><a href=#%e5%ae%9e%e6%88%98%e4%b8%80%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%e6%b5%81%e5%bc%8f%e8%be%93%e5%87%ba>实战一：文本生成（流式输出）</a></li><li><a href=#%e5%ae%9e%e6%88%98%e4%ba%8c%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90>实战二：文本分类（情感分析）</a></li><li><a href=#%e5%ae%9e%e6%88%98%e4%b8%89token%e8%ae%a1%e6%95%b0%e4%b8%8e%e6%88%90%e6%9c%ac%e4%bc%b0%e7%ae%97>实战三：Token计数与成本估算</a></li></ul></li><li><a href=#%e5%85%ad%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94>六、新手问答</a></li><li><a href=#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>七、本章小结</a></li></ul><hr><h2 id=引言穿越nlp发展史>引言：穿越NLP发展史<a class=anchor href=#%e5%bc%95%e8%a8%80%e7%a9%bf%e8%b6%8anlp%e5%8f%91%e5%b1%95%e5%8f%b2>#</a></h2><p>想象一下，你是一个从未接触过语言的外星人，突然被投放到地球。你看到人类用奇怪的符号（文字）交流，发出各种声音（语言）。你的任务是：<strong>理解并使用这些符号和声音</strong>。</p><p>这就是自然语言处理（NLP）面临的核心挑战：<strong>如何让计算机理解人类语言</strong>？</p><p>在过去的几十年里，人类尝试了各种方法：</p><ul><li><strong>1950-1990年代</strong>：基于规则的方法（专家系统）</li><li><strong>1990-2010年代</strong>：统计方法（词袋模型、n-gram）</li><li><strong>2013-2017年</strong>：词嵌入时代（Word2Vec、GloVe）</li><li><strong>2017-2020年</strong>：Transformer革命（BERT、GPT）</li><li><strong>2020年至今</strong>：大语言模型时代（GPT-3/4、Claude、ChatGPT）</li></ul><p>让我们一起回到起点，看看这段激动人心的演化历程。</p><hr><h2 id=一一段简史从词袋到大脑>一、一段简史：从"词袋"到"大脑"<a class=anchor href=#%e4%b8%80%e4%b8%80%e6%ae%b5%e7%ae%80%e5%8f%b2%e4%bb%8e%e8%af%8d%e8%a2%8b%e5%88%b0%e5%a4%a7%e8%84%91>#</a></h2><h3 id=词袋模型的局限>词袋模型的局限<a class=anchor href=#%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%b1%80%e9%99%90>#</a></h3><p>在深度学习出现之前，NLP领域最常用的方法是<strong>词袋模型</strong>（Bag of Words, BoW）。</p><p><strong>核心思想</strong>：把文本看作一个"词的袋子"，只关心词出现的频率，不关心词的顺序。</p><h4 id=举个例子>举个例子<a class=anchor href=#%e4%b8%be%e4%b8%aa%e4%be%8b%e5%ad%90>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>句子1</span><span class=p>:</span> <span class=s2>&#34;我爱自然语言处理&#34;</span>
</span></span><span class=line><span class=cl><span class=n>句子2</span><span class=p>:</span> <span class=s2>&#34;自然语言处理爱我&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 词袋表示（词频统计）</span>
</span></span><span class=line><span class=cl><span class=n>句子1</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;我&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;爱&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;自然&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;语言&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;处理&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>句子2</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;自然&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;语言&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;处理&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;爱&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;我&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：两个句子完全相同！❌</span></span></span></code></pre></div><p><strong>问题显而易见</strong>：</p><ol><li><strong>丢失了词序信息</strong>：&ldquo;我爱你&rdquo; vs &ldquo;你爱我&rdquo; 完全不同</li><li><strong>无法捕捉语义</strong>：&ldquo;国王&rdquo; 和 &ldquo;女王&rdquo; 的关系无法表示</li><li><strong>维度灾难</strong>：词表有10万个词，每个句子都是10万维的稀疏向量</li></ol><hr><h3 id=为什么需要更好的表示>为什么需要更好的表示<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%9b%b4%e5%a5%bd%e7%9a%84%e8%a1%a8%e7%a4%ba>#</a></h3><p><strong>问题</strong>：如何让计算机理解"国王"和"女王"的关系？</p><p>在词袋模型中：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;国王&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>  <span class=c1># 第34567个位置是1</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;女王&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>  <span class=c1># 第78901个位置是1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 这两个向量完全没有关系！</span></span></span></code></pre></div><p><strong>理想的表示</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;国王&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>  <span class=c1># 密集向量</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;女王&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mf>0.7</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>  <span class=c1># 相似的向量</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 可以计算相似度：</span>
</span></span><span class=line><span class=cl><span class=n>similarity</span><span class=p>(</span><span class=s2>&#34;国王&#34;</span><span class=p>,</span> <span class=s2>&#34;女王&#34;</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.85</span>  <span class=c1># 很相似！</span></span></span></code></pre></div><p>这就是<strong>词嵌入</strong>要解决的问题。</p><hr><h2 id=二词嵌入让计算机理解国王-男人女王>二、词嵌入：让计算机理解"国王-男人=女王"<a class=anchor href=#%e4%ba%8c%e8%af%8d%e5%b5%8c%e5%85%a5%e8%ae%a9%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%90%86%e8%a7%a3%e5%9b%bd%e7%8e%8b-%e7%94%b7%e4%ba%ba%e5%a5%b3%e7%8e%8b>#</a></h2><h3 id=分布式假设物以类聚词以群分>分布式假设：物以类聚，词以群分<a class=anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e5%81%87%e8%ae%be%e7%89%a9%e4%bb%a5%e7%b1%bb%e8%81%9a%e8%af%8d%e4%bb%a5%e7%be%a4%e5%88%86>#</a></h3><p>词嵌入的理论基础是<strong>分布式假设</strong>（Distributional Hypothesis）：</p><blockquote class=book-hint><p>&ldquo;You shall know a word by the company it keeps.&rdquo;
&ldquo;一个词的含义由它的上下文决定。&rdquo;
— <strong>J.R. Firth</strong>, 语言学家（1957）</p></blockquote><p><strong>直觉理解</strong>：</p><ul><li>&ldquo;国王"经常和"王冠、宫殿、统治"一起出现</li><li>&ldquo;女王"经常和"王冠、宫殿、统治"一起出现</li><li>&ldquo;猫"经常和"喵喵、爪子、宠物"一起出现</li></ul><p><strong>结论</strong>：上下文相似的词，语义也相似。</p><hr><h3 id=word2vec神奇的词向量>Word2Vec：神奇的词向量<a class=anchor href=#word2vec%e7%a5%9e%e5%a5%87%e7%9a%84%e8%af%8d%e5%90%91%e9%87%8f>#</a></h3><p><strong>Word2Vec</strong>（2013年，Google）是词嵌入时代的里程碑。</p><h4 id=核心思想>核心思想<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h4><p><strong>训练目标</strong>：给定一个词，预测它的上下文词（或反过来）。</p><p>两种训练方式：</p><ol><li><strong>Skip-gram</strong>：用中心词预测上下文</li><li><strong>CBOW</strong>：用上下文预测中心词</li></ol><h4 id=skip-gram-图示>Skip-gram 图示<a class=anchor href=#skip-gram-%e5%9b%be%e7%a4%ba>#</a></h4><pre class=mermaid>graph LR
    A[上下文窗口] --&gt; B[中心词: 国王]
    B --&gt; C[预测: 王冠]
    B --&gt; D[预测: 宫殿]
    B --&gt; E[预测: 统治]

    style B fill:#e1f5ff
    style C fill:#d4edda
    style D fill:#d4edda
    style E fill:#d4edda</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><h4 id=训练过程示例>训练过程示例<a class=anchor href=#%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b%e7%a4%ba%e4%be%8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练语料</span>
</span></span><span class=line><span class=cl><span class=n>句子</span><span class=p>:</span> <span class=s2>&#34;国王坐在华丽的王座上&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 窗口大小=2，中心词=&#34;国王&#34;</span>
</span></span><span class=line><span class=cl><span class=n>上下文</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;坐在&#34;</span><span class=p>,</span> <span class=s2>&#34;华丽&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练目标</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;国王&#34;</span><span class=n>的向量</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=n>预测</span><span class=s2>&#34;坐在&#34;</span><span class=n>和</span><span class=s2>&#34;华丽&#34;</span><span class=n>的概率要高</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 经过数百万次训练后</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;国王&#34;</span><span class=n>的向量</span> <span class=err>→</span> <span class=p>[</span><span class=mf>0.25</span><span class=p>,</span> <span class=mf>0.78</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.45</span><span class=p>,</span> <span class=mf>0.12</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;女王&#34;</span><span class=n>的向量</span> <span class=err>→</span> <span class=p>[</span><span class=mf>0.27</span><span class=p>,</span> <span class=mf>0.75</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.43</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 两个向量非常相似！</span></span></span></code></pre></div><hr><h4 id=神奇的类比能力>神奇的类比能力<a class=anchor href=#%e7%a5%9e%e5%a5%87%e7%9a%84%e7%b1%bb%e6%af%94%e8%83%bd%e5%8a%9b>#</a></h4><p>经过训练后，Word2Vec能学到惊人的语义关系：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 向量运算</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;国王&#34;</span> <span class=o>-</span> <span class=s2>&#34;男人&#34;</span> <span class=o>+</span> <span class=s2>&#34;女人&#34;</span> <span class=err>≈</span> <span class=s2>&#34;女王&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实际数值（简化版）</span>
</span></span><span class=line><span class=cl><span class=n>king</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>man</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>woman</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>king</span> <span class=o>-</span> <span class=n>man</span> <span class=o>+</span> <span class=n>woman</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 最接近的词就是 &#34;queen&#34;！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 更多例子</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;巴黎&#34;</span> <span class=o>-</span> <span class=s2>&#34;法国&#34;</span> <span class=o>+</span> <span class=s2>&#34;德国&#34;</span> <span class=err>≈</span> <span class=s2>&#34;柏林&#34;</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;走&#34;</span> <span class=o>-</span> <span class=s2>&#34;走了&#34;</span> <span class=o>+</span> <span class=s2>&#34;去&#34;</span> <span class=err>≈</span> <span class=s2>&#34;去了&#34;</span></span></span></code></pre></div><p><strong>为什么会这样？</strong></p><p>因为模型从语料中学到了：</p><ul><li>&ldquo;国王"和"女王"的关系 = &ldquo;男人"和"女人"的关系</li><li>这些关系都编码在向量空间的几何结构中</li></ul><pre class=mermaid>graph TD
    A[国王] --&gt;|性别转换| B[女王]
    C[男人] --&gt;|性别转换| D[女人]

    A --&gt;|去掉性别| C
    B --&gt;|去掉性别| D

    style A fill:#e1f5ff
    style B fill:#d4edda
    style C fill:#fff3cd
    style D fill:#ffe6e6</pre><hr><h3 id=词嵌入的局限性>词嵌入的局限性<a class=anchor href=#%e8%af%8d%e5%b5%8c%e5%85%a5%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7>#</a></h3><p>尽管Word2Vec非常强大，但它有一个致命缺陷：<strong>一词一义</strong>。</p><h4 id=问题示例>问题示例<a class=anchor href=#%e9%97%ae%e9%a2%98%e7%a4%ba%e4%be%8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>句子1</span><span class=p>:</span> <span class=s2>&#34;我去银行存钱&#34;</span>
</span></span><span class=line><span class=cl><span class=n>句子2</span><span class=p>:</span> <span class=s2>&#34;我在河岸边散步&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Word2Vec中，&#34;银行&#34;只有一个向量</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;银行&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：</span>
</span></span><span class=line><span class=cl><span class=c1># - 句子1中的&#34;银行&#34;是金融机构</span>
</span></span><span class=line><span class=cl><span class=c1># - 句子2中的&#34;银行&#34;是河岸（英文都是&#34;bank&#34;）</span>
</span></span><span class=line><span class=cl><span class=c1># - 但它们的向量表示完全相同！</span></span></span></code></pre></div><p><strong>根本原因</strong>：Word2Vec是<strong>静态嵌入</strong>，每个词只有一个固定的向量，无法根据上下文动态调整。</p><p><strong>我们需要什么？</strong></p><ul><li><strong>上下文相关的表示</strong>：同一个词在不同句子中有不同的向量</li><li><strong>这就是Transformer要解决的问题</strong></li></ul><hr><h2 id=三transformer革命从读死书到举一反三>三、Transformer革命：从"读死书"到"举一反三&rdquo;<a class=anchor href=#%e4%b8%89transformer%e9%9d%a9%e5%91%bd%e4%bb%8e%e8%af%bb%e6%ad%bb%e4%b9%a6%e5%88%b0%e4%b8%be%e4%b8%80%e5%8f%8d%e4%b8%89>#</a></h2><h3 id=rnn的困境梯度消失>RNN的困境：梯度消失<a class=anchor href=#rnn%e7%9a%84%e5%9b%b0%e5%a2%83%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1>#</a></h3><p>在Transformer之前，NLP主要使用<strong>循环神经网络</strong>（RNN）处理序列数据。</p><h4 id=rnn的工作方式>RNN的工作方式<a class=anchor href=#rnn%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%96%b9%e5%bc%8f>#</a></h4><pre class=mermaid>graph LR
    A[我] --&gt; B[爱]
    B --&gt; C[自然]
    C --&gt; D[语言]
    D --&gt; E[处理]

    A -.-&gt;|隐藏状态| B
    B -.-&gt;|隐藏状态| C
    C -.-&gt;|隐藏状态| D
    D -.-&gt;|隐藏状态| E

    style A fill:#e1f5ff
    style E fill:#d4edda</pre><p><strong>问题</strong>：处理长句子时，早期的信息会逐渐"消失&rdquo;。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>句子</span><span class=p>:</span> <span class=s2>&#34;我昨天去超市买了很多东西，包括牛奶、面包、鸡蛋，</span>
</span></span><span class=line><span class=cl>      <span class=n>然后我回家把它们放进冰箱</span><span class=err>，</span><span class=n>之后我发现</span><span class=err>【】</span><span class=n>忘记带了</span><span class=err>。</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># RNN处理到【】时：</span>
</span></span><span class=line><span class=cl><span class=c1># - &#34;我&#34;、&#34;昨天&#34;、&#34;超市&#34;的信息已经模糊</span>
</span></span><span class=line><span class=cl><span class=c1># - 很难准确填空</span></span></span></code></pre></div><p><strong>这就是著名的"梯度消失"问题</strong>。</p><hr><h3 id=self-attention理解上下文的艺术>Self-Attention：理解上下文的艺术<a class=anchor href=#self-attention%e7%90%86%e8%a7%a3%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e8%89%ba%e6%9c%af>#</a></h3><p><strong>Transformer的核心创新</strong>：Self-Attention（自注意力机制）。</p><h4 id=直觉理解>直觉理解<a class=anchor href=#%e7%9b%b4%e8%a7%89%e7%90%86%e8%a7%a3>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>句子</span><span class=p>:</span> <span class=s2>&#34;我去银行存钱&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Self-Attention会计算每个词之间的关联度</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;银行&#34;</span> <span class=n>关注</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;存钱&#34;</span> <span class=err>→</span> <span class=n>高权重</span><span class=err>（</span><span class=mf>0.8</span><span class=err>）</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;我&#34;</span> <span class=err>→</span> <span class=n>中等权重</span><span class=err>（</span><span class=mf>0.3</span><span class=err>）</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;去&#34;</span> <span class=err>→</span> <span class=n>低权重</span><span class=err>（</span><span class=mf>0.1</span><span class=err>）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：&#34;银行&#34;的向量会融合&#34;存钱&#34;的信息</span>
</span></span><span class=line><span class=cl><span class=c1># → 模型理解这里是&#34;金融机构&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>---</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>句子</span><span class=p>:</span> <span class=s2>&#34;我在河岸边散步&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Self-Attention会重新计算</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;银行&#34;</span> <span class=n>关注</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;河&#34;</span> <span class=err>→</span> <span class=n>高权重</span><span class=err>（</span><span class=mf>0.7</span><span class=err>）</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;散步&#34;</span> <span class=err>→</span> <span class=n>中等权重</span><span class=err>（</span><span class=mf>0.4</span><span class=err>）</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;在&#34;</span> <span class=err>→</span> <span class=n>低权重</span><span class=err>（</span><span class=mf>0.2</span><span class=err>）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：&#34;银行&#34;的向量会融合&#34;河&#34;的信息</span>
</span></span><span class=line><span class=cl><span class=c1># → 模型理解这里是&#34;河岸&#34;</span></span></span></code></pre></div><p><strong>可视化</strong>：</p><pre class=mermaid>graph TD
    A[我去银行存钱] --&gt; B{Self-Attention}
    B --&gt; C[银行 + 存钱信息]
    C --&gt; D[理解为金融机构]

    E[我在河岸边散步] --&gt; F{Self-Attention}
    F --&gt; G[银行 + 河的信息]
    G --&gt; H[理解为河岸]

    style C fill:#e1f5ff
    style G fill:#d4edda</pre><hr><h4 id=self-attention的优势>Self-Attention的优势<a class=anchor href=#self-attention%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h4><p><strong>对比RNN</strong>：</p><table><thead><tr><th>维度</th><th>RNN</th><th>Transformer</th></tr></thead><tbody><tr><td><strong>处理长文本</strong></td><td>❌ 梯度消失</td><td>✅ 直接关联任意位置</td></tr><tr><td><strong>并行计算</strong></td><td>❌ 必须顺序处理</td><td>✅ 可以并行</td></tr><tr><td><strong>训练速度</strong></td><td>慢</td><td>快（10倍以上）</td></tr><tr><td><strong>长距离依赖</strong></td><td>弱</td><td>强</td></tr></tbody></table><p><strong>关键公式</strong>（简化版）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 给定查询词 Q（如&#34;银行&#34;）</span>
</span></span><span class=line><span class=cl><span class=c1># 和所有键 K（句子中的其他词）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 计算注意力分数</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>Q</span> <span class=err>·</span> <span class=n>K</span>  <span class=c1># 点积</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 归一化为概率</span>
</span></span><span class=line><span class=cl><span class=n>attention_weights</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 加权求和值 V</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>attention_weights</span> <span class=err>·</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：output 是融合了上下文的新向量</span></span></span></code></pre></div><hr><h3 id=encoder-vs-decoder两种思维方式>Encoder vs Decoder：两种思维方式<a class=anchor href=#encoder-vs-decoder%e4%b8%a4%e7%a7%8d%e6%80%9d%e7%bb%b4%e6%96%b9%e5%bc%8f>#</a></h3><p>Transformer有两种架构：</p><h4 id=1-encoder编码器>1. Encoder（编码器）<a class=anchor href=#1-encoder%e7%bc%96%e7%a0%81%e5%99%a8>#</a></h4><p><strong>目标</strong>：理解输入文本的含义</p><pre class=mermaid>graph TD
    A[输入: 我爱NLP] --&gt; B[Encoder]
    B --&gt; C[双向Self-Attention]
    C --&gt; D[输出: 上下文向量]

    D --&gt; E[下游任务]
    E --&gt; F[文本分类]
    E --&gt; G[命名实体识别]

    style B fill:#e1f5ff
    style D fill:#d4edda</pre><p><strong>特点</strong>：</p><ul><li><strong>双向注意力</strong>：每个词可以看到前后所有词</li><li><strong>擅长理解</strong>：适合分类、提取任务</li></ul><hr><h4 id=2-decoder解码器>2. Decoder（解码器）<a class=anchor href=#2-decoder%e8%a7%a3%e7%a0%81%e5%99%a8>#</a></h4><p><strong>目标</strong>：根据已有信息生成下一个词</p><pre class=mermaid>graph TD
    A[输入: 今天天气] --&gt; B[Decoder]
    B --&gt; C[单向Self-Attention]
    C --&gt; D[预测下一个词]

    D --&gt; E[可能输出]
    E --&gt; F[很好 30%]
    E --&gt; G[不错 25%]
    E --&gt; H[真好 20%]

    style B fill:#e1f5ff
    style F fill:#d4edda</pre><p><strong>特点</strong>：</p><ul><li><strong>单向注意力</strong>（因果注意力）：只能看到当前词之前的内容</li><li><strong>擅长生成</strong>：适合续写、翻译、对话</li></ul><hr><p><strong>为什么不能双向？</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 假设使用双向注意力生成文本</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;今天天气&#34;</span>
</span></span><span class=line><span class=cl><span class=n>任务</span><span class=p>:</span> <span class=n>预测下一个词</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 如果能看到未来：</span>
</span></span><span class=line><span class=cl><span class=n>模型看到</span><span class=p>:</span> <span class=s2>&#34;今天天气【很好】，我们去...&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 答案已经泄露了！这是作弊！</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 单向注意力（遮挡未来）：</span>
</span></span><span class=line><span class=cl><span class=n>模型只看到</span><span class=p>:</span> <span class=s2>&#34;今天天气【？】&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 必须根据&#34;今天天气&#34;推理下一个词</span></span></span></code></pre></div><hr><h2 id=四认识两大模型家族>四、认识两大模型家族<a class=anchor href=#%e5%9b%9b%e8%ae%a4%e8%af%86%e4%b8%a4%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%ae%b6%e6%97%8f>#</a></h2><h3 id=bert双向理解的大师>BERT：双向理解的大师<a class=anchor href=#bert%e5%8f%8c%e5%90%91%e7%90%86%e8%a7%a3%e7%9a%84%e5%a4%a7%e5%b8%88>#</a></h3><p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers, 2018)</p><h4 id=架构>架构<a class=anchor href=#%e6%9e%b6%e6%9e%84>#</a></h4><pre class=mermaid>graph TD
    A[输入文本] --&gt; B[Token Embedding]
    B --&gt; C[Encoder层 x 12]
    C --&gt; D[输出向量]

    D --&gt; E[任务1: 完形填空MLM]
    D --&gt; F[任务2: 句子关系NSP]

    style C fill:#e1f5ff
    style D fill:#d4edda</pre><h4 id=预训练任务>预训练任务<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1>#</a></h4><p><strong>1. 完形填空（Masked Language Modeling, MLM）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>原句</span><span class=p>:</span> <span class=s2>&#34;我爱自然语言处理&#34;</span>
</span></span><span class=line><span class=cl><span class=n>遮挡</span><span class=p>:</span> <span class=s2>&#34;我爱[MASK]语言处理&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BERT任务：预测[MASK]是什么</span>
</span></span><span class=line><span class=cl><span class=n>模型预测</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;自然&#34;</span> <span class=err>→</span> <span class=mi>85</span><span class=o>%</span>  <span class=err>✅</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;人工&#34;</span> <span class=err>→</span> <span class=mi>10</span><span class=o>%</span>
</span></span><span class=line><span class=cl>  <span class=o>-</span> <span class=s2>&#34;编程&#34;</span> <span class=err>→</span> <span class=mi>3</span><span class=o>%</span></span></span></code></pre></div><p><strong>2. 句子关系判断（Next Sentence Prediction, NSP）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>句子A</span><span class=p>:</span> <span class=s2>&#34;今天天气很好&#34;</span>
</span></span><span class=line><span class=cl><span class=n>句子B</span><span class=p>:</span> <span class=s2>&#34;我们去公园吧&#34;</span>
</span></span><span class=line><span class=cl><span class=n>标签</span><span class=p>:</span> <span class=n>相关</span> <span class=err>✅</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>句子A</span><span class=p>:</span> <span class=s2>&#34;今天天气很好&#34;</span>
</span></span><span class=line><span class=cl><span class=n>句子B</span><span class=p>:</span> <span class=s2>&#34;量子力学很复杂&#34;</span>
</span></span><span class=line><span class=cl><span class=n>标签</span><span class=p>:</span> <span class=n>不相关</span> <span class=err>❌</span></span></span></code></pre></div><h4 id=适用场景>适用场景<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 文本分类</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;这部电影太棒了！&#34;</span>
</span></span><span class=line><span class=cl><span class=n>BERT</span><span class=p>:</span> <span class=p>[</span><span class=n>情感分析</span><span class=p>]</span> <span class=err>→</span> <span class=n>正面</span> <span class=err>✅</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 命名实体识别</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;乔布斯创立了苹果公司&#34;</span>
</span></span><span class=line><span class=cl><span class=n>BERT</span><span class=p>:</span> <span class=p>[</span><span class=n>实体提取</span><span class=p>]</span> <span class=err>→</span> <span class=s2>&#34;乔布斯&#34;</span><span class=p>(</span><span class=n>人名</span><span class=p>),</span> <span class=s2>&#34;苹果公司&#34;</span><span class=p>(</span><span class=n>组织</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问答系统</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;问题: 谁发明了电灯？上下文: 爱迪生在1879年发明了电灯。&#34;</span>
</span></span><span class=line><span class=cl><span class=n>BERT</span><span class=p>:</span> <span class=p>[</span><span class=n>答案提取</span><span class=p>]</span> <span class=err>→</span> <span class=s2>&#34;爱迪生&#34;</span></span></span></code></pre></div><hr><h3 id=gpt生成式的魔法师>GPT：生成式的魔法师<a class=anchor href=#gpt%e7%94%9f%e6%88%90%e5%bc%8f%e7%9a%84%e9%ad%94%e6%b3%95%e5%b8%88>#</a></h3><p><strong>GPT</strong> (Generative Pre-trained Transformer, 2018-2023)</p><h4 id=架构-1>架构<a class=anchor href=#%e6%9e%b6%e6%9e%84-1>#</a></h4><pre class=mermaid>graph TD
    A[输入文本] --&gt; B[Token Embedding]
    B --&gt; C[Decoder层 x 96]
    C --&gt; D[预测下一个词]

    D --&gt; E[采样生成]
    E --&gt; F[输出文本]

    style C fill:#e1f5ff
    style F fill:#d4edda</pre><h4 id=预训练任务-1>预训练任务<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1-1>#</a></h4><p><strong>自回归语言模型</strong>（Autoregressive Language Modeling）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;我爱自然语言&#34;</span>
</span></span><span class=line><span class=cl><span class=n>任务</span><span class=p>:</span> <span class=n>预测下一个词</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPT学习：</span>
</span></span><span class=line><span class=cl><span class=n>P</span><span class=p>(</span><span class=s2>&#34;处理&#34;</span> <span class=o>|</span> <span class=s2>&#34;我爱自然语言&#34;</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.65</span>  <span class=c1># 高概率</span>
</span></span><span class=line><span class=cl><span class=n>P</span><span class=p>(</span><span class=s2>&#34;规律&#34;</span> <span class=o>|</span> <span class=s2>&#34;我爱自然语言&#34;</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.10</span>
</span></span><span class=line><span class=cl><span class=n>P</span><span class=p>(</span><span class=s2>&#34;科学&#34;</span> <span class=o>|</span> <span class=s2>&#34;我爱自然语言&#34;</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.08</span></span></span></code></pre></div><h4 id=演化历程>演化历程<a class=anchor href=#%e6%bc%94%e5%8c%96%e5%8e%86%e7%a8%8b>#</a></h4><table><thead><tr><th>模型</th><th>年份</th><th>参数量</th><th>核心能力</th></tr></thead><tbody><tr><td>GPT-1</td><td>2018</td><td>117M</td><td>基础文本生成</td></tr><tr><td>GPT-2</td><td>2019</td><td>1.5B</td><td>零样本学习</td></tr><tr><td>GPT-3</td><td>2020</td><td>175B</td><td>上下文学习（ICL）</td></tr><tr><td>GPT-3.5</td><td>2022</td><td>175B</td><td>指令跟随（ChatGPT）</td></tr><tr><td>GPT-4</td><td>2023</td><td>~1.8T</td><td>多模态、推理增强</td></tr></tbody></table><hr><h3 id=两者的区别与适用场景>两者的区别与适用场景<a class=anchor href=#%e4%b8%a4%e8%80%85%e7%9a%84%e5%8c%ba%e5%88%ab%e4%b8%8e%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><p><strong>对比表</strong>：</p><table><thead><tr><th>维度</th><th>BERT (Encoder)</th><th>GPT (Decoder)</th></tr></thead><tbody><tr><td><strong>注意力方向</strong></td><td>双向</td><td>单向（因果）</td></tr><tr><td><strong>训练目标</strong></td><td>完形填空 + 句子关系</td><td>预测下一个词</td></tr><tr><td><strong>擅长任务</strong></td><td>理解、分类、提取</td><td>生成、对话、续写</td></tr><tr><td><strong>典型应用</strong></td><td>搜索、问答、NER</td><td>ChatGPT、代码生成</td></tr><tr><td><strong>参数量</strong></td><td>&lt;1B</td><td>>100B</td></tr><tr><td><strong>涌现能力</strong></td><td>弱</td><td>强（思维链、工具调用）</td></tr></tbody></table><p><strong>为什么GPT胜出？</strong></p><ol><li><strong>生成任务更通用</strong>：续写可以变成翻译、摘要、问答</li><li><strong>扩展性更好</strong>：Decoder架构更适合扩展到超大规模</li><li><strong>涌现能力</strong>：参数量超过100B后，出现思维链推理、工具调用等"魔法&rdquo;</li></ol><hr><h2 id=五动手实践与大模型对话>五、动手实践：与大模型对话<a class=anchor href=#%e4%ba%94%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%8e%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%af%b9%e8%af%9d>#</a></h2><p>现在，让我们用<strong>DeepSeek</strong>（国产大模型）实际体验大模型的能力。</p><h3 id=实战一文本生成流式输出>实战一：文本生成（流式输出）<a class=anchor href=#%e5%ae%9e%e6%88%98%e4%b8%80%e6%96%87%e6%9c%ac%e7%94%9f%e6%88%90%e6%b5%81%e5%bc%8f%e8%be%93%e5%87%ba>#</a></h3><p><strong>为什么需要流式输出？</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 非流式：用户等待10秒</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;写一篇500字文章&#34;</span><span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 10秒后一次性显示全部 ⏳</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 流式：像ChatGPT一样逐字显示</span>
</span></span><span class=line><span class=cl><span class=c1># 用户体验更好 ✅</span></span></span></code></pre></div><hr><h4 id=代码实现>代码实现<a class=anchor href=#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 配置DeepSeek API</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;sk-your-api-key&#34;</span><span class=p>,</span>  <span class=c1># 替换为你的API密钥</span>
</span></span><span class=line><span class=cl>    <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;https://api.deepseek.com&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 流式生成</span>
</span></span><span class=line><span class=cl><span class=n>stream</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;deepseek-chat&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;给我讲一个关于Transformer的故事&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>stream</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># 启用流式输出</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.7</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;AI回复：&#34;</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=n>flush</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 逐块接收内容</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>chunk</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>chunk</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=n>flush</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>()</span>  <span class=c1># 换行</span></span></span></code></pre></div><p><strong>运行效果</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>AI回复：从前，有一个神经网络叫RNN，它每天辛苦地处理文本...（逐字显示）</span></span></code></pre></div><hr><h4 id=高级带打字机效果>高级：带打字机效果<a class=anchor href=#%e9%ab%98%e7%ba%a7%e5%b8%a6%e6%89%93%e5%ad%97%e6%9c%ba%e6%95%88%e6%9e%9c>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>typewriter_effect</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>delay</span><span class=o>=</span><span class=mf>0.03</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;模拟打字机效果&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>char</span> <span class=ow>in</span> <span class=n>text</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>char</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=n>flush</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=n>delay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 流式 + 打字机</span>
</span></span><span class=line><span class=cl><span class=n>stream</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;deepseek-chat&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;解释Self-Attention&#34;</span><span class=p>}],</span>
</span></span><span class=line><span class=cl>    <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>chunk</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>typewriter_effect</span><span class=p>(</span><span class=n>chunk</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>)</span></span></span></code></pre></div><hr><h3 id=实战二文本分类情感分析>实战二：文本分类（情感分析）<a class=anchor href=#%e5%ae%9e%e6%88%98%e4%ba%8c%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e6%83%85%e6%84%9f%e5%88%86%e6%9e%90>#</a></h3><p><strong>任务</strong>：判断电影评论是正面还是负面。</p><h4 id=方法1零样本分类>方法1：零样本分类<a class=anchor href=#%e6%96%b9%e6%b3%951%e9%9b%b6%e6%a0%b7%e6%9c%ac%e5%88%86%e7%b1%bb>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sentiment_analysis</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;情感分析&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;你是一个情感分析专家。分析文本情感，只回答&#39;正面&#39;或&#39;负面&#39;。&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;分析以下评论的情感：</span><span class=se>\n</span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;deepseek-chat&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>=</span><span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span>  <span class=c1># 确定性输出</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>reviews</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;这部电影太棒了！演员演技精湛，剧情引人入胜。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;浪费时间，剧情拖沓，演技尴尬。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;还可以，有些地方不错，但整体一般。&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>review</span> <span class=ow>in</span> <span class=n>reviews</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>sentiment</span> <span class=o>=</span> <span class=n>sentiment_analysis</span><span class=p>(</span><span class=n>review</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;评论: </span><span class=si>{</span><span class=n>review</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;情感: </span><span class=si>{</span><span class=n>sentiment</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>评论: 这部电影太棒了！演员演技精湛，剧情引人入胜。
</span></span><span class=line><span class=cl>情感: 正面
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>评论: 浪费时间,剧情拖沓，演技尴尬。
</span></span><span class=line><span class=cl>情感: 负面
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>评论: 还可以，有些地方不错，但整体一般。
</span></span><span class=line><span class=cl>情感: 中性</span></span></code></pre></div><hr><h4 id=方法2少样本学习few-shot>方法2：少样本学习（Few-shot）<a class=anchor href=#%e6%96%b9%e6%b3%952%e5%b0%91%e6%a0%b7%e6%9c%ac%e5%ad%a6%e4%b9%a0few-shot>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>few_shot_sentiment</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;少样本情感分析&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;你是情感分析专家。&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;&#34;示例：
</span></span></span><span class=line><span class=cl><span class=s2>评论: &#34;太好看了！强烈推荐。&#34;
</span></span></span><span class=line><span class=cl><span class=s2>情感: 正面
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>评论: &#34;无聊透顶，不建议观看。&#34;
</span></span></span><span class=line><span class=cl><span class=s2>情感: 负面
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>评论: &#34;画面精美，但剧情一般。&#34;
</span></span></span><span class=line><span class=cl><span class=s2>情感: 中性
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>---
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>现在分析以下评论：
</span></span></span><span class=line><span class=cl><span class=s2>评论: &#34;</span><span class=si>{}</span><span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>情感:&#34;&#34;&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;deepseek-chat&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>=</span><span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>few_shot_sentiment</span><span class=p>(</span><span class=s2>&#34;特效炸裂，但故事太弱了。&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>  <span class=c1># 输出: 中性</span></span></span></code></pre></div><hr><h3 id=实战三token计数与成本估算>实战三：Token计数与成本估算<a class=anchor href=#%e5%ae%9e%e6%88%98%e4%b8%89token%e8%ae%a1%e6%95%b0%e4%b8%8e%e6%88%90%e6%9c%ac%e4%bc%b0%e7%ae%97>#</a></h3><p><strong>为什么关心Token数？</strong></p><ol><li><strong>计费单位</strong>：API按Token收费</li><li><strong>上下文限制</strong>：模型有最大Token窗口（如128K）</li><li><strong>速度影响</strong>：Token越多，推理越慢</li></ol><hr><h4 id=token计数>Token计数<a class=anchor href=#token%e8%ae%a1%e6%95%b0>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载Tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>encoding</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算Token数</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Transformer彻底改变了NLP领域&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>encoding</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;文本: </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token IDs: </span><span class=si>{</span><span class=n>tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出:</span>
</span></span><span class=line><span class=cl><span class=c1># Token IDs: [65387, 55040, 104762, 57095, 95885, 51343]</span>
</span></span><span class=line><span class=cl><span class=c1># Token数: 6</span></span></span></code></pre></div><hr><h4 id=成本估算>成本估算<a class=anchor href=#%e6%88%90%e6%9c%ac%e4%bc%b0%e7%ae%97>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>estimate_cost</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;deepseek-chat&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;估算对话成本&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>encoding</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算输入Token</span>
</span></span><span class=line><span class=cl>    <span class=n>input_tokens</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>msg</span> <span class=ow>in</span> <span class=n>messages</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>input_tokens</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>msg</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 假设输出200 tokens</span>
</span></span><span class=line><span class=cl>    <span class=n>output_tokens</span> <span class=o>=</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># DeepSeek参考定价</span>
</span></span><span class=line><span class=cl>    <span class=n>input_price</span> <span class=o>=</span> <span class=mf>0.001</span> <span class=o>/</span> <span class=mi>1000</span>   <span class=c1># ¥0.001每1K tokens</span>
</span></span><span class=line><span class=cl>    <span class=n>output_price</span> <span class=o>=</span> <span class=mf>0.002</span> <span class=o>/</span> <span class=mi>1000</span>  <span class=c1># ¥0.002每1K tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cost</span> <span class=o>=</span> <span class=n>input_tokens</span> <span class=o>*</span> <span class=n>input_price</span> <span class=o>+</span> <span class=n>output_tokens</span> <span class=o>*</span> <span class=n>output_price</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输入Token: </span><span class=si>{</span><span class=n>input_tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预计输出Token: </span><span class=si>{</span><span class=n>output_tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;预计成本: ¥</span><span class=si>{</span><span class=n>cost</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>cost</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;你是Python专家。&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;如何实现快速排序？&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>estimate_cost</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出:</span>
</span></span><span class=line><span class=cl><span class=c1># 输入Token: 18</span>
</span></span><span class=line><span class=cl><span class=c1># 预计输出Token: 200</span>
</span></span><span class=line><span class=cl><span class=c1># 预计成本: ¥0.000418</span></span></span></code></pre></div><hr><h4 id=上下文窗口管理>上下文窗口管理<a class=anchor href=#%e4%b8%8a%e4%b8%8b%e6%96%87%e7%aa%97%e5%8f%a3%e7%ae%a1%e7%90%86>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>trim_messages</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>4000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;裁剪消息历史以适应上下文窗口&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>encoding</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 保留system消息</span>
</span></span><span class=line><span class=cl>    <span class=n>system_msg</span> <span class=o>=</span> <span class=n>messages</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>if</span> <span class=n>messages</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;role&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;system&#34;</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>chat_history</span> <span class=o>=</span> <span class=n>messages</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=k>if</span> <span class=n>system_msg</span> <span class=k>else</span> <span class=n>messages</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>total_tokens</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>trimmed_history</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 从最新消息往前计算</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>msg</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=n>chat_history</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>msg_tokens</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoding</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>msg</span><span class=p>[</span><span class=s2>&#34;content&#34;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>total_tokens</span> <span class=o>+</span> <span class=n>msg_tokens</span> <span class=o>&gt;</span> <span class=n>max_tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>        <span class=n>trimmed_history</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>msg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>total_tokens</span> <span class=o>+=</span> <span class=n>msg_tokens</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 重新组合</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=p>[</span><span class=n>system_msg</span><span class=p>]</span> <span class=o>+</span> <span class=n>trimmed_history</span> <span class=k>if</span> <span class=n>system_msg</span> <span class=k>else</span> <span class=n>trimmed_history</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;原始消息数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;裁剪后消息数: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>result</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;总Token数: </span><span class=si>{</span><span class=n>total_tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>result</span></span></span></code></pre></div><hr><h2 id=六新手问答>六、新手问答<a class=anchor href=#%e5%85%ad%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94>#</a></h2><h3 id=q1-bert和gpt到底有什么区别>Q1: BERT和GPT到底有什么区别？<a class=anchor href=#q1-bert%e5%92%8cgpt%e5%88%b0%e5%ba%95%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab>#</a></h3><p><strong>简单记忆法</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>BERT = 完形填空高手（双向理解）
</span></span><span class=line><span class=cl>GPT = 作文写作能手（单向生成）</span></span></code></pre></div><p><strong>深入对比</strong>：</p><table><thead><tr><th>维度</th><th>BERT</th><th>GPT</th></tr></thead><tbody><tr><td><strong>训练方式</strong></td><td>遮挡词，预测被遮挡的</td><td>根据前文，预测下一个词</td></tr><tr><td><strong>注意力</strong></td><td>双向（能看到前后文）</td><td>单向（只能看到前文）</td></tr><tr><td><strong>擅长</strong></td><td>理解、分类</td><td>生成、对话</td></tr><tr><td><strong>典型应用</strong></td><td>搜索引擎、问答系统</td><td>ChatGPT、代码生成</td></tr></tbody></table><p><strong>为什么GPT更火？</strong></p><ul><li>生成任务包含了理解任务（续写→翻译、摘要）</li><li>扩展到超大规模后，涌现了推理能力</li></ul><hr><h3 id=q2-为什么transformer比rnn好>Q2: 为什么Transformer比RNN好？<a class=anchor href=#q2-%e4%b8%ba%e4%bb%80%e4%b9%88transformer%e6%af%94rnn%e5%a5%bd>#</a></h3><p><strong>核心差异</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># RNN：顺序处理（串行）</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;我 爱 自然 语言 处理&#34;</span>
</span></span><span class=line><span class=cl><span class=n>处理</span><span class=p>:</span> <span class=n>我</span> <span class=err>→</span> <span class=n>爱</span> <span class=err>→</span> <span class=n>自然</span> <span class=err>→</span> <span class=n>语言</span> <span class=err>→</span> <span class=n>处理</span>
</span></span><span class=line><span class=cl>      <span class=err>↓</span>    <span class=err>↓</span>     <span class=err>↓</span>      <span class=err>↓</span>      <span class=err>↓</span>
</span></span><span class=line><span class=cl>      <span class=n>h1</span> <span class=err>→</span> <span class=n>h2</span> <span class=err>→</span> <span class=n>h3</span> <span class=err>→</span> <span class=n>h4</span> <span class=err>→</span> <span class=n>h5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 处理到h5时，h1的信息已经模糊（梯度消失）</span>
</span></span><span class=line><span class=cl><span class=c1># 2. 必须顺序处理，无法并行</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>---</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Transformer：并行处理</span>
</span></span><span class=line><span class=cl><span class=n>输入</span><span class=p>:</span> <span class=s2>&#34;我 爱 自然 语言 处理&#34;</span>
</span></span><span class=line><span class=cl><span class=n>处理</span><span class=p>:</span> <span class=n>所有词同时进入</span> <span class=err>→</span> <span class=n>Self</span><span class=o>-</span><span class=n>Attention</span>
</span></span><span class=line><span class=cl>      <span class=err>↓</span>
</span></span><span class=line><span class=cl>      <span class=n>每个词都能直接看到其他所有词</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 优势：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 没有梯度消失（直接连接）</span>
</span></span><span class=line><span class=cl><span class=c1># 2. 可以并行计算（速度快10倍）</span></span></span></code></pre></div><hr><h3 id=q3-什么是涌现能力>Q3: 什么是"涌现能力&rdquo;？<a class=anchor href=#q3-%e4%bb%80%e4%b9%88%e6%98%af%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b>#</a></h3><p><strong>定义</strong>：模型规模达到某个临界点后，突然出现的新能力。</p><p><strong>案例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 参数量 &lt; 10B</span>
</span></span><span class=line><span class=cl><span class=n>模型</span><span class=p>:</span> <span class=s2>&#34;1 + 1 = ?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;2&#34;</span>  <span class=c1># 只是记忆</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>模型</span><span class=p>:</span> <span class=s2>&#34;小明有5个苹果，给了小红2个，还剩几个？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;3个苹果&#34;</span>  <span class=c1># 错误或不稳定</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>---</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 参数量 &gt; 100B (如GPT-3)</span>
</span></span><span class=line><span class=cl><span class=n>模型</span><span class=p>:</span> <span class=s2>&#34;1 + 1 = ?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;2&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>模型</span><span class=p>:</span> <span class=s2>&#34;小明有5个苹果，给了小红2个，还剩几个？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>输出</span><span class=p>:</span> <span class=s2>&#34;让我逐步思考：</span>
</span></span><span class=line><span class=cl>       <span class=n>初始</span><span class=err>：</span><span class=mi>5</span><span class=n>个</span>
</span></span><span class=line><span class=cl>       <span class=n>给出</span><span class=err>：</span><span class=mi>2</span><span class=n>个</span>
</span></span><span class=line><span class=cl>       <span class=n>剩余</span><span class=err>：</span><span class=mi>5</span> <span class=o>-</span> <span class=mi>2</span> <span class=o>=</span> <span class=mi>3</span><span class=n>个</span>
</span></span><span class=line><span class=cl>       <span class=n>答案</span><span class=err>：</span><span class=n>还剩3个苹果</span><span class=err>。</span><span class=s2>&#34;  # 涌现了思维链推理！</span></span></span></code></pre></div><p><strong>涌现能力包括</strong>：</p><ul><li>思维链推理（Chain-of-Thought）</li><li>上下文学习（In-Context Learning）</li><li>工具调用（Function Calling）</li></ul><hr><h3 id=q4-为什么需要这么大的模型>Q4: 为什么需要这么大的模型？<a class=anchor href=#q4-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e8%bf%99%e4%b9%88%e5%a4%a7%e7%9a%84%e6%a8%a1%e5%9e%8b>#</a></h3><p><strong>Scaling Law</strong>（缩放定律）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>模型性能 ∝ 参数量^0.73 × 训练数据量^0.28</span></span></code></pre></div><p><strong>数据说明</strong>：</p><table><thead><tr><th>模型</th><th>参数量</th><th>能力</th></tr></thead><tbody><tr><td>GPT-2</td><td>1.5B</td><td>基础续写</td></tr><tr><td>GPT-3</td><td>175B</td><td>零样本学习</td></tr><tr><td>GPT-4</td><td>~1.8T</td><td>复杂推理</td></tr></tbody></table><p><strong>类比</strong>：</p><ul><li>小模型 = 小学生：只能做简单题</li><li>大模型 = 博士：能处理复杂推理</li></ul><p><strong>但也有代价</strong>：</p><ul><li>训练成本：GPT-3训练费用 ~$5,000,000</li><li>推理速度：70B模型比7B慢10倍</li><li>部署成本：需要高端GPU</li></ul><hr><h3 id=q5-我应该学bert还是gpt>Q5: 我应该学BERT还是GPT？<a class=anchor href=#q5-%e6%88%91%e5%ba%94%e8%af%a5%e5%ad%a6bert%e8%bf%98%e6%98%afgpt>#</a></h3><p><strong>建议</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>当前建议：优先学GPT系列</span></span></code></pre></div><p><strong>原因</strong>：</p><ol><li><strong>应用更广</strong>：ChatGPT、Claude、文心一言都是GPT架构</li><li><strong>更易上手</strong>：只需调用API，无需训练模型</li><li><strong>生态更好</strong>：有LangChain、LlamaIndex等丰富工具链</li></ol><p><strong>学习路径</strong>：</p><pre class=mermaid>graph TD
    A[第1章: 理解Transformer基础] --&gt; B[第2章: 提示工程Prompt]
    B --&gt; C[第3章: 向量与嵌入]
    C --&gt; D[第4章: RAG检索增强]
    D --&gt; E[第5章: Agent应用]

    style A fill:#e1f5ff
    style E fill:#d4edda</pre><hr><h2 id=七本章小结>七、本章小结<a class=anchor href=#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=核心收获>核心收获<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%94%b6%e8%8e%b7>#</a></h3><ol><li><p><strong>NLP演化史</strong>：</p><ul><li>词袋模型 → 只看词频，丢失语序</li><li>Word2Vec → 学会语义，但一词一义</li><li>Transformer → 上下文相关，动态表示</li></ul></li><li><p><strong>Transformer核心</strong>：</p><ul><li>Self-Attention → 理解上下文</li><li>Encoder → 双向理解（BERT）</li><li>Decoder → 单向生成（GPT）</li></ul></li><li><p><strong>两大家族</strong>：</p><ul><li>BERT = 理解大师（分类、提取）</li><li>GPT = 生成魔法师（对话、续写）</li></ul></li><li><p><strong>实战能力</strong>：</p><ul><li>流式输出 → 提升用户体验</li><li>情感分析 → 零样本/少样本学习</li><li>Token管理 → 控制成本</li></ul></li></ol><hr><h3 id=思维导图>思维导图<a class=anchor href=#%e6%80%9d%e7%bb%b4%e5%af%bc%e5%9b%be>#</a></h3><pre class=mermaid>mindmap
  root((大语言模型))
    历史演进
      词袋模型
        丢失词序
        维度灾难
      Word2Vec
        分布式假设
        一词一义局限
      Transformer
        Self-Attention
        并行计算
    两大家族
      BERT编码器
        双向理解
        完形填空
        理解任务
      GPT解码器
        单向生成
        自回归
        生成任务
    核心技术
      Self-Attention
        上下文相关
        解决歧义
      Token化
        子词单元
        平衡效率
    实战应用
      文本生成
      文本分类
      成本管理</pre><hr><h3 id=下一章预告>下一章预告<a class=anchor href=#%e4%b8%8b%e4%b8%80%e7%ab%a0%e9%a2%84%e5%91%8a>#</a></h3><p>在**第2章《与模型对话：提示工程基础》**中，我们将深入探讨：</p><ol><li><strong>零样本 vs 少样本学习</strong>：如何让模型"举一反三&rdquo;</li><li><strong>思维链（Chain-of-Thought）</strong>：为什么"逐步思考"能提升准确率</li><li><strong>提示词工程</strong>：如何设计高质量Prompt</li><li><strong>温度与采样策略</strong>：精确控制模型输出</li></ol><p><strong>核心问题</strong>：</p><blockquote class=book-hint><p>&ldquo;如何让LLM从&rsquo;能用&rsquo;到&rsquo;好用&rsquo;？&rdquo;</p></blockquote></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>大模型笔记</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/ class="flex align-center"><span>第2章 与模型对话：提示工程基础</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#引言穿越nlp发展史>引言：穿越NLP发展史</a></li><li><a href=#一一段简史从词袋到大脑>一、一段简史：从"词袋"到"大脑"</a><ul><li><a href=#词袋模型的局限>词袋模型的局限</a><ul><li><a href=#举个例子>举个例子</a></li></ul></li><li><a href=#为什么需要更好的表示>为什么需要更好的表示</a></li></ul></li><li><a href=#二词嵌入让计算机理解国王-男人女王>二、词嵌入：让计算机理解"国王-男人=女王"</a><ul><li><a href=#分布式假设物以类聚词以群分>分布式假设：物以类聚，词以群分</a></li><li><a href=#word2vec神奇的词向量>Word2Vec：神奇的词向量</a><ul><li><a href=#核心思想>核心思想</a></li><li><a href=#skip-gram-图示>Skip-gram 图示</a></li><li><a href=#训练过程示例>训练过程示例</a></li><li><a href=#神奇的类比能力>神奇的类比能力</a></li></ul></li><li><a href=#词嵌入的局限性>词嵌入的局限性</a><ul><li><a href=#问题示例>问题示例</a></li></ul></li></ul></li><li><a href=#三transformer革命从读死书到举一反三>三、Transformer革命：从"读死书"到"举一反三&rdquo;</a><ul><li><a href=#rnn的困境梯度消失>RNN的困境：梯度消失</a><ul><li><a href=#rnn的工作方式>RNN的工作方式</a></li></ul></li><li><a href=#self-attention理解上下文的艺术>Self-Attention：理解上下文的艺术</a><ul><li><a href=#直觉理解>直觉理解</a></li><li><a href=#self-attention的优势>Self-Attention的优势</a></li></ul></li><li><a href=#encoder-vs-decoder两种思维方式>Encoder vs Decoder：两种思维方式</a><ul><li><a href=#1-encoder编码器>1. Encoder（编码器）</a></li><li><a href=#2-decoder解码器>2. Decoder（解码器）</a></li></ul></li></ul></li><li><a href=#四认识两大模型家族>四、认识两大模型家族</a><ul><li><a href=#bert双向理解的大师>BERT：双向理解的大师</a><ul><li><a href=#架构>架构</a></li><li><a href=#预训练任务>预训练任务</a></li><li><a href=#适用场景>适用场景</a></li></ul></li><li><a href=#gpt生成式的魔法师>GPT：生成式的魔法师</a><ul><li><a href=#架构-1>架构</a></li><li><a href=#预训练任务-1>预训练任务</a></li><li><a href=#演化历程>演化历程</a></li></ul></li><li><a href=#两者的区别与适用场景>两者的区别与适用场景</a></li></ul></li><li><a href=#五动手实践与大模型对话>五、动手实践：与大模型对话</a><ul><li><a href=#实战一文本生成流式输出>实战一：文本生成（流式输出）</a><ul><li><a href=#代码实现>代码实现</a></li><li><a href=#高级带打字机效果>高级：带打字机效果</a></li></ul></li><li><a href=#实战二文本分类情感分析>实战二：文本分类（情感分析）</a><ul><li><a href=#方法1零样本分类>方法1：零样本分类</a></li><li><a href=#方法2少样本学习few-shot>方法2：少样本学习（Few-shot）</a></li></ul></li><li><a href=#实战三token计数与成本估算>实战三：Token计数与成本估算</a><ul><li><a href=#token计数>Token计数</a></li><li><a href=#成本估算>成本估算</a></li><li><a href=#上下文窗口管理>上下文窗口管理</a></li></ul></li></ul></li><li><a href=#六新手问答>六、新手问答</a><ul><li><a href=#q1-bert和gpt到底有什么区别>Q1: BERT和GPT到底有什么区别？</a></li><li><a href=#q2-为什么transformer比rnn好>Q2: 为什么Transformer比RNN好？</a></li><li><a href=#q3-什么是涌现能力>Q3: 什么是"涌现能力&rdquo;？</a></li><li><a href=#q4-为什么需要这么大的模型>Q4: 为什么需要这么大的模型？</a></li><li><a href=#q5-我应该学bert还是gpt>Q5: 我应该学BERT还是GPT？</a></li></ul></li><li><a href=#七本章小结>七、本章小结</a><ul><li><a href=#核心收获>核心收获</a></li><li><a href=#思维导图>思维导图</a></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></div></aside></main></body></html>