<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第3章：语言的基石：分词与嵌入 (Tokenization & Embedding)# “Words are, in my not-so-humble opinion, our most inexhaustible source of magic.” — Albus Dumbledore
揭开 LLM 的第一个黑盒：理解机器如何将人类的语言转化为数学的语言。
目录# 一、分词：机器阅读的第一步 1. 为什么要分词？ 2. 主流分词算法详解 3. 前沿分词技术趋势 二、嵌入：赋予词语数学灵魂 1. 从 Token ID 到高维向量 2. 嵌入空间的几何奥秘 3. 上下文嵌入 vs 静态嵌入 三、代码实战：从零构建与使用 1. 实战：使用 TikToken 高效分词 2. 实战：可视化嵌入空间 3. 实战：构建语义搜索引擎 四、工程最佳实践 五、本章小结 一、分词：机器阅读的第一步# 在模型眼中，“我爱你” 不是情感的表达，而是一串数字。将文本转换为这串数字的过程，就是分词 (Tokenization)。
1. 为什么要分词？# 你可能会问：为什么不直接用字符（Character）或者单词（Word）作为最小单位？
❌ 方案 A：按字符切分 (Character-level)# 做法：&#34;apple&#34; → ['a', 'p', 'p', 'l', 'e'] 优点：词表极小（26个字母+符号），不会有未知词（OOV）。 缺点：序列太长。一句话变成几百个字符，模型注意力机制的计算量是序列长度的平方 ($O(N^2)$)，成本太高。而且单个字符缺乏语义。 ❌ 方案 B：按单词切分 (Word-level)# 做法：&#34;I love apples&#34; → ['I', 'love', 'apples'] 优点：语义完整，序列短。 缺点：词表爆炸。英语有几十万词，而且还要处理变形（run, running, ran）和新词（Covid-19, ChatGPT）。如果遇到词表中没有的词，只能由 <UNK> 代替，丢失信息。 ✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择# 核心思想：常用词保持完整，生僻词拆解为字根。 例子： apple (常用) → ['apple'] unbelievable (较长) → ['un', 'believ', 'able'] 优势： 平衡性：词表大小适中（通常 30k-150k）。 处理未知词：任何新词都可以拆成见过的子词。 多语言能力：不同语言共享子词结构。 2. 主流分词算法详解# 目前主流的大模型主要使用以下三种算法的变体：
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第3章 语言的基石：分词与嵌入"><meta property="og:description" content="第3章：语言的基石：分词与嵌入 (Tokenization & Embedding)# “Words are, in my not-so-humble opinion, our most inexhaustible source of magic.” — Albus Dumbledore
揭开 LLM 的第一个黑盒：理解机器如何将人类的语言转化为数学的语言。
目录# 一、分词：机器阅读的第一步 1. 为什么要分词？ 2. 主流分词算法详解 3. 前沿分词技术趋势 二、嵌入：赋予词语数学灵魂 1. 从 Token ID 到高维向量 2. 嵌入空间的几何奥秘 3. 上下文嵌入 vs 静态嵌入 三、代码实战：从零构建与使用 1. 实战：使用 TikToken 高效分词 2. 实战：可视化嵌入空间 3. 实战：构建语义搜索引擎 四、工程最佳实践 五、本章小结 一、分词：机器阅读的第一步# 在模型眼中，“我爱你” 不是情感的表达，而是一串数字。将文本转换为这串数字的过程，就是分词 (Tokenization)。
1. 为什么要分词？# 你可能会问：为什么不直接用字符（Character）或者单词（Word）作为最小单位？
❌ 方案 A：按字符切分 (Character-level)# 做法：&#34;apple&#34; → ['a', 'p', 'p', 'l', 'e'] 优点：词表极小（26个字母+符号），不会有未知词（OOV）。 缺点：序列太长。一句话变成几百个字符，模型注意力机制的计算量是序列长度的平方 ($O(N^2)$)，成本太高。而且单个字符缺乏语义。 ❌ 方案 B：按单词切分 (Word-level)# 做法：&#34;I love apples&#34; → ['I', 'love', 'apples'] 优点：语义完整，序列短。 缺点：词表爆炸。英语有几十万词，而且还要处理变形（run, running, ran）和新词（Covid-19, ChatGPT）。如果遇到词表中没有的词，只能由 <UNK> 代替，丢失信息。 ✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择# 核心思想：常用词保持完整，生僻词拆解为字根。 例子： apple (常用) → ['apple'] unbelievable (较长) → ['un', 'believ', 'able'] 优势： 平衡性：词表大小适中（通常 30k-150k）。 处理未知词：任何新词都可以拆成见过的子词。 多语言能力：不同语言共享子词结构。 2. 主流分词算法详解# 目前主流的大模型主要使用以下三种算法的变体："><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第3章 语言的基石：分词与嵌入"><meta itemprop=description content="第3章：语言的基石：分词与嵌入 (Tokenization & Embedding)# “Words are, in my not-so-humble opinion, our most inexhaustible source of magic.” — Albus Dumbledore
揭开 LLM 的第一个黑盒：理解机器如何将人类的语言转化为数学的语言。
目录# 一、分词：机器阅读的第一步 1. 为什么要分词？ 2. 主流分词算法详解 3. 前沿分词技术趋势 二、嵌入：赋予词语数学灵魂 1. 从 Token ID 到高维向量 2. 嵌入空间的几何奥秘 3. 上下文嵌入 vs 静态嵌入 三、代码实战：从零构建与使用 1. 实战：使用 TikToken 高效分词 2. 实战：可视化嵌入空间 3. 实战：构建语义搜索引擎 四、工程最佳实践 五、本章小结 一、分词：机器阅读的第一步# 在模型眼中，“我爱你” 不是情感的表达，而是一串数字。将文本转换为这串数字的过程，就是分词 (Tokenization)。
1. 为什么要分词？# 你可能会问：为什么不直接用字符（Character）或者单词（Word）作为最小单位？
❌ 方案 A：按字符切分 (Character-level)# 做法：&#34;apple&#34; → ['a', 'p', 'p', 'l', 'e'] 优点：词表极小（26个字母+符号），不会有未知词（OOV）。 缺点：序列太长。一句话变成几百个字符，模型注意力机制的计算量是序列长度的平方 ($O(N^2)$)，成本太高。而且单个字符缺乏语义。 ❌ 方案 B：按单词切分 (Word-level)# 做法：&#34;I love apples&#34; → ['I', 'love', 'apples'] 优点：语义完整，序列短。 缺点：词表爆炸。英语有几十万词，而且还要处理变形（run, running, ran）和新词（Covid-19, ChatGPT）。如果遇到词表中没有的词，只能由 <UNK> 代替，丢失信息。 ✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择# 核心思想：常用词保持完整，生僻词拆解为字根。 例子： apple (常用) → ['apple'] unbelievable (较长) → ['un', 'believ', 'able'] 优势： 平衡性：词表大小适中（通常 30k-150k）。 处理未知词：任何新词都可以拆成见过的子词。 多语言能力：不同语言共享子词结构。 2. 主流分词算法详解# 目前主流的大模型主要使用以下三种算法的变体："><meta itemprop=wordCount content="1826"><title>第3章 语言的基石：分词与嵌入 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle checked>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/ class=active>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第3章 语言的基石：分词与嵌入</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一分词机器阅读的第一步>一、分词：机器阅读的第一步</a><ul><li><a href=#1-为什么要分词>1. 为什么要分词？</a><ul><li><a href=#-方案-a按字符切分-character-level>❌ 方案 A：按字符切分 (Character-level)</a></li><li><a href=#-方案-b按单词切分-word-level>❌ 方案 B：按单词切分 (Word-level)</a></li><li><a href=#-方案-c子词切分-subword-level--现代-llm-的选择>✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择</a></li></ul></li><li><a href=#2-主流分词算法详解>2. 主流分词算法详解</a><ul><li><a href=#1-bpe-byte-pair-encoding>(1) BPE (Byte Pair Encoding)</a></li><li><a href=#2-wordpiece>(2) WordPiece</a></li><li><a href=#3-sentencepiece-unigram-算法>(3) SentencePiece (Unigram 算法)</a></li></ul></li><li><a href=#3-oov-问题与子词的优势>3. OOV 问题与子词的优势</a><ul><li><a href=#什么是-oov-out-of-vocabulary-问题>什么是 OOV (Out-of-Vocabulary) 问题？</a></li><li><a href=#子词分词如何解决-oov>子词分词如何解决 OOV？</a></li><li><a href=#子词粒度的权衡>子词粒度的权衡</a></li></ul></li><li><a href=#4-前沿分词技术趋势>4. 前沿分词技术趋势</a></li></ul></li><li><a href=#二嵌入赋予词语数学灵魂>二、嵌入：赋予词语数学灵魂</a><ul><li><a href=#1-从-token-id-到高维向量>1. 从 Token ID 到高维向量</a><ul><li><a href=#阶段-1token-id离散表示>阶段 1：Token ID（离散表示）</a></li><li><a href=#阶段-2one-hot-编码稀疏向量>阶段 2：One-Hot 编码（稀疏向量）</a></li><li><a href=#阶段-3dense-embedding稠密向量>阶段 3：Dense Embedding（稠密向量）</a></li></ul></li><li><a href=#2-嵌入空间的几何奥秘>2. 嵌入空间的几何奥秘</a></li><li><a href=#3-静态嵌入-vs-上下文嵌入>3. 静态嵌入 vs 上下文嵌入</a><ul><li><a href=#静态嵌入-static-embedding>静态嵌入 (Static Embedding)</a></li><li><a href=#上下文嵌入-contextual-embedding>上下文嵌入 (Contextual Embedding)</a></li></ul></li></ul></li><li><a href=#三代码实战从零构建与使用>三、代码实战：从零构建与使用</a><ul><li><a href=#1-实战从零训练-bpe-分词器>1. 实战：从零训练 BPE 分词器</a></li><li><a href=#2-实战使用-tiktoken-高效分词>2. 实战：使用 TikToken 高效分词</a></li><li><a href=#2-实战可视化嵌入空间>2. 实战：可视化嵌入空间</a></li><li><a href=#3-实战语义相似度计算与搜索>3. 实战：语义相似度计算与搜索</a><ul><li><a href=#方法-1使用-openai-embeddings-api最简单>方法 1：使用 OpenAI Embeddings API（最简单）</a></li><li><a href=#方法-2使用本地模型适合批量处理>方法 2：使用本地模型（适合批量处理）</a></li><li><a href=#核心知识点余弦相似度>核心知识点：余弦相似度</a></li></ul></li></ul></li><li><a href=#四工程最佳实践>四、工程最佳实践</a><ul><li><a href=#1-如何选择分词器>1. 如何选择分词器？</a></li><li><a href=#2-如何处理超长文本>2. 如何处理"超长文本"？</a></li><li><a href=#3-嵌入的选择>3. 嵌入的选择</a></li></ul></li><li><a href=#五本章小结>五、本章小结</a><ul><li><a href=#核心要点回顾>核心要点回顾</a></li><li><a href=#实战技能清单>实战技能清单</a></li><li><a href=#延伸阅读>延伸阅读</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第3章语言的基石分词与嵌入-tokenization--embedding>第3章：语言的基石：分词与嵌入 (Tokenization & Embedding)<a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e8%af%ad%e8%a8%80%e7%9a%84%e5%9f%ba%e7%9f%b3%e5%88%86%e8%af%8d%e4%b8%8e%e5%b5%8c%e5%85%a5-tokenization--embedding>#</a></h1><blockquote class=book-hint><p>&ldquo;Words are, in my not-so-humble opinion, our most inexhaustible source of magic.&rdquo; — Albus Dumbledore</p><p>揭开 LLM 的第一个黑盒：理解机器如何将人类的语言转化为数学的语言。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e5%88%86%e8%af%8d%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5>一、分词：机器阅读的第一步</a><ul><li><a href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%88%86%e8%af%8d>1. 为什么要分词？</a></li><li><a href=#2-%e4%b8%bb%e6%b5%81%e5%88%86%e8%af%8d%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3>2. 主流分词算法详解</a></li><li><a href=#3-%e5%89%8d%e6%b2%bf%e5%88%86%e8%af%8d%e6%8a%80%e6%9c%af%e8%b6%8b%e5%8a%bf>3. 前沿分词技术趋势</a></li></ul></li><li><a href=#%e4%ba%8c%e5%b5%8c%e5%85%a5%e8%b5%8b%e4%ba%88%e8%af%8d%e8%af%ad%e6%95%b0%e5%ad%a6%e7%81%b5%e9%ad%82>二、嵌入：赋予词语数学灵魂</a><ul><li><a href=#1-%e4%bb%8e-token-id-%e5%88%b0%e9%ab%98%e7%bb%b4%e5%90%91%e9%87%8f>1. 从 Token ID 到高维向量</a></li><li><a href=#2-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e5%87%a0%e4%bd%95%e5%a5%a5%e7%a7%98>2. 嵌入空间的几何奥秘</a></li><li><a href=#3-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b5%8c%e5%85%a5-vs-%e9%9d%99%e6%80%81%e5%b5%8c%e5%85%a5>3. 上下文嵌入 vs 静态嵌入</a></li></ul></li><li><a href=#%e4%b8%89%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e4%b8%8e%e4%bd%bf%e7%94%a8>三、代码实战：从零构建与使用</a><ul><li><a href=#1-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-tiktoken-%e9%ab%98%e6%95%88%e5%88%86%e8%af%8d>1. 实战：使用 TikToken 高效分词</a></li><li><a href=#2-%e5%ae%9e%e6%88%98%e5%8f%af%e8%a7%86%e5%8c%96%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4>2. 实战：可视化嵌入空间</a></li><li><a href=#3-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba%e8%af%ad%e4%b9%89%e6%90%9c%e7%b4%a2%e5%bc%95%e6%93%8e>3. 实战：构建语义搜索引擎</a></li></ul></li><li><a href=#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>四、工程最佳实践</a></li><li><a href=#%e4%ba%94%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>五、本章小结</a></li></ul><hr><h2 id=一分词机器阅读的第一步>一、分词：机器阅读的第一步<a class=anchor href=#%e4%b8%80%e5%88%86%e8%af%8d%e6%9c%ba%e5%99%a8%e9%98%85%e8%af%bb%e7%9a%84%e7%ac%ac%e4%b8%80%e6%ad%a5>#</a></h2><p>在模型眼中，&ldquo;我爱你&rdquo; 不是情感的表达，而是一串数字。将文本转换为这串数字的过程，就是<strong>分词 (Tokenization)</strong>。</p><h3 id=1-为什么要分词>1. 为什么要分词？<a class=anchor href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%88%86%e8%af%8d>#</a></h3><p>你可能会问：为什么不直接用字符（Character）或者单词（Word）作为最小单位？</p><h4 id=-方案-a按字符切分-character-level>❌ 方案 A：按字符切分 (Character-level)<a class=anchor href=#-%e6%96%b9%e6%a1%88-a%e6%8c%89%e5%ad%97%e7%ac%a6%e5%88%87%e5%88%86-character-level>#</a></h4><ul><li><strong>做法</strong>：<code>"apple"</code> → <code>['a', 'p', 'p', 'l', 'e']</code></li><li><strong>优点</strong>：词表极小（26个字母+符号），不会有未知词（OOV）。</li><li><strong>缺点</strong>：序列太长。一句话变成几百个字符，模型注意力机制的计算量是序列长度的平方 ($O(N^2)$)，成本太高。而且单个字符缺乏语义。</li></ul><h4 id=-方案-b按单词切分-word-level>❌ 方案 B：按单词切分 (Word-level)<a class=anchor href=#-%e6%96%b9%e6%a1%88-b%e6%8c%89%e5%8d%95%e8%af%8d%e5%88%87%e5%88%86-word-level>#</a></h4><ul><li><strong>做法</strong>：<code>"I love apples"</code> → <code>['I', 'love', 'apples']</code></li><li><strong>优点</strong>：语义完整，序列短。</li><li><strong>缺点</strong>：<strong>词表爆炸</strong>。英语有几十万词，而且还要处理变形（run, running, ran）和新词（Covid-19, ChatGPT）。如果遇到词表中没有的词，只能由 <code>&lt;UNK></code> 代替，丢失信息。</li></ul><h4 id=-方案-c子词切分-subword-level--现代-llm-的选择>✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择<a class=anchor href=#-%e6%96%b9%e6%a1%88-c%e5%ad%90%e8%af%8d%e5%88%87%e5%88%86-subword-level--%e7%8e%b0%e4%bb%a3-llm-%e7%9a%84%e9%80%89%e6%8b%a9>#</a></h4><ul><li><strong>核心思想</strong>：<strong>常用词保持完整，生僻词拆解为字根</strong>。</li><li><strong>例子</strong>：<ul><li><code>apple</code> (常用) → <code>['apple']</code></li><li><code>unbelievable</code> (较长) → <code>['un', 'believ', 'able']</code></li></ul></li><li><strong>优势</strong>：<ul><li><strong>平衡性</strong>：词表大小适中（通常 30k-150k）。</li><li><strong>处理未知词</strong>：任何新词都可以拆成见过的子词。</li><li><strong>多语言能力</strong>：不同语言共享子词结构。</li></ul></li></ul><hr><h3 id=2-主流分词算法详解>2. 主流分词算法详解<a class=anchor href=#2-%e4%b8%bb%e6%b5%81%e5%88%86%e8%af%8d%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3>#</a></h3><p>目前主流的大模型主要使用以下三种算法的变体：</p><h4 id=1-bpe-byte-pair-encoding>(1) BPE (Byte Pair Encoding)<a class=anchor href=#1-bpe-byte-pair-encoding>#</a></h4><p><strong>代表模型</strong>：GPT-2, GPT-3, GPT-4, Llama, Qwen, DeepSeek
<strong>原理</strong>：统计语料中相邻字符对的<strong>频率</strong>，不断合并最高频的对。</p><p><strong>算法详细演示</strong>：
假设我们有一个简单的语料库，包含以下单词和它们的频率：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>hug: 10次
</span></span><span class=line><span class=cl>pug: 5次
</span></span><span class=line><span class=cl>pun: 12次
</span></span><span class=line><span class=cl>bun: 4次
</span></span><span class=line><span class=cl>hugs: 5次</span></span></code></pre></div><p><strong>步骤 1</strong>：初始化，将每个单词拆成字符</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>h u g: 10
</span></span><span class=line><span class=cl>p u g: 5
</span></span><span class=line><span class=cl>p u n: 12
</span></span><span class=line><span class=cl>b u n: 4
</span></span><span class=line><span class=cl>h u g s: 5</span></span></code></pre></div><p><strong>步骤 2</strong>：统计所有相邻字符对的频率</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>(h, u): 15次  [出现在 hug(10) + hugs(5)]
</span></span><span class=line><span class=cl>(u, g): 20次  [出现在 hug(10) + pug(5) + hugs(5)]
</span></span><span class=line><span class=cl>(p, u): 17次  [出现在 pug(5) + pun(12)]
</span></span><span class=line><span class=cl>(u, n): 16次  [出现在 pun(12) + bun(4)]
</span></span><span class=line><span class=cl>(b, u): 4次
</span></span><span class=line><span class=cl>(g, s): 5次</span></span></code></pre></div><p><strong>步骤 3</strong>：选择频率最高的对 <code>(u, g)</code> 合并为 <code>ug</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>h ug: 10
</span></span><span class=line><span class=cl>p ug: 5
</span></span><span class=line><span class=cl>p u n: 12
</span></span><span class=line><span class=cl>b u n: 4
</span></span><span class=line><span class=cl>h ug s: 5</span></span></code></pre></div><p>词表更新：<code>[h, u, g, p, n, b, s, ug]</code></p><p><strong>步骤 4</strong>：继续统计并合并，例如下一步可能合并 <code>(h, ug)</code> → <code>hug</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>hug: 10
</span></span><span class=line><span class=cl>p ug: 5
</span></span><span class=line><span class=cl>p u n: 12
</span></span><span class=line><span class=cl>b u n: 4
</span></span><span class=line><span class=cl>hug s: 5</span></span></code></pre></div><p>通过这样反复迭代，最终得到一个平衡的词表。</p><blockquote class=book-hint><p><strong>💡 直觉理解</strong>：BPE 是一种数据压缩算法。它试图用最少的 Token 覆盖最多的文本内容。</p></blockquote><h4 id=2-wordpiece>(2) WordPiece<a class=anchor href=#2-wordpiece>#</a></h4><p><strong>代表模型</strong>：BERT, DistilBERT
<strong>原理</strong>：与 BPE 类似，但合并依据不是单纯的"频率"，而是<strong>似然值 (Likelihood)</strong> 的提升。</p><p>具体来说，WordPiece 在选择合并哪个字符对时，会计算：
$$
\text{score} = \frac{\text{freq}(x, y)}{\text{freq}(x) \times \text{freq}(y)}
$$</p><p>选择 score 最高的对进行合并。这样可以避免仅仅因为高频而合并不相关的字符。</p><p><strong>实际效果</strong>：在处理生僻词和形态变化时比纯频率方法更合理。</p><h4 id=3-sentencepiece-unigram-算法>(3) SentencePiece (Unigram 算法)<a class=anchor href=#3-sentencepiece-unigram-%e7%ae%97%e6%b3%95>#</a></h4><p><strong>代表模型</strong>：T5, ALBERT, XLNet, Llama (部分版本)
<strong>原理</strong>：与 BPE/WordPiece 相反，它采用<strong>自顶向下</strong>的策略。</p><p><strong>核心思想</strong>：</p><ol><li>从一个非常大的候选词表开始（包含所有可能的子词）</li><li>使用 Unigram 语言模型计算每个 Token 的重要性</li><li>逐步删除贡献最小的 Token，直到达到目标词表大小</li></ol><p><strong>优势</strong>：</p><ul><li><strong>语言无关</strong>：不依赖空格分词，可以直接处理中文、日文等语言</li><li><strong>可逆性</strong>：可以完美还原原始文本（包括空格）</li><li><strong>灵活性</strong>：支持多种采样策略</li></ul><blockquote class=book-hint><p><strong>💡 为什么叫 SentencePiece？</strong>
因为它直接处理原始句子，不需要预分词。这对于没有明确词界的语言（如中文）特别重要。</p></blockquote><hr><h3 id=3-oov-问题与子词的优势>3. OOV 问题与子词的优势<a class=anchor href=#3-oov-%e9%97%ae%e9%a2%98%e4%b8%8e%e5%ad%90%e8%af%8d%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h3><h4 id=什么是-oov-out-of-vocabulary-问题>什么是 OOV (Out-of-Vocabulary) 问题？<a class=anchor href=#%e4%bb%80%e4%b9%88%e6%98%af-oov-out-of-vocabulary-%e9%97%ae%e9%a2%98>#</a></h4><p><strong>传统词级分词的困境</strong>：
假设我们训练了一个词表，包含 50,000 个常见单词。但用户输入了一个新词：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;I love ChatGPT and DeepSeek-V3!&#34;</span></span></code></pre></div><p>如果词表中没有 &ldquo;ChatGPT&rdquo; 和 &ldquo;DeepSeek-V3&rdquo;，传统方法只能将它们替换为 <code>&lt;UNK></code> (Unknown Token)：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#34;I love &lt;UNK&gt; and &lt;UNK&gt;!&#34;</span></span></code></pre></div><p><strong>后果</strong>：</p><ul><li>信息完全丢失</li><li>模型无法理解这些新词</li><li>每次有新词出现都需要重新训练</li></ul><h4 id=子词分词如何解决-oov>子词分词如何解决 OOV？<a class=anchor href=#%e5%ad%90%e8%af%8d%e5%88%86%e8%af%8d%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3-oov>#</a></h4><p><strong>核心思想</strong>：<strong>任何词都可以拆解为已知的子词组合</strong>。</p><p><strong>示例对比</strong>：</p><table><thead><tr><th style=text-align:left>输入</th><th style=text-align:left>词级分词</th><th style=text-align:left>BPE 子词分词</th></tr></thead><tbody><tr><td style=text-align:left>ChatGPT</td><td style=text-align:left><code>&lt;UNK></code></td><td style=text-align:left><code>['Chat', 'GPT']</code></td></tr><tr><td style=text-align:left>DeepSeek-V3</td><td style=text-align:left><code>&lt;UNK></code></td><td style=text-align:left><code>['Deep', 'Seek', '-', 'V', '3']</code></td></tr><tr><td style=text-align:left>unbelievable</td><td style=text-align:left><code>&lt;UNK></code> (如果未见过)</td><td style=text-align:left><code>['un', 'believ', 'able']</code></td></tr><tr><td style=text-align:left>超级计算机</td><td style=text-align:left><code>&lt;UNK></code> (中文更严重)</td><td style=text-align:left><code>['超级', '计算', '机']</code></td></tr></tbody></table><p><strong>优势</strong>：</p><ol><li><strong>零 OOV</strong>：理论上任何词都可以拆解到字符级别，不会出现 <code>&lt;UNK></code></li><li><strong>语义保留</strong>：子词仍然保留部分语义（如 &ldquo;Chat&rdquo; 和 &ldquo;GPT&rdquo; 都有意义）</li><li><strong>词表可控</strong>：不需要存储所有可能的词，只需要合理数量的子词</li></ol><p><strong>实际例子</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>encoding_for_model</span><span class=p>(</span><span class=s2>&#34;gpt-4&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 常见词：保持完整</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;apple&#34;</span><span class=p>))</span>  <span class=c1># [23182]  -&gt; 1 个 Token</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 新造词：拆解为子词</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;ChatGPT&#34;</span><span class=p>))</span>  <span class=c1># [14326, 38, 2898]  -&gt; 3 个 Tokens</span>
</span></span><span class=line><span class=cl><span class=c1># 对应：[&#39;Chat&#39;, &#39;G&#39;, &#39;PT&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 超长词：自动拆解</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;antidisestablishmentarianism&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [519, 85342, 34500, 479, 8997, 2191]  -&gt; 6 个 Tokens</span></span></span></code></pre></div><h4 id=子词粒度的权衡>子词粒度的权衡<a class=anchor href=#%e5%ad%90%e8%af%8d%e7%b2%92%e5%ba%a6%e7%9a%84%e6%9d%83%e8%a1%a1>#</a></h4><table><thead><tr><th style=text-align:left>粒度</th><th style=text-align:left>Token 数量</th><th style=text-align:left>语义完整性</th><th style=text-align:left>适用场景</th></tr></thead><tbody><tr><td style=text-align:left><strong>粗粒度</strong>（大子词）</td><td style=text-align:left>少（序列短）</td><td style=text-align:left>高（常用词完整）</td><td style=text-align:left>通用文本、对话</td></tr><tr><td style=text-align:left><strong>细粒度</strong>（小子词）</td><td style=text-align:left>多（序列长）</td><td style=text-align:left>低（频繁拆解）</td><td style=text-align:left>代码、专业术语</td></tr><tr><td style=text-align:left><strong>字符级</strong></td><td style=text-align:left>最多</td><td style=text-align:left>最低</td><td style=text-align:left>极端情况（密码、Base64）</td></tr></tbody></table><p><strong>现代模型的选择</strong>：</p><ul><li>GPT-4：50,000+ 词表，中等粒度</li><li>Llama 3：128,000+ 词表，较粗粒度（更高效）</li><li>DeepSeek-V3：100,000+ 词表，优化中文（中文粒度更粗）</li></ul><hr><h3 id=4-前沿分词技术趋势>4. 前沿分词技术趋势<a class=anchor href=#4-%e5%89%8d%e6%b2%bf%e5%88%86%e8%af%8d%e6%8a%80%e6%9c%af%e8%b6%8b%e5%8a%bf>#</a></h3><p>随着 DeepSeek-V3、Claude 3.5/4.5 等模型的发布，分词技术出现了一些明显的新趋势：</p><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>早期模型 (GPT-2/BERT)</th><th style=text-align:left>现代模型 (Llama 3 / DeepSeek-V3 / Qwen 2.5)</th><th style=text-align:left>优势</th></tr></thead><tbody><tr><td style=text-align:left><strong>词表大小</strong></td><td style=text-align:left>~30k - 50k</td><td style=text-align:left><strong>100k - 150k</strong></td><td style=text-align:left>更大的词表意味着更多词被作为完整 Token，<strong>压缩率更高</strong>，推理更省钱。</td></tr><tr><td style=text-align:left><strong>数字处理</strong></td><td style=text-align:left>经常拆碎 (1998 -> 1, 9, 9, 8)</td><td style=text-align:left><strong>独立数字 Token</strong></td><td style=text-align:left>增强数学和代码处理能力。</td></tr><tr><td style=text-align:left><strong>空格处理</strong></td><td style=text-align:left>包含在词前 (Ġlove)</td><td style=text-align:left>更灵活的处理</td><td style=text-align:left>提高代码缩进的准确性。</td></tr><tr><td style=text-align:left><strong>多语言</strong></td><td style=text-align:left>英语为主</td><td style=text-align:left><strong>原生多语言</strong></td><td style=text-align:left>中文不再是"生僻词"，<code>"你好"</code> 是 1 个 Token 而非 2-3 个。</td></tr></tbody></table><blockquote class=book-hint><p><strong>⚠️ 为什么 GPT-4 数不清 &ldquo;Strawberry&rdquo; 里的 &lsquo;r&rsquo;？</strong>
这是分词带来的典型副作用。在 GPT-4 眼中，<code>Strawberry</code> 是一个完整的 Token（ID: 9699），它并不"看"到里面的字母。除非你强制它按字符处理（如：&ldquo;Spell the word strawberry&rdquo;）。</p></blockquote><hr><h2 id=二嵌入赋予词语数学灵魂>二、嵌入：赋予词语数学灵魂<a class=anchor href=#%e4%ba%8c%e5%b5%8c%e5%85%a5%e8%b5%8b%e4%ba%88%e8%af%8d%e8%af%ad%e6%95%b0%e5%ad%a6%e7%81%b5%e9%ad%82>#</a></h2><p>分词将文本变成了 ID 列表（如 <code>[12, 334, 98]</code>），但这只是索引。<strong>嵌入 (Embedding)</strong> 才是将这些索引转化为具有数学意义的向量的关键步骤。</p><h3 id=1-从-token-id-到高维向量>1. 从 Token ID 到高维向量<a class=anchor href=#1-%e4%bb%8e-token-id-%e5%88%b0%e9%ab%98%e7%bb%b4%e5%90%91%e9%87%8f>#</a></h3><h4 id=阶段-1token-id离散表示>阶段 1：Token ID（离散表示）<a class=anchor href=#%e9%98%b6%e6%ae%b5-1token-id%e7%a6%bb%e6%95%a3%e8%a1%a8%e7%a4%ba>#</a></h4><p>分词后，每个 Token 被赋予一个唯一的整数 ID：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;I love AI&#34;</span> <span class=err>→</span> <span class=p>[</span><span class=mi>40</span><span class=p>,</span> <span class=mi>3021</span><span class=p>,</span> <span class=mi>15592</span><span class=p>]</span></span></span></code></pre></div><p>这些数字本身没有任何数学意义，<code>40</code> 和 <code>41</code> 之间的"距离"不代表语义的相近。</p><h4 id=阶段-2one-hot-编码稀疏向量>阶段 2：One-Hot 编码（稀疏向量）<a class=anchor href=#%e9%98%b6%e6%ae%b5-2one-hot-%e7%bc%96%e7%a0%81%e7%a8%80%e7%96%8f%e5%90%91%e9%87%8f>#</a></h4><p><strong>理论上的中间步骤</strong>（实际工程中会跳过）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 假设词表大小 V = 50000</span>
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=s2>&#34;love&#34;</span> <span class=p>(</span><span class=n>ID</span><span class=o>=</span><span class=mi>3021</span><span class=p>)</span> <span class=err>→</span> <span class=n>One</span><span class=o>-</span><span class=n>Hot</span> <span class=n>向量</span><span class=err>：</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>  <span class=c1># 只有第 3021 位是 1，其余都是 0</span>
</span></span><span class=line><span class=cl>     <span class=err>↑</span>
</span></span><span class=line><span class=cl>  <span class=n>第</span> <span class=mi>3021</span> <span class=n>位</span></span></span></code></pre></div><p><strong>问题</strong>：</p><ul><li><strong>极度稀疏</strong>：50000 维的向量中只有 1 个非零值</li><li><strong>没有语义信息</strong>：&ldquo;love&rdquo; 和 &ldquo;hate&rdquo; 的向量完全正交，距离相等</li><li><strong>存储和计算浪费</strong>：矩阵乘法效率极低</li></ul><h4 id=阶段-3dense-embedding稠密向量>阶段 3：Dense Embedding（稠密向量）<a class=anchor href=#%e9%98%b6%e6%ae%b5-3dense-embedding%e7%a8%a0%e5%af%86%e5%90%91%e9%87%8f>#</a></h4><p><strong>定义</strong>：嵌入是一个将离散的 Token ID 映射到连续低维向量空间的<strong>查找表 (Lookup Table)</strong>。</p><p>$$
\text{Embedding}(x) = W_E[x]
$$</p><p>其中 $W_E \in \mathbb{R}^{V \times d}$ 是一个可学习的矩阵：</p><ul><li>$V$：词表大小（如 150,000）</li><li>$d$：嵌入维度（如 4096，远小于 V）</li></ul><p><strong>具体过程</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>150000</span>
</span></span><span class=line><span class=cl><span class=n>embed_dim</span> <span class=o>=</span> <span class=mi>4096</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 初始化嵌入矩阵（训练前是随机值，训练后包含语义）</span>
</span></span><span class=line><span class=cl><span class=n>embedding_matrix</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查表操作（本质是矩阵索引）</span>
</span></span><span class=line><span class=cl><span class=n>token_id</span> <span class=o>=</span> <span class=mi>3021</span>  <span class=c1># &#34;love&#34;</span>
</span></span><span class=line><span class=cl><span class=n>embedding_vector</span> <span class=o>=</span> <span class=n>embedding_matrix</span><span class=p>[</span><span class=n>token_id</span><span class=p>]</span>  <span class=c1># 形状: [4096]</span></span></span></code></pre></div><p><strong>直觉</strong>：
想象一个 4096 维的语义空间。</p><ul><li>&ldquo;猫&rdquo; 的向量指向 [0.1, 0.9, -0.3, &mldr;, 0.5]</li><li>&ldquo;狗&rdquo; 的向量指向 [0.2, 0.8, -0.2, &mldr;, 0.4]（接近"猫"）</li><li>&ldquo;桌子&rdquo; 的向量指向 [-0.5, 0.1, 0.9, &mldr;, -0.8]（远离"猫"和"狗"）</li></ul><p>模型通过训练，自动调整这些向量的位置，使得语义相近的词在空间中距离更近。</p><p><strong>优势对比</strong>：</p><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>One-Hot</th><th style=text-align:left>Dense Embedding</th></tr></thead><tbody><tr><td style=text-align:left>维度</td><td style=text-align:left>= 词表大小 (50k-150k)</td><td style=text-align:left>可控 (通常 768-4096)</td></tr><tr><td style=text-align:left>稀疏性</td><td style=text-align:left>极度稀疏 (只有 1 个非零值)</td><td style=text-align:left>稠密 (所有维度都有值)</td></tr><tr><td style=text-align:left>语义信息</td><td style=text-align:left>无（所有词等距）</td><td style=text-align:left>有（相似词距离近）</td></tr><tr><td style=text-align:left>存储效率</td><td style=text-align:left>低</td><td style=text-align:left>高</td></tr></tbody></table><h3 id=2-嵌入空间的几何奥秘>2. 嵌入空间的几何奥秘<a class=anchor href=#2-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e5%87%a0%e4%bd%95%e5%a5%a5%e7%a7%98>#</a></h3><p>嵌入空间最迷人的特性是其<strong>线性关系</strong>。经典的例子：</p><p>$$
\vec{King} - \vec{Man} + \vec{Woman} \approx \vec{Queen}
$$</p><p>这意味着模型不仅记住了词的含义，还理解了词与词之间的<strong>关系</strong>（如性别、首都-国家、过去式-现在式）。</p><h3 id=3-静态嵌入-vs-上下文嵌入>3. 静态嵌入 vs 上下文嵌入<a class=anchor href=#3-%e9%9d%99%e6%80%81%e5%b5%8c%e5%85%a5-vs-%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b5%8c%e5%85%a5>#</a></h3><p>这是理解现代 LLM 的一个关键区别：</p><h4 id=静态嵌入-static-embedding>静态嵌入 (Static Embedding)<a class=anchor href=#%e9%9d%99%e6%80%81%e5%b5%8c%e5%85%a5-static-embedding>#</a></h4><p><strong>代表技术</strong>：Word2Vec, GloVe（2013-2014 年的技术）</p><p><strong>特点</strong>：每个词有且只有<strong>一个固定的向量</strong>。</p><p><strong>问题</strong>：多义词无法区分</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;Apple flavor is sweet.&#34;</span>        <span class=c1># 苹果（水果）</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;Apple Inc. stock rises.&#34;</span>       <span class=c1># 苹果（公司）</span></span></span></code></pre></div><p>在静态嵌入中，两个 &ldquo;Apple&rdquo; 的向量完全相同，无法表达不同的语义。</p><h4 id=上下文嵌入-contextual-embedding>上下文嵌入 (Contextual Embedding)<a class=anchor href=#%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b5%8c%e5%85%a5-contextual-embedding>#</a></h4><p><strong>代表技术</strong>：Transformer 模型（BERT, GPT 等）</p><p><strong>特点</strong>：同一个词在不同上下文中会有<strong>不同的向量表示</strong>。</p><p><strong>工作流程</strong>（简化）：</p><ol><li><strong>输入层</strong>：<code>Apple</code> 先查表得到初始嵌入（此时还是静态的）</li><li><strong>Transformer 层</strong>：模型读取上下文，动态调整向量</li><li><strong>输出层</strong>：<code>Apple</code> 在 &ldquo;Apple flavor&rdquo; 和 &ldquo;Apple Inc.&rdquo; 中的最终向量已经完全不同</li></ol><p><strong>直觉理解</strong>：</p><ul><li>静态嵌入像是"字典定义"：一个词一个解释。</li><li>上下文嵌入像是"根据场景理解"：同一个词在不同句子里意思不同。</li></ul><blockquote class=book-hint><p><strong>💡 本章重点</strong>：我们主要关注<strong>输入层的静态 Embedding</strong>（即查找表）。至于 Transformer 如何利用注意力机制生成上下文嵌入，详见 [Part 2 第1章]。</p></blockquote><hr><h2 id=三代码实战从零构建与使用>三、代码实战：从零构建与使用<a class=anchor href=#%e4%b8%89%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e4%b8%8e%e4%bd%bf%e7%94%a8>#</a></h2><h3 id=1-实战从零训练-bpe-分词器>1. 实战：从零训练 BPE 分词器<a class=anchor href=#1-%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e8%ae%ad%e7%bb%83-bpe-%e5%88%86%e8%af%8d%e5%99%a8>#</a></h3><p>在使用现成的分词器之前，让我们先理解如何从零训练一个 BPE 分词器。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>功能：从零实现 BPE 分词器训练（教学版）
</span></span></span><span class=line><span class=cl><span class=s2>依赖：pip install tokenizers
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers</span> <span class=kn>import</span> <span class=n>Tokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers.models</span> <span class=kn>import</span> <span class=n>BPE</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers.trainers</span> <span class=kn>import</span> <span class=n>BpeTrainer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers.pre_tokenizers</span> <span class=kn>import</span> <span class=n>Whitespace</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 准备训练语料（实际应用中应该是大规模文本文件）</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;This is the Hugging Face Course.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;This chapter is about tokenization.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;This section shows several tokenizer algorithms.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Hopefully, you will be able to understand how they are trained and generate tokens.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 保存到文件（BPE 训练器需要文件路径）</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;corpus.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;w&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>line</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>line</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 初始化 BPE 分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>BPE</span><span class=p>(</span><span class=n>unk_token</span><span class=o>=</span><span class=s2>&#34;&lt;UNK&gt;&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 设置预分词器（按空格切分）</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>pre_tokenizer</span> <span class=o>=</span> <span class=n>Whitespace</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 配置训练器</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>BpeTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>vocab_size</span><span class=o>=</span><span class=mi>300</span><span class=p>,</span>  <span class=c1># 目标词表大小</span>
</span></span><span class=line><span class=cl>    <span class=n>special_tokens</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;&lt;PAD&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;UNK&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;BOS&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;EOS&gt;&#34;</span><span class=p>],</span>  <span class=c1># 特殊 Token</span>
</span></span><span class=line><span class=cl>    <span class=n>min_frequency</span><span class=o>=</span><span class=mi>2</span>  <span class=c1># 只保留出现至少 2 次的子词</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. 训练分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>files</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;corpus.txt&#34;</span><span class=p>],</span> <span class=n>trainer</span><span class=o>=</span><span class=n>trainer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 6. 测试分词效果</span>
</span></span><span class=line><span class=cl><span class=n>test_sentence</span> <span class=o>=</span> <span class=s2>&#34;This tokenization algorithm is awesome!&#34;</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>test_sentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;原始文本: </span><span class=si>{</span><span class=n>test_sentence</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token IDs: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>ids</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Tokens: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>tokens</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token 数量: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 7. 保存分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;my_bpe_tokenizer.json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>分词器已保存到 my_bpe_tokenizer.json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 8. 加载并使用</span>
</span></span><span class=line><span class=cl><span class=n>loaded_tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=o>.</span><span class=n>from_file</span><span class=p>(</span><span class=s2>&#34;my_bpe_tokenizer.json&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>重新加载后的分词结果:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>loaded_tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>test_sentence</span><span class=p>)</span><span class=o>.</span><span class=n>tokens</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>原始文本: This tokenization algorithm is awesome!
</span></span><span class=line><span class=cl>Token IDs: [51, 98, 156, 203, 45, 187, 12]
</span></span><span class=line><span class=cl>Tokens: [&#39;This&#39;, &#39;token&#39;, &#39;ization&#39;, &#39;algorithm&#39;, &#39;is&#39;, &#39;awesome&#39;, &#39;!&#39;]
</span></span><span class=line><span class=cl>Token 数量: 7
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>分词器已保存到 my_bpe_tokenizer.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>重新加载后的分词结果:
</span></span><span class=line><span class=cl>[&#39;This&#39;, &#39;token&#39;, &#39;ization&#39;, &#39;algorithm&#39;, &#39;is&#39;, &#39;awesome&#39;, &#39;!&#39;]</span></span></code></pre></div><blockquote class=book-hint><p><strong>💡 观察</strong>：训练后的分词器学会了将 &ldquo;tokenization&rdquo; 拆分为 &ldquo;token&rdquo; + &ldquo;ization&rdquo;，这是 BPE 从语料中学到的高频子词组合。</p></blockquote><h3 id=2-实战使用-tiktoken-高效分词>2. 实战：使用 TikToken 高效分词<a class=anchor href=#2-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-tiktoken-%e9%ab%98%e6%95%88%e5%88%86%e8%af%8d>#</a></h3><p><code>tiktoken</code> 是 OpenAI 开源的高性能 BPE 分词库，比 HuggingFace 的 tokenizer 快 3-6 倍。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>功能：演示使用 tiktoken 进行分词、ID转换及 Token 计费估算
</span></span></span><span class=line><span class=cl><span class=s2>依赖：pip install tiktoken
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>analyze_tokens</span><span class=p>(</span><span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;gpt-4o&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;--- Model: </span><span class=si>{</span><span class=n>model</span><span class=si>}</span><span class=s2> ---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 获取对应的编码器</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>encoding</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>encoding_for_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>KeyError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;这 Warning: Model </span><span class=si>{</span><span class=n>model</span><span class=si>}</span><span class=s2> using default cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>encoding</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 编码 (Text -&gt; Token IDs)</span>
</span></span><span class=line><span class=cl>    <span class=n>token_ids</span> <span class=o>=</span> <span class=n>encoding</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Text: </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Token IDs: </span><span class=si>{</span><span class=n>token_ids</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Count: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 解码 (Token IDs -&gt; Text)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 这一步可以让你看到每个 Token 到底对应什么文本</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Token Breakdown:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>tid</span> <span class=ow>in</span> <span class=n>token_ids</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>token_bytes</span> <span class=o>=</span> <span class=n>encoding</span><span class=o>.</span><span class=n>decode_single_token_bytes</span><span class=p>(</span><span class=n>tid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=n>tid</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>token_bytes</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  </span><span class=si>{</span><span class=n>tid</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>token_bytes</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试案例</span>
</span></span><span class=line><span class=cl><span class=n>text_sample</span> <span class=o>=</span> <span class=s2>&#34;Hello, world! 我爱大模型 2025.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>analyze_tokens</span><span class=p>(</span><span class=n>text_sample</span><span class=p>,</span> <span class=s2>&#34;gpt-4o&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=2-实战可视化嵌入空间>2. 实战：可视化嵌入空间<a class=anchor href=#2-%e5%ae%9e%e6%88%98%e5%8f%af%e8%a7%86%e5%8c%96%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4>#</a></h3><p>让我们看看这些高维向量在二维平面上是什么样子的。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>功能：使用 t-SNE/UMAP 可视化词嵌入
</span></span></span><span class=line><span class=cl><span class=s2>依赖：pip install numpy matplotlib scikit-learn openai
</span></span></span><span class=line><span class=cl><span class=s2>策略：使用 OpenAI 的 Embedding API 获取向量（也可以用本地模型）
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.manifold</span> <span class=kn>import</span> <span class=n>TSNE</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 准备测试词汇（选择有语义关系的词）</span>
</span></span><span class=line><span class=cl><span class=n>words</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;king&#34;</span><span class=p>,</span> <span class=s2>&#34;queen&#34;</span><span class=p>,</span> <span class=s2>&#34;prince&#34;</span><span class=p>,</span> <span class=s2>&#34;princess&#34;</span><span class=p>,</span>  <span class=c1># 王室</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;man&#34;</span><span class=p>,</span> <span class=s2>&#34;woman&#34;</span><span class=p>,</span> <span class=s2>&#34;boy&#34;</span><span class=p>,</span> <span class=s2>&#34;girl&#34;</span><span class=p>,</span>          <span class=c1># 性别</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;cat&#34;</span><span class=p>,</span> <span class=s2>&#34;dog&#34;</span><span class=p>,</span> <span class=s2>&#34;puppy&#34;</span><span class=p>,</span> <span class=s2>&#34;kitten&#34;</span><span class=p>,</span>        <span class=c1># 动物</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;car&#34;</span><span class=p>,</span> <span class=s2>&#34;bus&#34;</span><span class=p>,</span> <span class=s2>&#34;train&#34;</span><span class=p>,</span> <span class=s2>&#34;plane&#34;</span>          <span class=c1># 交通工具</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 获取嵌入向量（使用 OpenAI API）</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>  <span class=c1># 需要设置 OPENAI_API_KEY 环境变量</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_embeddings</span><span class=p>(</span><span class=n>texts</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;批量获取文本的嵌入向量&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>embeddings</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=nb>input</span><span class=o>=</span><span class=n>texts</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=n>model</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>data</span><span class=o>.</span><span class=n>embedding</span> <span class=k>for</span> <span class=n>data</span> <span class=ow>in</span> <span class=n>response</span><span class=o>.</span><span class=n>data</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>get_embeddings</span><span class=p>(</span><span class=n>words</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Embedding shape: </span><span class=si>{</span><span class=n>embeddings</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># 应该是 (16, 1536)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 使用 t-SNE 降维到 2D</span>
</span></span><span class=line><span class=cl><span class=n>tsne</span> <span class=o>=</span> <span class=n>TSNE</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>perplexity</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vis_data</span> <span class=o>=</span> <span class=n>tsne</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 可视化</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>colors</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;red&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;blue&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;green&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;purple&#39;</span><span class=p>]</span><span class=o>*</span><span class=mi>4</span>  <span class=c1># 按类别着色</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>vis_data</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>vis_data</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>colors</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>word</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>words</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>annotate</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>xy</span><span class=o>=</span><span class=p>(</span><span class=n>vis_data</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>vis_data</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>1</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>                 <span class=n>xytext</span><span class=o>=</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>textcoords</span><span class=o>=</span><span class=s1>&#39;offset points&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;right&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;bottom&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Word Embeddings Visualization (t-SNE)&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Dimension 1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Dimension 2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># plt.savefig(&#34;embedding_visualization.png&#34;, dpi=150)</span>
</span></span><span class=line><span class=cl><span class=c1># plt.show()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;可视化完成！观察：同类词应该聚集在一起。&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>如果你想使用本地模型</strong>（不依赖 API）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>使用 Hugging Face 的轻量级嵌入模型（无需 BERT 架构知识）
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载预训练的嵌入模型（这是一个封装好的工具，不需要理解内部）</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;all-MiniLM-L6-v2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>words</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;king&#34;</span><span class=p>,</span> <span class=s2>&#34;queen&#34;</span><span class=p>,</span> <span class=s2>&#34;man&#34;</span><span class=p>,</span> <span class=s2>&#34;woman&#34;</span><span class=p>,</span> <span class=s2>&#34;cat&#34;</span><span class=p>,</span> <span class=s2>&#34;dog&#34;</span><span class=p>,</span> <span class=s2>&#34;car&#34;</span><span class=p>,</span> <span class=s2>&#34;bus&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 一行代码获取嵌入</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>words</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Shape: </span><span class=si>{</span><span class=n>embeddings</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># (8, 384)</span>
</span></span><span class=line><span class=cl><span class=c1># 后续的降维和可视化代码与上面相同</span></span></span></code></pre></div><p><strong>使用 UMAP（通常比 t-SNE 更快更好）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># pip install umap-learn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>umap</span> <span class=kn>import</span> <span class=n>UMAP</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>reducer</span> <span class=o>=</span> <span class=n>UMAP</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vis_data</span> <span class=o>=</span> <span class=n>reducer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 绘图代码同上</span></span></span></code></pre></div><h3 id=3-实战语义相似度计算与搜索>3. 实战：语义相似度计算与搜索<a class=anchor href=#3-%e5%ae%9e%e6%88%98%e8%af%ad%e4%b9%89%e7%9b%b8%e4%bc%bc%e5%ba%a6%e8%ae%a1%e7%ae%97%e4%b8%8e%e6%90%9c%e7%b4%a2>#</a></h3><p>这是 Embedding 最经典的工业级应用，也是 <strong>RAG (检索增强生成)</strong> 的核心基础。</p><h4 id=方法-1使用-openai-embeddings-api最简单>方法 1：使用 OpenAI Embeddings API（最简单）<a class=anchor href=#%e6%96%b9%e6%b3%95-1%e4%bd%bf%e7%94%a8-openai-embeddings-api%e6%9c%80%e7%ae%80%e5%8d%95>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>功能：基于余弦相似度的语义搜索
</span></span></span><span class=line><span class=cl><span class=s2>依赖：pip install openai numpy
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cosine_similarity</span><span class=p>(</span><span class=n>vec1</span><span class=p>,</span> <span class=n>vec2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;计算两个向量的余弦相似度&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>dot_product</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>vec1</span><span class=p>,</span> <span class=n>vec2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>norm1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>vec1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>norm2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>vec2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dot_product</span> <span class=o>/</span> <span class=p>(</span><span class=n>norm1</span> <span class=o>*</span> <span class=n>norm2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 准备数据</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is eating food.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is eating a piece of bread.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The girl is carrying a baby.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is riding a horse.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A woman is playing violin.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;A man is eating pasta.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 获取嵌入向量</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_embedding</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>embeddings</span><span class=o>.</span><span class=n>create</span><span class=p>(</span><span class=nb>input</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span> <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>embedding</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>corpus_embeddings</span> <span class=o>=</span> <span class=p>[</span><span class=n>get_embedding</span><span class=p>(</span><span class=n>text</span><span class=p>)</span> <span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>corpus</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=n>get_embedding</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 计算相似度并排序</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>doc_emb</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>corpus_embeddings</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>query_embedding</span><span class=p>,</span> <span class=n>doc_emb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>score</span><span class=p>,</span> <span class=n>corpus</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>results</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 输出结果</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Query: </span><span class=si>{</span><span class=n>query</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>score</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>results</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Score: </span><span class=si>{</span><span class=n>score</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Query: A man is eating pasta.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Score: 0.8823 | A man is eating a piece of bread.
</span></span><span class=line><span class=cl>Score: 0.8654 | A man is eating food.
</span></span><span class=line><span class=cl>Score: 0.5421 | A man is riding a horse.
</span></span><span class=line><span class=cl>Score: 0.4102 | The girl is carrying a baby.
</span></span><span class=line><span class=cl>Score: 0.3876 | A woman is playing violin.</span></span></code></pre></div><blockquote class=book-hint><p><strong>💡 观察</strong>：模型成功识别出"eating pasta"与"eating bread/food"语义最接近，即使用词不完全相同！</p></blockquote><h4 id=方法-2使用本地模型适合批量处理>方法 2：使用本地模型（适合批量处理）<a class=anchor href=#%e6%96%b9%e6%b3%95-2%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e6%a8%a1%e5%9e%8b%e9%80%82%e5%90%88%e6%89%b9%e9%87%8f%e5%a4%84%e7%90%86>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>使用 sentence-transformers 库（无需了解内部架构）
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span><span class=p>,</span> <span class=n>util</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载模型（这个库封装了所有复杂度）</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;all-MiniLM-L6-v2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is eating food.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is eating a piece of bread.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The girl is carrying a baby.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A man is riding a horse.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;A woman is playing violin.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;A man is eating pasta.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 编码（一行搞定）</span>
</span></span><span class=line><span class=cl><span class=n>corpus_embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>convert_to_tensor</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>convert_to_tensor</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 计算相似度（内置函数）</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>util</span><span class=o>.</span><span class=n>cos_sim</span><span class=p>(</span><span class=n>query_embedding</span><span class=p>,</span> <span class=n>corpus_embeddings</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 排序并输出</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[(</span><span class=n>scores</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>corpus</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>corpus</span><span class=p>))]</span>
</span></span><span class=line><span class=cl><span class=n>results</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Query: </span><span class=si>{</span><span class=n>query</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>score</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>results</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Score: </span><span class=si>{</span><span class=n>score</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>text</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><h4 id=核心知识点余弦相似度>核心知识点：余弦相似度<a class=anchor href=#%e6%a0%b8%e5%bf%83%e7%9f%a5%e8%af%86%e7%82%b9%e4%bd%99%e5%bc%a6%e7%9b%b8%e4%bc%bc%e5%ba%a6>#</a></h4><p>余弦相似度衡量两个向量的方向是否一致（而非距离）：</p><p>$$
\text{cosine_similarity}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \times ||\vec{B}||} = \cos(\theta)
$$</p><ul><li><strong>取值范围</strong>：[-1, 1]<ul><li>1：方向完全相同（语义完全一致）</li><li>0：正交（无关）</li><li>-1：方向相反（语义相反，如反义词）</li></ul></li></ul><p><strong>为什么不用欧氏距离？</strong></p><ul><li>欧氏距离受向量长度影响，而余弦相似度只关心方向。</li><li>在高维空间中，余弦相似度更稳定。</li></ul><blockquote class=book-hint><p><strong>🔗 延伸阅读</strong>：关于如何将这个简单的搜索引擎扩展到百万级文档，详见 [Part 4 第2章：RAG 系统设计]。</p></blockquote><hr><h2 id=四工程最佳实践>四、工程最佳实践<a class=anchor href=#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>#</a></h2><h3 id=1-如何选择分词器>1. 如何选择分词器？<a class=anchor href=#1-%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e5%88%86%e8%af%8d%e5%99%a8>#</a></h3><ul><li><strong>通用文本</strong>：直接使用模型对应的分词器（<code>AutoTokenizer.from_pretrained(...)</code>）。</li><li><strong>代码场景</strong>：确保分词器对空格和缩进敏感（现代 LLM 分词器通常已优化）。</li><li><strong>中文场景</strong>：优先选择针对中文优化的模型（如 Qwen, DeepSeek, Yi），它们的中文压缩率远高于 Llama 原生版。</li></ul><blockquote class=book-hint><p><strong>💡 成本提示</strong>：在 API 计费时，DeepSeek/Qwen 的 Tokenizer 通常比 OpenAI 的 tokenizer 在中文上节省 30%-50% 的 Token 用量。</p></blockquote><h3 id=2-如何处理超长文本>2. 如何处理"超长文本"？<a class=anchor href=#2-%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e8%b6%85%e9%95%bf%e6%96%87%e6%9c%ac>#</a></h3><p>当文本超过 <code>max_tokens</code>（如 8192 或 128k）时：</p><ul><li><strong>截断 (Truncation)</strong>：丢弃多余部分（通常是不重要的开头或结尾）。</li><li><strong>滑动窗口 (Sliding Window)</strong>：将长文本切分成重叠的块，分别处理。</li><li><strong>RAG</strong>：先将长文本分块存入向量库，只检索相关的部分（详见 Part 4 第2章）。</li></ul><h3 id=3-嵌入的选择>3. 嵌入的选择<a class=anchor href=#3-%e5%b5%8c%e5%85%a5%e7%9a%84%e9%80%89%e6%8b%a9>#</a></h3><ul><li><strong>OpenAI text-embedding-3-small</strong>: 性价比极高，适合大多数通用场景。</li><li><strong>GTE-Qwen / BGE-M3</strong>: 当前中文和多语言检索效果最好的开源嵌入模型之一。</li><li><strong>Matryoshka Embeddings (俄罗斯套娃嵌入)</strong>: 新技术，允许你灵活截断嵌入向量的前 $k$ 维使用（例如只用前 256 维），在精度损失极小的情况下大幅减少存储空间。</li></ul><hr><h2 id=五本章小结>五、本章小结<a class=anchor href=#%e4%ba%94%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=核心要点回顾>核心要点回顾<a class=anchor href=#%e6%a0%b8%e5%bf%83%e8%a6%81%e7%82%b9%e5%9b%9e%e9%a1%be>#</a></h3><ol><li><p><strong>分词是基础</strong>：</p><ul><li>LLM 不读字，读 Token</li><li>现代 LLM 普遍使用 BPE/SentencePiece 算法</li><li>词表大小趋向于 100k-150k 以提升多语言效率</li><li>子词切分完美解决了 OOV 问题</li></ul></li><li><p><strong>嵌入是桥梁</strong>：</p><ul><li>Token ID → One-Hot → Dense Embedding 的三阶段转换</li><li>Embedding 层本质是一个可学习的查找表</li><li>将离散符号映射到连续的高维语义空间</li></ul></li><li><p><strong>几何即语义</strong>：</p><ul><li>向量空间中的距离代表语义相似度</li><li>余弦相似度是衡量语义距离的核心工具</li><li>这是 RAG 和语义搜索的数学基础</li></ul></li><li><p><strong>静态到动态</strong>：</p><ul><li>本章关注的是输入层的静态 Embedding（查找表）</li><li>Transformer 将静态嵌入转化为上下文相关的动态表示</li><li>这部分的原理将在下一部分详细展开</li></ul></li></ol><h3 id=实战技能清单>实战技能清单<a class=anchor href=#%e5%ae%9e%e6%88%98%e6%8a%80%e8%83%bd%e6%b8%85%e5%8d%95>#</a></h3><p>✅ 能够使用 <code>tiktoken</code> 进行高效分词和 Token 计数
✅ 理解 BPE/WordPiece/SentencePiece 三种算法的区别
✅ 能够获取和可视化嵌入向量（t-SNE/UMAP）
✅ 能够实现基于余弦相似度的语义搜索
✅ 理解 One-Hot 到 Dense Embedding 的转换过程</p><h3 id=延伸阅读>延伸阅读<a class=anchor href=#%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb>#</a></h3><ul><li><strong>Transformer 如何处理嵌入？</strong> → 详见 [Part 2 第1章：Transformer 核心揭秘]</li><li><strong>如何训练更好的嵌入模型？</strong> → 详见 [Part 3 第4章：创建更优的嵌入模型]</li><li><strong>如何构建生产级 RAG 系统？</strong> → 详见 [Part 4 第2章：检索增强生成]</li></ul><hr><p><strong>下一站</strong>：[Part 2 第1章：Transformer 核心揭秘]</p><p>我们即将进入 LLM 的心脏区域，看看这些嵌入向量是如何在 Self-Attention 的编织中涌现出智能的。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第2章 与模型对话：提示工程基础</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/ class="flex align-center"><span>第1章 Transformer核心揭秘</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一分词机器阅读的第一步>一、分词：机器阅读的第一步</a><ul><li><a href=#1-为什么要分词>1. 为什么要分词？</a><ul><li><a href=#-方案-a按字符切分-character-level>❌ 方案 A：按字符切分 (Character-level)</a></li><li><a href=#-方案-b按单词切分-word-level>❌ 方案 B：按单词切分 (Word-level)</a></li><li><a href=#-方案-c子词切分-subword-level--现代-llm-的选择>✅ 方案 C：子词切分 (Subword-level) —— 现代 LLM 的选择</a></li></ul></li><li><a href=#2-主流分词算法详解>2. 主流分词算法详解</a><ul><li><a href=#1-bpe-byte-pair-encoding>(1) BPE (Byte Pair Encoding)</a></li><li><a href=#2-wordpiece>(2) WordPiece</a></li><li><a href=#3-sentencepiece-unigram-算法>(3) SentencePiece (Unigram 算法)</a></li></ul></li><li><a href=#3-oov-问题与子词的优势>3. OOV 问题与子词的优势</a><ul><li><a href=#什么是-oov-out-of-vocabulary-问题>什么是 OOV (Out-of-Vocabulary) 问题？</a></li><li><a href=#子词分词如何解决-oov>子词分词如何解决 OOV？</a></li><li><a href=#子词粒度的权衡>子词粒度的权衡</a></li></ul></li><li><a href=#4-前沿分词技术趋势>4. 前沿分词技术趋势</a></li></ul></li><li><a href=#二嵌入赋予词语数学灵魂>二、嵌入：赋予词语数学灵魂</a><ul><li><a href=#1-从-token-id-到高维向量>1. 从 Token ID 到高维向量</a><ul><li><a href=#阶段-1token-id离散表示>阶段 1：Token ID（离散表示）</a></li><li><a href=#阶段-2one-hot-编码稀疏向量>阶段 2：One-Hot 编码（稀疏向量）</a></li><li><a href=#阶段-3dense-embedding稠密向量>阶段 3：Dense Embedding（稠密向量）</a></li></ul></li><li><a href=#2-嵌入空间的几何奥秘>2. 嵌入空间的几何奥秘</a></li><li><a href=#3-静态嵌入-vs-上下文嵌入>3. 静态嵌入 vs 上下文嵌入</a><ul><li><a href=#静态嵌入-static-embedding>静态嵌入 (Static Embedding)</a></li><li><a href=#上下文嵌入-contextual-embedding>上下文嵌入 (Contextual Embedding)</a></li></ul></li></ul></li><li><a href=#三代码实战从零构建与使用>三、代码实战：从零构建与使用</a><ul><li><a href=#1-实战从零训练-bpe-分词器>1. 实战：从零训练 BPE 分词器</a></li><li><a href=#2-实战使用-tiktoken-高效分词>2. 实战：使用 TikToken 高效分词</a></li><li><a href=#2-实战可视化嵌入空间>2. 实战：可视化嵌入空间</a></li><li><a href=#3-实战语义相似度计算与搜索>3. 实战：语义相似度计算与搜索</a><ul><li><a href=#方法-1使用-openai-embeddings-api最简单>方法 1：使用 OpenAI Embeddings API（最简单）</a></li><li><a href=#方法-2使用本地模型适合批量处理>方法 2：使用本地模型（适合批量处理）</a></li><li><a href=#核心知识点余弦相似度>核心知识点：余弦相似度</a></li></ul></li></ul></li><li><a href=#四工程最佳实践>四、工程最佳实践</a><ul><li><a href=#1-如何选择分词器>1. 如何选择分词器？</a></li><li><a href=#2-如何处理超长文本>2. 如何处理"超长文本"？</a></li><li><a href=#3-嵌入的选择>3. 嵌入的选择</a></li></ul></li><li><a href=#五本章小结>五、本章小结</a><ul><li><a href=#核心要点回顾>核心要点回顾</a></li><li><a href=#实战技能清单>实战技能清单</a></li><li><a href=#延伸阅读>延伸阅读</a></li></ul></li></ul></nav></div></aside></main></body></html>