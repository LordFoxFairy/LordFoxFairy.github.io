<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第1章：模型压缩与推理加速# 让大模型"瘦身"，从显存杀手变成生产力工具。
目录# 第一节：量化技术详解 1.1 量化技术概览 1.2 GPTQ 原理与实战 1.3 AWQ 原理与实战 1.4 量化实战对比 第二节：剪枝技术 2.1 结构化剪枝 vs 非结构化剪枝 2.2 实战：SparseGPT 第三节：知识蒸馏 3.1 核心思想 3.2 实战：蒸馏 BERT 第四节：显存估算与优化 4.1 显存占用计算公式 4.2 KV Cache 优化 本章小结 第一节：量化技术详解# 1.1 量化技术概览# 量化 (Quantization) 是将模型权重和激活值从高精度（如 FP16/BF16）转换为低精度（如 INT8, INT4）的过程。
核心收益：
显存占用剧减：INT4 模型显存仅为 FP16 的 1/4。 内存带宽压力减轻：这是 LLM 推理的主要瓶颈。 计算加速：整数运算比浮点运算更快（取决于硬件支持）。 主流方案对比 (SOTA)：
特性 GPTQ AWQ EXL2 (ExLlamaV2) bitsandbytes (BnB) 全称 GPT-Quantization Activation-aware Weight Quantization ExLlamaV2 Quantization - 核心原理 逐层量化，利用Hessian矩阵最小化误差 保护1%的关键"显著"权重通道 混合精度量化 (2-8 bit混合) 运行时动态量化 (LLM.int8()) 量化时间 慢 (需校准数据) 较快 (需校准数据) 慢 (极其精细的搜索) 无 (加载时量化) 推理速度 快 快 极快 (针对性CUDA优化) 较慢 (非计算密集型) 主要用途 早期主流，通用性好 边缘端、低比特高精度 生产环境高性能推理 训练微调 (QLoRA) 显存颗粒度 固定 (4-bit/8-bit) 固定 灵活 (如 4.65 bpw) 固定 1.2 GPTQ vs AWQ vs EXL2 深度解析# 1. GPTQ (Generative Pre-trained Transformer Quantization)# 早期最流行的 Post-Training Quantization (PTQ) 方法。
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 模型压缩与推理加速"><meta property="og:description" content='第1章：模型压缩与推理加速# 让大模型"瘦身"，从显存杀手变成生产力工具。
目录# 第一节：量化技术详解 1.1 量化技术概览 1.2 GPTQ 原理与实战 1.3 AWQ 原理与实战 1.4 量化实战对比 第二节：剪枝技术 2.1 结构化剪枝 vs 非结构化剪枝 2.2 实战：SparseGPT 第三节：知识蒸馏 3.1 核心思想 3.2 实战：蒸馏 BERT 第四节：显存估算与优化 4.1 显存占用计算公式 4.2 KV Cache 优化 本章小结 第一节：量化技术详解# 1.1 量化技术概览# 量化 (Quantization) 是将模型权重和激活值从高精度（如 FP16/BF16）转换为低精度（如 INT8, INT4）的过程。
核心收益：
显存占用剧减：INT4 模型显存仅为 FP16 的 1/4。 内存带宽压力减轻：这是 LLM 推理的主要瓶颈。 计算加速：整数运算比浮点运算更快（取决于硬件支持）。 主流方案对比 (SOTA)：
特性 GPTQ AWQ EXL2 (ExLlamaV2) bitsandbytes (BnB) 全称 GPT-Quantization Activation-aware Weight Quantization ExLlamaV2 Quantization - 核心原理 逐层量化，利用Hessian矩阵最小化误差 保护1%的关键"显著"权重通道 混合精度量化 (2-8 bit混合) 运行时动态量化 (LLM.int8()) 量化时间 慢 (需校准数据) 较快 (需校准数据) 慢 (极其精细的搜索) 无 (加载时量化) 推理速度 快 快 极快 (针对性CUDA优化) 较慢 (非计算密集型) 主要用途 早期主流，通用性好 边缘端、低比特高精度 生产环境高性能推理 训练微调 (QLoRA) 显存颗粒度 固定 (4-bit/8-bit) 固定 灵活 (如 4.65 bpw) 固定 1.2 GPTQ vs AWQ vs EXL2 深度解析# 1. GPTQ (Generative Pre-trained Transformer Quantization)# 早期最流行的 Post-Training Quantization (PTQ) 方法。'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 模型压缩与推理加速"><meta itemprop=description content='第1章：模型压缩与推理加速# 让大模型"瘦身"，从显存杀手变成生产力工具。
目录# 第一节：量化技术详解 1.1 量化技术概览 1.2 GPTQ 原理与实战 1.3 AWQ 原理与实战 1.4 量化实战对比 第二节：剪枝技术 2.1 结构化剪枝 vs 非结构化剪枝 2.2 实战：SparseGPT 第三节：知识蒸馏 3.1 核心思想 3.2 实战：蒸馏 BERT 第四节：显存估算与优化 4.1 显存占用计算公式 4.2 KV Cache 优化 本章小结 第一节：量化技术详解# 1.1 量化技术概览# 量化 (Quantization) 是将模型权重和激活值从高精度（如 FP16/BF16）转换为低精度（如 INT8, INT4）的过程。
核心收益：
显存占用剧减：INT4 模型显存仅为 FP16 的 1/4。 内存带宽压力减轻：这是 LLM 推理的主要瓶颈。 计算加速：整数运算比浮点运算更快（取决于硬件支持）。 主流方案对比 (SOTA)：
特性 GPTQ AWQ EXL2 (ExLlamaV2) bitsandbytes (BnB) 全称 GPT-Quantization Activation-aware Weight Quantization ExLlamaV2 Quantization - 核心原理 逐层量化，利用Hessian矩阵最小化误差 保护1%的关键"显著"权重通道 混合精度量化 (2-8 bit混合) 运行时动态量化 (LLM.int8()) 量化时间 慢 (需校准数据) 较快 (需校准数据) 慢 (极其精细的搜索) 无 (加载时量化) 推理速度 快 快 极快 (针对性CUDA优化) 较慢 (非计算密集型) 主要用途 早期主流，通用性好 边缘端、低比特高精度 生产环境高性能推理 训练微调 (QLoRA) 显存颗粒度 固定 (4-bit/8-bit) 固定 灵活 (如 4.65 bpw) 固定 1.2 GPTQ vs AWQ vs EXL2 深度解析# 1. GPTQ (Generative Pre-trained Transformer Quantization)# 早期最流行的 Post-Training Quantization (PTQ) 方法。'><meta itemprop=wordCount content="543"><title>第1章 模型压缩与推理加速 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle checked>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/ class=active>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 模型压缩与推理加速</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节量化技术详解>第一节：量化技术详解</a><ul><li><a href=#11-量化技术概览>1.1 量化技术概览</a></li><li><a href=#12-gptq-vs-awq-vs-exl2-深度解析>1.2 GPTQ vs AWQ vs EXL2 深度解析</a><ul><li><a href=#1-gptq-generative-pre-trained-transformer-quantization>1. GPTQ (Generative Pre-trained Transformer Quantization)</a></li><li><a href=#2-awq-activation-aware-weight-quantization>2. AWQ (Activation-aware Weight Quantization)</a></li><li><a href=#3-exl2-exllamav2>3. EXL2 (ExLlamaV2)</a></li></ul></li><li><a href=#13-实战使用-autogptq-和-autoawq>1.3 实战：使用 AutoGPTQ 和 AutoAWQ</a><ul><li><a href=#准备环境>准备环境</a></li><li><a href=#脚本-1-使用-autogptq-量化-llama-3-8b>脚本 1: 使用 AutoGPTQ 量化 LLaMA-3-8B</a></li><li><a href=#脚本-2-使用-autoawq-量化>脚本 2: 使用 AutoAWQ 量化</a></li></ul></li></ul></li><li><a href=#第二节kv-cache-量化-前沿趋势>第二节：KV Cache 量化 (前沿趋势)</a><ul><li><a href=#21-什么是-kv-cache为什么它是显存杀手>2.1 什么是 KV Cache？为什么它是显存杀手？</a></li><li><a href=#22-k-v-cache-quantization-fp8--int4>2.2 K-V Cache Quantization (FP8 / INT4)</a></li></ul></li><li><a href=#第三节剪枝与蒸馏-简介>第三节：剪枝与蒸馏 (简介)</a><ul><li><a href=#31-结构化剪枝-structured-pruning>3.1 结构化剪枝 (Structured Pruning)</a></li><li><a href=#32-知识蒸馏-knowledge-distillation>3.2 知识蒸馏 (Knowledge Distillation)</a></li></ul></li><li><a href=#第1章小结>第1章小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第1章模型压缩与推理加速>第1章：模型压缩与推理加速<a class=anchor href=#%e7%ac%ac1%e7%ab%a0%e6%a8%a1%e5%9e%8b%e5%8e%8b%e7%bc%a9%e4%b8%8e%e6%8e%a8%e7%90%86%e5%8a%a0%e9%80%9f>#</a></h1><blockquote class=book-hint><p>让大模型"瘦身"，从显存杀手变成生产力工具。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3>第一节：量化技术详解</a><ul><li><a href=#11-%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e6%a6%82%e8%a7%88>1.1 量化技术概览</a></li><li><a href=#12-gptq-%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ae%9e%e6%88%98>1.2 GPTQ 原理与实战</a></li><li><a href=#13-awq-%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ae%9e%e6%88%98>1.3 AWQ 原理与实战</a></li><li><a href=#14-%e9%87%8f%e5%8c%96%e5%ae%9e%e6%88%98%e5%af%b9%e6%af%94>1.4 量化实战对比</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e8%8a%82%e5%89%aa%e6%9e%9d%e6%8a%80%e6%9c%af>第二节：剪枝技术</a><ul><li><a href=#21-%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d-vs-%e9%9d%9e%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d>2.1 结构化剪枝 vs 非结构化剪枝</a></li><li><a href=#22-%e5%ae%9e%e6%88%98sparsegpt>2.2 实战：SparseGPT</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f>第三节：知识蒸馏</a><ul><li><a href=#31-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>3.1 核心思想</a></li><li><a href=#32-%e5%ae%9e%e6%88%98%e8%92%b8%e9%a6%8f-bert>3.2 实战：蒸馏 BERT</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e8%8a%82%e6%98%be%e5%ad%98%e4%bc%b0%e7%ae%97%e4%b8%8e%e4%bc%98%e5%8c%96>第四节：显存估算与优化</a><ul><li><a href=#41-%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e8%ae%a1%e7%ae%97%e5%85%ac%e5%bc%8f>4.1 显存占用计算公式</a></li><li><a href=#42-kv-cache-%e4%bc%98%e5%8c%96>4.2 KV Cache 优化</a></li></ul></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li></ul><hr><h2 id=第一节量化技术详解>第一节：量化技术详解<a class=anchor href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3>#</a></h2><h3 id=11-量化技术概览>1.1 量化技术概览<a class=anchor href=#11-%e9%87%8f%e5%8c%96%e6%8a%80%e6%9c%af%e6%a6%82%e8%a7%88>#</a></h3><p><strong>量化 (Quantization)</strong> 是将模型权重和激活值从高精度（如 FP16/BF16）转换为低精度（如 INT8, INT4）的过程。</p><p><strong>核心收益</strong>：</p><ol><li><strong>显存占用剧减</strong>：INT4 模型显存仅为 FP16 的 1/4。</li><li><strong>内存带宽压力减轻</strong>：这是 LLM 推理的主要瓶颈。</li><li><strong>计算加速</strong>：整数运算比浮点运算更快（取决于硬件支持）。</li></ol><p><strong>主流方案对比 (SOTA)</strong>：</p><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>GPTQ</th><th style=text-align:left>AWQ</th><th style=text-align:left>EXL2 (ExLlamaV2)</th><th style=text-align:left>bitsandbytes (BnB)</th></tr></thead><tbody><tr><td style=text-align:left><strong>全称</strong></td><td style=text-align:left>GPT-Quantization</td><td style=text-align:left>Activation-aware Weight Quantization</td><td style=text-align:left>ExLlamaV2 Quantization</td><td style=text-align:left>-</td></tr><tr><td style=text-align:left><strong>核心原理</strong></td><td style=text-align:left>逐层量化，利用Hessian矩阵最小化误差</td><td style=text-align:left>保护1%的关键"显著"权重通道</td><td style=text-align:left>混合精度量化 (2-8 bit混合)</td><td style=text-align:left>运行时动态量化 (LLM.int8())</td></tr><tr><td style=text-align:left><strong>量化时间</strong></td><td style=text-align:left>慢 (需校准数据)</td><td style=text-align:left>较快 (需校准数据)</td><td style=text-align:left>慢 (极其精细的搜索)</td><td style=text-align:left>无 (加载时量化)</td></tr><tr><td style=text-align:left><strong>推理速度</strong></td><td style=text-align:left>快</td><td style=text-align:left>快</td><td style=text-align:left><strong>极快</strong> (针对性CUDA优化)</td><td style=text-align:left>较慢 (非计算密集型)</td></tr><tr><td style=text-align:left><strong>主要用途</strong></td><td style=text-align:left>早期主流，通用性好</td><td style=text-align:left>边缘端、低比特高精度</td><td style=text-align:left><strong>生产环境高性能推理</strong></td><td style=text-align:left>训练微调 (QLoRA)</td></tr><tr><td style=text-align:left><strong>显存颗粒度</strong></td><td style=text-align:left>固定 (4-bit/8-bit)</td><td style=text-align:left>固定</td><td style=text-align:left><strong>灵活</strong> (如 4.65 bpw)</td><td style=text-align:left>固定</td></tr></tbody></table><hr><h3 id=12-gptq-vs-awq-vs-exl2-深度解析>1.2 GPTQ vs AWQ vs EXL2 深度解析<a class=anchor href=#12-gptq-vs-awq-vs-exl2-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90>#</a></h3><h4 id=1-gptq-generative-pre-trained-transformer-quantization>1. GPTQ (Generative Pre-trained Transformer Quantization)<a class=anchor href=#1-gptq-generative-pre-trained-transformer-quantization>#</a></h4><p>早期最流行的 Post-Training Quantization (PTQ) 方法。</p><ul><li><strong>原理</strong>：基于 OBS (Optimal Brain Surgeon) 理论，通过二阶信息（Hessian 逆矩阵）来补偿量化带来的误差。它不是简单地四舍五入，而是调整未量化的权重来弥补已量化权重造成的损失。</li><li><strong>关键点</strong>：逐列更新权重，误差最小化。</li></ul><h4 id=2-awq-activation-aware-weight-quantization>2. AWQ (Activation-aware Weight Quantization)<a class=anchor href=#2-awq-activation-aware-weight-quantization>#</a></h4><ul><li><strong>核心发现</strong>：权重的重要性并不是平等的。<strong>大约 1% 的权重通道对精度影响巨大</strong>，而这些通道通常对应较大的<strong>激活值</strong>（Activation）。</li><li><strong>原理</strong>：不直接保护权重，而是保护对应激活值较大的权重通道。通过缩放（Scaling）技巧，将量化误差从重要权重转移到非重要权重上。</li><li><strong>优势</strong>：这种“激活感知”比 GPTQ 的盲目数学优化在某些场景下有更好的泛化能力，且主要为了这种格式设计了极快的推理 Kernel。</li></ul><h4 id=3-exl2-exllamav2>3. EXL2 (ExLlamaV2)<a class=anchor href=#3-exl2-exllamav2>#</a></h4><p>这是目前<strong>单卡推理速度最快</strong>的量化格式。</p><ul><li><strong>原理</strong>：不仅仅是 4-bit。EXL2 允许<strong>混合精度</strong>，例如可以设定目标为 <code>4.65 bpw</code> (bits per weight)。它会根据每一层对整体误差的敏感度，给重要的层分配 5-bit 或 6-bit，给不重要的层分配 3-bit 或 2-bit。</li><li><strong>优势</strong>：<ul><li><strong>显存利用率极致</strong>：可以正好卡在 24GB 显存显卡中塞入 70B 模型的极度压缩版，或者 34B 模型的高精度版。</li><li><strong>推理速度</strong>：作者重写了全套 CUDA Kernel，速度极快。</li></ul></li></ul><hr><h3 id=13-实战使用-autogptq-和-autoawq>1.3 实战：使用 AutoGPTQ 和 AutoAWQ<a class=anchor href=#13-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-autogptq-%e5%92%8c-autoawq>#</a></h3><h4 id=准备环境>准备环境<a class=anchor href=#%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install auto-gptq autoawq optimum</span></span></code></pre></div><h4 id=脚本-1-使用-autogptq-量化-llama-3-8b>脚本 1: 使用 AutoGPTQ 量化 LLaMA-3-8B<a class=anchor href=#%e8%84%9a%e6%9c%ac-1-%e4%bd%bf%e7%94%a8-autogptq-%e9%87%8f%e5%8c%96-llama-3-8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>auto_gptq</span> <span class=kn>import</span> <span class=n>AutoGPTQForCausalLM</span><span class=p>,</span> <span class=n>BaseQuantizeConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 设定路径</span>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Meta-Llama-3-8B&#34;</span>
</span></span><span class=line><span class=cl><span class=n>quantized_model_dir</span> <span class=o>=</span> <span class=s2>&#34;Llama-3-8B-GPTQ-4bit&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 准备分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>use_fast</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 准备量化数据 (校准集)</span>
</span></span><span class=line><span class=cl><span class=c1># GPTQ 需要看少量真实数据来计算 Hessian 矩阵</span>
</span></span><span class=line><span class=cl><span class=n>examples</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;Automated quantization is amazing!&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;Large language models are the future.&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 在生产环境中，这里应该加载真实数据集的子集，如 wikiText2 或 C4</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 配置量化参数</span>
</span></span><span class=line><span class=cl><span class=n>quantize_config</span> <span class=o>=</span> <span class=n>BaseQuantizeConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>bits</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>             <span class=c1># 量化到 4-bit</span>
</span></span><span class=line><span class=cl>    <span class=n>group_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>     <span class=c1># 典型的 group size，越小精度越高但显存略增</span>
</span></span><span class=line><span class=cl>    <span class=n>desc_act</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>     <span class=c1># 是否根据激活值重排 (提高精度但可能影响推理速度)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. 加载模型并量化</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoGPTQForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>quantize_config</span><span class=o>=</span><span class=n>quantize_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 开始量化</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;开始量化...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>quantize</span><span class=p>(</span><span class=n>examples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 6. 保存</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>save_quantized</span><span class=p>(</span><span class=n>quantized_model_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>quantized_model_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;量化完成，已保存至 </span><span class=si>{</span><span class=n>quantized_model_dir</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><h4 id=脚本-2-使用-autoawq-量化>脚本 2: 使用 AutoAWQ 量化<a class=anchor href=#%e8%84%9a%e6%9c%ac-2-%e4%bd%bf%e7%94%a8-autoawq-%e9%87%8f%e5%8c%96>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>awq</span> <span class=kn>import</span> <span class=n>AutoAWQForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Meta-Llama-3-8B&#34;</span>
</span></span><span class=line><span class=cl><span class=n>quant_path</span> <span class=o>=</span> <span class=s2>&#34;Llama-3-8B-AWQ-4bit&#34;</span>
</span></span><span class=line><span class=cl><span class=n>quant_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;zero_point&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;q_group_size&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;w_bit&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;version&#34;</span><span class=p>:</span> <span class=s2>&#34;GEMM&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoAWQForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 量化</span>
</span></span><span class=line><span class=cl><span class=c1># AWQ 也需要校准集来寻找&#34;显著&#34;通道</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;开始 AWQ 量化...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>quantize</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>quant_config</span><span class=o>=</span><span class=n>quant_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>calib_data</span><span class=o>=</span><span class=s2>&#34;pileval&#34;</span> <span class=c1># AutoAWQ 内置支持一些数据集名称，也可以传 list</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 保存</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>save_quantized</span><span class=p>(</span><span class=n>quant_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>quant_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;AWQ 量化完成: </span><span class=si>{</span><span class=n>quant_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=第二节kv-cache-量化-前沿趋势>第二节：KV Cache 量化 (前沿趋势)<a class=anchor href=#%e7%ac%ac%e4%ba%8c%e8%8a%82kv-cache-%e9%87%8f%e5%8c%96-%e5%89%8d%e6%b2%bf%e8%b6%8b%e5%8a%bf>#</a></h2><h3 id=21-什么是-kv-cache为什么它是显存杀手>2.1 什么是 KV Cache？为什么它是显存杀手？<a class=anchor href=#21-%e4%bb%80%e4%b9%88%e6%98%af-kv-cache%e4%b8%ba%e4%bb%80%e4%b9%88%e5%ae%83%e6%98%af%e6%98%be%e5%ad%98%e6%9d%80%e6%89%8b>#</a></h3><p>在 LLM 生成过程中，每生成一个 Token，都需要之前所有 Token 的 Key 和 Value 矩阵参与计算。为了加速，我们把这些矩阵存在显存里，这就是 <strong>KV Cache</strong>。</p><ul><li><strong>问题</strong>：随着上下文长度（Context Length）增加，KV Cache 呈线性增长。</li><li><strong>计算公式</strong>：
$$ \text{Size} = 2 \times \text{Batch} \times \text{Layers} \times \text{Heads} \times \text{Head_Dim} \times \text{Seq_Len} \times \text{Precision} $$</li><li><strong>示例</strong>：对于 LLaMA-3-70B，FP16 精度，Batch=1，Sequence=128k：<ul><li>显存占用可能高达 <strong>数十GB</strong>，甚至超过模型权重本身！</li></ul></li></ul><h3 id=22-k-v-cache-quantization-fp8--int4>2.2 K-V Cache Quantization (FP8 / INT4)<a class=anchor href=#22-k-v-cache-quantization-fp8--int4>#</a></h3><p>为了解决长窗口（Long Context）推理的显存瓶颈，仅量化权重已经不够了，必须对 KV Cache 动刀。</p><p><strong>趋势</strong>：从 FP16 KV Cache -> <strong>FP8 KV Cache</strong> (甚至 INT4)。</p><p><strong>原理</strong>：</p><ul><li>Key 和 Value 矩阵的数值分布通常比权重更不规则（存在 Outliers）。</li><li>但是，通过合适的 scale factor，可以将它们压缩到 FP8 (e4m3 或 e5m2 格式)。</li><li><strong>vLLM</strong> 和 <strong>FlashAttention</strong> 已经原生支持 FP8 KV Cache。</li></ul><p><strong>收益</strong>：</p><ul><li><strong>显存减半</strong>：128k 上下文的显存需求直接减半。</li><li><strong>吞吐增加</strong>：同等显存下，可以支持 <code>Batch Size</code> 翻倍。</li></ul><p><strong>vLLM 中开启 KV Cache 量化</strong>：</p><p>只是启动时的简单参数：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 启动 vLLM server 时添加 --kv-cache-dtype</span>
</span></span><span class=line><span class=cl>python -m vllm.entrypoints.openai.api_server <span class=se>\
</span></span></span><span class=line><span class=cl>    --model meta-llama/Meta-Llama-3-8B-Instruct <span class=se>\
</span></span></span><span class=line><span class=cl>    --kv-cache-dtype fp8</span></span></code></pre></div><p>vLLM 会自动处理 FP8 的转换。对于一些新模型，这几乎是<strong>无损</strong>的。</p><hr><h2 id=第三节剪枝与蒸馏-简介>第三节：剪枝与蒸馏 (简介)<a class=anchor href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e5%89%aa%e6%9e%9d%e4%b8%8e%e8%92%b8%e9%a6%8f-%e7%ae%80%e4%bb%8b>#</a></h2><p>虽然量化是当前最主流的手段，但剪枝和蒸馏在端侧模型（On-Device LLM）中仍占有一席之地。</p><h3 id=31-结构化剪枝-structured-pruning>3.1 结构化剪枝 (Structured Pruning)<a class=anchor href=#31-%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d-structured-pruning>#</a></h3><ul><li><strong>原理</strong>：直接移除模型中的整行、整列、甚至整层。</li><li><strong>LLM-Pruner</strong>：通过分析梯度信息，通过移除对 Loss 影响最小的结构。</li><li><strong>稀疏度</strong>：通常 LLM 很难在剪枝超过 20%-30% 的情况下保持核心推理能力，这也是为什么目前量化（可以压到 25% 体积）比剪枝更受欢迎的原因。</li></ul><h3 id=32-知识蒸馏-knowledge-distillation>3.2 知识蒸馏 (Knowledge Distillation)<a class=anchor href=#32-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f-knowledge-distillation>#</a></h3><ul><li><strong>原理</strong>：Teacher 模型（如 GPT-4）指导 Student 模型（如 LLaMA-1B）。</li><li><strong>白盒蒸馏</strong>：Student 学习 Teacher 的 Logits 分布（不仅学答案，还学"为什么"）。</li><li><strong>黑盒蒸馏</strong>：Student 仅学习 Teacher 生成的文本数据（合成数据训练）。目前 DeepSeek-R1-Distill 系列就是这种模式，效果惊人。</li></ul><hr><h2 id=第1章小结>第1章小结<a class=anchor href=#%e7%ac%ac1%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><ol><li><strong>首选量化</strong>：对于生产环境，<strong>AWQ</strong> 和 <strong>GPTQ</strong> 是标准选择。如果有极致性能需求且有时间调试，<strong>EXL2</strong> 是最佳选择。</li><li><strong>KV Cache 也要压</strong>：长文本时代，<strong>FP8 KV Cache</strong> 是标配，能极大提升吞吐和最大上下文长度。</li><li><strong>工具链成熟</strong>：<code>AutoGPTQ</code> 和 <code>AutoAWQ</code> 让量化变得像 <code>model.save()</code> 一样简单。</li></ol><p><strong>下一章预告</strong>：
模型压缩只是第一步。如何把压缩后的模型变成为数千人服务的高性能 API？我们将深入 <strong>vLLM</strong> 及其核心技术 <strong>PagedAttention</strong>。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第5章 端到端LLM项目实战</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/ class="flex align-center"><span>第2章 vLLM高性能推理</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节量化技术详解>第一节：量化技术详解</a><ul><li><a href=#11-量化技术概览>1.1 量化技术概览</a></li><li><a href=#12-gptq-vs-awq-vs-exl2-深度解析>1.2 GPTQ vs AWQ vs EXL2 深度解析</a><ul><li><a href=#1-gptq-generative-pre-trained-transformer-quantization>1. GPTQ (Generative Pre-trained Transformer Quantization)</a></li><li><a href=#2-awq-activation-aware-weight-quantization>2. AWQ (Activation-aware Weight Quantization)</a></li><li><a href=#3-exl2-exllamav2>3. EXL2 (ExLlamaV2)</a></li></ul></li><li><a href=#13-实战使用-autogptq-和-autoawq>1.3 实战：使用 AutoGPTQ 和 AutoAWQ</a><ul><li><a href=#准备环境>准备环境</a></li><li><a href=#脚本-1-使用-autogptq-量化-llama-3-8b>脚本 1: 使用 AutoGPTQ 量化 LLaMA-3-8B</a></li><li><a href=#脚本-2-使用-autoawq-量化>脚本 2: 使用 AutoAWQ 量化</a></li></ul></li></ul></li><li><a href=#第二节kv-cache-量化-前沿趋势>第二节：KV Cache 量化 (前沿趋势)</a><ul><li><a href=#21-什么是-kv-cache为什么它是显存杀手>2.1 什么是 KV Cache？为什么它是显存杀手？</a></li><li><a href=#22-k-v-cache-quantization-fp8--int4>2.2 K-V Cache Quantization (FP8 / INT4)</a></li></ul></li><li><a href=#第三节剪枝与蒸馏-简介>第三节：剪枝与蒸馏 (简介)</a><ul><li><a href=#31-结构化剪枝-structured-pruning>3.1 结构化剪枝 (Structured Pruning)</a></li><li><a href=#32-知识蒸馏-knowledge-distillation>3.2 知识蒸馏 (Knowledge Distillation)</a></li></ul></li><li><a href=#第1章小结>第1章小结</a></li></ul></nav></div></aside></main></body></html>