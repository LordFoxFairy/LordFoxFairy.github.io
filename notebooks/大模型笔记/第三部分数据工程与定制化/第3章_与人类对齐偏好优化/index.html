<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第3章：与人类对齐：偏好优化 (Preference Alignment)# “Alignment is the art of getting what you want, not just what you asked for.”
即使是最强的预训练模型，也只是学会了"续写"。是偏好优化让它学会了"对话"、“拒绝"和"价值观”。
目录# 一、对齐三原则与 SFT 的局限 1. HHH 原则：有用、诚实、无害 2. 为什么 SFT 还不够？ 二、经典路线：RLHF (PPO) 1. 训练 Reward Model (奖励模型) 2. PPO 算法核心：KL 散度与 Policy 更新 3. 实战：手动实现 PPO Step 三、现代路线：DPO (Direct Preference Optimization) 1. DPO 的数学魔术：无需 Reward Model 2. DPO vs PPO：谁赢了？ 3. 实战：使用 TRL 训练 DPO 四、前沿变体：KTO / IPO / ORPO 1. KTO: 如果只有赞和踩，没有比较对 2. IPO: 修复 DPO 的长度偏好问题 3. ORPO: 连 SFT 都不需要了？ 4. SPIN: 自我对弈，无需人工数据 五、最新进展与趋势 1. 从 RLHF 到 RLAIF (AI Feedback) 2. Online DPO: 摆脱静态数据集 3. 多目标对齐：不只是 HHH 4. 对齐税 (Alignment Tax) 5. 主流模型的对齐策略 六、本章小结 一、对齐三原则与 SFT 的局限# 1. HHH 原则：有用、诚实、无害# OpenAI 定义了对齐的三大支柱：
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第3章 与人类对齐：偏好优化"><meta property="og:description" content='第3章：与人类对齐：偏好优化 (Preference Alignment)# “Alignment is the art of getting what you want, not just what you asked for.”
即使是最强的预训练模型，也只是学会了"续写"。是偏好优化让它学会了"对话"、“拒绝"和"价值观”。
目录# 一、对齐三原则与 SFT 的局限 1. HHH 原则：有用、诚实、无害 2. 为什么 SFT 还不够？ 二、经典路线：RLHF (PPO) 1. 训练 Reward Model (奖励模型) 2. PPO 算法核心：KL 散度与 Policy 更新 3. 实战：手动实现 PPO Step 三、现代路线：DPO (Direct Preference Optimization) 1. DPO 的数学魔术：无需 Reward Model 2. DPO vs PPO：谁赢了？ 3. 实战：使用 TRL 训练 DPO 四、前沿变体：KTO / IPO / ORPO 1. KTO: 如果只有赞和踩，没有比较对 2. IPO: 修复 DPO 的长度偏好问题 3. ORPO: 连 SFT 都不需要了？ 4. SPIN: 自我对弈，无需人工数据 五、最新进展与趋势 1. 从 RLHF 到 RLAIF (AI Feedback) 2. Online DPO: 摆脱静态数据集 3. 多目标对齐：不只是 HHH 4. 对齐税 (Alignment Tax) 5. 主流模型的对齐策略 六、本章小结 一、对齐三原则与 SFT 的局限# 1. HHH 原则：有用、诚实、无害# OpenAI 定义了对齐的三大支柱：'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第3章 与人类对齐：偏好优化"><meta itemprop=description content='第3章：与人类对齐：偏好优化 (Preference Alignment)# “Alignment is the art of getting what you want, not just what you asked for.”
即使是最强的预训练模型，也只是学会了"续写"。是偏好优化让它学会了"对话"、“拒绝"和"价值观”。
目录# 一、对齐三原则与 SFT 的局限 1. HHH 原则：有用、诚实、无害 2. 为什么 SFT 还不够？ 二、经典路线：RLHF (PPO) 1. 训练 Reward Model (奖励模型) 2. PPO 算法核心：KL 散度与 Policy 更新 3. 实战：手动实现 PPO Step 三、现代路线：DPO (Direct Preference Optimization) 1. DPO 的数学魔术：无需 Reward Model 2. DPO vs PPO：谁赢了？ 3. 实战：使用 TRL 训练 DPO 四、前沿变体：KTO / IPO / ORPO 1. KTO: 如果只有赞和踩，没有比较对 2. IPO: 修复 DPO 的长度偏好问题 3. ORPO: 连 SFT 都不需要了？ 4. SPIN: 自我对弈，无需人工数据 五、最新进展与趋势 1. 从 RLHF 到 RLAIF (AI Feedback) 2. Online DPO: 摆脱静态数据集 3. 多目标对齐：不只是 HHH 4. 对齐税 (Alignment Tax) 5. 主流模型的对齐策略 六、本章小结 一、对齐三原则与 SFT 的局限# 1. HHH 原则：有用、诚实、无害# OpenAI 定义了对齐的三大支柱：'><meta itemprop=wordCount content="4287"><title>第3章 与人类对齐：偏好优化 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle checked>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/ class=active>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第3章 与人类对齐：偏好优化</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一对齐三原则与-sft-的局限>一、对齐三原则与 SFT 的局限</a><ul><li><a href=#1-hhh-原则有用诚实无害>1. HHH 原则：有用、诚实、无害</a></li><li><a href=#2-为什么-sft-还不够>2. 为什么 SFT 还不够？</a><ul><li><a href=#21-深层原理sft-与偏好学习的数学空间本质不同>2.1 深层原理：SFT 与偏好学习的数学空间本质不同</a></li></ul></li></ul></li><li><a href=#二经典路线rlhf-ppo>二、经典路线：RLHF (PPO)</a><ul><li><a href=#1-训练-reward-model-奖励模型>1. 训练 Reward Model (奖励模型)</a><ul><li><a href=#11-bradley-terry-模型从概率到排序>1.1 Bradley-Terry 模型：从概率到排序</a></li><li><a href=#12-代码实现最小化-reward-model>1.2 代码实现：最小化 Reward Model</a></li></ul></li><li><a href=#2-ppo-算法核心kl-散度与-policy-更新>2. PPO 算法核心：KL 散度与 Policy 更新</a><ul><li><a href=#21-深度解析β-参数为什么不能太大也不能太小>2.1 深度解析：β 参数为什么不能太大也不能太小？</a></li><li><a href=#22-深度解析clipped-objective-为什么是这个形式>2.2 深度解析：Clipped Objective 为什么是这个形式？</a></li></ul></li><li><a href=#3-实战手动实现-ppo-step>3. 实战：手动实现 PPO Step</a></li></ul></li><li><a href=#三现代路线dpo-direct-preference-optimization>三、现代路线：DPO (Direct Preference Optimization)</a><ul><li><a href=#1-dpo-的数学魔术从-rlhf-到直接优化>1. DPO 的数学魔术：从 RLHF 到直接优化</a><ul><li><a href=#11-核心洞察reward-可以用-policy-表示>1.1 核心洞察：Reward 可以用 Policy 表示</a></li><li><a href=#12-逆向变换从-policy-反推-reward>1.2 逆向变换：从 Policy 反推 Reward</a></li><li><a href=#121-深度理解配分函数-zx-为什么会消掉>1.2.1 深度理解：配分函数 Z(x) 为什么会"消掉&rdquo;？</a></li><li><a href=#13-代入-bradley-terry-模型>1.3 代入 Bradley-Terry 模型</a></li><li><a href=#14-手写-dpo-losspytorch-实现>1.4 手写 DPO Loss：PyTorch 实现</a></li></ul></li><li><a href=#2-dpo-vs-ppo谁赢了>2. DPO vs PPO：谁赢了？</a><ul><li><a href=#21-深度解析dpo-为什么比-ppo-更稳定>2.1 深度解析：DPO 为什么比 PPO 更稳定？</a></li></ul></li><li><a href=#3-dpo-实战要点>3. DPO 实战要点</a></li></ul></li><li><a href=#四前沿变体kto--ipo--orpo>四、前沿变体：KTO / IPO / ORPO</a><ul><li><a href=#1-kto-如果只有赞和踩没有比较对>1. KTO: 如果只有赞和踩，没有比较对</a></li><li><a href=#2-ipo-修复-dpo-的长度偏好问题>2. IPO: 修复 DPO 的长度偏好问题</a><ul><li><a href=#21-深度解析为什么平方损失能消除长度偏好>2.1 深度解析：为什么平方损失能消除长度偏好？</a></li></ul></li><li><a href=#3-orpo-连-sft-都不需要了>3. ORPO: 连 SFT 都不需要了？</a><ul><li><a href=#31-深度解析odds-ratio-的数学魔法>3.1 深度解析：Odds Ratio 的数学魔法</a></li></ul></li><li><a href=#4-spin-自我对弈无需人工数据>4. SPIN: 自我对弈，无需人工数据</a></li></ul></li><li><a href=#五最新进展与趋势>五、最新进展与趋势</a><ul><li><a href=#1-simpo连-reference-model-都不需要了>1. SimPO：连 Reference Model 都不需要了</a><ul><li><a href=#11-simpo-的数学推导>1.1 SimPO 的数学推导</a></li><li><a href=#12-手写-simpo-loss>1.2 手写 SimPO Loss</a></li></ul></li><li><a href=#2-从-rlhf-到-rlaif-ai-feedback>2. 从 RLHF 到 RLAIF (AI Feedback)</a></li><li><a href=#2-online-dpo-摆脱静态数据集>2. Online DPO: 摆脱静态数据集</a></li><li><a href=#3-多目标对齐不只是-hhh>3. 多目标对齐：不只是 HHH</a></li><li><a href=#4-对齐税-alignment-tax>4. 对齐税 (Alignment Tax)</a></li><li><a href=#5-主流模型的对齐策略>5. 主流模型的对齐策略</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a><ul><li><a href=#核心要点>核心要点</a></li><li><a href=#实践建议>实践建议</a></li><li><a href=#延伸阅读>延伸阅读</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第3章与人类对齐偏好优化-preference-alignment>第3章：与人类对齐：偏好优化 (Preference Alignment)<a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e4%b8%8e%e4%ba%ba%e7%b1%bb%e5%af%b9%e9%bd%90%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96-preference-alignment>#</a></h1><blockquote class=book-hint><p>&ldquo;Alignment is the art of getting what you want, not just what you asked for.&rdquo;</p><p>即使是最强的预训练模型，也只是学会了"续写"。是偏好优化让它学会了"对话"、&ldquo;拒绝"和"价值观&rdquo;。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e5%af%b9%e9%bd%90%e4%b8%89%e5%8e%9f%e5%88%99%e4%b8%8e-sft-%e7%9a%84%e5%b1%80%e9%99%90>一、对齐三原则与 SFT 的局限</a><ul><li><a href=#1-hhh-%e5%8e%9f%e5%88%99%e6%9c%89%e7%94%a8%e8%af%9a%e5%ae%9e%e6%97%a0%e5%ae%b3>1. HHH 原则：有用、诚实、无害</a></li><li><a href=#2-%e4%b8%ba%e4%bb%80%e4%b9%88-sft-%e8%bf%98%e4%b8%8d%e5%a4%9f>2. 为什么 SFT 还不够？</a></li></ul></li><li><a href=#%e4%ba%8c%e7%bb%8f%e5%85%b8%e8%b7%af%e7%ba%bfrlhf-ppo>二、经典路线：RLHF (PPO)</a><ul><li><a href=#1-%e8%ae%ad%e7%bb%83-reward-model-%e5%a5%96%e5%8a%b1%e6%a8%a1%e5%9e%8b>1. 训练 Reward Model (奖励模型)</a></li><li><a href=#2-ppo-%e7%ae%97%e6%b3%95%e6%a0%b8%e5%bf%83kl-%e6%95%a3%e5%ba%a6%e4%b8%8e-policy-%e6%9b%b4%e6%96%b0>2. PPO 算法核心：KL 散度与 Policy 更新</a></li><li><a href=#3-%e5%ae%9e%e6%88%98%e6%89%8b%e5%8a%a8%e5%ae%9e%e7%8e%b0-ppo-step>3. 实战：手动实现 PPO Step</a></li></ul></li><li><a href=#%e4%b8%89%e7%8e%b0%e4%bb%a3%e8%b7%af%e7%ba%bfdpo-direct-preference-optimization>三、现代路线：DPO (Direct Preference Optimization)</a><ul><li><a href=#1-dpo-%e7%9a%84%e6%95%b0%e5%ad%a6%e9%ad%94%e6%9c%af%e6%97%a0%e9%9c%80-reward-model>1. DPO 的数学魔术：无需 Reward Model</a></li><li><a href=#2-dpo-vs-ppo%e8%b0%81%e8%b5%a2%e4%ba%86>2. DPO vs PPO：谁赢了？</a></li><li><a href=#3-%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8-trl-%e8%ae%ad%e7%bb%83-dpo>3. 实战：使用 TRL 训练 DPO</a></li></ul></li><li><a href=#%e5%9b%9b%e5%89%8d%e6%b2%bf%e5%8f%98%e4%bd%93kto--ipo--orpo>四、前沿变体：KTO / IPO / ORPO</a><ul><li><a href=#1-kto-%e5%a6%82%e6%9e%9c%e5%8f%aa%e6%9c%89%e8%b5%9e%e5%92%8c%e8%b8%a9%e6%b2%a1%e6%9c%89%e6%af%94%e8%be%83%e5%af%b9>1. KTO: 如果只有赞和踩，没有比较对</a></li><li><a href=#2-ipo-%e4%bf%ae%e5%a4%8d-dpo-%e7%9a%84%e9%95%bf%e5%ba%a6%e5%81%8f%e5%a5%bd%e9%97%ae%e9%a2%98>2. IPO: 修复 DPO 的长度偏好问题</a></li><li><a href=#3-orpo-%e8%bf%9e-sft-%e9%83%bd%e4%b8%8d%e9%9c%80%e8%a6%81%e4%ba%86>3. ORPO: 连 SFT 都不需要了？</a></li><li><a href=#4-spin-%e8%87%aa%e6%88%91%e5%af%b9%e5%bc%88%e6%97%a0%e9%9c%80%e4%ba%ba%e5%b7%a5%e6%95%b0%e6%8d%ae>4. SPIN: 自我对弈，无需人工数据</a></li></ul></li><li><a href=#%e4%ba%94%e6%9c%80%e6%96%b0%e8%bf%9b%e5%b1%95%e4%b8%8e%e8%b6%8b%e5%8a%bf>五、最新进展与趋势</a><ul><li><a href=#1-%e4%bb%8e-rlhf-%e5%88%b0-rlaif-ai-feedback>1. 从 RLHF 到 RLAIF (AI Feedback)</a></li><li><a href=#2-online-dpo-%e6%91%86%e8%84%b1%e9%9d%99%e6%80%81%e6%95%b0%e6%8d%ae%e9%9b%86>2. Online DPO: 摆脱静态数据集</a></li><li><a href=#3-%e5%a4%9a%e7%9b%ae%e6%a0%87%e5%af%b9%e9%bd%90%e4%b8%8d%e5%8f%aa%e6%98%af-hhh>3. 多目标对齐：不只是 HHH</a></li><li><a href=#4-%e5%af%b9%e9%bd%90%e7%a8%8e-alignment-tax>4. 对齐税 (Alignment Tax)</a></li><li><a href=#5-%e4%b8%bb%e6%b5%81%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%af%b9%e9%bd%90%e7%ad%96%e7%95%a5>5. 主流模型的对齐策略</a></li></ul></li><li><a href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>六、本章小结</a></li></ul><hr><h2 id=一对齐三原则与-sft-的局限>一、对齐三原则与 SFT 的局限<a class=anchor href=#%e4%b8%80%e5%af%b9%e9%bd%90%e4%b8%89%e5%8e%9f%e5%88%99%e4%b8%8e-sft-%e7%9a%84%e5%b1%80%e9%99%90>#</a></h2><h3 id=1-hhh-原则有用诚实无害>1. HHH 原则：有用、诚实、无害<a class=anchor href=#1-hhh-%e5%8e%9f%e5%88%99%e6%9c%89%e7%94%a8%e8%af%9a%e5%ae%9e%e6%97%a0%e5%ae%b3>#</a></h3><p>OpenAI 定义了对齐的三大支柱：</p><ul><li><strong>Helpful (有用)</strong>: 能够解决用户问题。</li><li><strong>Honest (诚实)</strong>: 不编造事实 (Hallucination)，不知道就说不知道。</li><li><strong>Harmless (无害)</strong>: 不生成暴力、色情、歧视内容。</li></ul><h3 id=2-为什么-sft-还不够>2. 为什么 SFT 还不够？<a class=anchor href=#2-%e4%b8%ba%e4%bb%80%e4%b9%88-sft-%e8%bf%98%e4%b8%8d%e5%a4%9f>#</a></h3><p>SFT (Supervised Fine-Tuning) 的训练目标是：
$$ L_{SFT} = -\log P(y_{label} \mid x) $$</p><p>SFT 只能学会**&ldquo;模仿&rdquo;<strong>标准答案，但无法理解</strong>&ldquo;好与坏&rdquo;**的程度。</p><ul><li>对于问题 &ldquo;如何制造炸弹？"，SFT 模型可能会模仿训练集里的高智商回答，给出一份完美的炸弹制作教程。这很 Helpful，但不 Harmless。</li><li>我们希望模型知道：即使你的回答在语法上很完美，但因为它是有害的，所以分数极低。</li></ul><h4 id=21-深层原理sft-与偏好学习的数学空间本质不同>2.1 深层原理：SFT 与偏好学习的数学空间本质不同<a class=anchor href=#21-%e6%b7%b1%e5%b1%82%e5%8e%9f%e7%90%86sft-%e4%b8%8e%e5%81%8f%e5%a5%bd%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%95%b0%e5%ad%a6%e7%a9%ba%e9%97%b4%e6%9c%ac%e8%b4%a8%e4%b8%8d%e5%90%8c>#</a></h4><p><strong>为什么 SFT 学不会偏好？</strong></p><p>SFT 的目标是最大化 $P(y|x)$，这是一个<strong>生成目标</strong>（Generative Objective）。
偏好学习需要的是<strong>比较目标</strong>（Comparative Objective）：$P(y_w \succ y_l | x)$。</p><p>两者的数学空间完全不同：</p><ul><li><strong>SFT</strong>：在概率空间中优化 —— 让模型输出接近标准答案</li><li><strong>偏好学习</strong>：在排序空间中优化 —— 让模型理解哪个答案更好</li></ul><p><strong>类比理解</strong>：</p><ul><li>SFT：教学生<strong>模仿范文写作</strong>（生成能力）</li><li>RLHF/DPO：教学生<strong>判断哪篇文章更好</strong>（评判能力）</li></ul><p>这就像让一个画家"临摹名画"和"鉴定真伪"是两种不同的能力。</p><p><strong>为什么需要"成对比较"而非"绝对打分&rdquo;？</strong></p><p>心理学研究发现：</p><ul><li>人类对<strong>绝对质量</strong>的判断不稳定（今天打 8 分，明天可能打 7 分）</li><li>但<strong>相对比较</strong>是稳定的（&ldquo;A 比 B 好"这个判断不会变）</li></ul><p>实验数据（来自 OpenAI InstructGPT 论文）：</p><table><thead><tr><th style=text-align:left>标注方式</th><th style=text-align:center>标注者一致性 (Kappa)</th><th style=text-align:center>数据效率</th></tr></thead><tbody><tr><td style=text-align:left>绝对打分（1-10分）</td><td style=text-align:center>0.61</td><td style=text-align:center>低</td></tr><tr><td style=text-align:left>成对比较（A vs B）</td><td style=text-align:center>0.85</td><td style=text-align:center>高</td></tr></tbody></table><p>Bradley-Terry 模型正是建立在"人类偏好可以通过潜在 reward 函数建模"的假设上。</p><hr><h2 id=二经典路线rlhf-ppo>二、经典路线：RLHF (PPO)<a class=anchor href=#%e4%ba%8c%e7%bb%8f%e5%85%b8%e8%b7%af%e7%ba%bfrlhf-ppo>#</a></h2><p>Reinforcement Learning from Human Feedback (RLHF) 是 ChatGPT 成功的关键。它把微调分成了三步：SFT -> RM -> PPO。</p><h3 id=1-训练-reward-model-奖励模型>1. 训练 Reward Model (奖励模型)<a class=anchor href=#1-%e8%ae%ad%e7%bb%83-reward-model-%e5%a5%96%e5%8a%b1%e6%a8%a1%e5%9e%8b>#</a></h3><h4 id=11-bradley-terry-模型从概率到排序>1.1 Bradley-Terry 模型：从概率到排序<a class=anchor href=#11-bradley-terry-%e6%a8%a1%e5%9e%8b%e4%bb%8e%e6%a6%82%e7%8e%87%e5%88%b0%e6%8e%92%e5%ba%8f>#</a></h4><p>我们需要一个能模仿人类打分的模型 $r_\phi(x, y)$。
输入：提示词 $x$，回答 $y$。输出：标量分数。</p><p><strong>训练数据</strong>：成对比较数据 (Pairwise Data)。
Human: &ldquo;写首诗&rdquo;</p><ul><li>A: &ldquo;窗前明月光&mldr;&rdquo; (人类觉得更好)</li><li>B: &ldquo;月亮很大&mldr;&rdquo;</li></ul><p><strong>Bradley-Terry 模型假设</strong>：人类偏好可以用奖励差的 sigmoid 建模。</p><p>给定两个回答 $y_w$ (winner) 和 $y_l$ (loser)，人类选择 $y_w$ 的概率为：
$$ P(y_w \succ y_l \mid x) = \frac{\exp(r_\phi(x, y_w))}{\exp(r_\phi(x, y_w)) + \exp(r_\phi(x, y_l))} = \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) $$</p><p>其中 $\sigma(z) = \frac{1}{1 + e^{-z}}$ 是 sigmoid 函数。</p><p><strong>Loss Function (负对数似然)</strong>:
$$ L_{RM} = -\mathbb{E}<em>{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(r</em>\phi(x, y_w) - r_\phi(x, y_l)) \right] $$</p><p><strong>直观理解</strong>：</p><ul><li>如果 $r_\phi(x, y_w) \gg r_\phi(x, y_l)$，则 $\sigma(\cdot) \to 1$，loss 接近 0</li><li>训练目标：拉大胜者和败者的分数差距</li></ul><h4 id=12-代码实现最小化-reward-model>1.2 代码实现：最小化 Reward Model<a class=anchor href=#12-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e6%9c%80%e5%b0%8f%e5%8c%96-reward-model>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Reward Model 核心实现
</span></span></span><span class=line><span class=cl><span class=s2>架构：Base LM + Linear Head → 标量分数
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RewardModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>base_model_name</span><span class=o>=</span><span class=s2>&#34;gpt2&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>base_model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>base_model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reward_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>base_model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 取最后一个有效 token 的隐藏状态</span>
</span></span><span class=line><span class=cl>        <span class=n>last_hidden</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>reward_head</span><span class=p>(</span><span class=n>last_hidden</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Bradley-Terry Loss</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>reward_loss</span><span class=p>(</span><span class=n>r_winner</span><span class=p>,</span> <span class=n>r_loser</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    输入：winner 和 loser 的奖励分数（标量）
</span></span></span><span class=line><span class=cl><span class=s2>    输出：Bradley-Terry Loss
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>r_winner</span> <span class=o>-</span> <span class=n>r_loser</span><span class=p>))</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例（伪代码）</span>
</span></span><span class=line><span class=cl><span class=c1># r_w = model(winner_ids, winner_mask)</span>
</span></span><span class=line><span class=cl><span class=c1># r_l = model(loser_ids, loser_mask)</span>
</span></span><span class=line><span class=cl><span class=c1># loss = reward_loss(r_w, r_l)</span></span></span></code></pre></div><p><strong>关键点</strong>：</p><ul><li>Reward Model 是一个<strong>回归问题</strong>，输出标量分数</li><li>训练目标：让胜者分数 > 败者分数，差距越大越好</li><li>实际应用中，RM 通常基于 SFT 模型初始化</li></ul><h3 id=2-ppo-算法核心kl-散度与-policy-更新>2. PPO 算法核心：KL 散度与 Policy 更新<a class=anchor href=#2-ppo-%e7%ae%97%e6%b3%95%e6%a0%b8%e5%bf%83kl-%e6%95%a3%e5%ba%a6%e4%b8%8e-policy-%e6%9b%b4%e6%96%b0>#</a></h3><p>有了奖励模型，我们就可以用强化学习来训练策略模型 $\pi_\theta$。</p><p><strong>目标函数</strong>：
$$ \max_\theta \mathbb{E}<em>{x \sim \mathcal{D}, y \sim \pi</em>\theta(\cdot|x)} \left[ r_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \right] $$</p><p>其中：</p><ul><li>$\pi_\theta$: 当前策略模型（待优化）</li><li>$\pi_{ref}$: 参考模型（通常是 SFT 模型，frozen）</li><li>$r_\phi(x, y)$: 奖励模型的评分</li><li>$\beta$: KL 惩罚系数（通常取 0.01-0.1）</li><li>$\mathcal{D}$: 提示词分布</li></ul><p>关键在于 <strong>KL 散度惩罚 (KL Penalty)</strong>：</p><ul><li>$\pi_{ref}$ 是原始的 SFT 模型。</li><li>我们希望模型分数变高，但<strong>不要偏离 SFT 模型太远</strong>。</li><li>如果没有 KL 惩罚，模型会利用 Reward Model 的漏洞 (Reward Hacking)，生成乱码来骗取高分。</li></ul><h4 id=21-深度解析β-参数为什么不能太大也不能太小>2.1 深度解析：β 参数为什么不能太大也不能太小？<a class=anchor href=#21-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%ce%b2-%e5%8f%82%e6%95%b0%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e5%a4%aa%e5%a4%a7%e4%b9%9f%e4%b8%8d%e8%83%bd%e5%a4%aa%e5%b0%8f>#</a></h4><p><strong>β 的物理意义</strong>：控制"追求高奖励"和"保持原有分布"之间的权衡。</p><p><strong>β 太小（如 0.001）的灾难</strong>：</p><p>问题：模型会<strong>过度优化</strong> reward，导致 <strong>Reward Hacking</strong>。</p><p>实际案例（来自 OpenAI 早期实验）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Prompt: &#34;写一首赞美春天的诗&#34;
</span></span><span class=line><span class=cl>正常输出: &#34;春风拂柳绿如烟，万物复苏...&#34;
</span></span><span class=line><span class=cl>β=0.001: &#34;好好好好好好好好好好好好...&#34; (重复 token 骗取高分)</span></span></code></pre></div><p>数学原因：</p><ul><li>KL 约束太弱：$\beta \cdot D_{KL}(\pi_\theta | \pi_{ref}) \approx 0$</li><li>模型可以任意偏离 $\pi_{ref}$，寻找 Reward Model 的漏洞</li><li>例如：生成高频词、重复 token、或触发 RM 的过拟合模式</li></ul><p><strong>β 太大（如 1.0）的问题</strong>：</p><p>问题：模型被<strong>锁死</strong>在 $\pi_{ref}$ 附近，无法学习新行为。</p><p>数学原因：</p><ul><li>KL 惩罚主导目标函数：$\beta \cdot D_{KL} \gg r_\phi(x, y)$</li><li>任何偏离 $\pi_{ref}$ 的行为都被严厉惩罚</li><li>结果：模型退化为 SFT 模型，RLHF 训练无效</li></ul><p><strong>最优 β 的选择（经验法则）</strong>：</p><p>理论依据（信息论）：
$$\beta^* \approx \frac{1}{\mathbb{E}[r_{max} - r_{min}]}$$</p><p>即：β 应该是"奖励动态范围"的倒数。</p><p>实践建议：</p><ul><li><strong>标准值</strong>：0.01 - 0.1（允许 10%-30% 的概率变化）</li><li><strong>安全任务</strong>（如客服机器人）：0.05 - 0.1（更保守）</li><li><strong>创意任务</strong>（如故事生成）：0.01 - 0.05（更自由）</li></ul><p><strong>可视化理解</strong>：</p><p>想象一条倒 U 型曲线：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>性能 ^
</span></span><span class=line><span class=cl>     |        *  (最优点)
</span></span><span class=line><span class=cl>     |      *   *
</span></span><span class=line><span class=cl>     |    *       *
</span></span><span class=line><span class=cl>     |  *           *
</span></span><span class=line><span class=cl>     +-------------------&gt; β
</span></span><span class=line><span class=cl>     0.001  0.05 0.1    1.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     β太小：     β最优：    β太大：
</span></span><span class=line><span class=cl>     Reward      平衡       模型
</span></span><span class=line><span class=cl>     Hacking              锁死</span></span></code></pre></div><p>OpenAI InstructGPT 实验数据：</p><table><thead><tr><th style=text-align:center>β 值</th><th style=text-align:center>KL 散度</th><th style=text-align:center>Reward</th><th style=text-align:center>人类评分</th></tr></thead><tbody><tr><td style=text-align:center>0.001</td><td style=text-align:center>15.2</td><td style=text-align:center>8.5</td><td style=text-align:center>6.2 (质量差)</td></tr><tr><td style=text-align:center>0.02</td><td style=text-align:center>2.1</td><td style=text-align:center>7.8</td><td style=text-align:center><strong>8.9</strong> (最优)</td></tr><tr><td style=text-align:center>0.1</td><td style=text-align:center>0.5</td><td style=text-align:center>6.2</td><td style=text-align:center>7.1 (保守)</td></tr><tr><td style=text-align:center>1.0</td><td style=text-align:center>0.05</td><td style=text-align:center>5.1</td><td style=text-align:center>5.8 (退化)</td></tr></tbody></table><p><strong>PPO 的核心创新：Clipped Surrogate Objective</strong></p><p>标准的策略梯度更新可能导致训练不稳定。PPO 通过限制策略更新幅度来解决这个问题：</p><p>$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right] $$</p><p>其中：</p><ul><li>$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}$: 新旧策略的概率比</li><li>$\hat{A}_t$: 优势函数 (Advantage)，衡量当前动作比平均好多少</li><li>$\epsilon$: 裁剪范围（通常取 0.2），防止更新过大</li></ul><h4 id=22-深度解析clipped-objective-为什么是这个形式>2.2 深度解析：Clipped Objective 为什么是这个形式？<a class=anchor href=#22-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90clipped-objective-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af%e8%bf%99%e4%b8%aa%e5%bd%a2%e5%bc%8f>#</a></h4><p><strong>为什么需要 clip？传统策略梯度的崩溃问题</strong></p><p>问题场景：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>假设某个低概率动作 a 突然获得高 reward：
</span></span><span class=line><span class=cl>- 旧策略：π_old(a|s) = 0.01
</span></span><span class=line><span class=cl>- 新策略：π_new(a|s) = 0.95
</span></span><span class=line><span class=cl>- 概率比 r_t = 0.95/0.01 = 95（暴涨 95 倍！）</span></span></code></pre></div><p>如果直接用策略梯度：$L = r_t \cdot A_t$</p><ul><li>当 $A_t = 5$ 时，梯度 = 95 × 5 = 475（梯度爆炸）</li><li>导致下一步更新过大，模型性能突然崩溃</li></ul><p><strong>为什么是 min(r_t · A, clip(r_t) · A) 这个形式？</strong></p><p>PPO 的设计哲学：<strong>保守更新，宁可慢也不要崩</strong>。</p><p>分情况分析：</p><ol><li><p><strong>当 $A_t > 0$（好动作，想增加概率）</strong>：</p><ul><li>如果 $r_t > 1 + \epsilon$（概率已经增加超过阈值）：<ul><li>不裁剪：继续增加 → 可能过度</li><li>PPO：clip 到 $1 + \epsilon$ → 停止增加</li></ul></li><li>结果：允许概率增加，但不超过 $(1+\epsilon)$ 倍</li></ul></li><li><p><strong>当 $A_t &lt; 0$（坏动作，想减少概率）</strong>：</p><ul><li>如果 $r_t &lt; 1 - \epsilon$（概率已经减少超过阈值）：<ul><li>不裁剪：继续减少 → 可能过度</li><li>PPO：clip 到 $1 - \epsilon$ → 停止减少</li></ul></li><li>结果：允许概率减少，但不超过 $(1-\epsilon)$ 倍</li></ul></li></ol><p><strong>为什么 $\epsilon = 0.2$？</strong></p><p>来源：Trust Region Policy Optimization (TRPO) 的近似。</p><p>TRPO 的约束：$D_{KL}(\pi_{old} | \pi_{new}) \leq \delta$</p><p>通过泰勒展开近似：
$$D_{KL} \approx \frac{1}{2} \mathbb{E}[(r_t - 1)^2] \leq \delta$$</p><p>求解得：$|r_t - 1| \leq \sqrt{2\delta}$</p><p>当 $\delta = 0.02$ 时，$\epsilon \approx 0.2$（经验最优值）</p><p><strong>实验对比：不同 $\epsilon$ 的影响</strong></p><table><thead><tr><th style=text-align:center>$\epsilon$ 值</th><th style=text-align:center>训练稳定性</th><th style=text-align:center>收敛速度</th><th style=text-align:center>最终性能</th><th style=text-align:left>适用场景</th></tr></thead><tbody><tr><td style=text-align:center>0.1</td><td style=text-align:center>极高</td><td style=text-align:center>慢</td><td style=text-align:center>中</td><td style=text-align:left>高风险任务（安全关键）</td></tr><tr><td style=text-align:center>0.2</td><td style=text-align:center>高</td><td style=text-align:center>中</td><td style=text-align:center><strong>高</strong></td><td style=text-align:left><strong>标准推荐</strong></td></tr><tr><td style=text-align:center>0.3</td><td style=text-align:center>中</td><td style=text-align:center>快</td><td style=text-align:center>中</td><td style=text-align:left>探索性训练</td></tr><tr><td style=text-align:center>0.5</td><td style=text-align:center>低</td><td style=text-align:center>很快</td><td style=text-align:center>低</td><td style=text-align:left>不推荐（易崩溃）</td></tr><tr><td style=text-align:center>无 clip</td><td style=text-align:center>极低</td><td style=text-align:center>不稳定</td><td style=text-align:center>崩溃</td><td style=text-align:left>论文对比基线</td></tr></tbody></table><p><strong>Advantage 函数：为什么不直接用 Reward？</strong></p><p><strong>问题场景</strong>：
假设所有动作的 reward 都是正的：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>状态 s 下的三个动作：
</span></span><span class=line><span class=cl>- 动作 a1: reward = 1.0
</span></span><span class=line><span class=cl>- 动作 a2: reward = 1.5
</span></span><span class=line><span class=cl>- 动作 a3: reward = 0.8</span></span></code></pre></div><p>如果直接用 reward 作为更新信号：</p><ul><li>所有动作的概率都会增加（因为 reward > 0）</li><li>这不合理！我们只想增加 a2，减少 a3</li></ul><p><strong>Advantage 的解决方案</strong>：</p><p>定义 Value 函数 $V(s)$：状态 $s$ 的<strong>平均价值</strong>
$$V(s) = \mathbb{E}_a[Q(s, a)] = 1.1 \text{ (平均值)}$$</p><p>Advantage 函数：
$$A(s, a) = Q(s, a) - V(s) = \text{&ldquo;比平均好多少&rdquo;}$$</p><p>结果：</p><ul><li>$A(s, a1) = 1.0 - 1.1 = -0.1$ (减少概率)</li><li>$A(s, a2) = 1.5 - 1.1 = +0.4$ (增加概率)</li><li>$A(s, a3) = 0.8 - 1.1 = -0.3$ (减少概率)</li></ul><p><strong>类比理解</strong>：</p><ul><li>Reward：考试的绝对分数（80 分、90 分、70 分）</li><li>Value：班级平均分（85 分）</li><li>Advantage：你比平均水平好多少（-5、+5、-15）</li></ul><p>只有"超过平均"的行为才会被鼓励！</p><h3 id=3-实战手动实现-ppo-step>3. 实战：手动实现 PPO Step<a class=anchor href=#3-%e5%ae%9e%e6%88%98%e6%89%8b%e5%8a%a8%e5%ae%9e%e7%8e%b0-ppo-step>#</a></h3><p>虽然现在常用 <code>trl.PPOTrainer</code>，但理解内部逻辑很重要。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>手动实现 PPO 的核心逻辑
</span></span></span><span class=line><span class=cl><span class=s2>输入：策略模型、参考模型、奖励信号
</span></span></span><span class=line><span class=cl><span class=s2>输出：策略损失
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gather_log_probs</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    从 logits 中提取对应 labels 的 log_probs
</span></span></span><span class=line><span class=cl><span class=s2>    输入：logits (batch, seq_len, vocab_size), labels (batch, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>    输出：log_probs (batch, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>log_probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 选择对应 token 的概率</span>
</span></span><span class=line><span class=cl>    <span class=n>selected_log_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>log_probs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=n>labels</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>selected_log_probs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_advantages</span><span class=p>(</span><span class=n>rewards</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span> <span class=n>lam</span><span class=o>=</span><span class=mf>0.95</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    计算 GAE (Generalized Advantage Estimation)
</span></span></span><span class=line><span class=cl><span class=s2>    输入：rewards (batch, seq_len), values (batch, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>    输出：advantages (batch, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>advantages</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>rewards</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>last_gae</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>rewards</span><span class=p>))):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>t</span> <span class=o>==</span> <span class=nb>len</span><span class=p>(</span><span class=n>rewards</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>next_value</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>next_value</span> <span class=o>=</span> <span class=n>values</span><span class=p>[</span><span class=n>t</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>delta</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>next_value</span> <span class=o>-</span> <span class=n>values</span><span class=p>[</span><span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>advantages</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>=</span> <span class=n>last_gae</span> <span class=o>=</span> <span class=n>delta</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>lam</span> <span class=o>*</span> <span class=n>last_gae</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>advantages</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ppo_step</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_model</span><span class=p>,</span> <span class=n>ref_model</span><span class=p>,</span> <span class=n>value_model</span><span class=p>,</span> <span class=n>reward_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span> <span class=n>response_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>kl_coef</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>clip_range</span><span class=o>=</span><span class=mf>0.2</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    完整的 PPO 更新步骤
</span></span></span><span class=line><span class=cl><span class=s2>    输入：
</span></span></span><span class=line><span class=cl><span class=s2>        - policy_model: 当前策略模型 (需要梯度)
</span></span></span><span class=line><span class=cl><span class=s2>        - ref_model: 参考模型 (frozen)
</span></span></span><span class=line><span class=cl><span class=s2>        - value_model: 价值函数 (Critic)
</span></span></span><span class=line><span class=cl><span class=s2>        - reward_model: 奖励模型 (frozen)
</span></span></span><span class=line><span class=cl><span class=s2>        - input_ids: prompt + response 的 token IDs
</span></span></span><span class=line><span class=cl><span class=s2>        - response_ids: 仅 response 部分的 token IDs
</span></span></span><span class=line><span class=cl><span class=s2>        - attention_mask: 掩码
</span></span></span><span class=line><span class=cl><span class=s2>    输出：policy_loss (标量)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 计算 Reward Model 的分数</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=n>reward_model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>  <span class=c1># (batch,)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 计算参考模型的 log_probs (frozen)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_logits</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_logprobs</span> <span class=o>=</span> <span class=n>gather_log_probs</span><span class=p>(</span><span class=n>ref_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span> <span class=n>response_ids</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 计算当前策略的 log_probs</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_logits</span> <span class=o>=</span> <span class=n>policy_model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_logprobs</span> <span class=o>=</span> <span class=n>gather_log_probs</span><span class=p>(</span><span class=n>policy_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span> <span class=n>response_ids</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. 计算 KL 散度惩罚</span>
</span></span><span class=line><span class=cl>    <span class=n>kl_div</span> <span class=o>=</span> <span class=p>(</span><span class=n>policy_logprobs</span> <span class=o>-</span> <span class=n>ref_logprobs</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (batch,)</span>
</span></span><span class=line><span class=cl>    <span class=n>penalized_rewards</span> <span class=o>=</span> <span class=n>rewards</span> <span class=o>-</span> <span class=n>kl_coef</span> <span class=o>*</span> <span class=n>kl_div</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5. 计算价值函数（用于 Advantage）</span>
</span></span><span class=line><span class=cl>    <span class=n>values</span> <span class=o>=</span> <span class=n>value_model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>  <span class=c1># (batch,)</span>
</span></span><span class=line><span class=cl>    <span class=n>advantages</span> <span class=o>=</span> <span class=n>penalized_rewards</span> <span class=o>-</span> <span class=n>values</span>  <span class=c1># 简化版，实际应使用 GAE</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 6. 保存旧的 log_probs（用于 ratio 计算）</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>old_logprobs</span> <span class=o>=</span> <span class=n>policy_logprobs</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 7. PPO Clipped Loss</span>
</span></span><span class=line><span class=cl>    <span class=n>ratio</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>policy_logprobs</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=n>old_logprobs</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>))</span>  <span class=c1># (batch,)</span>
</span></span><span class=line><span class=cl>    <span class=n>surr1</span> <span class=o>=</span> <span class=n>ratio</span> <span class=o>*</span> <span class=n>advantages</span>
</span></span><span class=line><span class=cl>    <span class=n>surr2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>ratio</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>clip_range</span><span class=p>,</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>clip_range</span><span class=p>)</span> <span class=o>*</span> <span class=n>advantages</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>surr1</span><span class=p>,</span> <span class=n>surr2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 8. Value Loss (MSE)</span>
</span></span><span class=line><span class=cl>    <span class=n>value_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>values</span><span class=p>,</span> <span class=n>penalized_rewards</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>policy_loss</span><span class=p>,</span> <span class=n>value_loss</span><span class=p>,</span> <span class=n>kl_div</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例（伪代码）</span>
</span></span><span class=line><span class=cl><span class=c1># optimizer_policy = torch.optim.AdamW(policy_model.parameters(), lr=1e-6)</span>
</span></span><span class=line><span class=cl><span class=c1># optimizer_value = torch.optim.AdamW(value_model.parameters(), lr=1e-5)</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># for batch in dataloader:</span>
</span></span><span class=line><span class=cl><span class=c1>#     loss_p, loss_v, kl = ppo_step(policy_model, ref_model, value_model, reward_model,</span>
</span></span><span class=line><span class=cl><span class=c1>#                                    batch[&#34;input_ids&#34;], batch[&#34;response_ids&#34;], batch[&#34;attention_mask&#34;])</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer_policy.zero_grad()</span>
</span></span><span class=line><span class=cl><span class=c1>#     loss_p.backward()</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer_policy.step()</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer_value.zero_grad()</span>
</span></span><span class=line><span class=cl><span class=c1>#     loss_v.backward()</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer_value.step()</span>
</span></span><span class=line><span class=cl><span class=c1>#     print(f&#34;Policy Loss: {loss_p.item():.4f}, Value Loss: {loss_v.item():.4f}, KL: {kl:.4f}&#34;)</span></span></span></code></pre></div><p><strong>为什么 PPO 很复杂？</strong></p><p>从代码可以看出，RLHF 需要同时维护 4 个模型：</p><ol><li><strong>Policy Model ($\pi_\theta$)</strong>: 待训练的策略</li><li><strong>Ref Model ($\pi_{ref}$)</strong>: 冻结的参考模型</li><li><strong>Reward Model ($r_\phi$)</strong>: 冻结的奖励模型</li><li><strong>Value Model (Critic)</strong>: 用于估计状态价值</li></ol><p>这导致：</p><ul><li>显存占用巨大（4个7B模型 = 112GB+）</li><li>训练不稳定（需精心调节 lr, clip_range, kl_coef）</li><li>实现复杂（需要 RL 框架，如 <code>trl.PPOTrainer</code>）</li></ul><hr><h2 id=三现代路线dpo-direct-preference-optimization>三、现代路线：DPO (Direct Preference Optimization)<a class=anchor href=#%e4%b8%89%e7%8e%b0%e4%bb%a3%e8%b7%af%e7%ba%bfdpo-direct-preference-optimization>#</a></h2><p>PPO 极其复杂，需要同时加载 4 个模型（Actor, Critic, Ref, Reward），显存占用巨大，且训练不稳定。
2023 年，Stanford 团队提出的 DPO 改变了游戏规则。</p><h3 id=1-dpo-的数学魔术从-rlhf-到直接优化>1. DPO 的数学魔术：从 RLHF 到直接优化<a class=anchor href=#1-dpo-%e7%9a%84%e6%95%b0%e5%ad%a6%e9%ad%94%e6%9c%af%e4%bb%8e-rlhf-%e5%88%b0%e7%9b%b4%e6%8e%a5%e4%bc%98%e5%8c%96>#</a></h3><h4 id=11-核心洞察reward-可以用-policy-表示>1.1 核心洞察：Reward 可以用 Policy 表示<a class=anchor href=#11-%e6%a0%b8%e5%bf%83%e6%b4%9e%e5%af%9freward-%e5%8f%af%e4%bb%a5%e7%94%a8-policy-%e8%a1%a8%e7%a4%ba>#</a></h4><p>回顾 RLHF 的目标函数：
$$ \max_{\pi_\theta} \mathbb{E}<em>{x \sim \mathcal{D}, y \sim \pi</em>\theta(\cdot|x)} \left[ r_\phi(x, y) - \beta \mathbb{D}<em>{KL}(\pi</em>\theta | \pi_{ref}) \right] $$</p><p>展开 KL 散度（在 $y$ 的分布上）：
$$ \mathbb{D}<em>{KL}(\pi</em>\theta | \pi_{ref}) = \mathbb{E}<em>{y \sim \pi</em>\theta} \left[ \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \right] $$</p><p>因此目标变为：
$$ \max_{\pi_\theta} \mathbb{E}<em>{y \sim \pi</em>\theta} \left[ r_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \right] $$</p><p><strong>关键推导</strong>：这个优化问题有闭式解！</p><p>最优策略 $\pi^<em>(y|x)$ 满足：
$$ \pi^</em>(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r^*(x, y)\right) $$</p><p>其中 $Z(x) = \sum_y \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r^*(x, y)\right)$ 是配分函数。</p><h4 id=12-逆向变换从-policy-反推-reward>1.2 逆向变换：从 Policy 反推 Reward<a class=anchor href=#12-%e9%80%86%e5%90%91%e5%8f%98%e6%8d%a2%e4%bb%8e-policy-%e5%8f%8d%e6%8e%a8-reward>#</a></h4><p>将上式改写，两边同时除以 $\pi_{ref}$ 再取对数：
$$ \log \frac{\pi^<em>(y|x)}{\pi_{ref}(y|x)} = \frac{1}{\beta} r^</em>(x, y) - \log Z(x) $$</p><p>移项得到<strong>隐式奖励函数</strong>：
$$ r^<em>(x, y) = \beta \log \frac{\pi^</em>(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x) $$</p><p><strong>关键发现</strong>：$Z(x)$ 只依赖于 $x$，在比较两个回答时会消掉！</p><h4 id=121-深度理解配分函数-zx-为什么会消掉>1.2.1 深度理解：配分函数 Z(x) 为什么会"消掉&rdquo;？<a class=anchor href=#121-%e6%b7%b1%e5%ba%a6%e7%90%86%e8%a7%a3%e9%85%8d%e5%88%86%e5%87%bd%e6%95%b0-zx-%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e6%b6%88%e6%8e%89>#</a></h4><p><strong>数学推导（更直观的版本）</strong>：</p><p>从隐式奖励函数出发：
$$r^<em>(x, y) = \beta \log \frac{\pi^</em>(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)$$</p><p>计算两个回答的奖励差：
$$
\begin{align}
r^<em>(x, y_w) - r^</em>(x, y_l) &= \beta \log \frac{\pi^<em>(y_w|x)}{\pi_{ref}(y_w|x)} + \cancel{\beta \log Z(x)} \
&\quad - \beta \log \frac{\pi^</em>(y_l|x)}{\pi_{ref}(y_l|x)} - \cancel{\beta \log Z(x)} \
&= \beta \left( \log \frac{\pi^<em>(y_w|x)}{\pi^</em>(y_l|x)} - \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right)
\end{align}
$$</p><p><strong>物理直觉</strong>：</p><p>$Z(x)$ 是<strong>归一化常数</strong>，只依赖于 prompt $x$，与具体回答 $y$ 无关。</p><p>类比 1：比较两个学生的成绩</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>学生 A：实际分数 85，班级平均 75 → 相对分数 +10
</span></span><span class=line><span class=cl>学生 B：实际分数 80，班级平均 75 → 相对分数 +5
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>比较 A 和 B 时：
</span></span><span class=line><span class=cl>- 绝对分数差：85 - 80 = 5
</span></span><span class=line><span class=cl>- 相对分数差：(+10) - (+5) = 5
</span></span><span class=line><span class=cl>- &#34;班级平均 75&#34;这个常数在相减时抵消了！</span></span></code></pre></div><p>类比 2：比较两个城市的房价</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>北京：房子 A = 500万，房子 B = 400万
</span></span><span class=line><span class=cl>上海：房子 A = 480万，房子 B = 380万
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>问题：哪个房子 A 相对更好？
</span></span><span class=line><span class=cl>- 如果考虑城市因素 Z(北京) = 城市溢价
</span></span><span class=line><span class=cl>- 比较时：(500 - Z) vs (400 - Z)
</span></span><span class=line><span class=cl>- Z 会消掉，只看房子本身的差异</span></span></code></pre></div><p><strong>决策树可视化</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>            Prompt x (Z(x) 在这里产生)
</span></span><span class=line><span class=cl>           /                    \
</span></span><span class=line><span class=cl>      y_w: &#34;好回答&#34;              y_l: &#34;坏回答&#34;
</span></span><span class=line><span class=cl>         ↓                          ↓
</span></span><span class=line><span class=cl>   r(x,y_w) = β·log[π/π_ref] + β·log Z(x)
</span></span><span class=line><span class=cl>   r(x,y_l) = β·log[π/π_ref] + β·log Z(x)
</span></span><span class=line><span class=cl>         \                          /
</span></span><span class=line><span class=cl>          \                        /
</span></span><span class=line><span class=cl>           \                      /
</span></span><span class=line><span class=cl>            r(x,y_w) - r(x,y_l)
</span></span><span class=line><span class=cl>                    ↓
</span></span><span class=line><span class=cl>            β·log Z(x) - β·log Z(x) = 0 ✓</span></span></code></pre></div><p><strong>关键洞察</strong>：</p><p>DPO 的天才之处在于：我们<strong>不需要计算</strong> $Z(x)$！</p><ul><li>Bradley-Terry 模型只关心"偏好概率"：$P(y_w \succ y_l)$</li><li>这是一个<strong>比较</strong>操作，$Z(x)$ 自动消掉</li><li>因此可以直接用 policy 的 log-ratio 代替 reward</li></ul><h4 id=13-代入-bradley-terry-模型>1.3 代入 Bradley-Terry 模型<a class=anchor href=#13-%e4%bb%a3%e5%85%a5-bradley-terry-%e6%a8%a1%e5%9e%8b>#</a></h4><p>回顾人类偏好模型：
$$ P(y_w \succ y_l \mid x) = \sigma(r^<em>(x, y_w) - r^</em>(x, y_l)) $$</p><p>代入隐式奖励：
$$
\begin{align}
P(y_w \succ y_l \mid x) &= \sigma\left( \beta \log \frac{\pi^<em>(y_w|x)}{\pi_{ref}(y_w|x)} + \cancel{\beta \log Z(x)} - \beta \log \frac{\pi^</em>(y_l|x)}{\pi_{ref}(y_l|x)} - \cancel{\beta \log Z(x)} \right) \
&= \sigma\left( \beta \log \frac{\pi^<em>(y_w|x)}{\pi^</em>(y_l|x)} - \beta \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right)
\end{align}
$$</p><p><strong>DPO Loss（负对数似然）</strong>：
$$
\boxed{
L_{DPO}(\pi_\theta) = -\mathbb{E}<em>{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi</em>\theta(y_w|x)}{\pi_\theta(y_l|x)} - \beta \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right) \right]
}
$$</p><p><strong>人话解释</strong>：</p><ol><li>我们不需要训练单独的 Reward Model</li><li>直接优化 Policy，让 $\frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)}$ 的比值大于 $\frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)}$</li><li>KL 惩罚隐式地编码在公式中（通过与 $\pi_{ref}$ 的比率）</li></ol><h4 id=14-手写-dpo-losspytorch-实现>1.4 手写 DPO Loss：PyTorch 实现<a class=anchor href=#14-%e6%89%8b%e5%86%99-dpo-losspytorch-%e5%ae%9e%e7%8e%b0>#</a></h4><p>DPO Loss 的核心只有几行代码！</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>手写 DPO Loss 的 PyTorch 实现
</span></span></span><span class=line><span class=cl><span class=s2>输入：policy 和 ref 模型的 logits
</span></span></span><span class=line><span class=cl><span class=s2>输出：DPO Loss
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_log_probs</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    从 logits 中提取对应 labels 的 log-probabilities
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        logits: (batch_size, seq_len, vocab_size)
</span></span></span><span class=line><span class=cl><span class=s2>        labels: (batch_size, seq_len)
</span></span></span><span class=line><span class=cl><span class=s2>    返回:
</span></span></span><span class=line><span class=cl><span class=s2>        log_probs: (batch_size,) - 每个序列的总 log-prob
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算 log-softmax</span>
</span></span><span class=line><span class=cl>    <span class=n>log_probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 选择对应 token 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=c1># gather: 从 log_probs 中按 labels 的索引取值</span>
</span></span><span class=line><span class=cl>    <span class=n>per_token_log_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>log_probs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>index</span><span class=o>=</span><span class=n>labels</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 对序列长度求和（忽略 padding）</span>
</span></span><span class=line><span class=cl>    <span class=c1># 假设 labels = -100 的位置是 padding</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>labels</span> <span class=o>!=</span> <span class=o>-</span><span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>per_token_log_probs</span> <span class=o>*</span> <span class=n>mask</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dpo_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logps</span><span class=p>,</span>    <span class=c1># π_θ(y_w|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logps</span><span class=p>,</span>  <span class=c1># π_θ(y_l|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_chosen_logps</span><span class=p>,</span>       <span class=c1># π_ref(y_w|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_rejected_logps</span><span class=p>,</span>     <span class=c1># π_ref(y_l|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span>                <span class=c1># KL 惩罚系数</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    DPO Loss 的核心实现
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        policy_*_logps: (batch_size,) - policy 模型的 log-probabilities
</span></span></span><span class=line><span class=cl><span class=s2>        ref_*_logps: (batch_size,) - reference 模型的 log-probabilities
</span></span></span><span class=line><span class=cl><span class=s2>        beta: KL 惩罚系数（典型值 0.1）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回:
</span></span></span><span class=line><span class=cl><span class=s2>        loss: 标量 - DPO Loss
</span></span></span><span class=line><span class=cl><span class=s2>        metrics: dict - 用于监控的指标
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算 log-ratio</span>
</span></span><span class=line><span class=cl>    <span class=n>pi_logratios</span> <span class=o>=</span> <span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>policy_rejected_logps</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_logratios</span> <span class=o>=</span> <span class=n>ref_chosen_logps</span> <span class=o>-</span> <span class=n>ref_rejected_logps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># DPO Loss: -log σ(β * (π_logratios - ref_logratios))</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>pi_logratios</span> <span class=o>-</span> <span class=n>ref_logratios</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>logits</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算监控指标</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=c1># 隐式奖励：r(x,y) = β * log(π/π_ref)</span>
</span></span><span class=line><span class=cl>        <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>ref_chosen_logps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_rejected_logps</span> <span class=o>-</span> <span class=n>ref_rejected_logps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_margin</span> <span class=o>=</span> <span class=p>(</span><span class=n>chosen_rewards</span> <span class=o>-</span> <span class=n>rejected_rewards</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 准确率：chosen 的奖励是否 &gt; rejected</span>
</span></span><span class=line><span class=cl>        <span class=n>accuracy</span> <span class=o>=</span> <span class=p>(</span><span class=n>chosen_rewards</span> <span class=o>&gt;</span> <span class=n>rejected_rewards</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;reward_margin&#34;</span><span class=p>:</span> <span class=n>reward_margin</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;accuracy&#34;</span><span class=p>:</span> <span class=n>accuracy</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=p>,</span> <span class=n>metrics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 完整训练步骤示例</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_step</span><span class=p>(</span><span class=n>policy_model</span><span class=p>,</span> <span class=n>ref_model</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    一个完整的 DPO 训练步骤
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    batch 包含:
</span></span></span><span class=line><span class=cl><span class=s2>        - chosen_input_ids: (B, L_chosen)
</span></span></span><span class=line><span class=cl><span class=s2>        - rejected_input_ids: (B, L_rejected)
</span></span></span><span class=line><span class=cl><span class=s2>        - chosen_labels: (B, L_chosen)
</span></span></span><span class=line><span class=cl><span class=s2>        - rejected_labels: (B, L_rejected)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 前向传播 - Policy Model</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logits</span> <span class=o>=</span> <span class=n>policy_model</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;chosen_input_ids&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logits</span> <span class=o>=</span> <span class=n>policy_model</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;rejected_input_ids&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logps</span> <span class=o>=</span> <span class=n>compute_log_probs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_chosen_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span>  <span class=c1># 去掉最后一个 token</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;chosen_labels&#34;</span><span class=p>][:,</span> <span class=mi>1</span><span class=p>:]</span>     <span class=c1># 去掉第一个 token</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logps</span> <span class=o>=</span> <span class=n>compute_log_probs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_rejected_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;rejected_labels&#34;</span><span class=p>][:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 前向传播 - Reference Model (frozen)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_chosen_logits</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;chosen_input_ids&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_rejected_logits</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;rejected_input_ids&#34;</span><span class=p>])</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>ref_chosen_logps</span> <span class=o>=</span> <span class=n>compute_log_probs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_chosen_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;chosen_labels&#34;</span><span class=p>][:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_rejected_logps</span> <span class=o>=</span> <span class=n>compute_log_probs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>ref_rejected_logits</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:],</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;rejected_labels&#34;</span><span class=p>][:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 计算 DPO Loss</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=p>,</span> <span class=n>metrics</span> <span class=o>=</span> <span class=n>dpo_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_chosen_logps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>policy_rejected_logps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_chosen_logps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_rejected_logps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>beta</span><span class=o>=</span><span class=n>beta</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=p>,</span> <span class=n>metrics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例（伪代码）</span>
</span></span><span class=line><span class=cl><span class=c1># optimizer = torch.optim.AdamW(policy_model.parameters(), lr=5e-7)</span>
</span></span><span class=line><span class=cl><span class=c1># for batch in dataloader:</span>
</span></span><span class=line><span class=cl><span class=c1>#     loss, metrics = train_step(policy_model, ref_model, batch)</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer.zero_grad()</span>
</span></span><span class=line><span class=cl><span class=c1>#     loss.backward()</span>
</span></span><span class=line><span class=cl><span class=c1>#     optimizer.step()</span>
</span></span><span class=line><span class=cl><span class=c1>#     print(f&#34;Loss: {metrics[&#39;loss&#39;]:.4f}, Reward Margin: {metrics[&#39;reward_margin&#39;]:.4f}&#34;)</span></span></span></code></pre></div><p><strong>代码关键点</strong>：</p><ol><li><strong>Log-Prob 计算</strong>：使用 <code>log_softmax</code> + <code>gather</code> 提取每个 token 的概率，再求和</li><li><strong>DPO Loss 核心</strong>：只有一行！<code>-F.logsigmoid(beta * (pi_logratios - ref_logratios))</code></li><li><strong>Ref Model 冻结</strong>：使用 <code>torch.no_grad()</code> 避免计算梯度</li><li><strong>监控指标</strong>：<ul><li><code>reward_margin</code>：chosen 和 rejected 的隐式奖励差</li><li><code>accuracy</code>：chosen 的奖励是否大于 rejected</li></ul></li></ol><h3 id=2-dpo-vs-ppo谁赢了>2. DPO vs PPO：谁赢了？<a class=anchor href=#2-dpo-vs-ppo%e8%b0%81%e8%b5%a2%e4%ba%86>#</a></h3><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>PPO (RLHF)</th><th style=text-align:left>DPO</th></tr></thead><tbody><tr><td style=text-align:left><strong>稳定性</strong></td><td style=text-align:left>极低，对超参敏感</td><td style=text-align:left>极高，像 SFT 一样稳</td></tr><tr><td style=text-align:left><strong>显存占用</strong></td><td style=text-align:left>巨大 (4个模型)</td><td style=text-align:left>低 (2个模型: Policy + Ref)</td></tr><tr><td style=text-align:left><strong>实现难度</strong></td><td style=text-align:left>困难</td><td style=text-align:left>简单 (几行代码)</td></tr><tr><td style=text-align:left><strong>效果</strong></td><td style=text-align:left>理论上限高，上限由RM决定</td><td style=text-align:left>实测与 PPO 持平甚至更好</td></tr></tbody></table><p>目前 (2025)，SOTA 模型如 Llama-3, Qwen-2 都在使用 DPO 及其变体。</p><h4 id=21-深度解析dpo-为什么比-ppo-更稳定>2.1 深度解析：DPO 为什么比 PPO 更稳定？<a class=anchor href=#21-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90dpo-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%af%94-ppo-%e6%9b%b4%e7%a8%b3%e5%ae%9a>#</a></h4><p><strong>从信息论视角理解</strong></p><p><strong>PPO 的不稳定性来源</strong>：</p><ol><li><p><strong>四个模型的复合误差</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Policy 更新 → 影响 → Reward 估计
</span></span><span class=line><span class=cl>     ↓                    ↓
</span></span><span class=line><span class=cl>Critic 更新 ← 影响 ← Advantage 计算
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>误差会累积放大（类似误差传播）</span></span></code></pre></div></li><li><p><strong>On-policy 采样问题</strong></p><ul><li>每次更新后，旧的经验数据就<strong>过时</strong>了</li><li>需要重新采样（sample inefficient）</li><li>数据分布不断变化 → 训练不稳定</li></ul></li></ol><p><strong>DPO 的稳定性来源</strong>：</p><ol><li><p><strong>Off-policy 训练</strong></p><ul><li>使用<strong>固定</strong>的 preference dataset</li><li>不需要重新采样</li><li>数据分布恒定 → 训练稳定</li></ul></li><li><p><strong>直接优化闭式解</strong></p><ul><li>目标函数是<strong>凸的</strong>（在 log 空间）</li><li>类似于分类问题（BCE Loss）</li><li>收敛性有理论保证</li></ul></li></ol><p><strong>数学证明（简化版）</strong>：</p><p>DPO 的 Hessian 矩阵（二阶导数）：
$$H_{DPO} = \mathbb{E}\left[\sigma(z)(1-\sigma(z)) \cdot \nabla^2 \log \pi\right]$$</p><p>关键特性：</p><ul><li>$\sigma(z)(1-\sigma(z)) \in [0, 0.25]$（有界！）</li><li>梯度更新幅度<strong>自动受限</strong>，不会爆炸</li></ul><p>PPO 的梯度：
$$\nabla L_{PPO} = \mathbb{E}\left[\frac{\pi_{new}}{\pi_{old}} \cdot A \cdot \nabla \log \pi\right]$$</p><p>问题：</p><ul><li>$\frac{\pi_{new}}{\pi_{old}}$ 可能非常大（如 > 10）</li><li>Advantage $A$ 的估计也有方差</li><li>两者相乘 → 梯度可能爆炸</li></ul><p><strong>实验数据</strong>（训练 100 steps 的梯度统计）：</p><table><thead><tr><th style=text-align:left>方法</th><th style=text-align:center>梯度范数均值</th><th style=text-align:center>梯度范数标准差</th><th style=text-align:center>Loss 震荡幅度</th></tr></thead><tbody><tr><td style=text-align:left>PPO</td><td style=text-align:center>2.3</td><td style=text-align:center><strong>8.5</strong> (高方差)</td><td style=text-align:center>±1.2 (剧烈)</td></tr><tr><td style=text-align:left>DPO</td><td style=text-align:center>0.8</td><td style=text-align:center><strong>1.2</strong> (低方差)</td><td style=text-align:center>±0.3 (平稳)</td></tr></tbody></table><p><strong>可视化</strong>（训练曲线对比）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Loss
</span></span><span class=line><span class=cl>  ^
</span></span><span class=line><span class=cl>  | PPO: ~~~~∿∿~~~∿∿~~  (震荡)
</span></span><span class=line><span class=cl>  |
</span></span><span class=line><span class=cl>  | DPO: ￣￣＼__＼_    (平滑下降)
</span></span><span class=line><span class=cl>  +-------------------&gt; Steps</span></span></code></pre></div><p><strong>β 在 DPO 中的物理意义（与 PPO 不同）</strong></p><p>在 PPO 中：</p><ul><li>$\beta$ 控制 KL 惩罚的<strong>强度</strong></li><li>单位：无量纲（纯比例系数）</li></ul><p>在 DPO 中：</p><ul><li>$\beta$ 是<strong>温度参数</strong>（temperature）</li><li>单位：reward 的倒数</li><li>来自统计物理的 Boltzmann 分布</li></ul><p><strong>温度的直觉</strong>：</p><p>$\beta$ 小（如 0.01）：<strong>高温状态</strong></p><ul><li>分布"陡峭"：小的 reward 差异 → 大的概率变化</li><li>模型对偏好<strong>非常敏感</strong></li><li>类比：冰块（固态）—— 分子排列整齐</li></ul><p>$\beta$ 大（如 1.0）：<strong>低温状态</strong></p><ul><li>分布"平缓"：需要很大的 reward 差异才改变概率</li><li>模型对偏好<strong>不敏感</strong></li><li>类比：水蒸气（气态）—— 分子随机运动</li></ul><p><strong>最优 $\beta$ 的理论公式</strong>（来自统计力学）：
$$\beta_{optimal} \approx \frac{1}{\mathbb{E}[|r_w - r_l|]}$$</p><p>即：$\beta$ 应该是"平均 reward 差"的倒数。</p><p><strong>实践建议</strong>：</p><ol><li>先在小数据集（1000 条）上扫描 $\beta \in {0.01, 0.05, 0.1, 0.3, 0.5}$</li><li>观察训练中的 accuracy 曲线</li><li>选择 accuracy <strong>最先达到 90%</strong> 的 $\beta$</li></ol><p><strong>Sigmoid 函数可视化</strong>（不同 $\beta$ 的影响）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>P(chosen)
</span></span><span class=line><span class=cl>  ^
</span></span><span class=line><span class=cl>1 |         β=0.5
</span></span><span class=line><span class=cl>  |       /
</span></span><span class=line><span class=cl>  |      /  β=0.1
</span></span><span class=line><span class=cl>  |     / /
</span></span><span class=line><span class=cl>0.5|----//-----------  (当 Δr=0 时)
</span></span><span class=line><span class=cl>  |   //
</span></span><span class=line><span class=cl>  |  / /   β=0.01
</span></span><span class=line><span class=cl>  | /
</span></span><span class=line><span class=cl>0 +-------------------&gt; Δr (reward 差)
</span></span><span class=line><span class=cl>    -2  -1  0  1  2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>解读：
</span></span><span class=line><span class=cl>- β=0.1 时，Δr=1 已经让 P(chosen) 从 0.5 → 0.73
</span></span><span class=line><span class=cl>- β=0.01 时，需要 Δr=10 才能达到相同效果
</span></span><span class=line><span class=cl>- β=0.5 时，Δr=0.5 就足够了</span></span></code></pre></div><h3 id=3-dpo-实战要点>3. DPO 实战要点<a class=anchor href=#3-dpo-%e5%ae%9e%e6%88%98%e8%a6%81%e7%82%b9>#</a></h3><p><strong>数据格式准备</strong>：</p><p>DPO 训练需要标准的 JSONL 格式，这种"三元组"是必须的：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;prompt&#34;</span><span class=p>:</span> <span class=s2>&#34;如何用 Python 读取 JSON 文件？&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;chosen&#34;</span><span class=p>:</span> <span class=s2>&#34;使用内置的 json 库：\n```python\nimport json\nwith open(&#39;file.json&#39;) as f:\n    data = json.load(f)\n```\n这是最标准的方法。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rejected&#34;</span><span class=p>:</span> <span class=s2>&#34;你可以用 Pandas 读取。\n```python\nimport pandas as pd\ndf = pd.read_json(&#39;file.json&#39;)\n```\n虽然可行，但对于简单读取来说太重了。&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>关键超参数</strong>：</p><table><thead><tr><th style=text-align:left>参数</th><th style=text-align:left>推荐值</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>beta</strong></td><td style=text-align:left>0.1 - 0.5</td><td style=text-align:left>KL 惩罚系数，越大模型越保守</td></tr><tr><td style=text-align:left><strong>learning_rate</strong></td><td style=text-align:left>1e-7 - 5e-7</td><td style=text-align:left>DPO 的 lr 要比 SFT 小 5-10 倍</td></tr><tr><td style=text-align:left><strong>batch_size</strong></td><td style=text-align:left>2 - 4</td><td style=text-align:left>显存占用是 SFT 的 2 倍（需同时处理 chosen 和 rejected）</td></tr></tbody></table><p><strong>常见错误</strong>：</p><ul><li>❌ 使用预训练模型（未 SFT）直接训练 DPO：效果极差，必须先 SFT</li><li>❌ learning_rate 太大：导致模型崩溃，loss 变成 NaN</li><li>❌ beta 设置为 0：模型会过拟合偏好数据，丧失生成能力</li></ul><p><strong>工具库推荐</strong>：</p><ul><li><code>TRL</code> (HuggingFace): <code>DPOTrainer</code> 是工业标准实现（详见 Part 5 第 3 章）</li><li><code>LLaMA-Factory</code>: 零代码 DPO 训练（详见 Part 5 第 2 章）</li></ul><hr><h2 id=四前沿变体kto--ipo--orpo>四、前沿变体：KTO / IPO / ORPO<a class=anchor href=#%e5%9b%9b%e5%89%8d%e6%b2%bf%e5%8f%98%e4%bd%93kto--ipo--orpo>#</a></h2><p>DPO 虽然好，但它需要<strong>成对数据 (Paired Data)</strong>。这很难搞：你得找两句话，还得判断谁好谁坏。</p><h3 id=1-kto-如果只有赞和踩没有比较对>1. KTO: 如果只有赞和踩，没有比较对<a class=anchor href=#1-kto-%e5%a6%82%e6%9e%9c%e5%8f%aa%e6%9c%89%e8%b5%9e%e5%92%8c%e8%b8%a9%e6%b2%a1%e6%9c%89%e6%af%94%e8%be%83%e5%af%b9>#</a></h3><p><strong>KTO (Kahneman-Tversky Optimization)</strong> 不需要成对数据。
它只需要：$(x, y, label)$，其中 label 是 true (赞) 或 false (踩)。</p><p><strong>核心思想</strong>：利用前景理论 (Prospect Theory)</p><ul><li>人类对"损失"的厌恶 > 对"收益"的喜悦</li><li>如果一个回答被点赞，小幅增加其概率</li><li>如果被点踩，大幅降低其概率</li></ul><p><strong>KTO Loss</strong>：
$$
L_{KTO} = \mathbb{E}_{(x,y,l)} \left[
\begin{cases}</p><ul><li>\lambda_D \cdot \sigma \left( \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} - z_{ref} \right), & l = 1 \text{ (desirable)} \</li><li>\lambda_U \cdot \sigma \left( z_{ref} - \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)} \right), & l = 0 \text{ (undesirable)}
\end{cases}
\right]
$$</li></ul><p>其中：</p><ul><li>$\lambda_D, \lambda_U$: 不对称系数（通常 $\lambda_U > \lambda_D$，惩罚比奖励强）</li><li>$z_{ref}$: 参考点（用于归一化）</li></ul><p><strong>代码实现（核心）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>kto_loss</span><span class=p>(</span><span class=n>policy_logp</span><span class=p>,</span> <span class=n>ref_logp</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>lambda_D</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>lambda_U</span><span class=o>=</span><span class=mf>1.5</span><span class=p>,</span> <span class=n>z_ref</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    KTO Loss 核心实现
</span></span></span><span class=line><span class=cl><span class=s2>    label: True (desirable) or False (undesirable)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>implicit_reward</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_logp</span> <span class=o>-</span> <span class=n>ref_logp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>label</span><span class=p>:</span>  <span class=c1># desirable</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=o>-</span><span class=n>lambda_D</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>implicit_reward</span> <span class=o>-</span> <span class=n>z_ref</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>  <span class=c1># undesirable</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=o>-</span><span class=n>lambda_U</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>z_ref</span> <span class=o>-</span> <span class=n>implicit_reward</span><span class=p>)</span></span></span></code></pre></div><p><strong>适用场景</strong>：</p><ul><li>✅ 只有点赞/点踩数据（如社交媒体评论）</li><li>✅ 标注成本高，无法做成对比较</li><li>❌ 需要精细控制偏好（DPO 更好）</li></ul><h3 id=2-ipo-修复-dpo-的长度偏好问题>2. IPO: 修复 DPO 的长度偏好问题<a class=anchor href=#2-ipo-%e4%bf%ae%e5%a4%8d-dpo-%e7%9a%84%e9%95%bf%e5%ba%a6%e5%81%8f%e5%a5%bd%e9%97%ae%e9%a2%98>#</a></h3><p><strong>问题</strong>：DPO 倾向于生成更长的回答（即使质量不高），因为长句子的 log-likelihood 更高。</p><p><strong>IPO (Identity Preference Optimization)</strong> 修改了 DPO 的 Loss 函数：</p><p>$$
L_{IPO} = \mathbb{E} \left[ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)} - \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} - \frac{1}{\beta} \right)^2 \right]
$$</p><p>关键变化：</p><ul><li>用<strong>平方损失</strong>代替对数损失（更稳定）</li><li>减去常数项 $\frac{1}{\beta}$（消除长度偏好）</li></ul><h4 id=21-深度解析为什么平方损失能消除长度偏好>2.1 深度解析：为什么平方损失能消除长度偏好？<a class=anchor href=#21-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90%e4%b8%ba%e4%bb%80%e4%b9%88%e5%b9%b3%e6%96%b9%e6%8d%9f%e5%a4%b1%e8%83%bd%e6%b6%88%e9%99%a4%e9%95%bf%e5%ba%a6%e5%81%8f%e5%a5%bd>#</a></h4><p><strong>DPO 的长度偏好问题</strong></p><p>DPO Loss：$-\log \sigma(\beta \cdot \Delta r)$，其中：
$$\Delta r = \log \pi(y_w|x) - \log \pi(y_l|x) = \sum_{t=1}^{T_w} \log P(y_t^w|&mldr;) - \sum_{t=1}^{T_l} \log P(y_t^l|&mldr;)$$</p><p><strong>问题根源</strong>：</p><ul><li>$\log \pi(y|x)$ 是所有 token 的 log 概率<strong>求和</strong></li><li>长序列的 log 概率更负（如 -50 vs -30）</li><li>即使质量相同，长序列的 log 概率<strong>绝对值更大</strong></li></ul><p><strong>实际案例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>问题：&#34;Python 中如何读取文件？&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>回答 A（简洁，20 tokens）：
</span></span><span class=line><span class=cl>&#34;使用 open() 函数：
</span></span><span class=line><span class=cl>with open(&#39;file.txt&#39;) as f:
</span></span><span class=line><span class=cl>    data = f.read()&#34;
</span></span><span class=line><span class=cl>→ log π(A|x) = -25
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>回答 B（冗长，50 tokens）：
</span></span><span class=line><span class=cl>&#34;在 Python 编程语言中，读取文件是一个常见的操作。
</span></span><span class=line><span class=cl>首先，我们需要使用内置的 open() 函数来打开文件...
</span></span><span class=line><span class=cl>（大量解释）
</span></span><span class=line><span class=cl>with open(&#39;file.txt&#39;, &#39;r&#39;) as f:
</span></span><span class=line><span class=cl>    data = f.read()
</span></span><span class=line><span class=cl>这就是读取文件的方法。&#34;
</span></span><span class=line><span class=cl>→ log π(B|x) = -65
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>如果 A 和 B 质量相当（都被选为 chosen）：
</span></span><span class=line><span class=cl>- DPO 会倾向于生成 B（因为 |log π(B)| 更大）
</span></span><span class=line><span class=cl>- 模型学到：&#34;写长一点，分数更高&#34;</span></span></code></pre></div><p><strong>梯度分析</strong>（为什么 DPO 会利用长度）：</p><p>DPO 的梯度：
$$\nabla L_{DPO} \propto \sigma&rsquo;(\beta \cdot \Delta r) \cdot \nabla \Delta r$$</p><p>当 $y_w$ 更长时：</p><ul><li>$\Delta r$ 的绝对值变大</li><li>$\sigma&rsquo;(\beta \cdot \Delta r)$ 在远离 0 时梯度更小（sigmoid 饱和）</li><li>但 $\nabla \Delta r$ 包含更多 token，总梯度仍然大</li><li>结果：模型被鼓励生成更长的序列</li></ul><p><strong>IPO 的解决方案</strong></p><p>核心思想：用<strong>平方损失</strong> + <strong>目标阈值</strong>代替 log-sigmoid。</p><p>$$L_{IPO} = \left(\Delta r - \frac{1}{\beta}\right)^2$$</p><p><strong>关键点 1：平方损失的梯度</strong>
$$\nabla L_{IPO} = 2\left(\Delta r - \frac{1}{\beta}\right) \cdot \nabla \Delta r$$</p><p><strong>关键点 2：目标阈值 $\frac{1}{\beta}$</strong></p><ul><li>当 $\Delta r = \frac{1}{\beta}$ 时，loss = 0（<strong>目标点</strong>）</li><li>无论序列长度，只要 $\Delta r$ 达到阈值，梯度就归零</li><li>超过阈值后，梯度甚至变负（惩罚过度优化）</li></ul><p><strong>长度不敏感性分析</strong>：</p><p>场景 1：长序列（50 tokens）</p><ul><li>$\Delta r = 2.0$（因为序列长，累积差异大）</li><li>如果 $\frac{1}{\beta} = 1.0$：<ul><li>$L_{IPO} = (2.0 - 1.0)^2 = 1.0$</li><li>梯度 ∝ $(2.0 - 1.0) = 1.0$ → <strong>停止优化</strong></li></ul></li></ul><p>场景 2：短序列（20 tokens）</p><ul><li>$\Delta r = 0.5$（因为序列短，累积差异小）</li><li>如果 $\frac{1}{\beta} = 1.0$：<ul><li>$L_{IPO} = (0.5 - 1.0)^2 = 0.25$</li><li>梯度 ∝ $(0.5 - 1.0) = -0.5$ → <strong>继续优化</strong></li></ul></li></ul><p><strong>类比理解</strong>：</p><p>DPO：考试要求"分数差距越大越好"</p><ul><li>长文章容易拉开差距（字多自然差距大）</li><li>短文章难拉开差距（字少差距有限）</li><li>结果：鼓励写长文章</li></ul><p>IPO：考试要求"分数差距达到 10 分即可"</p><ul><li>长文章：差距 20 分？停！已经够了</li><li>短文章：差距 5 分？继续努力</li><li>达到标准后，长短文章一视同仁</li></ul><p><strong>实验数据</strong>（统计训练后模型的生成长度）：</p><table><thead><tr><th style=text-align:left>方法</th><th style=text-align:center>平均长度 (tokens)</th><th style=text-align:center>长度标准差</th><th style=text-align:center>Quality (人类评分)</th></tr></thead><tbody><tr><td style=text-align:left>DPO</td><td style=text-align:center>247</td><td style=text-align:center>82 (高方差)</td><td style=text-align:center>7.8</td></tr><tr><td style=text-align:left>IPO</td><td style=text-align:center>182</td><td style=text-align:center>45 (低方差)</td><td style=text-align:center><strong>8.2</strong> (更好)</td></tr><tr><td style=text-align:left>SFT</td><td style=text-align:center>165</td><td style=text-align:center>38</td><td style=text-align:center>7.5</td></tr></tbody></table><p><strong>关键发现</strong>：</p><ul><li>IPO 的生成长度接近 SFT（不过度冗长）</li><li>质量反而更高（简洁性是优点）</li></ul><p><strong>实现（需要手动修改 DPOTrainer）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># IPO Loss 的核心实现</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ipo_loss</span><span class=p>(</span><span class=n>policy_logps_w</span><span class=p>,</span> <span class=n>policy_logps_l</span><span class=p>,</span> <span class=n>ref_logps_w</span><span class=p>,</span> <span class=n>ref_logps_l</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    IPO 损失函数
</span></span></span><span class=line><span class=cl><span class=s2>    输入：chosen 和 rejected 的 log-probabilities
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算对数比率差</span>
</span></span><span class=line><span class=cl>    <span class=n>pi_ratio</span> <span class=o>=</span> <span class=n>policy_logps_w</span> <span class=o>-</span> <span class=n>policy_logps_l</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_ratio</span> <span class=o>=</span> <span class=n>ref_logps_w</span> <span class=o>-</span> <span class=n>ref_logps_l</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># IPO Loss: (π_ratio - ref_ratio - 1/β)^2</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=p>((</span><span class=n>pi_ratio</span> <span class=o>-</span> <span class=n>ref_ratio</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>/</span> <span class=n>beta</span><span class=p>))</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span></span></span></code></pre></div><h3 id=3-orpo-连-sft-都不需要了>3. ORPO: 连 SFT 都不需要了？<a class=anchor href=#3-orpo-%e8%bf%9e-sft-%e9%83%bd%e4%b8%8d%e9%9c%80%e8%a6%81%e4%ba%86>#</a></h3><p>传统流程：Pretrain -> SFT -> DPO（两阶段）。
<strong>ORPO (Odds Ratio Preference Optimization)</strong> 试图把 SFT 和 DPO 合二为一。</p><p><strong>核心思想</strong>：在 SFT Loss 基础上，加一个 Odds Ratio 惩罚项。</p><p>$$
L_{ORPO} = L_{SFT}(y_w) + \lambda \cdot \mathbb{E} \left[ \log \sigma \left( \log \frac{\text{odds}<em>\theta(y_w|x)}{\text{odds}</em>\theta(y_l|x)} \right) \right]
$$</p><p>其中 Odds Ratio（胜率比）定义为：
$$
\text{odds}<em>\theta(y|x) = \frac{P</em>\theta(y|x)}{1 - P_\theta(y|x)}
$$</p><p><strong>与 DPO 的区别</strong>：</p><ul><li>DPO: 需要先 SFT，再用偏好数据微调</li><li>ORPO: 直接在预训练模型上同时做 SFT 和偏好优化</li></ul><p><strong>优势</strong>：</p><ul><li>节省一半训练时间（一次训练完成两个目标）</li><li>在 Mistral-7B 上实测效果优于 SFT+DPO</li></ul><p><strong>劣势</strong>：</p><ul><li>超参数敏感（$\lambda$ 需要精心调节）</li><li>对数据质量要求极高</li></ul><h4 id=31-深度解析odds-ratio-的数学魔法>3.1 深度解析：Odds Ratio 的数学魔法<a class=anchor href=#31-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90odds-ratio-%e7%9a%84%e6%95%b0%e5%ad%a6%e9%ad%94%e6%b3%95>#</a></h4><p><strong>什么是 Odds（胜率）？</strong></p><p>概率 $P$：事件发生的可能性（0 到 1）
$$\text{Odds} = \frac{P}{1-P} \quad \text{（胜率，0 到 }\infty\text{）}$$</p><p><strong>例子</strong>：</p><table><thead><tr><th style=text-align:center>概率 P</th><th style=text-align:center>Odds</th><th style=text-align:left>含义</th></tr></thead><tbody><tr><td style=text-align:center>0.5</td><td style=text-align:center>1</td><td style=text-align:left>50% 概率（五五开）</td></tr><tr><td style=text-align:center>0.9</td><td style=text-align:center>9</td><td style=text-align:left>90% 概率（9:1 的胜率）</td></tr><tr><td style=text-align:center>0.99</td><td style=text-align:center>99</td><td style=text-align:left>99% 概率（99:1 的胜率）</td></tr><tr><td style=text-align:center>0.999</td><td style=text-align:center>999</td><td style=text-align:left>99.9% 概率（999:1 的胜率）</td></tr></tbody></table><p><strong>为什么用 Odds 而非 Probability？</strong></p><p><strong>问题场景</strong>：
假设两个回答的概率接近但有差距：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>P(y_w|x) = 0.9
</span></span><span class=line><span class=cl>P(y_l|x) = 0.8</span></span></code></pre></div><p><strong>概率比 vs Odds 比</strong>：</p><ul><li>概率比：$\frac{0.9}{0.8} = 1.125$（差距不明显）</li><li>Odds 比：$\frac{9}{4} = 2.25$（差距被放大）</li></ul><p><strong>更极端的例子</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>P(y_w|x) = 0.99
</span></span><span class=line><span class=cl>P(y_l|x) = 0.98</span></span></code></pre></div><ul><li>概率比：$\frac{0.99}{0.98} = 1.01$（几乎看不出差距）</li><li>Odds 比：$\frac{99}{49} = 2.02$（差距仍然明显）</li></ul><p><strong>数学优势</strong>：</p><ol><li><p><strong>对数空间的对称性</strong>：
$$\log(\text{Odds Ratio}) = \log(\text{Odds}_w) - \log(\text{Odds}_l)$$
与 log 概率比类似，但<strong>动态范围更大</strong></p></li><li><p><strong>极端概率下更敏感</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>概率：0.99 → 0.999，仅提升 0.009
</span></span><span class=line><span class=cl>Odds：99 → 999，提升 10 倍！</span></span></code></pre></div></li></ol><p><strong>ORPO 为什么能合并 SFT 和 DPO？</strong></p><p>$$L_{ORPO} = \underbrace{L_{SFT}(y_w)}<em>{\text{模仿项}} + \lambda \cdot \underbrace{L</em>{OR}(y_w, y_l)}_{\text{对比项}}$$</p><p><strong>关键洞察</strong>：两个目标都作用在 $y_w$ 上！</p><ul><li><p><strong>SFT 项</strong>：$-\log P_\theta(y_w|x)$</p><ul><li>增加 $P(y_w|x)$（学会生成好回答）</li></ul></li><li><p><strong>OR 项</strong>：$-\log \sigma\left(\log \frac{\text{Odds}(y_w)}{\text{Odds}(y_l)}\right)$</p><ul><li>增加 $\frac{\text{Odds}(y_w)}{\text{Odds}(y_l)}$（偏好好回答而非坏回答）</li></ul></li></ul><p><strong>协同效应</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>SFT：让 P(y_w|x) 从 0.1 → 0.6（模仿能力↑）
</span></span><span class=line><span class=cl>OR： 让 Odds(y_w)/Odds(y_l) 从 1 → 5（偏好强度↑）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>结果：既提升生成能力，又建立偏好</span></span></code></pre></div><p><strong>为什么传统方法不行？</strong></p><p>传统 SFT + DPO 需要<strong>两阶段</strong>：</p><ol><li><p><strong>阶段 1（SFT）</strong>：模仿 $y_w$</p><ul><li>问题：也会模仿 $y_l$（如果它在 SFT 数据中）</li></ul></li><li><p><strong>阶段 2（DPO）</strong>：对比 $y_w$ vs $y_l$</p><ul><li>问题：需要"遗忘" SFT 阶段学到的 $y_l$</li></ul></li></ol><p>ORPO 一步到位：</p><ul><li>在模仿 $y_w$ 的同时，<strong>主动远离</strong> $y_l$</li><li>无需"遗忘"过程</li></ul><p><strong>数学表达</strong>：</p><p>ORPO 的梯度（对 $\theta$ 求导）：
$$\nabla L_{ORPO} = \underbrace{\nabla L_{SFT}(y_w)}<em>{\text{拉向 }y_w} + \lambda \cdot \left(\underbrace{\nabla \log \text{Odds}(y_w)}</em>{\text{增强 }y_w} - \underbrace{\nabla \log \text{Odds}(y_l)}_{\text{削弱 }y_l}\right)$$</p><p><strong>可视化</strong>（概率空间中的优化轨迹）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>P(y_l|x) ^
</span></span><span class=line><span class=cl>         |
</span></span><span class=line><span class=cl>    0.8  |     ×初始点
</span></span><span class=line><span class=cl>         |      ↘
</span></span><span class=line><span class=cl>    0.6  |        ↘SFT+DPO
</span></span><span class=line><span class=cl>         |          ×
</span></span><span class=line><span class=cl>    0.4  |            ↘
</span></span><span class=line><span class=cl>         |              ×终点
</span></span><span class=line><span class=cl>    0.2  |
</span></span><span class=line><span class=cl>         +-----------------&gt; P(y_w|x)
</span></span><span class=line><span class=cl>         0.2  0.4  0.6  0.8
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>ORPO路径：直接斜向右下（同时增强 y_w，削弱 y_l）
</span></span><span class=line><span class=cl>SFT+DPO：先右移（SFT），再下移（DPO）</span></span></code></pre></div><p><strong>为什么 $\lambda$ 很敏感？</strong></p><p>$\lambda$ 控制"模仿"和"对比"的权重：</p><table><thead><tr><th style=text-align:center>$\lambda$ 值</th><th style=text-align:center>SFT 权重</th><th style=text-align:center>OR 权重</th><th style=text-align:left>结果</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>100%</td><td style=text-align:center>0%</td><td style=text-align:left>退化为纯 SFT</td></tr><tr><td style=text-align:center>0.1</td><td style=text-align:center>90%</td><td style=text-align:center>10%</td><td style=text-align:left>OR 太弱，对比不足</td></tr><tr><td style=text-align:center><strong>0.5</strong></td><td style=text-align:center><strong>67%</strong></td><td style=text-align:center><strong>33%</strong></td><td style=text-align:left><strong>平衡（推荐）</strong></td></tr><tr><td style=text-align:center>1.0</td><td style=text-align:center>50%</td><td style=text-align:center>50%</td><td style=text-align:left>OR 可能过强</td></tr><tr><td style=text-align:center>5.0</td><td style=text-align:center>17%</td><td style=text-align:center>83%</td><td style=text-align:left>模型崩溃（过度对比）</td></tr></tbody></table><p><strong>实验曲线</strong>（Mistral-7B 上的结果）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>性能 ^
</span></span><span class=line><span class=cl>     |        *  (λ=0.5, 最优)
</span></span><span class=line><span class=cl> 8.5 |      *   *
</span></span><span class=line><span class=cl>     |    *       *
</span></span><span class=line><span class=cl> 8.0 |  *           *
</span></span><span class=line><span class=cl>     | *               *
</span></span><span class=line><span class=cl> 7.5 +----------------------&gt; λ
</span></span><span class=line><span class=cl>     0   0.5   1.0   2.0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>解读：
</span></span><span class=line><span class=cl>- λ=0：纯 SFT，性能 7.5
</span></span><span class=line><span class=cl>- λ=0.5：最优平衡，性能 8.7
</span></span><span class=line><span class=cl>- λ&gt;1.0：过度对比，模型退化</span></span></code></pre></div><p><strong>代价与适用场景</strong>：</p><p><strong>何时用 ORPO？</strong></p><ul><li>✅ 数据质量极高（$y_w$ 确实比 $y_l$ 好很多）</li><li>✅ 计算资源有限（只能训练一次）</li><li>✅ 任务明确（如安全对齐）</li></ul><p><strong>何时不用 ORPO？</strong></p><ul><li>❌ 数据质量参差不齐</li><li>❌ 需要精细控制（两阶段更灵活）</li><li>❌ 主观任务（如创意写作）</li></ul><h3 id=4-spin-自我对弈无需人工数据>4. SPIN: 自我对弈，无需人工数据<a class=anchor href=#4-spin-%e8%87%aa%e6%88%91%e5%af%b9%e5%bc%88%e6%97%a0%e9%9c%80%e4%ba%ba%e5%b7%a5%e6%95%b0%e6%8d%ae>#</a></h3><p><strong>SPIN (Self-Play Fine-Tuning)</strong>：模型通过与自己对弈来自我提升。</p><p><strong>算法流程</strong>：</p><ol><li><strong>迭代 t=0</strong>: 用 SFT 模型 $\pi_0$ 生成回答 $y^{gen}_0$</li><li><strong>构造偏好对</strong>: $(x, y^{SFT}, y^{gen}_0)$，其中 $y^{SFT}$ 是人工标注的"好"答案</li><li><strong>DPO 训练</strong>: 优化 $\pi_1$，使其偏好 $y^{SFT}$ 而非 $y^{gen}_0$</li><li><strong>迭代 t=1</strong>: 用 $\pi_1$ 生成新回答 $y^{gen}_1$</li><li><strong>重复</strong>: 直到模型无法区分自己的输出和 SFT 数据（收敛）</li></ol><p><strong>数学表达</strong>：
$$
\pi_{t+1} = \arg\max_\pi \mathbb{E}_{x \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi(y^{SFT}|x)}{\pi(y^{gen}_t|x)} \right) \right]
$$</p><p><strong>代码实现（伪代码）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>SPIN 自我对弈训练流程
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>DPOTrainer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 初始化模型（SFT 后的模型）</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;sft_model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 迭代训练</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>  <span class=c1># 通常 3-5 轮即可</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;=== SPIN Iteration </span><span class=si>{</span><span class=n>iteration</span><span class=si>}</span><span class=s2> ===&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 用当前模型生成回答</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_responses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>prompt</span> <span class=ow>in</span> <span class=n>prompts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>generated_responses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 构造偏好数据（SFT 数据为 chosen，生成数据为 rejected）</span>
</span></span><span class=line><span class=cl>    <span class=n>preference_data</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;prompt&#34;</span><span class=p>:</span> <span class=n>p</span><span class=p>,</span> <span class=s2>&#34;chosen&#34;</span><span class=p>:</span> <span class=n>sft_response</span><span class=p>,</span> <span class=s2>&#34;rejected&#34;</span><span class=p>:</span> <span class=n>gen_response</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span><span class=p>,</span> <span class=n>sft_response</span><span class=p>,</span> <span class=n>gen_response</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=n>sft_responses</span><span class=p>,</span> <span class=n>generated_responses</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. DPO 训练一轮</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span> <span class=o>=</span> <span class=n>DPOTrainer</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>train_dataset</span><span class=o>=</span><span class=n>preference_data</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. 评估：当生成质量接近 SFT 数据时停止</span>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>evaluate_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>accuracy</span> <span class=o>&gt;</span> <span class=mf>0.95</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>break</span></span></span></code></pre></div><p><strong>实验结果</strong>：</p><ul><li>在 GSM8K（数学推理）上，SPIN 使 Llama-2-7B 从 36% 提升到 <strong>58%</strong></li><li>无需额外标注数据，仅靠自我对弈</li></ul><p><strong>适用场景</strong>：</p><ul><li>✅ 有高质量 SFT 数据，但无偏好标注</li><li>✅ 任务有明确对错（数学、代码）</li><li>❌ 主观任务（创意写作）效果不明显</li></ul><hr><h2 id=五最新进展与趋势>五、最新进展与趋势<a class=anchor href=#%e4%ba%94%e6%9c%80%e6%96%b0%e8%bf%9b%e5%b1%95%e4%b8%8e%e8%b6%8b%e5%8a%bf>#</a></h2><h3 id=1-simpo连-reference-model-都不需要了>1. SimPO：连 Reference Model 都不需要了<a class=anchor href=#1-simpo%e8%bf%9e-reference-model-%e9%83%bd%e4%b8%8d%e9%9c%80%e8%a6%81%e4%ba%86>#</a></h3><p><strong>问题</strong>：DPO 虽然比 PPO 简单，但仍需要加载一个冻结的 Reference Model，占用显存。</p><p><strong>SimPO (Simple Preference Optimization, 2024)</strong> 的核心洞察：</p><blockquote class=book-hint><p>用<strong>序列长度归一化</strong>代替 Reference Model！</p></blockquote><h4 id=11-simpo-的数学推导>1.1 SimPO 的数学推导<a class=anchor href=#11-simpo-%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8e%a8%e5%af%bc>#</a></h4><p>传统 DPO Loss：
$$ L_{DPO} = -\log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_\theta(y_l|x)} - \beta \log \frac{\pi_{ref}(y_w|x)}{\pi_{ref}(y_l|x)} \right) $$</p><p><strong>SimPO 的替换</strong>：</p><ul><li>移除 $\pi_{ref}$ 项</li><li>对 log-prob 进行<strong>长度归一化</strong>（average log-prob）</li><li>引入 reward margin $\gamma$（类似 SVM 的 margin）</li></ul><p>$$
\boxed{
L_{SimPO} = -\log \sigma \left( \beta \left( \frac{\log \pi_\theta(y_w|x)}{|y_w|} - \frac{\log \pi_\theta(y_l|x)}{|y_l|} \right) - \gamma \right)
}
$$</p><p>其中：</p><ul><li>$|y|$: 序列长度（token 数）</li><li>$\gamma$: reward margin，通常取 0.5-2.0</li></ul><p><strong>关键优势</strong>：</p><ol><li><strong>零显存开销</strong>：不需要加载 Reference Model</li><li><strong>更好的长度泛化</strong>：长度归一化天然避免长度偏好</li><li><strong>隐式奖励更稳定</strong>：margin $\gamma$ 提供了更大的容错空间</li></ol><h4 id=12-手写-simpo-loss>1.2 手写 SimPO Loss<a class=anchor href=#12-%e6%89%8b%e5%86%99-simpo-loss>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>SimPO Loss 的 PyTorch 实现
</span></span></span><span class=line><span class=cl><span class=s2>对比 DPO：无需 Reference Model！
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>simpo_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logps</span><span class=p>,</span>    <span class=c1># π_θ(y_w|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logps</span><span class=p>,</span>  <span class=c1># π_θ(y_l|x) 的 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>chosen_lengths</span><span class=p>,</span>         <span class=c1># y_w 的长度</span>
</span></span><span class=line><span class=cl>    <span class=n>rejected_lengths</span><span class=p>,</span>       <span class=c1># y_l 的长度</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span><span class=o>=</span><span class=mf>2.0</span><span class=p>,</span>               <span class=c1># 温度系数（SimPO 通常用更大的 beta）</span>
</span></span><span class=line><span class=cl>    <span class=n>gamma</span><span class=o>=</span><span class=mf>1.0</span>               <span class=c1># reward margin</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    SimPO Loss 核心实现
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    参数:
</span></span></span><span class=line><span class=cl><span class=s2>        policy_*_logps: (batch_size,) - log-probabilities（总和）
</span></span></span><span class=line><span class=cl><span class=s2>        *_lengths: (batch_size,) - 序列长度
</span></span></span><span class=line><span class=cl><span class=s2>        beta: 温度系数（典型值 1.0-5.0）
</span></span></span><span class=line><span class=cl><span class=s2>        gamma: reward margin（典型值 0.5-2.0）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    返回:
</span></span></span><span class=line><span class=cl><span class=s2>        loss: 标量 - SimPO Loss
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 长度归一化：平均 log-prob</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_log_prob_chosen</span> <span class=o>=</span> <span class=n>policy_chosen_logps</span> <span class=o>/</span> <span class=n>chosen_lengths</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_log_prob_rejected</span> <span class=o>=</span> <span class=n>policy_rejected_logps</span> <span class=o>/</span> <span class=n>rejected_lengths</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># SimPO Loss: -log σ(β * (avg_logp_w - avg_logp_l) - γ)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>avg_log_prob_chosen</span> <span class=o>-</span> <span class=n>avg_log_prob_rejected</span><span class=p>)</span> <span class=o>-</span> <span class=n>gamma</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>logits</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 监控指标</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=c1># 隐式奖励（归一化后的）</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_margin</span> <span class=o>=</span> <span class=p>(</span><span class=n>avg_log_prob_chosen</span> <span class=o>-</span> <span class=n>avg_log_prob_rejected</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>accuracy</span> <span class=o>=</span> <span class=p>(</span><span class=n>avg_log_prob_chosen</span> <span class=o>&gt;</span> <span class=n>avg_log_prob_rejected</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;loss&#34;</span><span class=p>:</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;reward_margin&#34;</span><span class=p>:</span> <span class=n>reward_margin</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;accuracy&#34;</span><span class=p>:</span> <span class=n>accuracy</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=p>,</span> <span class=n>metrics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 对比：DPO vs SimPO</span>
</span></span><span class=line><span class=cl><span class=c1># DPO:   需要 2 个模型（policy + ref），需要计算 ref_logps</span>
</span></span><span class=line><span class=cl><span class=c1># SimPO: 只需 1 个模型（policy），直接用长度归一化</span></span></span></code></pre></div><p><strong>实验结果</strong>（2024 年论文）：</p><table><thead><tr><th style=text-align:left>任务</th><th style=text-align:center>DPO (7B)</th><th style=text-align:center>SimPO (7B)</th><th style=text-align:center>显存节省</th></tr></thead><tbody><tr><td style=text-align:left>AlpacaEval</td><td style=text-align:center>82.3%</td><td style=text-align:center><strong>85.7%</strong></td><td style=text-align:center><strong>40%</strong></td></tr><tr><td style=text-align:left>MT-Bench</td><td style=text-align:center>7.45</td><td style=text-align:center><strong>7.68</strong></td><td style=text-align:center><strong>40%</strong></td></tr></tbody></table><p><strong>适用场景</strong>：</p><ul><li>✅ 显存受限（单卡训练 7B 模型）</li><li>✅ 需要更好的长度泛化</li><li>❌ 需要强约束（DPO 的 ref model 提供更强的 KL 约束）</li></ul><h3 id=2-从-rlhf-到-rlaif-ai-feedback>2. 从 RLHF 到 RLAIF (AI Feedback)<a class=anchor href=#2-%e4%bb%8e-rlhf-%e5%88%b0-rlaif-ai-feedback>#</a></h3><p><strong>问题</strong>：人类标注成本高（$10-30/小时），速度慢，还有标注者偏见。</p><p><strong>解决方案</strong>：用 GPT-4/Claude 等强模型来生成偏好数据。</p><p><strong>Constitutional AI (Anthropic)</strong>：</p><ol><li>定义"宪法"（价值观规则），如"拒绝有害请求"</li><li>让 AI 自己评判回答是否符合宪法</li><li>用 AI 生成的偏好数据训练模型</li></ol><p><strong>实战：通用 Judge Prompt 模板</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>JUDGE_PROMPT</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>其实你是一个公正的裁判。请根据以下用户问题，对比两个助手的回答，选出更好的一个。
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>[用户问题]
</span></span></span><span class=line><span class=cl><span class=si>{prompt}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>[助手 A]
</span></span></span><span class=line><span class=cl><span class=si>{response_a}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>[助手 B]
</span></span></span><span class=line><span class=cl><span class=si>{response_b}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>[评判标准]
</span></span></span><span class=line><span class=cl><span class=s2>1. 有用性：是否解决了问题？
</span></span></span><span class=line><span class=cl><span class=s2>2. 安全性：是否有害？
</span></span></span><span class=line><span class=cl><span class=s2>3. 简洁性：是否啰嗦？
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>请输出 JSON 格式：
</span></span></span><span class=line><span class=cl><span class=s2>{{
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;reasoning&#34;: &#34;简短的分析理由...&#34;,
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;winner&#34;: &#34;A&#34; // 或 &#34;B&#34; 或 &#34;tie&#34;
</span></span></span><span class=line><span class=cl><span class=s2>}}
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><p><strong>效果</strong>：Claude-2 的 Harmlessness 指标提升 40%，完全基于 RLAIF。</p><h3 id=2-online-dpo-摆脱静态数据集>2. Online DPO: 摆脱静态数据集<a class=anchor href=#2-online-dpo-%e6%91%86%e8%84%b1%e9%9d%99%e6%80%81%e6%95%b0%e6%8d%ae%e9%9b%86>#</a></h3><p><strong>传统 DPO 问题</strong>：使用固定的偏好数据集，无法适应模型迭代。</p><p><strong>Online DPO</strong>：</p><ul><li>训练过程中实时生成 rejected 样本</li><li>每个 epoch 使用当前模型生成新的负样本</li><li>类似 SPIN，但不需要 SFT 数据作为 chosen</li></ul><p><strong>优势</strong>：</p><ul><li>数据永不过时（always on-policy）</li><li>避免分布偏移（distribution shift）</li></ul><h3 id=3-多目标对齐不只是-hhh>3. 多目标对齐：不只是 HHH<a class=anchor href=#3-%e5%a4%9a%e7%9b%ae%e6%a0%87%e5%af%b9%e9%bd%90%e4%b8%8d%e5%8f%aa%e6%98%af-hhh>#</a></h3><p>现代对齐不止考虑 Helpful、Honest、Harmless，还包括：</p><ul><li><strong>Factuality (事实性)</strong>：减少幻觉</li><li><strong>Safety (安全性)</strong>：防止 Jailbreak</li><li><strong>Reasoning (推理能力)</strong>：保持逻辑链</li><li><strong>Efficiency (效率)</strong>：生成简洁的回答（避免冗长）</li></ul><p><strong>多目标 DPO</strong>：
$$
L = \alpha_1 L_{helpful} + \alpha_2 L_{harmless} + \alpha_3 L_{factual} + \alpha_4 L_{concise}
$$</p><p>每个目标使用不同的偏好数据集，联合优化。</p><h3 id=4-对齐税-alignment-tax>4. 对齐税 (Alignment Tax)<a class=anchor href=#4-%e5%af%b9%e9%bd%90%e7%a8%8e-alignment-tax>#</a></h3><p><strong>现象</strong>：对齐训练会损害模型的原始能力（如代码生成、数学推理）。</p><p><strong>原因</strong>：</p><ul><li>过度的 safety 训练导致模型"过于谨慎"</li><li>KL 惩罚限制了模型的表达能力</li></ul><p><strong>解决方案</strong>：</p><ul><li><strong>Targeted Alignment</strong>: 只对特定领域（如 safety）做对齐，保留其他能力</li><li><strong>Iterative DPO</strong>: 多轮小步迭代，而非一次大步</li><li><strong>Weak-to-Strong Generalization</strong>: 用弱模型的偏好数据训练强模型</li></ul><h3 id=5-主流模型的对齐策略>5. 主流模型的对齐策略<a class=anchor href=#5-%e4%b8%bb%e6%b5%81%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%af%b9%e9%bd%90%e7%ad%96%e7%95%a5>#</a></h3><table><thead><tr><th style=text-align:left>模型</th><th style=text-align:left>对齐方法</th><th style=text-align:left>数据来源</th></tr></thead><tbody><tr><td style=text-align:left><strong>GPT-4</strong></td><td style=text-align:left>RLHF (PPO)</td><td style=text-align:left>人工标注 + RLAIF</td></tr><tr><td style=text-align:left><strong>Claude 3</strong></td><td style=text-align:left>Constitutional AI (DPO)</td><td style=text-align:left>完全 RLAIF</td></tr><tr><td style=text-align:left><strong>Llama-3</strong></td><td style=text-align:left>DPO + IPO</td><td style=text-align:left>人工标注</td></tr><tr><td style=text-align:left><strong>Qwen-2</strong></td><td style=text-align:left>ORPO (单阶段)</td><td style=text-align:left>人工标注 + 自我对弈</td></tr><tr><td style=text-align:left><strong>DeepSeek-V2</strong></td><td style=text-align:left>Online DPO</td><td style=text-align:left>RLAIF + 多目标对齐</td></tr></tbody></table><p><strong>趋势总结</strong>：</p><ul><li>✅ DPO 取代 PPO（稳定性 + 效率）</li><li>✅ RLAIF 取代人工标注（成本 + 规模）</li><li>✅ 单阶段训练（ORPO）成为新宠</li><li>✅ 多目标对齐成为标配</li></ul><hr><h2 id=六本章小结>六、本章小结<a class=anchor href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=核心要点>核心要点<a class=anchor href=#%e6%a0%b8%e5%bf%83%e8%a6%81%e7%82%b9>#</a></h3><ol><li><p><strong>对齐是刚需</strong>：没有对齐的模型是危险且不可用的。HHH（Helpful, Honest, Harmless）是基本原则。</p></li><li><p><strong>RLHF 已成过去式</strong>：PPO 虽然理论优雅，但工程上复杂度太高（4 个模型，不稳定），已被 DPO 取代。</p></li><li><p><strong>DPO 是现代标准</strong>：通过数学推导消除 Reward Model，将 RL 转化为分类问题，训练稳定且效果出色。</p></li><li><p><strong>多种变体各有千秋</strong>：</p><ul><li><strong>KTO</strong>: 适用于只有点赞/踩数据的场景</li><li><strong>IPO</strong>: 修复 DPO 的长度偏好问题</li><li><strong>ORPO</strong>: 单阶段训练，省时省力</li><li><strong>SPIN</strong>: 自我对弈，无需额外标注</li></ul></li><li><p><strong>未来趋势</strong>：</p><ul><li>从 Human Feedback 走向 AI Feedback (RLAIF)</li><li>从离线训练走向在线训练 (Online DPO)</li><li>从单目标走向多目标对齐</li><li>从两阶段（SFT+DPO）走向单阶段（ORPO）</li></ul></li></ol><h3 id=实践建议>实践建议<a class=anchor href=#%e5%ae%9e%e8%b7%b5%e5%bb%ba%e8%ae%ae>#</a></h3><p><strong>如果你是工程师</strong>：</p><ul><li>优先使用 <code>TRL</code> 库的 <code>DPOTrainer</code>（成熟稳定）</li><li>beta 参数从 0.1 开始调试</li><li>确保先做 SFT，再做 DPO（除非用 ORPO）</li><li>监控 KL 散度，避免模型偏离过远</li></ul><p><strong>如果你是研究员</strong>：</p><ul><li>探索 ORPO/SPIN 等单阶段方法</li><li>尝试 RLAIF（用 GPT-4 生成偏好数据）</li><li>研究多目标对齐（factuality + safety + reasoning）</li></ul><h3 id=延伸阅读>延伸阅读<a class=anchor href=#%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb>#</a></h3><p><strong>核心论文</strong>：</p><ol><li><strong>RLHF</strong>: <em>Training language models to follow instructions with human feedback</em> (OpenAI, 2022)</li><li><strong>DPO</strong>: <em>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em> (Stanford, 2023)</li><li><strong>KTO</strong>: <em>Kahneman-Tversky Optimization</em> (Cornell, 2024)</li><li><strong>ORPO</strong>: <em>Odds Ratio Preference Optimization</em> (KAIST, 2024)</li><li><strong>SPIN</strong>: <em>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</em> (UCLA, 2024)</li></ol><p><strong>工具库</strong>：</p><ul><li><a href=https://github.com/huggingface/trl>TRL (Transformer Reinforcement Learning)</a> - HuggingFace 官方库</li><li><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a> - 零代码 DPO 训练</li><li><a href=https://github.com/OpenLLMAI/OpenRLHF>OpenRLHF</a> - 开源 RLHF 框架</li></ul><hr><p><strong>下一章预告：</strong> 第4章 - 创建更优的嵌入模型</p><p>除了生成模型，Embedding 模型也是 LLM 生态的重要部分。下一章我们将探讨对比学习、InfoNCE 和 MTEB 榜单，教你训练媲美 OpenAI Ada-002 的嵌入模型。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第2章 微调你的专属模型</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/ class="flex align-center"><span>第4章 创建更优的嵌入模型</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一对齐三原则与-sft-的局限>一、对齐三原则与 SFT 的局限</a><ul><li><a href=#1-hhh-原则有用诚实无害>1. HHH 原则：有用、诚实、无害</a></li><li><a href=#2-为什么-sft-还不够>2. 为什么 SFT 还不够？</a><ul><li><a href=#21-深层原理sft-与偏好学习的数学空间本质不同>2.1 深层原理：SFT 与偏好学习的数学空间本质不同</a></li></ul></li></ul></li><li><a href=#二经典路线rlhf-ppo>二、经典路线：RLHF (PPO)</a><ul><li><a href=#1-训练-reward-model-奖励模型>1. 训练 Reward Model (奖励模型)</a><ul><li><a href=#11-bradley-terry-模型从概率到排序>1.1 Bradley-Terry 模型：从概率到排序</a></li><li><a href=#12-代码实现最小化-reward-model>1.2 代码实现：最小化 Reward Model</a></li></ul></li><li><a href=#2-ppo-算法核心kl-散度与-policy-更新>2. PPO 算法核心：KL 散度与 Policy 更新</a><ul><li><a href=#21-深度解析β-参数为什么不能太大也不能太小>2.1 深度解析：β 参数为什么不能太大也不能太小？</a></li><li><a href=#22-深度解析clipped-objective-为什么是这个形式>2.2 深度解析：Clipped Objective 为什么是这个形式？</a></li></ul></li><li><a href=#3-实战手动实现-ppo-step>3. 实战：手动实现 PPO Step</a></li></ul></li><li><a href=#三现代路线dpo-direct-preference-optimization>三、现代路线：DPO (Direct Preference Optimization)</a><ul><li><a href=#1-dpo-的数学魔术从-rlhf-到直接优化>1. DPO 的数学魔术：从 RLHF 到直接优化</a><ul><li><a href=#11-核心洞察reward-可以用-policy-表示>1.1 核心洞察：Reward 可以用 Policy 表示</a></li><li><a href=#12-逆向变换从-policy-反推-reward>1.2 逆向变换：从 Policy 反推 Reward</a></li><li><a href=#121-深度理解配分函数-zx-为什么会消掉>1.2.1 深度理解：配分函数 Z(x) 为什么会"消掉&rdquo;？</a></li><li><a href=#13-代入-bradley-terry-模型>1.3 代入 Bradley-Terry 模型</a></li><li><a href=#14-手写-dpo-losspytorch-实现>1.4 手写 DPO Loss：PyTorch 实现</a></li></ul></li><li><a href=#2-dpo-vs-ppo谁赢了>2. DPO vs PPO：谁赢了？</a><ul><li><a href=#21-深度解析dpo-为什么比-ppo-更稳定>2.1 深度解析：DPO 为什么比 PPO 更稳定？</a></li></ul></li><li><a href=#3-dpo-实战要点>3. DPO 实战要点</a></li></ul></li><li><a href=#四前沿变体kto--ipo--orpo>四、前沿变体：KTO / IPO / ORPO</a><ul><li><a href=#1-kto-如果只有赞和踩没有比较对>1. KTO: 如果只有赞和踩，没有比较对</a></li><li><a href=#2-ipo-修复-dpo-的长度偏好问题>2. IPO: 修复 DPO 的长度偏好问题</a><ul><li><a href=#21-深度解析为什么平方损失能消除长度偏好>2.1 深度解析：为什么平方损失能消除长度偏好？</a></li></ul></li><li><a href=#3-orpo-连-sft-都不需要了>3. ORPO: 连 SFT 都不需要了？</a><ul><li><a href=#31-深度解析odds-ratio-的数学魔法>3.1 深度解析：Odds Ratio 的数学魔法</a></li></ul></li><li><a href=#4-spin-自我对弈无需人工数据>4. SPIN: 自我对弈，无需人工数据</a></li></ul></li><li><a href=#五最新进展与趋势>五、最新进展与趋势</a><ul><li><a href=#1-simpo连-reference-model-都不需要了>1. SimPO：连 Reference Model 都不需要了</a><ul><li><a href=#11-simpo-的数学推导>1.1 SimPO 的数学推导</a></li><li><a href=#12-手写-simpo-loss>1.2 手写 SimPO Loss</a></li></ul></li><li><a href=#2-从-rlhf-到-rlaif-ai-feedback>2. 从 RLHF 到 RLAIF (AI Feedback)</a></li><li><a href=#2-online-dpo-摆脱静态数据集>2. Online DPO: 摆脱静态数据集</a></li><li><a href=#3-多目标对齐不只是-hhh>3. 多目标对齐：不只是 HHH</a></li><li><a href=#4-对齐税-alignment-tax>4. 对齐税 (Alignment Tax)</a></li><li><a href=#5-主流模型的对齐策略>5. 主流模型的对齐策略</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a><ul><li><a href=#核心要点>核心要点</a></li><li><a href=#实践建议>实践建议</a></li><li><a href=#延伸阅读>延伸阅读</a></li></ul></li></ul></nav></div></aside></main></body></html>