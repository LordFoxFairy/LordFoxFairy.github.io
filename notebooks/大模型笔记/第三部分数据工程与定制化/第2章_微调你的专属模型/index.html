<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第2章 微调你的专属模型：从原理到实战的完全指南# “微调不是魔法，而是精准的外科手术 —— 在冻结的知识海洋中，只激活你需要的那几个神经元。”
目录# 引言：为什么需要微调？ 一、微调的本质：Loss函数视角 1.1 预训练 vs 微调：目标函数的差异 1.2 SFT Loss 图解：Token级掩码表 二、显存账单：为什么全量微调这么贵？ 2.1 显存占用的四大开销 2.2 AdamW的12字节秘密 2.3 全量微调 vs PEFT 显存对比 三、LoRA核心：低秩适配的数学本质 3.1 核心公式：秩分解 3.2 LoRA架构图 3.3 深度问答：为什么不能全0初始化？ 3.4 PyTorch原生实现：LoRALinear 四、LoRA家族演进：从QLoRA到GaLore 4.1 QLoRA：量化的艺术 4.1.1 NF4量化原理 4.2 DoRA：方向与幅度的解耦 4.2.1 DoRA完整实现 4.3 GaLore：梯度低秩投影 4.3.1 原理推导 4.3.2 算法流程 4.4 LoRA+：非对称学习率 4.5 其他变种简介 AdaLoRA：自适应秩分配 VeRA：极致参数效率 五、微调深度理解 5.1 指令数据构建的艺术 (1) 指令数据的黄金标准 (2) Self-Instruct: 用GPT-4生成训练数据 (3) 数据增强技术 5.2 灾难性遗忘 (Catastrophic Forgetting) 缓解策略1: 混合训练数据 缓解策略2: Elastic Weight Consolidation (EWC) 5.3 多任务微调 (Multi-task Fine-tuning) (1) 任务标识符 (Task Prefix) (2) 任务特定适配器 5.4 持续学习 (Continual Learning) (1) 渐进式LoRA (Progressive LoRA) (2) 知识蒸馏 (Knowledge Distillation) (3) 完整持续学习流程 六、工程实战：用TRL库微调模型 6.1 完整训练流程 6.2 关键技术详解 6.2.1 Chat Template的正确使用 6.2.2 Padding避坑指南 6.2.3 NEFTune：嵌入层加噪技巧 6.3 推理与部署 七、模型合并技术 7.1 线性插值合并 (Weight Averaging) 7.2 SLERP: 球面线性插值 7.3 TIES: 修剪、选举与合并 7.4 DARE: 丢弃与重缩放 7.5 Task Arithmetic: 任务算术 7.6 合并方法对比与选择 八、总结：微调知识地图 8.1 核心公式速查表 8.2 方法选择决策树 8.3 实战建议清单 8.3.1 数据准备 8.3.2 超参数调优 8.3.3 显存优化 8.3.4 训练技巧 8.3.5 推理优化 8.4 常见问题排查 8.5 进阶资源 💡 新手问答：从困惑到理解 结语：微调的艺术与科学 引言：为什么需要微调？# 想象一下，你拥有一位博学的教授（预训练模型），他知晓天文地理，但对你公司的业务一无所知。微调（Fine-tuning）就像是给他补习专业课程，让他在保留通用知识的同时，掌握你的领域专长。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第2章 微调你的专属模型"><meta property="og:description" content="第2章 微调你的专属模型：从原理到实战的完全指南# “微调不是魔法，而是精准的外科手术 —— 在冻结的知识海洋中，只激活你需要的那几个神经元。”
目录# 引言：为什么需要微调？ 一、微调的本质：Loss函数视角 1.1 预训练 vs 微调：目标函数的差异 1.2 SFT Loss 图解：Token级掩码表 二、显存账单：为什么全量微调这么贵？ 2.1 显存占用的四大开销 2.2 AdamW的12字节秘密 2.3 全量微调 vs PEFT 显存对比 三、LoRA核心：低秩适配的数学本质 3.1 核心公式：秩分解 3.2 LoRA架构图 3.3 深度问答：为什么不能全0初始化？ 3.4 PyTorch原生实现：LoRALinear 四、LoRA家族演进：从QLoRA到GaLore 4.1 QLoRA：量化的艺术 4.1.1 NF4量化原理 4.2 DoRA：方向与幅度的解耦 4.2.1 DoRA完整实现 4.3 GaLore：梯度低秩投影 4.3.1 原理推导 4.3.2 算法流程 4.4 LoRA+：非对称学习率 4.5 其他变种简介 AdaLoRA：自适应秩分配 VeRA：极致参数效率 五、微调深度理解 5.1 指令数据构建的艺术 (1) 指令数据的黄金标准 (2) Self-Instruct: 用GPT-4生成训练数据 (3) 数据增强技术 5.2 灾难性遗忘 (Catastrophic Forgetting) 缓解策略1: 混合训练数据 缓解策略2: Elastic Weight Consolidation (EWC) 5.3 多任务微调 (Multi-task Fine-tuning) (1) 任务标识符 (Task Prefix) (2) 任务特定适配器 5.4 持续学习 (Continual Learning) (1) 渐进式LoRA (Progressive LoRA) (2) 知识蒸馏 (Knowledge Distillation) (3) 完整持续学习流程 六、工程实战：用TRL库微调模型 6.1 完整训练流程 6.2 关键技术详解 6.2.1 Chat Template的正确使用 6.2.2 Padding避坑指南 6.2.3 NEFTune：嵌入层加噪技巧 6.3 推理与部署 七、模型合并技术 7.1 线性插值合并 (Weight Averaging) 7.2 SLERP: 球面线性插值 7.3 TIES: 修剪、选举与合并 7.4 DARE: 丢弃与重缩放 7.5 Task Arithmetic: 任务算术 7.6 合并方法对比与选择 八、总结：微调知识地图 8.1 核心公式速查表 8.2 方法选择决策树 8.3 实战建议清单 8.3.1 数据准备 8.3.2 超参数调优 8.3.3 显存优化 8.3.4 训练技巧 8.3.5 推理优化 8.4 常见问题排查 8.5 进阶资源 💡 新手问答：从困惑到理解 结语：微调的艺术与科学 引言：为什么需要微调？# 想象一下，你拥有一位博学的教授（预训练模型），他知晓天文地理，但对你公司的业务一无所知。微调（Fine-tuning）就像是给他补习专业课程，让他在保留通用知识的同时，掌握你的领域专长。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第2章 微调你的专属模型"><meta itemprop=description content="第2章 微调你的专属模型：从原理到实战的完全指南# “微调不是魔法，而是精准的外科手术 —— 在冻结的知识海洋中，只激活你需要的那几个神经元。”
目录# 引言：为什么需要微调？ 一、微调的本质：Loss函数视角 1.1 预训练 vs 微调：目标函数的差异 1.2 SFT Loss 图解：Token级掩码表 二、显存账单：为什么全量微调这么贵？ 2.1 显存占用的四大开销 2.2 AdamW的12字节秘密 2.3 全量微调 vs PEFT 显存对比 三、LoRA核心：低秩适配的数学本质 3.1 核心公式：秩分解 3.2 LoRA架构图 3.3 深度问答：为什么不能全0初始化？ 3.4 PyTorch原生实现：LoRALinear 四、LoRA家族演进：从QLoRA到GaLore 4.1 QLoRA：量化的艺术 4.1.1 NF4量化原理 4.2 DoRA：方向与幅度的解耦 4.2.1 DoRA完整实现 4.3 GaLore：梯度低秩投影 4.3.1 原理推导 4.3.2 算法流程 4.4 LoRA+：非对称学习率 4.5 其他变种简介 AdaLoRA：自适应秩分配 VeRA：极致参数效率 五、微调深度理解 5.1 指令数据构建的艺术 (1) 指令数据的黄金标准 (2) Self-Instruct: 用GPT-4生成训练数据 (3) 数据增强技术 5.2 灾难性遗忘 (Catastrophic Forgetting) 缓解策略1: 混合训练数据 缓解策略2: Elastic Weight Consolidation (EWC) 5.3 多任务微调 (Multi-task Fine-tuning) (1) 任务标识符 (Task Prefix) (2) 任务特定适配器 5.4 持续学习 (Continual Learning) (1) 渐进式LoRA (Progressive LoRA) (2) 知识蒸馏 (Knowledge Distillation) (3) 完整持续学习流程 六、工程实战：用TRL库微调模型 6.1 完整训练流程 6.2 关键技术详解 6.2.1 Chat Template的正确使用 6.2.2 Padding避坑指南 6.2.3 NEFTune：嵌入层加噪技巧 6.3 推理与部署 七、模型合并技术 7.1 线性插值合并 (Weight Averaging) 7.2 SLERP: 球面线性插值 7.3 TIES: 修剪、选举与合并 7.4 DARE: 丢弃与重缩放 7.5 Task Arithmetic: 任务算术 7.6 合并方法对比与选择 八、总结：微调知识地图 8.1 核心公式速查表 8.2 方法选择决策树 8.3 实战建议清单 8.3.1 数据准备 8.3.2 超参数调优 8.3.3 显存优化 8.3.4 训练技巧 8.3.5 推理优化 8.4 常见问题排查 8.5 进阶资源 💡 新手问答：从困惑到理解 结语：微调的艺术与科学 引言：为什么需要微调？# 想象一下，你拥有一位博学的教授（预训练模型），他知晓天文地理，但对你公司的业务一无所知。微调（Fine-tuning）就像是给他补习专业课程，让他在保留通用知识的同时，掌握你的领域专长。"><meta itemprop=wordCount content="3283"><title>第2章 微调你的专属模型 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle checked>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/ class=active>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第2章 微调你的专属模型</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#引言为什么需要微调>引言：为什么需要微调？</a></li><li><a href=#一微调的本质loss函数视角>一、微调的本质：Loss函数视角</a><ul><li><a href=#11-预训练-vs-微调目标函数的差异>1.1 预训练 vs 微调：目标函数的差异</a></li><li><a href=#12-sft-loss-图解token级掩码表>1.2 SFT Loss 图解：Token级掩码表</a></li></ul></li><li><a href=#二显存账单为什么全量微调这么贵>二、显存账单：为什么全量微调这么贵？</a><ul><li><a href=#21-显存占用的四大开销>2.1 显存占用的四大开销</a></li><li><a href=#22-adamw的12字节秘密>2.2 AdamW的12字节秘密</a></li><li><a href=#23-全量微调-vs-peft-显存对比>2.3 全量微调 vs PEFT 显存对比</a></li></ul></li><li><a href=#三lora核心低秩适配的数学本质>三、LoRA核心：低秩适配的数学本质</a><ul><li><a href=#31-核心公式秩分解>3.1 核心公式：秩分解</a></li><li><a href=#32-lora架构图>3.2 LoRA架构图</a></li><li><a href=#33-深度问答为什么不能全0初始化>3.3 深度问答：为什么不能全0初始化？</a></li><li><a href=#34-pytorch原生实现loralinear>3.4 PyTorch原生实现：LoRALinear</a></li></ul></li><li><a href=#四lora家族演进从qlora到galore>四、LoRA家族演进：从QLoRA到GaLore</a><ul><li><a href=#41-qlora量化的艺术>4.1 QLoRA：量化的艺术</a><ul><li><a href=#411-nf4量化原理>4.1.1 NF4量化原理</a></li></ul></li><li><a href=#42-dora方向与幅度的解耦>4.2 DoRA：方向与幅度的解耦</a><ul><li><a href=#421-dora完整实现>4.2.1 DoRA完整实现</a></li></ul></li><li><a href=#43-galore梯度低秩投影>4.3 GaLore：梯度低秩投影</a><ul><li><a href=#431-原理推导>4.3.1 原理推导</a></li><li><a href=#432-算法流程>4.3.2 算法流程</a></li></ul></li><li><a href=#44-lora非对称学习率>4.4 LoRA+：非对称学习率</a></li><li><a href=#45-其他变种简介>4.5 其他变种简介</a><ul><li><a href=#adalora自适应秩分配>AdaLoRA：自适应秩分配</a></li><li><a href=#vera极致参数效率>VeRA：极致参数效率</a></li></ul></li></ul></li><li><a href=#五微调深度理解>五、微调深度理解</a><ul><li><a href=#51-指令数据构建的艺术>5.1 指令数据构建的艺术</a><ul><li><a href=#1-指令数据的黄金标准>(1) 指令数据的黄金标准</a></li><li><a href=#2-self-instruct-用gpt-4生成训练数据>(2) Self-Instruct: 用GPT-4生成训练数据</a></li><li><a href=#3-数据增强技术>(3) 数据增强技术</a></li></ul></li><li><a href=#52-灾难性遗忘-catastrophic-forgetting>5.2 灾难性遗忘 (Catastrophic Forgetting)</a><ul><li><a href=#缓解策略1-混合训练数据>缓解策略1: 混合训练数据</a></li><li><a href=#缓解策略2-elastic-weight-consolidation-ewc>缓解策略2: Elastic Weight Consolidation (EWC)</a></li></ul></li><li><a href=#53-多任务微调-multi-task-fine-tuning>5.3 多任务微调 (Multi-task Fine-tuning)</a><ul><li><a href=#1-任务标识符-task-prefix>(1) 任务标识符 (Task Prefix)</a></li><li><a href=#2-任务特定适配器>(2) 任务特定适配器</a></li></ul></li><li><a href=#54-持续学习-continual-learning>5.4 持续学习 (Continual Learning)</a><ul><li><a href=#1-渐进式lora-progressive-lora>(1) 渐进式LoRA (Progressive LoRA)</a></li><li><a href=#2-知识蒸馏-knowledge-distillation>(2) 知识蒸馏 (Knowledge Distillation)</a></li><li><a href=#3-完整持续学习流程>(3) 完整持续学习流程</a></li></ul></li></ul></li><li><a href=#六工程实战用trl库微调模型>六、工程实战：用TRL库微调模型</a><ul><li><a href=#61-完整训练流程>6.1 完整训练流程</a></li><li><a href=#62-关键技术详解>6.2 关键技术详解</a><ul><li><a href=#621-chat-template的正确使用>6.2.1 Chat Template的正确使用</a></li><li><a href=#622-padding避坑指南>6.2.2 Padding避坑指南</a></li><li><a href=#623-neftune嵌入层加噪技巧>6.2.3 NEFTune：嵌入层加噪技巧</a></li></ul></li><li><a href=#63-推理与部署>6.3 推理与部署</a></li></ul></li><li><a href=#七模型合并技术>七、模型合并技术</a><ul><li><a href=#71-线性插值合并-weight-averaging>7.1 线性插值合并 (Weight Averaging)</a></li><li><a href=#72-slerp-球面线性插值>7.2 SLERP: 球面线性插值</a></li><li><a href=#73-ties-修剪选举与合并-trim-elect-sign--merge>7.3 TIES: 修剪、选举与合并 (Trim, Elect, Sign & Merge)</a></li><li><a href=#74-dare-丢弃与重缩放-drop-and-rescale>7.4 DARE: 丢弃与重缩放 (Drop and Rescale)</a></li><li><a href=#75-task-arithmetic-任务算术>7.5 Task Arithmetic: 任务算术</a></li><li><a href=#76-合并方法对比与选择>7.6 合并方法对比与选择</a></li></ul></li><li><a href=#八总结微调知识地图>八、总结：微调知识地图</a><ul><li><a href=#81-核心公式速查表>8.1 核心公式速查表</a></li><li><a href=#82-方法选择决策树>8.2 方法选择决策树</a></li><li><a href=#83-实战建议清单>8.3 实战建议清单</a><ul><li><a href=#831-数据准备>8.3.1 数据准备</a></li><li><a href=#832-超参数调优>8.3.2 超参数调优</a></li><li><a href=#833-显存优化>8.3.3 显存优化</a></li><li><a href=#834-训练技巧>8.3.4 训练技巧</a></li><li><a href=#835-推理优化>8.3.5 推理优化</a></li></ul></li><li><a href=#84-常见问题排查>8.4 常见问题排查</a></li><li><a href=#85-进阶资源>8.5 进阶资源</a></li></ul></li><li><a href=#-新手问答从困惑到理解>💡 新手问答：从困惑到理解</a></li><li><a href=#结语微调的艺术与科学>结语：微调的艺术与科学</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第2章-微调你的专属模型从原理到实战的完全指南>第2章 微调你的专属模型：从原理到实战的完全指南<a class=anchor href=#%e7%ac%ac2%e7%ab%a0-%e5%be%ae%e8%b0%83%e4%bd%a0%e7%9a%84%e4%b8%93%e5%b1%9e%e6%a8%a1%e5%9e%8b%e4%bb%8e%e5%8e%9f%e7%90%86%e5%88%b0%e5%ae%9e%e6%88%98%e7%9a%84%e5%ae%8c%e5%85%a8%e6%8c%87%e5%8d%97>#</a></h1><blockquote class=book-hint><p>&ldquo;微调不是魔法，而是精准的外科手术 —— 在冻结的知识海洋中，只激活你需要的那几个神经元。”</p></blockquote><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%be%ae%e8%b0%83>引言：为什么需要微调？</a></li><li><a href=#%e4%b8%80%e5%be%ae%e8%b0%83%e7%9a%84%e6%9c%ac%e8%b4%a8loss%e5%87%bd%e6%95%b0%e8%a7%86%e8%a7%92>一、微调的本质：Loss函数视角</a><ul><li><a href=#11-%e9%a2%84%e8%ae%ad%e7%bb%83-vs-%e5%be%ae%e8%b0%83%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e7%9a%84%e5%b7%ae%e5%bc%82>1.1 预训练 vs 微调：目标函数的差异</a></li><li><a href=#12-sft-loss-%e5%9b%be%e8%a7%a3token%e7%ba%a7%e6%8e%a9%e7%a0%81%e8%a1%a8>1.2 SFT Loss 图解：Token级掩码表</a></li></ul></li><li><a href=#%e4%ba%8c%e6%98%be%e5%ad%98%e8%b4%a6%e5%8d%95%e4%b8%ba%e4%bb%80%e4%b9%88%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83%e8%bf%99%e4%b9%88%e8%b4%b5>二、显存账单：为什么全量微调这么贵？</a><ul><li><a href=#21-%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e7%9a%84%e5%9b%9b%e5%a4%a7%e5%bc%80%e9%94%80>2.1 显存占用的四大开销</a></li><li><a href=#22-adamw%e7%9a%8412%e5%ad%97%e8%8a%82%e7%a7%98%e5%af%86>2.2 AdamW的12字节秘密</a></li><li><a href=#23-%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83-vs-peft-%e6%98%be%e5%ad%98%e5%af%b9%e6%af%94>2.3 全量微调 vs PEFT 显存对比</a></li></ul></li><li><a href=#%e4%b8%89lora%e6%a0%b8%e5%bf%83%e4%bd%8e%e7%a7%a9%e9%80%82%e9%85%8d%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8>三、LoRA核心：低秩适配的数学本质</a><ul><li><a href=#31-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e7%a7%a9%e5%88%86%e8%a7%a3>3.1 核心公式：秩分解</a></li><li><a href=#32-lora%e6%9e%b6%e6%9e%84%e5%9b%be>3.2 LoRA架构图</a></li><li><a href=#33-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e5%85%a80%e5%88%9d%e5%a7%8b%e5%8c%96>3.3 深度问答：为什么不能全0初始化？</a></li><li><a href=#34-pytorch%e5%8e%9f%e7%94%9f%e5%ae%9e%e7%8e%b0loralinear>3.4 PyTorch原生实现：LoRALinear</a></li></ul></li><li><a href=#%e5%9b%9blora%e5%ae%b6%e6%97%8f%e6%bc%94%e8%bf%9b%e4%bb%8eqlora%e5%88%b0galore>四、LoRA家族演进：从QLoRA到GaLore</a><ul><li><a href=#41-qlora%e9%87%8f%e5%8c%96%e7%9a%84%e8%89%ba%e6%9c%af>4.1 QLoRA：量化的艺术</a><ul><li><a href=#411-nf4%e9%87%8f%e5%8c%96%e5%8e%9f%e7%90%86>4.1.1 NF4量化原理</a></li></ul></li><li><a href=#42-dora%e6%96%b9%e5%90%91%e4%b8%8e%e5%b9%85%e5%ba%a6%e7%9a%84%e8%a7%a3%e8%80%a6>4.2 DoRA：方向与幅度的解耦</a><ul><li><a href=#421-dora%e5%ae%8c%e6%95%b4%e5%ae%9e%e7%8e%b0>4.2.1 DoRA完整实现</a></li></ul></li><li><a href=#43-galore%e6%a2%af%e5%ba%a6%e4%bd%8e%e7%a7%a9%e6%8a%95%e5%bd%b1>4.3 GaLore：梯度低秩投影</a><ul><li><a href=#431-%e5%8e%9f%e7%90%86%e6%8e%a8%e5%af%bc>4.3.1 原理推导</a></li><li><a href=#432-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>4.3.2 算法流程</a></li></ul></li><li><a href=#44-lora%e9%9d%9e%e5%af%b9%e7%a7%b0%e5%ad%a6%e4%b9%a0%e7%8e%87>4.4 LoRA+：非对称学习率</a></li><li><a href=#45-%e5%85%b6%e4%bb%96%e5%8f%98%e7%a7%8d%e7%ae%80%e4%bb%8b>4.5 其他变种简介</a><ul><li><a href=#adalora%e8%87%aa%e9%80%82%e5%ba%94%e7%a7%a9%e5%88%86%e9%85%8d>AdaLoRA：自适应秩分配</a></li><li><a href=#vera%e6%9e%81%e8%87%b4%e5%8f%82%e6%95%b0%e6%95%88%e7%8e%87>VeRA：极致参数效率</a></li></ul></li></ul></li><li><a href=#%e4%ba%94%e5%be%ae%e8%b0%83%e6%b7%b1%e5%ba%a6%e7%90%86%e8%a7%a3>五、微调深度理解</a><ul><li><a href=#51-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e6%9e%84%e5%bb%ba%e7%9a%84%e8%89%ba%e6%9c%af>5.1 指令数据构建的艺术</a><ul><li><a href=#1-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e7%9a%84%e9%bb%84%e9%87%91%e6%a0%87%e5%87%86>(1) 指令数据的黄金标准</a></li><li><a href=#2-self-instruct-%e7%94%a8gpt-4%e7%94%9f%e6%88%90%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae>(2) Self-Instruct: 用GPT-4生成训练数据</a></li><li><a href=#3-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%e6%8a%80%e6%9c%af>(3) 数据增强技术</a></li></ul></li><li><a href=#52-%e7%81%be%e9%9a%be%e6%80%a7%e9%81%97%e5%bf%98-catastrophic-forgetting>5.2 灾难性遗忘 (Catastrophic Forgetting)</a><ul><li><a href=#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a51-%e6%b7%b7%e5%90%88%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae>缓解策略1: 混合训练数据</a></li><li><a href=#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a52-elastic-weight-consolidation-ewc>缓解策略2: Elastic Weight Consolidation (EWC)</a></li></ul></li><li><a href=#53-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%be%ae%e8%b0%83-multi-task-fine-tuning>5.3 多任务微调 (Multi-task Fine-tuning)</a><ul><li><a href=#1-%e4%bb%bb%e5%8a%a1%e6%a0%87%e8%af%86%e7%ac%a6-task-prefix>(1) 任务标识符 (Task Prefix)</a></li><li><a href=#2-%e4%bb%bb%e5%8a%a1%e7%89%b9%e5%ae%9a%e9%80%82%e9%85%8d%e5%99%a8>(2) 任务特定适配器</a></li></ul></li><li><a href=#54-%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0-continual-learning>5.4 持续学习 (Continual Learning)</a><ul><li><a href=#1-%e6%b8%90%e8%bf%9b%e5%bc%8flora-progressive-lora>(1) 渐进式LoRA (Progressive LoRA)</a></li><li><a href=#2-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f-knowledge-distillation>(2) 知识蒸馏 (Knowledge Distillation)</a></li><li><a href=#3-%e5%ae%8c%e6%95%b4%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0%e6%b5%81%e7%a8%8b>(3) 完整持续学习流程</a></li></ul></li></ul></li><li><a href=#%e5%85%ad%e5%b7%a5%e7%a8%8b%e5%ae%9e%e6%88%98%e7%94%a8trl%e5%ba%93%e5%be%ae%e8%b0%83%e6%a8%a1%e5%9e%8b>六、工程实战：用TRL库微调模型</a><ul><li><a href=#61-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b>6.1 完整训练流程</a></li><li><a href=#62-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3>6.2 关键技术详解</a><ul><li><a href=#621-chat-template%e7%9a%84%e6%ad%a3%e7%a1%ae%e4%bd%bf%e7%94%a8>6.2.1 Chat Template的正确使用</a></li><li><a href=#622-padding%e9%81%bf%e5%9d%91%e6%8c%87%e5%8d%97>6.2.2 Padding避坑指南</a></li><li><a href=#623-neftune%e5%b5%8c%e5%85%a5%e5%b1%82%e5%8a%a0%e5%99%aa%e6%8a%80%e5%b7%a7>6.2.3 NEFTune：嵌入层加噪技巧</a></li></ul></li><li><a href=#63-%e6%8e%a8%e7%90%86%e4%b8%8e%e9%83%a8%e7%bd%b2>6.3 推理与部署</a></li></ul></li><li><a href=#%e4%b8%83%e6%a8%a1%e5%9e%8b%e5%90%88%e5%b9%b6%e6%8a%80%e6%9c%af>七、模型合并技术</a><ul><li><a href=#71-%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc%e5%90%88%e5%b9%b6-weight-averaging>7.1 线性插值合并 (Weight Averaging)</a></li><li><a href=#72-slerp-%e7%90%83%e9%9d%a2%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc>7.2 SLERP: 球面线性插值</a></li><li><a href=#73-ties-%e4%bf%ae%e5%89%aa%e9%80%89%e4%b8%be%e4%b8%8e%e5%90%88%e5%b9%b6>7.3 TIES: 修剪、选举与合并</a></li><li><a href=#74-dare-%e4%b8%a2%e5%bc%83%e4%b8%8e%e9%87%8d%e7%bc%a9%e6%94%be>7.4 DARE: 丢弃与重缩放</a></li><li><a href=#75-task-arithmetic-%e4%bb%bb%e5%8a%a1%e7%ae%97%e6%9c%af>7.5 Task Arithmetic: 任务算术</a></li><li><a href=#76-%e5%90%88%e5%b9%b6%e6%96%b9%e6%b3%95%e5%af%b9%e6%af%94%e4%b8%8e%e9%80%89%e6%8b%a9>7.6 合并方法对比与选择</a></li></ul></li><li><a href=#%e5%85%ab%e6%80%bb%e7%bb%93%e5%be%ae%e8%b0%83%e7%9f%a5%e8%af%86%e5%9c%b0%e5%9b%be>八、总结：微调知识地图</a><ul><li><a href=#81-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5%e8%a1%a8>8.1 核心公式速查表</a></li><li><a href=#82-%e6%96%b9%e6%b3%95%e9%80%89%e6%8b%a9%e5%86%b3%e7%ad%96%e6%a0%91>8.2 方法选择决策树</a></li><li><a href=#83-%e5%ae%9e%e6%88%98%e5%bb%ba%e8%ae%ae%e6%b8%85%e5%8d%95>8.3 实战建议清单</a><ul><li><a href=#831-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87>8.3.1 数据准备</a></li><li><a href=#832-%e8%b6%85%e5%8f%82%e6%95%b0%e8%b0%83%e4%bc%98>8.3.2 超参数调优</a></li><li><a href=#833-%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96>8.3.3 显存优化</a></li><li><a href=#834-%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7>8.3.4 训练技巧</a></li><li><a href=#835-%e6%8e%a8%e7%90%86%e4%bc%98%e5%8c%96>8.3.5 推理优化</a></li></ul></li><li><a href=#84-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98%e6%8e%92%e6%9f%a5>8.4 常见问题排查</a></li><li><a href=#85-%e8%bf%9b%e9%98%b6%e8%b5%84%e6%ba%90>8.5 进阶资源</a></li></ul></li><li><a href=#-%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94%e4%bb%8e%e5%9b%b0%e6%83%91%e5%88%b0%e7%90%86%e8%a7%a3>💡 新手问答：从困惑到理解</a></li><li><a href=#%e7%bb%93%e8%af%ad%e5%be%ae%e8%b0%83%e7%9a%84%e8%89%ba%e6%9c%af%e4%b8%8e%e7%a7%91%e5%ad%a6>结语：微调的艺术与科学</a></li></ul><h2 id=引言为什么需要微调>引言：为什么需要微调？<a class=anchor href=#%e5%bc%95%e8%a8%80%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%be%ae%e8%b0%83>#</a></h2><p>想象一下，你拥有一位博学的教授（预训练模型），他知晓天文地理，但对你公司的业务一无所知。微调（Fine-tuning）就像是给他补习专业课程，让他在保留通用知识的同时，掌握你的领域专长。</p><p><strong>本章学习路径</strong>：</p><pre class=mermaid>graph LR
    A[微调本质] --&gt; B[显存优化]
    B --&gt; C[LoRA核心原理]
    C --&gt; D[LoRA家族演进]
    D --&gt; E[工程实战]
    E --&gt; F[模型合并]

    style A fill:#e1f5ff
    style C fill:#fff3e0
    style E fill:#f3e5f5</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><hr><h2 id=一微调的本质loss函数视角>一、微调的本质：Loss函数视角<a class=anchor href=#%e4%b8%80%e5%be%ae%e8%b0%83%e7%9a%84%e6%9c%ac%e8%b4%a8loss%e5%87%bd%e6%95%b0%e8%a7%86%e8%a7%92>#</a></h2><h3 id=11-预训练-vs-微调目标函数的差异>1.1 预训练 vs 微调：目标函数的差异<a class=anchor href=#11-%e9%a2%84%e8%ae%ad%e7%bb%83-vs-%e5%be%ae%e8%b0%83%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e7%9a%84%e5%b7%ae%e5%bc%82>#</a></h3><table><thead><tr><th>阶段</th><th>目标函数</th><th>学习内容</th><th>数据规模</th></tr></thead><tbody><tr><td><strong>预训练</strong></td><td>$\mathcal{L}<em>{\text{pretrain}} = -\sum</em>{t=1}^{T} \log P(x_t \mid x_{&lt;t})$</td><td>通用语言模式</td><td>TB级</td></tr><tr><td><strong>微调</strong></td><td>$\mathcal{L}<em>{\text{SFT}} = -\sum</em>{t \in \text{answer}} \log P(x_t \mid x_{&lt;t})$</td><td>特定任务格式</td><td>GB级</td></tr></tbody></table><p><strong>关键差异</strong>：微调只计算<strong>答案部分</strong>的损失，这通过 <strong>Token级掩码（Mask）</strong> 实现。</p><h3 id=12-sft-loss-图解token级掩码表>1.2 SFT Loss 图解：Token级掩码表<a class=anchor href=#12-sft-loss-%e5%9b%be%e8%a7%a3token%e7%ba%a7%e6%8e%a9%e7%a0%81%e8%a1%a8>#</a></h3><p>假设我们要微调一个医疗问答模型，输入序列是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>[BOS] 用户: 头痛怎么办? [SEP] 助手: 建议多休息，必要时就医。[EOS]</span></span></code></pre></div><p><strong>Token级Loss计算表</strong>：</p><table><thead><tr><th>Position</th><th>Token</th><th>计算Loss？</th><th>原因</th></tr></thead><tbody><tr><td>0</td><td><code>[BOS]</code></td><td>❌</td><td>起始符</td></tr><tr><td>1</td><td><code>用户</code></td><td>❌</td><td>指令部分</td></tr><tr><td>2</td><td><code>:</code></td><td>❌</td><td>指令部分</td></tr><tr><td>3</td><td><code>头痛</code></td><td>❌</td><td>指令部分</td></tr><tr><td>4</td><td><code>怎么办</code></td><td>❌</td><td>指令部分</td></tr><tr><td>5</td><td><code>?</code></td><td>❌</td><td>指令部分</td></tr><tr><td>6</td><td><code>[SEP]</code></td><td>❌</td><td>分隔符</td></tr><tr><td>7</td><td><code>助手</code></td><td>❌</td><td>答案前缀</td></tr><tr><td>8</td><td><code>:</code></td><td>❌</td><td>答案前缀</td></tr><tr><td>9</td><td><code>建议</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>10</td><td><code>多</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>11</td><td><code>休息</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>12</td><td><code>，</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>13</td><td><code>必要时</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>14</td><td><code>就医</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>15</td><td><code>。</code></td><td>✅</td><td><strong>答案内容</strong></td></tr><tr><td>16</td><td><code>[EOS]</code></td><td>✅</td><td><strong>答案内容</strong></td></tr></tbody></table><p><strong>代码实现</strong>（PyTorch）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_sft_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        logits: [batch, seq_len, vocab_size]
</span></span></span><span class=line><span class=cl><span class=s2>        labels: [batch, seq_len]  # -100 表示忽略该位置
</span></span></span><span class=line><span class=cl><span class=s2>        attention_mask: [batch, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 只计算labels != -100的位置</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>ignore_index</span><span class=o>=-</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>shift_logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>shift_labels</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>:]</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>shift_logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>shift_logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>        <span class=n>shift_labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实际使用：构造labels</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_sft_labels</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>answer_start_idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;只在答案部分计算loss&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span><span class=p>[:,</span> <span class=p>:</span><span class=n>answer_start_idx</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=mi>100</span>  <span class=c1># 掩码指令部分</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span></span></span></code></pre></div><p><strong>可视化损失计算</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>指令部分（掩码）         答案部分（计算Loss）
</span></span><span class=line><span class=cl>┌──────────────┐        ┌──────────────┐
</span></span><span class=line><span class=cl>│ 用户: 头痛... │ -100   │ 建议多休息... │ Loss
</span></span><span class=line><span class=cl>└──────────────┘        └──────────────┘
</span></span><span class=line><span class=cl>      ↓                        ↓
</span></span><span class=line><span class=cl>  不反向传播                 更新梯度</span></span></code></pre></div><hr><h2 id=二显存账单为什么全量微调这么贵>二、显存账单：为什么全量微调这么贵？<a class=anchor href=#%e4%ba%8c%e6%98%be%e5%ad%98%e8%b4%a6%e5%8d%95%e4%b8%ba%e4%bb%80%e4%b9%88%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83%e8%bf%99%e4%b9%88%e8%b4%b5>#</a></h2><h3 id=21-显存占用的四大开销>2.1 显存占用的四大开销<a class=anchor href=#21-%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8%e7%9a%84%e5%9b%9b%e5%a4%a7%e5%bc%80%e9%94%80>#</a></h3><p>假设模型参数量为 $\Phi$（单位：十亿参数）：</p><table><thead><tr><th>项目</th><th>每参数开销</th><th>7B模型占用</th><th>公式</th></tr></thead><tbody><tr><td><strong>模型参数</strong></td><td>2 bytes (FP16)</td><td>14 GB</td><td>$2\Phi$</td></tr><tr><td><strong>梯度</strong></td><td>2 bytes</td><td>14 GB</td><td>$2\Phi$</td></tr><tr><td><strong>优化器状态</strong></td><td>8 bytes</td><td>56 GB</td><td>$8\Phi$</td></tr><tr><td><strong>激活值</strong></td><td>可变</td><td>~10 GB</td><td>取决于batch size</td></tr><tr><td><strong>总计</strong></td><td>-</td><td><strong>~94 GB</strong></td><td>$12\Phi + \text{激活}$</td></tr></tbody></table><h3 id=22-adamw的12字节秘密>2.2 AdamW的12字节秘密<a class=anchor href=#22-adamw%e7%9a%8412%e5%ad%97%e8%8a%82%e7%a7%98%e5%af%86>#</a></h3><p><strong>AdamW优化器</strong>需要存储：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码：AdamW状态</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer_state</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;m&#39;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>param</span><span class=p>),</span>      <span class=c1># 一阶矩（均值） - 4 bytes (FP32)</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;v&#39;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>param</span><span class=p>),</span>      <span class=c1># 二阶矩（方差） - 4 bytes (FP32)</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;grad&#39;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>    <span class=c1># 梯度缓存 - 4 bytes (FP32)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1># 总计：4 + 4 + 4 = 12 bytes/参数</span></span></span></code></pre></div><p><strong>为什么是FP32而不是FP16？</strong></p><ul><li>优化器需要累积微小更新（如 $10^{-5}$）</li><li>FP16精度不足会导致<strong>数值下溢</strong>（vanishing updates）</li></ul><h3 id=23-全量微调-vs-peft-显存对比>2.3 全量微调 vs PEFT 显存对比<a class=anchor href=#23-%e5%85%a8%e9%87%8f%e5%be%ae%e8%b0%83-vs-peft-%e6%98%be%e5%ad%98%e5%af%b9%e6%af%94>#</a></h3><p>以 <strong>Llama-3-8B</strong> 为例（8B参数）：</p><table><thead><tr><th>方法</th><th>可训练参数</th><th>优化器状态</th><th>总显存</th><th>A100(80GB)可行性</th></tr></thead><tbody><tr><td><strong>全量微调</strong></td><td>8B (100%)</td><td>8B × 12 = 96 GB</td><td>~120 GB</td><td>❌ 需要2卡</td></tr><tr><td><strong>LoRA</strong> (r=16)</td><td>21M (0.26%)</td><td>21M × 12 = 252 MB</td><td>~30 GB</td><td>✅ 单卡</td></tr><tr><td><strong>QLoRA</strong> (4bit)</td><td>21M (0.26%)</td><td>21M × 12 = 252 MB</td><td>~12 GB</td><td>✅ 单卡</td></tr></tbody></table><p><strong>计算公式</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># LoRA可训练参数量</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lora_params</span><span class=p>(</span><span class=n>model_dim</span><span class=p>,</span> <span class=n>rank</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>params_per_layer</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>model_dim</span> <span class=o>*</span> <span class=n>rank</span>  <span class=c1># W_A + W_B</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>params_per_layer</span> <span class=o>*</span> <span class=n>num_layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Llama-3-8B: 4096维度, 32层, rank=16</span>
</span></span><span class=line><span class=cl><span class=n>lora_params</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span><span class=mi>388</span><span class=p>,</span><span class=mi>608</span> <span class=err>≈</span> <span class=mi>8</span><span class=n>M</span> <span class=p>(</span><span class=n>仅Attention层</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=三lora核心低秩适配的数学本质>三、LoRA核心：低秩适配的数学本质<a class=anchor href=#%e4%b8%89lora%e6%a0%b8%e5%bf%83%e4%bd%8e%e7%a7%a9%e9%80%82%e9%85%8d%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8>#</a></h2><h3 id=31-核心公式秩分解>3.1 核心公式：秩分解<a class=anchor href=#31-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e7%a7%a9%e5%88%86%e8%a7%a3>#</a></h3><p><strong>原始想法</strong>：直接更新权重矩阵 $W \in \mathbb{R}^{d \times k}$
$$W&rsquo; = W + \Delta W$$</p><p><strong>LoRA创新</strong>：将 $\Delta W$ 分解为两个低秩矩阵
$$\Delta W = BA$$
其中：</p><ul><li>$B \in \mathbb{R}^{d \times r}$（Down-projection）</li><li>$A \in \mathbb{R}^{r \times k}$（Up-projection）</li><li>$r \ll \min(d, k)$（秩远小于原维度）</li></ul><p><strong>前向传播</strong>：
$$h = W_0 x + \Delta W x = W_0 x + BAx$$</p><h3 id=32-lora架构图>3.2 LoRA架构图<a class=anchor href=#32-lora%e6%9e%b6%e6%9e%84%e5%9b%be>#</a></h3><pre class=mermaid>graph TD
    Input[输入 x ∈ R^k] --&gt; Frozen[冻结权重 W0]
    Input --&gt; LoRA_A[LoRA-A ∈ R^r×k]

    Frozen --&gt; Add[相加 +]
    LoRA_A --&gt; LoRA_B[LoRA-B ∈ R^d×r]
    LoRA_B --&gt; Scale[缩放 α/r]
    Scale --&gt; Add

    Add --&gt; Output[输出 h ∈ R^d]

    style Frozen fill:#e8eaf6,stroke:#3f51b5
    style LoRA_A fill:#fff3e0,stroke:#ff9800
    style LoRA_B fill:#fff3e0,stroke:#ff9800
    style Scale fill:#e0f7fa,stroke:#00acc1

    classDef frozen stroke-width:4px
    class Frozen frozen</pre><p><strong>数学表达</strong>：
$$h = W_0 x + \frac{\alpha}{r} BAx$$
其中 $\alpha$ 是缩放因子（通常设为 $r$，使初始化时 $\Delta W \approx 0$）。</p><h3 id=33-深度问答为什么不能全0初始化>3.3 深度问答：为什么不能全0初始化？<a class=anchor href=#33-%e6%b7%b1%e5%ba%a6%e9%97%ae%e7%ad%94%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e5%85%a80%e5%88%9d%e5%a7%8b%e5%8c%96>#</a></h3><p><strong>问题</strong>：如果 $B = 0$ 和 $A = 0$，会发生什么？</p><p><strong>答案</strong>：<strong>梯度死锁（Gradient Lockout）</strong></p><p><strong>推导过程</strong>：</p><ol><li>假设初始化 $B = 0, A = 0$</li><li>前向传播：$\Delta W = BA = 0$</li><li>计算梯度：
$$\frac{\partial \mathcal{L}}{\partial B} = \frac{\partial \mathcal{L}}{\partial (\Delta W)} \cdot A^T = \frac{\partial \mathcal{L}}{\partial (\Delta W)} \cdot 0 = 0$$
$$\frac{\partial \mathcal{L}}{\partial A} = B^T \cdot \frac{\partial \mathcal{L}}{\partial (\Delta W)} = 0 \cdot \frac{\partial \mathcal{L}}{\partial (\Delta W)} = 0$$</li><li><strong>梯度恒为0</strong> → <strong>参数永远不更新</strong></li></ol><p><strong>正确初始化策略</strong>：</p><ul><li>$A \sim \mathcal{N}(0, \sigma^2)$：高斯初始化</li><li>$B = 0$：零初始化（确保初始时 $\Delta W = 0$）</li></ul><p>这样可以保证：</p><ul><li>初始状态：$\Delta W = B \cdot A = 0 \cdot A = 0$（不改变预训练权重）</li><li>有效梯度：$\frac{\partial \mathcal{L}}{\partial B} = \frac{\partial \mathcal{L}}{\partial (\Delta W)} \cdot A^T \neq 0$</li></ul><h3 id=34-pytorch原生实现loralinear>3.4 PyTorch原生实现：LoRALinear<a class=anchor href=#34-pytorch%e5%8e%9f%e7%94%9f%e5%ae%9e%e7%8e%b0loralinear>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LoRALinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;LoRA线性层：完整PyTorch原生实现&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>in_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>out_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>alpha</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 冻结的预训练权重</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>bias</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># LoRA参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>=</span> <span class=n>rank</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>/</span> <span class=n>rank</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 低秩矩阵</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>in_features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>out_features</span><span class=p>,</span> <span class=n>rank</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Dropout（可选）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span> <span class=k>if</span> <span class=n>dropout</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Kaiming初始化A，零初始化B&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>kaiming_uniform_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=p>,</span> <span class=n>a</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        前向传播: h = W0*x + (α/r)*B*A*x
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch, seq_len, in_features]
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            h: [batch, seq_len, out_features]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 基础输出（冻结）</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># LoRA增量</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=o>.</span><span class=n>T</span>  <span class=c1># [*, rank]</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=n>lora_out</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span><span class=o>.</span><span class=n>T</span>         <span class=c1># [*, out_features]</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=n>lora_out</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>result</span> <span class=o>+</span> <span class=n>lora_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>merge_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;合并LoRA权重到主权重（推理优化）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>delta_w</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=n>delta_w</span>
</span></span><span class=line><span class=cl>            <span class=c1># 清空LoRA参数以节省显存</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_lora_state_dict</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;只保存LoRA参数&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;lora_A&#39;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;lora_B&#39;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>layer</span> <span class=o>=</span> <span class=n>LoRALinear</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 统计参数量</span>
</span></span><span class=line><span class=cl><span class=n>frozen_params</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>layer</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>trainable_params</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=p>[</span><span class=n>layer</span><span class=o>.</span><span class=n>lora_A</span><span class=p>,</span> <span class=n>layer</span><span class=o>.</span><span class=n>lora_B</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;冻结参数: </span><span class=si>{</span><span class=n>frozen_params</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>frozen_params</span> <span class=o>*</span> <span class=mi>2</span> <span class=o>/</span> <span class=mf>1e9</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> GB)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;可训练参数: </span><span class=si>{</span><span class=n>trainable_params</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2> (</span><span class=si>{</span><span class=n>trainable_params</span> <span class=o>*</span> <span class=mi>2</span> <span class=o>/</span> <span class=mf>1e9</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> GB)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;参数比例: </span><span class=si>{</span><span class=n>trainable_params</span> <span class=o>/</span> <span class=n>frozen_params</span> <span class=o>*</span> <span class=mi>100</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出：</span>
</span></span><span class=line><span class=cl><span class=c1># 冻结参数: 16,777,216 (0.03 GB)</span>
</span></span><span class=line><span class=cl><span class=c1># 可训练参数: 131,072 (0.0003 GB)</span>
</span></span><span class=line><span class=cl><span class=c1># 参数比例: 0.78%</span></span></span></code></pre></div><p><strong>关键设计要点</strong>：</p><ol><li><strong>权重冻结</strong>：<code>requires_grad = False</code></li><li><strong>缩放因子</strong>：$\alpha/r$ 平衡学习率</li><li><strong>初始化策略</strong>：确保 $\Delta W_0 = 0$</li><li><strong>推理优化</strong>：<code>merge_weights()</code> 避免推理开销</li></ol><hr><h2 id=四lora家族演进从qlora到galore>四、LoRA家族演进：从QLoRA到GaLore<a class=anchor href=#%e5%9b%9blora%e5%ae%b6%e6%97%8f%e6%bc%94%e8%bf%9b%e4%bb%8eqlora%e5%88%b0galore>#</a></h2><h3 id=41-qlora量化的艺术>4.1 QLoRA：量化的艺术<a class=anchor href=#41-qlora%e9%87%8f%e5%8c%96%e7%9a%84%e8%89%ba%e6%9c%af>#</a></h3><p><strong>核心思想</strong>：基础模型用4bit存储，LoRA适配器用FP16训练。</p><h4 id=411-nf4量化原理>4.1.1 NF4量化原理<a class=anchor href=#411-nf4%e9%87%8f%e5%8c%96%e5%8e%9f%e7%90%86>#</a></h4><p><strong>普通INT4量化</strong>：均匀分割 $[-1, 1]$ 为16个区间
<strong>NF4（Normal Float 4）</strong>：根据<strong>正态分布</strong>的分位数选择量化点</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># NF4量化点（从标准正态分布的分位数推导）</span>
</span></span><span class=line><span class=cl><span class=n>NF4_QUANTILES</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=o>-</span><span class=mf>1.0000</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.6962</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5251</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3949</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=o>-</span><span class=mf>0.2844</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1848</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.0911</span><span class=p>,</span>  <span class=mf>0.0000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=mf>0.0796</span><span class=p>,</span>  <span class=mf>0.1609</span><span class=p>,</span>  <span class=mf>0.2461</span><span class=p>,</span>  <span class=mf>0.3379</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=mf>0.4407</span><span class=p>,</span>  <span class=mf>0.5626</span><span class=p>,</span>  <span class=mf>0.7230</span><span class=p>,</span>  <span class=mf>1.0000</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>quantize_nf4</span><span class=p>(</span><span class=n>tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;NF4量化&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 标准化到[-1, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>absmax</span> <span class=o>=</span> <span class=n>tensor</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>max</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>normalized</span> <span class=o>=</span> <span class=n>tensor</span> <span class=o>/</span> <span class=p>(</span><span class=n>absmax</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 查找最近的量化点</span>
</span></span><span class=line><span class=cl>    <span class=n>distances</span> <span class=o>=</span> <span class=p>(</span><span class=n>normalized</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=n>NF4_QUANTILES</span><span class=p>)</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>indices</span> <span class=o>=</span> <span class=n>distances</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 量化值（4bit索引）</span>
</span></span><span class=line><span class=cl>    <span class=n>quantized</span> <span class=o>=</span> <span class=n>NF4_QUANTILES</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 反量化</span>
</span></span><span class=line><span class=cl>    <span class=n>dequantized</span> <span class=o>=</span> <span class=n>quantized</span> <span class=o>*</span> <span class=n>absmax</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dequantized</span><span class=p>,</span> <span class=n>indices</span><span class=p>,</span> <span class=n>absmax</span></span></span></code></pre></div><p><strong>QLoRA显存计算</strong>：</p><p>$$\text{显存} = \underbrace{0.5 \Phi}<em>{\text{4bit模型}} + \underbrace{2 \Phi</em>{\text{LoRA}}}<em>{\text{FP16适配器}} + \underbrace{12 \Phi</em>{\text{LoRA}}}_{\text{优化器状态}}$$</p><p>对于Llama-3-8B + LoRA(r=16)：</p><ul><li>基础模型：$8B \times 0.5 = 4GB$</li><li>LoRA参数：$21M \times 2 = 42MB$</li><li>优化器：$21M \times 12 = 252MB$</li><li><strong>总计：~5GB</strong>（可在消费级GPU运行）</li></ul><h3 id=42-dora方向与幅度的解耦>4.2 DoRA：方向与幅度的解耦<a class=anchor href=#42-dora%e6%96%b9%e5%90%91%e4%b8%8e%e5%b9%85%e5%ba%a6%e7%9a%84%e8%a7%a3%e8%80%a6>#</a></h3><p><strong>核心公式</strong>：
$$W&rsquo; = W_0 + \Delta W = m \frac{V + \Delta V}{|V + \Delta V|}$$</p><p>其中：</p><ul><li>$m = |W_0|$：权重的<strong>幅度</strong>（Magnitude）</li><li>$V = W_0 / m$：权重的<strong>方向</strong>（Direction）</li><li>$\Delta V = BA$：LoRA更新方向</li></ul><p><strong>架构对比</strong>：</p><table><thead><tr><th>方法</th><th>更新方式</th><th>公式</th><th>参数量</th></tr></thead><tbody><tr><td><strong>LoRA</strong></td><td>加法</td><td>$W&rsquo; = W_0 + BA$</td><td>$2dr$</td></tr><tr><td><strong>DoRA</strong></td><td>方向+幅度</td><td>$W&rsquo; = m \frac{W_0 + BA}{|W_0 + BA|}$</td><td>$2dr + d$</td></tr></tbody></table><h4 id=421-dora完整实现>4.2.1 DoRA完整实现<a class=anchor href=#421-dora%e5%ae%8c%e6%95%b4%e5%ae%9e%e7%8e%b0>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DoRALinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;DoRA: Direction + Magnitude分解&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>in_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>out_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>alpha</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 冻结的预训练权重</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># LoRA参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rank</span> <span class=o>=</span> <span class=n>rank</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>/</span> <span class=n>rank</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>in_features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>out_features</span><span class=p>,</span> <span class=n>rank</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># DoRA特有：幅度向量</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>magnitude</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>out_features</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span> <span class=k>if</span> <span class=n>dropout</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 初始化</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;初始化策略&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>kaiming_uniform_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=p>,</span> <span class=n>a</span><span class=o>=</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 幅度初始化为W0的Row Norm</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>weight_norm</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>magnitude</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>weight_norm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        前向传播: h = m * (V + ΔV) / ||V + ΔV||
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: [batch, seq_len, in_features]
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            h: [batch, seq_len, out_features]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算方向更新</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=n>lora_out</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_out</span> <span class=o>=</span> <span class=n>lora_out</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 组合权重</span>
</span></span><span class=line><span class=cl>        <span class=n>combined_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span> <span class=o>+</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>lora_B</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>lora_A</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 标准化方向</span>
</span></span><span class=line><span class=cl>        <span class=n>weight_norm</span> <span class=o>=</span> <span class=n>combined_weight</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>normalized_weight</span> <span class=o>=</span> <span class=n>combined_weight</span> <span class=o>/</span> <span class=p>(</span><span class=n>weight_norm</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 应用幅度</span>
</span></span><span class=line><span class=cl>        <span class=n>final_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>magnitude</span> <span class=o>*</span> <span class=n>normalized_weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算输出</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>final_weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>extra_repr</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=sa>f</span><span class=s1>&#39;in_features=</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>in_features</span><span class=si>}</span><span class=s1>, out_features=</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>out_features</span><span class=si>}</span><span class=s1>, rank=</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>rank</span><span class=si>}</span><span class=s1>&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 性能对比测试</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>benchmark_dora_vs_lora</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>4096</span><span class=p>)</span>  <span class=c1># [batch, seq, dim]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>lora</span> <span class=o>=</span> <span class=n>LoRALinear</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=mi>4096</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dora</span> <span class=o>=</span> <span class=n>DoRALinear</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=mi>4096</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播时间</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>lora</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span> <span class=o>=</span> <span class=n>dora</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dora_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;LoRA时间: </span><span class=si>{</span><span class=n>lora_time</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;DoRA时间: </span><span class=si>{</span><span class=n>dora_time</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;DoRA额外开销: </span><span class=si>{</span><span class=p>(</span><span class=n>dora_time</span> <span class=o>-</span> <span class=n>lora_time</span><span class=p>)</span> <span class=o>/</span> <span class=n>lora_time</span> <span class=o>*</span> <span class=mi>100</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>DoRA的优势</strong>：</p><ol><li><strong>更稳定的训练</strong>：幅度和方向独立更新</li><li><strong>更好的泛化</strong>：类似Weight Normalization的正则化效果</li><li><strong>代价</strong>：前向传播多一次归一化操作（~10%开销）</li></ol><h3 id=43-galore梯度低秩投影>4.3 GaLore：梯度低秩投影<a class=anchor href=#43-galore%e6%a2%af%e5%ba%a6%e4%bd%8e%e7%a7%a9%e6%8a%95%e5%bd%b1>#</a></h3><p><strong>核心思想</strong>：不是低秩分解权重，而是<strong>低秩投影梯度</strong>。</p><h4 id=431-原理推导>4.3.1 原理推导<a class=anchor href=#431-%e5%8e%9f%e7%90%86%e6%8e%a8%e5%af%bc>#</a></h4><p>标准梯度下降：
$$W_{t+1} = W_t - \eta \nabla_W \mathcal{L}$$</p><p>GaLore投影：
$$W_{t+1} = W_t - \eta P_r(\nabla_W \mathcal{L})$$</p><p>其中 $P_r(\cdot)$ 是秩-$r$ 投影算子：</p><p>$$P_r(G) = U_r U_r^T G \quad \text{（左投影）}$$
$$P_r(G) = G V_r V_r^T \quad \text{（右投影）}$$</p><p>$U_r, V_r$ 是 $G$ 的前 $r$ 个奇异向量（SVD分解）。</p><h4 id=432-算法流程>4.3.2 算法流程<a class=anchor href=#432-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.optim.optimizer</span> <span class=kn>import</span> <span class=n>Optimizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GaLoreAdamW</span><span class=p>(</span><span class=n>Optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;GaLore优化器：梯度低秩投影&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>rank</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>update_proj_gap</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>200</span><span class=p>,</span>  <span class=c1># 每200步更新投影矩阵</span>
</span></span><span class=line><span class=cl>        <span class=n>scale</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>proj_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;std&#34;</span>  <span class=c1># &#34;std&#34;, &#34;reverse&#34;, &#34;right&#34;, &#34;left&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>defaults</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span> <span class=n>update_proj_gap</span><span class=o>=</span><span class=n>update_proj_gap</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>scale</span><span class=o>=</span><span class=n>scale</span><span class=p>,</span> <span class=n>proj_type</span><span class=o>=</span><span class=n>proj_type</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>defaults</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=k>continue</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>grad</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>p</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 初始化状态</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>state</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;exp_avg&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;exp_avg_sq&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;projector&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 更新投影矩阵</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;step&#39;</span><span class=p>]</span> <span class=o>%</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;update_proj_gap&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>state</span><span class=p>[</span><span class=s1>&#39;projector&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_projector</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>grad</span><span class=p>,</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;rank&#39;</span><span class=p>],</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;proj_type&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 投影梯度</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;projector&#39;</span><span class=p>]</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>grad_proj</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_project</span><span class=p>(</span><span class=n>grad</span><span class=p>,</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;projector&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>grad_proj</span> <span class=o>=</span> <span class=n>grad</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># AdamW更新</span>
</span></span><span class=line><span class=cl>                <span class=n>exp_avg</span><span class=p>,</span> <span class=n>exp_avg_sq</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;exp_avg&#39;</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=s1>&#39;exp_avg_sq&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>beta1</span><span class=p>,</span> <span class=n>beta2</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>exp_avg</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=n>beta1</span><span class=p>)</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=n>grad_proj</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>exp_avg_sq</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=n>beta2</span><span class=p>)</span><span class=o>.</span><span class=n>addcmul_</span><span class=p>(</span><span class=n>grad_proj</span><span class=p>,</span> <span class=n>grad_proj</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>denom</span> <span class=o>=</span> <span class=n>exp_avg_sq</span><span class=o>.</span><span class=n>sqrt</span><span class=p>()</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=mf>1e-8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>step_size</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=n>group</span><span class=p>[</span><span class=s1>&#39;scale&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>p</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=n>exp_avg</span> <span class=o>/</span> <span class=n>denom</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=-</span><span class=n>step_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_get_projector</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grad</span><span class=p>,</span> <span class=n>rank</span><span class=p>,</span> <span class=n>proj_type</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;计算SVD投影矩阵&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>matrix</span> <span class=o>=</span> <span class=n>grad</span> <span class=k>if</span> <span class=n>grad</span><span class=o>.</span><span class=n>dim</span><span class=p>()</span> <span class=o>==</span> <span class=mi>2</span> <span class=k>else</span> <span class=n>grad</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># SVD分解（只取前r个奇异向量）</span>
</span></span><span class=line><span class=cl>        <span class=n>U</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>Vt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svd</span><span class=p>(</span><span class=n>matrix</span><span class=p>,</span> <span class=n>full_matrices</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>U_r</span> <span class=o>=</span> <span class=n>U</span><span class=p>[:,</span> <span class=p>:</span><span class=n>rank</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>Vt_r</span> <span class=o>=</span> <span class=n>Vt</span><span class=p>[:</span><span class=n>rank</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>proj_type</span> <span class=o>==</span> <span class=s2>&#34;left&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>U_r</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>proj_type</span> <span class=o>==</span> <span class=s2>&#34;right&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>Vt_r</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>  <span class=c1># &#34;std&#34;</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=p>(</span><span class=n>U_r</span><span class=p>,</span> <span class=n>Vt_r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_project</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grad</span><span class=p>,</span> <span class=n>projector</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;应用投影&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>projector</span><span class=p>,</span> <span class=nb>tuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>U_r</span><span class=p>,</span> <span class=n>Vt_r</span> <span class=o>=</span> <span class=n>projector</span>
</span></span><span class=line><span class=cl>            <span class=c1># 双边投影: U_r @ U_r^T @ G @ Vt_r^T @ Vt_r</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>U_r</span> <span class=o>@</span> <span class=p>(</span><span class=n>U_r</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>grad</span> <span class=o>@</span> <span class=n>Vt_r</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>@</span> <span class=n>Vt_r</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 单边投影</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>projector</span> <span class=o>@</span> <span class=p>(</span><span class=n>projector</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MyTransformer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>GaLoreAdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span></span></span></code></pre></div><p><strong>GaLore的优势</strong>：</p><ul><li><strong>全参数更新</strong>：不像LoRA只更新部分层</li><li><strong>显存效率</strong>：优化器状态只存储低秩投影（$r \ll d$）</li><li><strong>适用场景</strong>：从头预训练小模型（如1B参数）</li></ul><p><strong>显存对比</strong>（以Llama-1B为例）：</p><table><thead><tr><th>方法</th><th>优化器状态</th><th>梯度存储</th><th>总显存</th></tr></thead><tbody><tr><td><strong>AdamW</strong></td><td>1B × 12 = 12GB</td><td>1B × 2 = 2GB</td><td>14GB</td></tr><tr><td><strong>GaLore (r=128)</strong></td><td>128M × 12 = 1.5GB</td><td>128M × 2 = 256MB</td><td>~2GB</td></tr></tbody></table><h3 id=44-lora非对称学习率>4.4 LoRA+：非对称学习率<a class=anchor href=#44-lora%e9%9d%9e%e5%af%b9%e7%a7%b0%e5%ad%a6%e4%b9%a0%e7%8e%87>#</a></h3><p><strong>核心观察</strong>：LoRA中 $A$ 和 $B$ 的作用不对称</p><ul><li>$A$：输入投影（类似特征提取）</li><li>$B$：输出投影（类似分类器）</li></ul><p><strong>改进策略</strong>：使用不同学习率
$$\eta_B = \lambda \cdot \eta_A \quad (\lambda > 1)$$</p><p>通常设置 $\lambda = 16$。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># LoRA+参数组设置</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=s1>&#39;lora_A&#39;</span> <span class=ow>in</span> <span class=n>n</span><span class=p>],</span>
</span></span><span class=line><span class=cl>     <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=s1>&#39;lora_B&#39;</span> <span class=ow>in</span> <span class=n>n</span><span class=p>],</span>
</span></span><span class=line><span class=cl>     <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-3</span><span class=p>},</span>  <span class=c1># 10x更大</span>
</span></span><span class=line><span class=cl><span class=p>])</span></span></span></code></pre></div><h3 id=45-其他变种简介>4.5 其他变种简介<a class=anchor href=#45-%e5%85%b6%e4%bb%96%e5%8f%98%e7%a7%8d%e7%ae%80%e4%bb%8b>#</a></h3><h4 id=adalora自适应秩分配>AdaLoRA：自适应秩分配<a class=anchor href=#adalora%e8%87%aa%e9%80%82%e5%ba%94%e7%a7%a9%e5%88%86%e9%85%8d>#</a></h4><ul><li><strong>思想</strong>：根据重要性动态调整每层的秩</li><li><strong>方法</strong>：通过奇异值大小剪枝不重要的秩</li><li><strong>适用</strong>：参数预算有限且层间差异大的场景</li></ul><h4 id=vera极致参数效率>VeRA：极致参数效率<a class=anchor href=#vera%e6%9e%81%e8%87%b4%e5%8f%82%e6%95%b0%e6%95%88%e7%8e%87>#</a></h4><ul><li><strong>思想</strong>：共享随机投影矩阵，只训练缩放向量</li><li><strong>公式</strong>：$\Delta W = b \cdot d^T$（$b, d$ 是向量）</li><li><strong>参数量</strong>：仅 $2d$（比LoRA少100倍）</li></ul><hr><h2 id=五微调深度理解>五、微调深度理解<a class=anchor href=#%e4%ba%94%e5%be%ae%e8%b0%83%e6%b7%b1%e5%ba%a6%e7%90%86%e8%a7%a3>#</a></h2><p>微调不仅是跑通代码，更是对数据分布的重塑。本章我们将深入探讨数据构建、遗忘问题以及多任务学习的内在机制。</p><h3 id=51-指令数据构建的艺术>5.1 指令数据构建的艺术<a class=anchor href=#51-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e6%9e%84%e5%bb%ba%e7%9a%84%e8%89%ba%e6%9c%af>#</a></h3><blockquote class=book-hint><p>&ldquo;Garbage In, Garbage Out.&rdquo; 在微调阶段，数据质量的重要性远超数量。</p></blockquote><h4 id=1-指令数据的黄金标准>(1) 指令数据的黄金标准<a class=anchor href=#1-%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e7%9a%84%e9%bb%84%e9%87%91%e6%a0%87%e5%87%86>#</a></h4><p>高质量的SFT数据应具备以下特征：</p><ol><li><strong>多样性 (Diversity)</strong>：覆盖各种任务类型（写作、推理、编码、翻译）。</li><li><strong>复杂性 (Complexity)</strong>：指令应包含多步约束（如"请用Python写一个排序算法，并解释时间复杂度，输出格式为Markdown&rdquo;）。</li><li><strong>正确性 (Correctness)</strong>：答案必须客观准确，无幻觉。</li></ol><h4 id=2-self-instruct-用gpt-4生成训练数据>(2) Self-Instruct: 用GPT-4生成训练数据<a class=anchor href=#2-self-instruct-%e7%94%a8gpt-4%e7%94%9f%e6%88%90%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae>#</a></h4><p>当缺乏标注数据时，利用强模型（Teacher）生成数据是标准做法。</p><p><strong>Self-Instruct 流程图</strong>：</p><pre class=mermaid>graph TD
A[种子指令库&lt;br&gt;175条人工撰写] --&gt;|1.采样| B[构造生成Prompt]
B --&gt;|2.生成| C[GPT-4/Claude-3]
C --&gt;|3.输出| D[新指令 &amp; 实例]
D --&gt;|4.过滤| E{质量过滤&lt;br&gt;ROUGE相似度小于0.7}
E --&gt;|通过| F[指令池]
E --&gt;|失败| G[丢弃]
F --&gt;|循环| A</pre><p><strong>Evol-Instruct (WizardLM)</strong>：
仅仅生成相似指令是不够的，我们需要<strong>进化</strong>指令难度：</p><ul><li><strong>深度进化</strong>：增加约束条件、增加推理步骤。</li><li><strong>广度进化</strong>：转换话题、增加罕见概念。</li></ul><h4 id=3-数据增强技术>(3) 数据增强技术<a class=anchor href=#3-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba%e6%8a%80%e6%9c%af>#</a></h4><p>如果你只有少量数据（如100条），可以使用以下技术扩充：</p><table><thead><tr><th>方法</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><strong>Back-Translation</strong></td><td>中文 $\to$ 英文 $\to$ 中文</td><td>&ldquo;我喜欢AI&rdquo; $\to$ &ldquo;I like AI&rdquo; $\to$ &ldquo;我对人工智能很感兴趣&rdquo;</td></tr><tr><td><strong>Rewrite</strong></td><td>让模型改写语气/风格</td><td>&ldquo;帮我请假&rdquo; $\to$ &ldquo;请帮我以此理由撰写一份正式的请假邮件&mldr;&rdquo;</td></tr><tr><td><strong>CoT Expansion</strong></td><td>将直接回答扩展为思维链</td><td>问：&ldquo;3+5=?&rdquo;<br>答：&ldquo;3+5=8&rdquo; $\to$ &ldquo;首先计算个位&mldr;结果是8&rdquo;</td></tr></tbody></table><h3 id=52-灾难性遗忘-catastrophic-forgetting>5.2 灾难性遗忘 (Catastrophic Forgetting)<a class=anchor href=#52-%e7%81%be%e9%9a%be%e6%80%a7%e9%81%97%e5%bf%98-catastrophic-forgetting>#</a></h3><p>微调就像给模型"洗脑"。如果微调数据分布与预训练分布差异过大，模型会迅速忘记原有的通用能力（如推理、常识）。</p><h4 id=缓解策略1-混合训练数据>缓解策略1: 混合训练数据<a class=anchor href=#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a51-%e6%b7%b7%e5%90%88%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae>#</a></h4><p>最简单有效的方法：在微调数据中混入 <strong>1%-5% 的通用预训练数据</strong>（如Wiki、Books）。</p><ul><li><strong>优点</strong>：操作简单，效果显著。</li><li><strong>缺点</strong>：增加了训练成本。</li></ul><h4 id=缓解策略2-elastic-weight-consolidation-ewc>缓解策略2: Elastic Weight Consolidation (EWC)<a class=anchor href=#%e7%bc%93%e8%a7%a3%e7%ad%96%e7%95%a52-elastic-weight-consolidation-ewc>#</a></h4><p>EWC（弹性权重巩固）的核心思想是：<strong>保护重要参数，允许不重要参数大幅更新</strong>。</p><p><strong>数学原理</strong>：
在Loss中加入正则项，惩罚对旧任务重要参数的改变：</p><p>$$ \mathcal{L} = \mathcal{L}<em>{\text{new}}(\theta) + \frac{\lambda}{2} \sum</em>{i} F_i (\theta_i - \theta_{i, \text{old}})^2 $$</p><p>其中：</p><ul><li>$F_i$ 是 <strong>Fisher信息矩阵</strong> 的对角线元素，代表参数 $\theta_i$ 对旧任务的重要性。</li><li>$\theta_{i, \text{old}}$ 是旧任务训练后的参数值。</li><li>$\lambda$ 是正则化强度的超参数。</li></ul><p><strong>PyTorch实现核心逻辑</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>ewc_loss</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>fisher_matrix</span><span class=p>,</span> <span class=n>old_params</span><span class=p>,</span> <span class=n>lambda_ewc</span><span class=o>=</span><span class=mf>0.4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>fisher_matrix</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Fisher值越大，惩罚越重</span>
</span></span><span class=line><span class=cl>            <span class=n>_loss</span> <span class=o>=</span> <span class=n>fisher_matrix</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>param</span> <span class=o>-</span> <span class=n>old_params</span><span class=p>[</span><span class=n>name</span><span class=p>])</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>+=</span> <span class=n>_loss</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>lambda_ewc</span> <span class=o>*</span> <span class=n>loss</span></span></span></code></pre></div><h3 id=53-多任务微调-multi-task-fine-tuning>5.3 多任务微调 (Multi-task Fine-tuning)<a class=anchor href=#53-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%be%ae%e8%b0%83-multi-task-fine-tuning>#</a></h3><p>与其让模型专精一件事，不如让它成为"多面手"。MFT（Multi-task Fine-tuning）能显著提升模型的泛化能力。</p><h4 id=1-任务标识符-task-prefix>(1) 任务标识符 (Task Prefix)<a class=anchor href=#1-%e4%bb%bb%e5%8a%a1%e6%a0%87%e8%af%86%e7%ac%a6-task-prefix>#</a></h4><p>显式告诉模型当前是什么任务。</p><ul><li><strong>输入格式</strong>：<ul><li><code>[Translation] Hello world -> ?</code></li><li><code>[Coding] Write a quick sort -> ?</code></li><li><code>[Summarization] Alice is a girl... -> ?</code></li></ul></li></ul><h4 id=2-任务特定适配器>(2) 任务特定适配器<a class=anchor href=#2-%e4%bb%bb%e5%8a%a1%e7%89%b9%e5%ae%9a%e9%80%82%e9%85%8d%e5%99%a8>#</a></h4><p>不同任务使用独立的LoRA模块，共享基础模型。</p><pre class=mermaid>graph TD
    Input --&gt; Base[基础模型Transformer]
    Base --&gt; Router{任务路由}
    Router --&gt;|任务A| LoRA_A[LoRA A: 医疗]
    Router --&gt;|任务B| LoRA_B[LoRA B: 法律]
    Router --&gt;|任务C| LoRA_C[LoRA C: 代码]
    LoRA_A --&gt; Output
    LoRA_B --&gt; Output
    LoRA_C --&gt; Output</pre><h3 id=54-持续学习-continual-learning>5.4 持续学习 (Continual Learning)<a class=anchor href=#54-%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0-continual-learning>#</a></h3><p>世界在变，知识在更新。我们需要模型能够持续学习新知识，而不是重新训练。</p><h4 id=1-渐进式lora-progressive-lora>(1) 渐进式LoRA (Progressive LoRA)<a class=anchor href=#1-%e6%b8%90%e8%bf%9b%e5%bc%8flora-progressive-lora>#</a></h4><p>当有新任务到来时：</p><ol><li><strong>冻结</strong> 基础模型和旧任务的 LoRA。</li><li><strong>初始化</strong> 一个新的 LoRA 模块学习新任务。</li><li>推理时，根据任务选择激活哪个 LoRA，或者将它们合并。</li></ol><h4 id=2-知识蒸馏-knowledge-distillation>(2) 知识蒸馏 (Knowledge Distillation)<a class=anchor href=#2-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f-knowledge-distillation>#</a></h4><p>用旧模型（Teacher）指导新模型（Student），迫使新模型在学习新数据时，输出分布尽量接近旧模型。</p><p>$$ \mathcal{L} = \alpha \mathcal{L}<em>{\text{SFT}} + (1-\alpha) \mathcal{L}</em>{\text{KD}}(P_{\text{student}}, P_{\text{teacher}}) $$</p><h4 id=3-完整持续学习流程>(3) 完整持续学习流程<a class=anchor href=#3-%e5%ae%8c%e6%95%b4%e6%8c%81%e7%bb%ad%e5%ad%a6%e4%b9%a0%e6%b5%81%e7%a8%8b>#</a></h4><ol><li><strong>阶段 1</strong>：在通用语料上预训练。</li><li><strong>阶段 2</strong>：在多任务指令集上SFT（MFT）。</li><li><strong>阶段 3</strong>：定期收集新数据，使用 Replay（数据回放）或 LoRA 增量更新。</li></ol><hr><h2 id=六工程实战用trl库微调模型>六、工程实战：用TRL库微调模型<a class=anchor href=#%e5%85%ad%e5%b7%a5%e7%a8%8b%e5%ae%9e%e6%88%98%e7%94%a8trl%e5%ba%93%e5%be%ae%e8%b0%83%e6%a8%a1%e5%9e%8b>#</a></h2><h3 id=61-完整训练流程>6.1 完整训练流程<a class=anchor href=#61-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>SFTTrainer</span><span class=p>,</span> <span class=n>SFTConfig</span><span class=p>,</span> <span class=n>DataCollatorForCompletionOnlyLM</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 1. 加载模型和Tokenizer ==========</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Llama-3.2-3B-Instruct&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 2. 配置LoRA ==========</span>
</span></span><span class=line><span class=cl><span class=n>lora_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>                            <span class=c1># 秩</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>                   <span class=c1># 缩放因子（通常是r的2倍）</span>
</span></span><span class=line><span class=cl>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span>                 <span class=c1># 目标层</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;q_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;k_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;v_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;o_proj&#34;</span><span class=p>,</span>  <span class=c1># Attention</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gate_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;up_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;down_proj&#34;</span>      <span class=c1># FFN</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bias</span><span class=o>=</span><span class=s2>&#34;none&#34;</span><span class=p>,</span>                     <span class=c1># 不训练bias</span>
</span></span><span class=line><span class=cl>    <span class=n>task_type</span><span class=o>=</span><span class=s2>&#34;CAUSAL_LM&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>print_trainable_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 输出：trainable params: 20,971,520 || all params: 3,213,978,624 || trainable%: 0.65%</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 3. 数据处理 ==========</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;tatsu-lab/alpaca&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train[:1000]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 关键：设置padding方向</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&#34;right&#34;</span>  <span class=c1># SFT必须用right padding！</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 应用聊天模板</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>format_chat</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>example</span><span class=p>[</span><span class=s2>&#34;instruction&#34;</span><span class=p>]},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>example</span><span class=p>[</span><span class=s2>&#34;output&#34;</span><span class=p>]}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=p>)}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>format_chat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 4. 数据Collator（关键：只计算答案Loss）==========</span>
</span></span><span class=line><span class=cl><span class=c1># 自动识别response_template，只在助手回复部分计算loss</span>
</span></span><span class=line><span class=cl><span class=n>response_template</span> <span class=o>=</span> <span class=s2>&#34;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>collator</span> <span class=o>=</span> <span class=n>DataCollatorForCompletionOnlyLM</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>response_template</span><span class=o>=</span><span class=n>response_template</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>mlm</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 5. 训练配置 ==========</span>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>SFTConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./llama3-lora-sft&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 基础设置</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>      <span class=c1># 等效batch size = 16</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 优化器</span>
</span></span><span class=line><span class=cl>    <span class=n>optim</span><span class=o>=</span><span class=s2>&#34;adamw_torch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_ratio</span><span class=o>=</span><span class=mf>0.03</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 学习率调度</span>
</span></span><span class=line><span class=cl>    <span class=n>lr_scheduler_type</span><span class=o>=</span><span class=s2>&#34;cosine&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 显存优化</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_checkpointing</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bf16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>                          <span class=c1># 使用BF16混合精度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 日志</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>save_strategy</span><span class=o>=</span><span class=s2>&#34;epoch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># NEFTune（噪声注入技巧）</span>
</span></span><span class=line><span class=cl>    <span class=n>neftune_noise_alpha</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>              <span class=c1># 在嵌入层加噪声</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 其他</span>
</span></span><span class=line><span class=cl>    <span class=n>max_seq_length</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset_text_field</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>,</span>          <span class=c1># 数据集中的文本字段</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 6. 开始训练 ==========</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>data_collator</span><span class=o>=</span><span class=n>collator</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 7. 保存模型 ==========</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>save_model</span><span class=p>(</span><span class=s2>&#34;./final_lora_model&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=62-关键技术详解>6.2 关键技术详解<a class=anchor href=#62-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3>#</a></h3><h4 id=621-chat-template的正确使用>6.2.1 Chat Template的正确使用<a class=anchor href=#621-chat-template%e7%9a%84%e6%ad%a3%e7%a1%ae%e4%bd%bf%e7%94%a8>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Llama-3的聊天格式</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>You are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>用户问题&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>助手回答&lt;|eot_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 自动应用模板</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;You are a helpful assistant.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;什么是量子纠缠？&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;量子纠缠是...&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>formatted</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>           <span class=c1># 返回字符串而非token ids</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>False</span>  <span class=c1># 训练时不加生成提示符</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><h4 id=622-padding避坑指南>6.2.2 Padding避坑指南<a class=anchor href=#622-padding%e9%81%bf%e5%9d%91%e6%8c%87%e5%8d%97>#</a></h4><p><strong>错误示范</strong>：使用 <code>padding_side="left"</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ❌ 错误：left padding会破坏因果注意力</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&#34;left&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例数据</span>
</span></span><span class=line><span class=cl><span class=n>texts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;短句&#34;</span><span class=p>,</span> <span class=s2>&#34;这是一个很长很长的句子&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>texts</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：</span>
</span></span><span class=line><span class=cl><span class=c1># [PAD][PAD][PAD] 短 句</span>
</span></span><span class=line><span class=cl><span class=c1># 这 是 一 个 很 长 ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 问题：模型会在PAD位置计算loss！</span></span></span></code></pre></div><p><strong>正确做法</strong>：使用 <code>padding_side="right"</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ✅ 正确：right padding保持因果结构</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&#34;right&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果：</span>
</span></span><span class=line><span class=cl><span class=c1># 短 句 [PAD][PAD][PAD]</span>
</span></span><span class=line><span class=cl><span class=c1># 这 是 一 个 很 长 ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 只在真实token上计算loss</span></span></span></code></pre></div><p><strong>原理</strong>：</p><ul><li>Left padding：破坏序列的因果顺序（模型会"看到未来"）</li><li>Right padding：保持因果结构，PAD在末尾可通过attention mask忽略</li></ul><h4 id=623-neftune嵌入层加噪技巧>6.2.3 NEFTune：嵌入层加噪技巧<a class=anchor href=#623-neftune%e5%b5%8c%e5%85%a5%e5%b1%82%e5%8a%a0%e5%99%aa%e6%8a%80%e5%b7%a7>#</a></h4><p><strong>原理</strong>：在训练时给输入嵌入添加随机噪声，提升泛化能力。</p><p>$$e&rsquo;_i = e_i + \mathcal{N}(0, \alpha^2 / \sqrt{Ld})$$</p><p>其中：</p><ul><li>$e_i$：原始嵌入</li><li>$L$：序列长度</li><li>$d$：嵌入维度</li><li>$\alpha$：噪声强度（通常5-15）</li></ul><p><strong>实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NEFTuneEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;NEFTune嵌入层包装器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embedding_layer</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>embedding_layer</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 计算噪声标准差</span>
</span></span><span class=line><span class=cl>            <span class=n>seq_len</span><span class=p>,</span> <span class=n>emb_dim</span> <span class=o>=</span> <span class=n>embeddings</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>embeddings</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>noise_std</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>/</span> <span class=p>(</span><span class=n>seq_len</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>)</span> <span class=o>**</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 添加噪声</span>
</span></span><span class=line><span class=cl>            <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span> <span class=o>*</span> <span class=n>noise_std</span>
</span></span><span class=line><span class=cl>            <span class=n>embeddings</span> <span class=o>=</span> <span class=n>embeddings</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TRL自动支持（在SFTConfig中设置）</span>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>SFTConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>neftune_noise_alpha</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>  <span class=c1># 自动应用NEFTune</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><p><strong>效果</strong>：</p><ul><li>提升1-3个点的下游任务性能</li><li>防止过拟合小数据集</li><li>几乎无额外计算开销</li></ul><h3 id=63-推理与部署>6.3 推理与部署<a class=anchor href=#63-%e6%8e%a8%e7%90%86%e4%b8%8e%e9%83%a8%e7%bd%b2>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>PeftModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 方法1：保持LoRA分离（灵活） ==========</span>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;meta-llama/Llama-3.2-3B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>PeftModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=s2>&#34;./final_lora_model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 推理</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;用户问题&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 方法2：合并权重（快速） ==========</span>
</span></span><span class=line><span class=cl><span class=c1># 合并LoRA到基础模型</span>
</span></span><span class=line><span class=cl><span class=n>merged_model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 保存合并后的模型</span>
</span></span><span class=line><span class=cl><span class=n>merged_model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&#34;./merged_model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&#34;./merged_model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 之后可以当作普通模型加载</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;./merged_model&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=七模型合并技术>七、模型合并技术<a class=anchor href=#%e4%b8%83%e6%a8%a1%e5%9e%8b%e5%90%88%e5%b9%b6%e6%8a%80%e6%9c%af>#</a></h2><blockquote class=book-hint><p>&ldquo;The whole is greater than the sum of its parts.&rdquo; —— 亚里士多德</p></blockquote><p>当我们有多个微调好的模型（例如一个懂医学，一个懂法律），如何将它们的能力合二为一，而不需要重新训练？这就是模型合并（Model Merging）的魔力。</p><h3 id=71-线性插值合并-weight-averaging>7.1 线性插值合并 (Weight Averaging)<a class=anchor href=#71-%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc%e5%90%88%e5%b9%b6-weight-averaging>#</a></h3><p>最朴素的方法：直接对权重求平均。</p><p>$$ W_{\text{merged}} = \alpha W_A + (1-\alpha) W_B $$</p><ul><li><strong>适用场景</strong>：两个模型微调自同一个基座，且微调轨迹相似（例如同一个Epoch的不同Checkpoint）。</li><li><strong>局限性</strong>：在高维空间中，简单平均可能会导致权重落入"Loss盆地"之间的壁垒，导致性能下降。</li></ul><h3 id=72-slerp-球面线性插值>7.2 SLERP: 球面线性插值<a class=anchor href=#72-slerp-%e7%90%83%e9%9d%a2%e7%ba%bf%e6%80%a7%e6%8f%92%e5%80%bc>#</a></h3><p><strong>Spherical Linear Interpolation (SLERP)</strong> 解决了线性插值导致的"幅度塌缩"问题。它将权重看作高维球面上的一点，沿着大圆弧进行插值。</p><p><strong>核心公式</strong>：</p><p>$$ \text{SLERP}(v_1, v_2, t) = \frac{\sin((1-t)\Omega)}{\sin \Omega} v_1 + \frac{\sin(t\Omega)}{\sin \Omega} v_2 $$</p><p>其中 $\Omega$ 是向量 $v_1$ 和 $v_2$ 之间的夹角。</p><p><strong>代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>slerp</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>v0</span><span class=p>,</span> <span class=n>v1</span><span class=p>,</span> <span class=n>dot_threshold</span><span class=o>=</span><span class=mf>0.9995</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    球面线性插值
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        t: 插值系数 [0, 1]
</span></span></span><span class=line><span class=cl><span class=s2>        v0, v1: 两个形状相同的张量
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 归一化</span>
</span></span><span class=line><span class=cl>    <span class=n>v0_norm</span> <span class=o>=</span> <span class=n>v0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>v0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>v1_norm</span> <span class=o>=</span> <span class=n>v1</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>v1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算夹角余弦</span>
</span></span><span class=line><span class=cl>    <span class=n>dot</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>v0_norm</span> <span class=o>*</span> <span class=n>v1_norm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 如果两个向量几乎平行，直接线性插值</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>dot</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>dot_threshold</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>v0</span> <span class=o>+</span> <span class=n>t</span> <span class=o>*</span> <span class=n>v1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 计算夹角</span>
</span></span><span class=line><span class=cl>    <span class=n>theta_0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>acos</span><span class=p>(</span><span class=n>dot</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sin_theta_0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>theta_0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>theta_t</span> <span class=o>=</span> <span class=n>theta_0</span> <span class=o>*</span> <span class=n>t</span>
</span></span><span class=line><span class=cl>    <span class=n>sin_theta_t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>theta_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>s0</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>theta_0</span> <span class=o>-</span> <span class=n>theta_t</span><span class=p>)</span> <span class=o>/</span> <span class=n>sin_theta_0</span>
</span></span><span class=line><span class=cl>    <span class=n>s1</span> <span class=o>=</span> <span class=n>sin_theta_t</span> <span class=o>/</span> <span class=n>sin_theta_0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>s0</span> <span class=o>*</span> <span class=n>v0</span> <span class=o>+</span> <span class=n>s1</span> <span class=o>*</span> <span class=n>v1</span></span></span></code></pre></div><h3 id=73-ties-修剪选举与合并-trim-elect-sign--merge>7.3 TIES: 修剪、选举与合并 (Trim, Elect, Sign & Merge)<a class=anchor href=#73-ties-%e4%bf%ae%e5%89%aa%e9%80%89%e4%b8%be%e4%b8%8e%e5%90%88%e5%b9%b6-trim-elect-sign--merge>#</a></h3><p><strong>解决冲突的神器</strong>。当两个模型修改同一个参数时，如果一个要变大，一个要变小，通过平均会抵消变成0。TIES 拒绝这种"平庸的妥协"。</p><p><strong>算法三步走</strong>：</p><ol><li><strong>Trim（修剪）</strong>：只保留变化最大的 Top-K% 参数，重置其余微小变化（视为噪声）。<ul><li>$$\hat{\tau}_t = \text{TopK}(|\tau_t|, 20%)$$</li></ul></li><li><strong>Elect（选举）</strong>：对每个参数位置，统计各个模型的更新符号（+1 或 -1）。由于符号和总和决定主导方向。<ul><li>$$\text{sign}_{\text{final}} = \text{sign}(\sum \hat{\tau}_t)$$</li></ul></li><li><strong>Merge（合并）</strong>：只计算方向一致的更新的平均值。</li></ol><h3 id=74-dare-丢弃与重缩放-drop-and-rescale>7.4 DARE: 丢弃与重缩放 (Drop and Rescale)<a class=anchor href=#74-dare-%e4%b8%a2%e5%bc%83%e4%b8%8e%e9%87%8d%e7%bc%a9%e6%94%be-drop-and-rescale>#</a></h3><p><strong>DARE</strong> 发现了一个惊人的现象：微调后的 Delta 参数中，有很多是冗余的。</p><p><strong>核心操作</strong>：</p><ol><li><strong>Drop</strong>：随机丢弃 $p%$（例如90%）的 Delta 参数（置为0）。</li><li><strong>Rescale</strong>：为了保持期望不变，将剩余参数放大 $1/(1-p)$ 倍。</li></ol><p>$$ W_{\text{DARE}} = W_{\text{base}} + \frac{1}{1-p} \cdot \text{Mask}(W_{\text{fine-tuned}} - W_{\text{base}}) $$</p><p><strong>优势</strong>：极大地减少了参数冲突概率，能够合并多达 10+ 个模型而不发生性能崩溃。</p><h3 id=75-task-arithmetic-任务算术>7.5 Task Arithmetic: 任务算术<a class=anchor href=#75-task-arithmetic-%e4%bb%bb%e5%8a%a1%e7%ae%97%e6%9c%af>#</a></h3><p>将模型的能力视为向量，进行算术运算。</p><ul><li><p><strong>公式</strong>：$\tau_{\text{task}} = W_{\text{task}} - W_{\text{base}}$</p></li><li><p><strong>类比</strong>：Word2Vec 中的 <code>King - Man + Woman = Queen</code></p></li><li><p><strong>操作</strong>：
$$ W_{\text{new}} = W_{\text{base}} + \lambda_1 \tau_{\text{math}} + \lambda_2 \tau_{\text{code}} - \lambda_3 \tau_{\text{toxic}} $$</p><p>通过减去 &ldquo;Toxic Vector&rdquo;，甚至可以消除模型的有害性！</p></li></ul><h3 id=76-合并方法对比与选择>7.6 合并方法对比与选择<a class=anchor href=#76-%e5%90%88%e5%b9%b6%e6%96%b9%e6%b3%95%e5%af%b9%e6%af%94%e4%b8%8e%e9%80%89%e6%8b%a9>#</a></h3><table><thead><tr><th>方法</th><th>适用场景</th><th>计算成本</th><th>推荐指数</th></tr></thead><tbody><tr><td><strong>Linear</strong></td><td>同源微调，差异小</td><td>⭐</td><td>⭐⭐</td></tr><tr><td><strong>SLERP</strong></td><td>两个模型融合，追求平滑</td><td>⭐⭐</td><td>⭐⭐⭐⭐</td></tr><tr><td><strong>TIES</strong></td><td>解决强烈冲突</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td><strong>DARE</strong></td><td>合并大量模型 (>3个)</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td><strong>Task Arithmetic</strong></td><td>操控特定能力（增/删）</td><td>⭐⭐</td><td>⭐⭐⭐⭐</td></tr></tbody></table><hr><h2 id=八总结微调知识地图>八、总结：微调知识地图<a class=anchor href=#%e5%85%ab%e6%80%bb%e7%bb%93%e5%be%ae%e8%b0%83%e7%9f%a5%e8%af%86%e5%9c%b0%e5%9b%be>#</a></h2><h3 id=81-核心公式速查表>8.1 核心公式速查表<a class=anchor href=#81-%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5%e8%a1%a8>#</a></h3><table><thead><tr><th>概念</th><th>公式</th><th>含义</th></tr></thead><tbody><tr><td><strong>SFT Loss</strong></td><td>$\mathcal{L} = -\sum_{t \in \text{answer}} \log P(x_t \mid x_{&lt;t})$</td><td>只在答案部分计算</td></tr><tr><td><strong>LoRA</strong></td><td>$h = W_0 x + \frac{\alpha}{r} BAx$</td><td>低秩增量</td></tr><tr><td><strong>DoRA</strong></td><td>$W&rsquo; = m \frac{W_0 + BA}{|W_0 + BA|}$</td><td>方向+幅度</td></tr><tr><td><strong>GaLore</strong></td><td>$W_{t+1} = W_t - \eta P_r(\nabla_W)$</td><td>梯度投影</td></tr><tr><td><strong>显存占用</strong></td><td>$\text{Memory} = 2\Phi + 12\Phi_{\text{trainable}}$</td><td>AdamW开销</td></tr></tbody></table><h3 id=82-方法选择决策树>8.2 方法选择决策树<a class=anchor href=#82-%e6%96%b9%e6%b3%95%e9%80%89%e6%8b%a9%e5%86%b3%e7%ad%96%e6%a0%91>#</a></h3><pre class=mermaid>graph TD
    Start[开始微调项目] --&gt; Q1{显存是否充足?}

    Q1 --&gt;|&gt;= 80GB| Full[全量微调]
    Q1 --&gt;|&lt; 80GB| Q2{需要多任务?}

    Q2 --&gt;|是| Q3{显存 &gt;= 24GB?}
    Q2 --&gt;|否| Q4{显存 &gt;= 16GB?}

    Q3 --&gt;|是| LoRA[LoRA&lt;br/&gt;r=16-64]
    Q3 --&gt;|否| QLoRA[QLoRA&lt;br/&gt;4bit量化]

    Q4 --&gt;|是| LoRA2[LoRA&lt;br/&gt;r=8-16]
    Q4 --&gt;|否| QLoRA2[QLoRA&lt;br/&gt;r=8]

    Full --&gt; Merge[合并策略]
    LoRA --&gt; Merge
    LoRA2 --&gt; Merge
    QLoRA --&gt; Merge
    QLoRA2 --&gt; Merge

    Merge --&gt; End[部署]

    style Start fill:#e1f5ff
    style Full fill:#c8e6c9
    style LoRA fill:#fff9c4
    style LoRA2 fill:#fff9c4
    style QLoRA fill:#ffccbc
    style QLoRA2 fill:#ffccbc
    style End fill:#f3e5f5</pre><h3 id=83-实战建议清单>8.3 实战建议清单<a class=anchor href=#83-%e5%ae%9e%e6%88%98%e5%bb%ba%e8%ae%ae%e6%b8%85%e5%8d%95>#</a></h3><h4 id=831-数据准备>8.3.1 数据准备<a class=anchor href=#831-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87>#</a></h4><ul><li><input checked disabled type=checkbox> 使用 <code>tokenizer.apply_chat_template</code> 格式化对话</li><li><input disabled type=checkbox> 设置 <code>padding_side="right"</code></li><li><input disabled type=checkbox> 用 <code>DataCollatorForCompletionOnlyLM</code> 掩码指令</li><li><input disabled type=checkbox> 检查数据长度分布，设置合理的 <code>max_seq_length</code></li></ul><h4 id=832-超参数调优>8.3.2 超参数调优<a class=anchor href=#832-%e8%b6%85%e5%8f%82%e6%95%b0%e8%b0%83%e4%bc%98>#</a></h4><ul><li><input disabled type=checkbox> <strong>学习率</strong>：LoRA用 <code>2e-4</code>，全量用 <code>5e-6</code></li><li><input disabled type=checkbox> <strong>秩</strong>：从 <code>r=8</code> 开始，不够再加到 <code>16/32</code></li><li><input disabled type=checkbox> <strong>Alpha</strong>：通常设为 <code>2*r</code></li><li><input disabled type=checkbox> <strong>Warmup</strong>：3-5% 的训练步数</li><li><input disabled type=checkbox> <strong>Batch size</strong>：用梯度累积达到有效batch 16-32</li></ul><h4 id=833-显存优化>8.3.3 显存优化<a class=anchor href=#833-%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96>#</a></h4><ul><li><input disabled type=checkbox> 启用 <code>gradient_checkpointing=True</code>（省30-40%显存）</li><li><input disabled type=checkbox> 使用 <code>bf16=True</code>（推荐）或 <code>fp16=True</code></li><li><input disabled type=checkbox> 尝试 <code>torch.compile(model)</code> 加速（PyTorch 2.0+）</li><li><input disabled type=checkbox> QLoRA场景开启 <code>bnb_4bit_compute_dtype=torch.bfloat16</code></li></ul><h4 id=834-训练技巧>8.3.4 训练技巧<a class=anchor href=#834-%e8%ae%ad%e7%bb%83%e6%8a%80%e5%b7%a7>#</a></h4><ul><li><input disabled type=checkbox> 启用 <code>neftune_noise_alpha=5</code> 提升泛化</li><li><input disabled type=checkbox> 每个epoch后在验证集评估，防止过拟合</li><li><input disabled type=checkbox> 保存多个checkpoint，对比选最优</li><li><input disabled type=checkbox> 用 <code>wandb</code> 或 <code>tensorboard</code> 监控训练曲线</li></ul><h4 id=835-推理优化>8.3.5 推理优化<a class=anchor href=#835-%e6%8e%a8%e7%90%86%e4%bc%98%e5%8c%96>#</a></h4><ul><li><input disabled type=checkbox> 训练后调用 <code>model.merge_and_unload()</code> 合并权重</li><li><input disabled type=checkbox> 使用 <code>vLLM</code> 或 <code>TGI</code> 部署高并发服务</li><li><input disabled type=checkbox> 考虑进一步量化（INT8/INT4）降低推理成本</li></ul><h3 id=84-常见问题排查>8.4 常见问题排查<a class=anchor href=#84-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98%e6%8e%92%e6%9f%a5>#</a></h3><table><thead><tr><th>问题</th><th>可能原因</th><th>解决方案</th></tr></thead><tbody><tr><td><strong>Loss不下降</strong></td><td>学习率过大/过小</td><td>尝试 <code>1e-4</code> 到 <code>5e-4</code> 之间</td></tr><tr><td><strong>过拟合严重</strong></td><td>数据量太小</td><td>增加dropout、减小秩r、用NEFTune</td></tr><tr><td><strong>显存溢出</strong></td><td>Batch size太大</td><td>减小batch size，增加梯度累积步数</td></tr><tr><td><strong>训练很慢</strong></td><td>序列太长</td><td>截断到512/1024，开启Flash Attention</td></tr><tr><td><strong>推理结果差</strong></td><td>只训练了Attention</td><td>增加FFN层到<code>target_modules</code></td></tr></tbody></table><h3 id=85-进阶资源>8.5 进阶资源<a class=anchor href=#85-%e8%bf%9b%e9%98%b6%e8%b5%84%e6%ba%90>#</a></h3><p><strong>论文必读</strong>：</p><ol><li><strong>LoRA</strong>: <a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><strong>QLoRA</strong>: <a href=https://arxiv.org/abs/2305.14314>QLoRA: Efficient Finetuning of Quantized LLMs</a></li><li><strong>DoRA</strong>: <a href=https://arxiv.org/abs/2402.09353>DoRA: Weight-Decomposed Low-Rank Adaptation</a></li><li><strong>GaLore</strong>: <a href=https://arxiv.org/abs/2403.03507>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li></ol><p><strong>开源工具</strong>：</p><ul><li><strong>Hugging Face TRL</strong>: <a href=https://github.com/huggingface/trl>https://github.com/huggingface/trl</a></li><li><strong>Axolotl</strong>: <a href=https://github.com/OpenAccess-AI-Collective/axolotl>https://github.com/OpenAccess-AI-Collective/axolotl</a></li><li><strong>LLaMA-Factory</strong>: <a href=https://github.com/hiyouga/LLaMA-Factory>https://github.com/hiyouga/LLaMA-Factory</a></li></ul><p><strong>数据集</strong>：</p><ul><li><strong>Alpaca</strong>: 指令跟随基础数据</li><li><strong>Dolly-15k</strong>: 开源商用友好</li><li><strong>OpenOrca</strong>: 高质量合成数据</li><li><strong>UltraChat</strong>: 多轮对话数据</li></ul><hr><h2 id=-新手问答从困惑到理解>💡 新手问答：从困惑到理解<a class=anchor href=#-%e6%96%b0%e6%89%8b%e9%97%ae%e7%ad%94%e4%bb%8e%e5%9b%b0%e6%83%91%e5%88%b0%e7%90%86%e8%a7%a3>#</a></h2><p><strong>Q1: 微调需要多少数据？是越多越好吗？</strong>
A: <strong>绝对不是</strong>。对于SFT，数据质量远比数量重要。</p><ul><li><strong>Rule of Thumb</strong>：几百条高质量（人工精修）指令 > 几万条低质量合成指令。</li><li>LIMA论文证明：仅用 <strong>1000条</strong> 精选数据，微调效果就能匹敌用50000条数据的模型。</li></ul><p><strong>Q2: 我的显只有24GB（3090/4090），能微调多大的模型？</strong>
A: 使用 <strong>QLoRA (4bit)</strong> 技术：</p><ul><li><strong>7B/8B模型</strong>：只需 ~6-8GB 显存，单卡轻松跑。</li><li><strong>13B/14B模型</strong>：只需 ~12-14GB 显存，单卡可跑。</li><li><strong>30B-70B模型</strong>：配合 <strong>CPU Offload</strong> 或多卡，也可以尝试，但速度较慢。</li></ul><p><strong>Q3: 微调后，模型好像变"笨"了，以前认识的人现在不认识了？</strong>
A: 这是 <strong>灾难性遗忘 (Catastrophic Forgetting)</strong>。</p><ul><li><strong>原因</strong>：模型过度拟合了特定领域的指令分布。</li><li><strong>解法</strong>：在微调数据中混入 <strong>1% - 5%</strong> 的通用数据（如Wiki、GSM8K数学题），或者使用 LoRA（因为它保留了原始权重）。</li></ul><p><strong>Q4: Loss一直在下降，但模型回答全是乱码或重复，为什么？</strong>
A: 这通常是 <strong>Padding方向</strong> 或 <strong>EOS Token</strong> 的问题。</p><ul><li>检查 <code>tokenizer.padding_side="right"</code>（SFT必须右对齐）。</li><li>检查训练数据末尾是否手动加了 <code>&lt;|eot_id|></code> 或 <code>&lt;/s></code>，否则模型不知道何时停止，会一直复读。</li></ul><p><strong>Q5: 什么时候该用 RLHF (DPO/PPO)？</strong>
A: SFT 只能教会模型 <strong>&ldquo;怎么说话&rdquo;</strong>（格式、语气），RLHF 才能教会模型 <strong>&ldquo;说什么话&rdquo;</strong>（价值观、偏好）。</p><ul><li>如果你只是想让模型按特定JSON格式输出，SFT足矣。</li><li>如果你想让模型更安全、更诚实、或者在开放域对话中更有趣，需要 RLHF。</li></ul><hr><h2 id=结语微调的艺术与科学>结语：微调的艺术与科学<a class=anchor href=#%e7%bb%93%e8%af%ad%e5%be%ae%e8%b0%83%e7%9a%84%e8%89%ba%e6%9c%af%e4%b8%8e%e7%a7%91%e5%ad%a6>#</a></h2><p>微调不是简单的"调参数"，而是在<strong>保留通用知识</strong>和<strong>学习专业技能</strong>之间的精妙平衡。LoRA及其家族方法告诉我们：<strong>有时候，改变一小部分，就能产生巨大的影响</strong>。</p><p>记住三个关键原则：</p><ol><li><strong>精准的损失计算</strong>：只在需要学习的部分计算Loss</li><li><strong>高效的参数更新</strong>：用低秩分解减少可训练参数</li><li><strong>稳定的训练过程</strong>：正确的初始化和超参数设置</li></ol><p>现在，拿起你的数据集，开始微调你的第一个专属模型吧！</p><hr><p><strong>本章检查清单</strong>：</p><ul><li><input checked disabled type=checkbox> 理解SFT的Token级掩码机制</li><li><input checked disabled type=checkbox> 掌握显存占用的12字节计算</li><li><input checked disabled type=checkbox> 实现LoRA/DoRA的PyTorch代码</li><li><input checked disabled type=checkbox> 了解QLoRA/GaLore的原理</li><li><input checked disabled type=checkbox> 用TRL库完成完整训练流程</li><li><input checked disabled type=checkbox> 学会模型合并与部署技巧</li></ul><p><strong>下一章预告</strong>：《第3章 强化学习人类反馈（RLHF）：让模型学会"讨好"用户》—— 我们将探索如何通过PPO/DPO算法，让模型输出更符合人类偏好。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第1章 数据工程基础</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/ class="flex align-center"><span>第3章 与人类对齐：偏好优化</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#引言为什么需要微调>引言：为什么需要微调？</a></li><li><a href=#一微调的本质loss函数视角>一、微调的本质：Loss函数视角</a><ul><li><a href=#11-预训练-vs-微调目标函数的差异>1.1 预训练 vs 微调：目标函数的差异</a></li><li><a href=#12-sft-loss-图解token级掩码表>1.2 SFT Loss 图解：Token级掩码表</a></li></ul></li><li><a href=#二显存账单为什么全量微调这么贵>二、显存账单：为什么全量微调这么贵？</a><ul><li><a href=#21-显存占用的四大开销>2.1 显存占用的四大开销</a></li><li><a href=#22-adamw的12字节秘密>2.2 AdamW的12字节秘密</a></li><li><a href=#23-全量微调-vs-peft-显存对比>2.3 全量微调 vs PEFT 显存对比</a></li></ul></li><li><a href=#三lora核心低秩适配的数学本质>三、LoRA核心：低秩适配的数学本质</a><ul><li><a href=#31-核心公式秩分解>3.1 核心公式：秩分解</a></li><li><a href=#32-lora架构图>3.2 LoRA架构图</a></li><li><a href=#33-深度问答为什么不能全0初始化>3.3 深度问答：为什么不能全0初始化？</a></li><li><a href=#34-pytorch原生实现loralinear>3.4 PyTorch原生实现：LoRALinear</a></li></ul></li><li><a href=#四lora家族演进从qlora到galore>四、LoRA家族演进：从QLoRA到GaLore</a><ul><li><a href=#41-qlora量化的艺术>4.1 QLoRA：量化的艺术</a><ul><li><a href=#411-nf4量化原理>4.1.1 NF4量化原理</a></li></ul></li><li><a href=#42-dora方向与幅度的解耦>4.2 DoRA：方向与幅度的解耦</a><ul><li><a href=#421-dora完整实现>4.2.1 DoRA完整实现</a></li></ul></li><li><a href=#43-galore梯度低秩投影>4.3 GaLore：梯度低秩投影</a><ul><li><a href=#431-原理推导>4.3.1 原理推导</a></li><li><a href=#432-算法流程>4.3.2 算法流程</a></li></ul></li><li><a href=#44-lora非对称学习率>4.4 LoRA+：非对称学习率</a></li><li><a href=#45-其他变种简介>4.5 其他变种简介</a><ul><li><a href=#adalora自适应秩分配>AdaLoRA：自适应秩分配</a></li><li><a href=#vera极致参数效率>VeRA：极致参数效率</a></li></ul></li></ul></li><li><a href=#五微调深度理解>五、微调深度理解</a><ul><li><a href=#51-指令数据构建的艺术>5.1 指令数据构建的艺术</a><ul><li><a href=#1-指令数据的黄金标准>(1) 指令数据的黄金标准</a></li><li><a href=#2-self-instruct-用gpt-4生成训练数据>(2) Self-Instruct: 用GPT-4生成训练数据</a></li><li><a href=#3-数据增强技术>(3) 数据增强技术</a></li></ul></li><li><a href=#52-灾难性遗忘-catastrophic-forgetting>5.2 灾难性遗忘 (Catastrophic Forgetting)</a><ul><li><a href=#缓解策略1-混合训练数据>缓解策略1: 混合训练数据</a></li><li><a href=#缓解策略2-elastic-weight-consolidation-ewc>缓解策略2: Elastic Weight Consolidation (EWC)</a></li></ul></li><li><a href=#53-多任务微调-multi-task-fine-tuning>5.3 多任务微调 (Multi-task Fine-tuning)</a><ul><li><a href=#1-任务标识符-task-prefix>(1) 任务标识符 (Task Prefix)</a></li><li><a href=#2-任务特定适配器>(2) 任务特定适配器</a></li></ul></li><li><a href=#54-持续学习-continual-learning>5.4 持续学习 (Continual Learning)</a><ul><li><a href=#1-渐进式lora-progressive-lora>(1) 渐进式LoRA (Progressive LoRA)</a></li><li><a href=#2-知识蒸馏-knowledge-distillation>(2) 知识蒸馏 (Knowledge Distillation)</a></li><li><a href=#3-完整持续学习流程>(3) 完整持续学习流程</a></li></ul></li></ul></li><li><a href=#六工程实战用trl库微调模型>六、工程实战：用TRL库微调模型</a><ul><li><a href=#61-完整训练流程>6.1 完整训练流程</a></li><li><a href=#62-关键技术详解>6.2 关键技术详解</a><ul><li><a href=#621-chat-template的正确使用>6.2.1 Chat Template的正确使用</a></li><li><a href=#622-padding避坑指南>6.2.2 Padding避坑指南</a></li><li><a href=#623-neftune嵌入层加噪技巧>6.2.3 NEFTune：嵌入层加噪技巧</a></li></ul></li><li><a href=#63-推理与部署>6.3 推理与部署</a></li></ul></li><li><a href=#七模型合并技术>七、模型合并技术</a><ul><li><a href=#71-线性插值合并-weight-averaging>7.1 线性插值合并 (Weight Averaging)</a></li><li><a href=#72-slerp-球面线性插值>7.2 SLERP: 球面线性插值</a></li><li><a href=#73-ties-修剪选举与合并-trim-elect-sign--merge>7.3 TIES: 修剪、选举与合并 (Trim, Elect, Sign & Merge)</a></li><li><a href=#74-dare-丢弃与重缩放-drop-and-rescale>7.4 DARE: 丢弃与重缩放 (Drop and Rescale)</a></li><li><a href=#75-task-arithmetic-任务算术>7.5 Task Arithmetic: 任务算术</a></li><li><a href=#76-合并方法对比与选择>7.6 合并方法对比与选择</a></li></ul></li><li><a href=#八总结微调知识地图>八、总结：微调知识地图</a><ul><li><a href=#81-核心公式速查表>8.1 核心公式速查表</a></li><li><a href=#82-方法选择决策树>8.2 方法选择决策树</a></li><li><a href=#83-实战建议清单>8.3 实战建议清单</a><ul><li><a href=#831-数据准备>8.3.1 数据准备</a></li><li><a href=#832-超参数调优>8.3.2 超参数调优</a></li><li><a href=#833-显存优化>8.3.3 显存优化</a></li><li><a href=#834-训练技巧>8.3.4 训练技巧</a></li><li><a href=#835-推理优化>8.3.5 推理优化</a></li></ul></li><li><a href=#84-常见问题排查>8.4 常见问题排查</a></li><li><a href=#85-进阶资源>8.5 进阶资源</a></li></ul></li><li><a href=#-新手问答从困惑到理解>💡 新手问答：从困惑到理解</a></li><li><a href=#结语微调的艺术与科学>结语：微调的艺术与科学</a></li></ul></nav></div></aside></main></body></html>