<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第4章：高性能嵌入模型实战 (Embedding Models)# “Good representations are the foundation of AI.” —— 优秀的表示层是人工智能的基石。本章将从零开始，深入探讨如何构建用于语义检索（Semantic Search）和 RAG 的高性能嵌入模型。
目录# 第一节：嵌入模型的本质与架构 1.1 为什么需要嵌入模型？（语义鸿沟） 1.2 嵌入模型核心架构 (Bi vs Cross) 1.3 嵌入空间的数学本质 1.4 SOTA嵌入模型对比 1.5 本节小结 第二节：对比学习与InfoNCE损失 2.1 对比学习的核心思想 2.2 InfoNCE损失函数详解 2.3 In-Batch Negatives 高效训练策略 2.4 关键超参：温度系数 ($\tau$) 的影响 2.5 实战：实现对比学习训练器 2.6 本节小结 第三节：数据工程：难负样本挖掘 3.1 为什么需要难负样本？ 3.2 静态挖掘：BM25策略 3.3 动态挖掘：ANCE算法 3.4 合成数据：LLM蒸馏 (Data Distillation) 3.5 本节小结 第四节：多任务联合训练与嵌套表示 4.1 为什么需要多任务训练？ 4.2 多任务训练框架 4.3 Matryoshka嵌套嵌入 (MRL) 4.4 本节小结 第五节：从零实战：训练与部署 5.1 完整训练流程代码 5.2 模型评估：MTEB基准 5.3 生产环境部署建议 第4章小结 思考练习 参考资料 第一节：嵌入模型的本质与架构# 1.1 为什么需要嵌入模型？（语义鸿沟）# 在深入技术细节前，我们先回答一个根本问题：为什么传统的关键词搜索（如 ElasticSearch 的默认设置）在 AI 时代不够用了？
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第4章 创建更优的嵌入模型"><meta property="og:description" content="第4章：高性能嵌入模型实战 (Embedding Models)# “Good representations are the foundation of AI.” —— 优秀的表示层是人工智能的基石。本章将从零开始，深入探讨如何构建用于语义检索（Semantic Search）和 RAG 的高性能嵌入模型。
目录# 第一节：嵌入模型的本质与架构 1.1 为什么需要嵌入模型？（语义鸿沟） 1.2 嵌入模型核心架构 (Bi vs Cross) 1.3 嵌入空间的数学本质 1.4 SOTA嵌入模型对比 1.5 本节小结 第二节：对比学习与InfoNCE损失 2.1 对比学习的核心思想 2.2 InfoNCE损失函数详解 2.3 In-Batch Negatives 高效训练策略 2.4 关键超参：温度系数 ($\tau$) 的影响 2.5 实战：实现对比学习训练器 2.6 本节小结 第三节：数据工程：难负样本挖掘 3.1 为什么需要难负样本？ 3.2 静态挖掘：BM25策略 3.3 动态挖掘：ANCE算法 3.4 合成数据：LLM蒸馏 (Data Distillation) 3.5 本节小结 第四节：多任务联合训练与嵌套表示 4.1 为什么需要多任务训练？ 4.2 多任务训练框架 4.3 Matryoshka嵌套嵌入 (MRL) 4.4 本节小结 第五节：从零实战：训练与部署 5.1 完整训练流程代码 5.2 模型评估：MTEB基准 5.3 生产环境部署建议 第4章小结 思考练习 参考资料 第一节：嵌入模型的本质与架构# 1.1 为什么需要嵌入模型？（语义鸿沟）# 在深入技术细节前，我们先回答一个根本问题：为什么传统的关键词搜索（如 ElasticSearch 的默认设置）在 AI 时代不够用了？"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第4章 创建更优的嵌入模型"><meta itemprop=description content="第4章：高性能嵌入模型实战 (Embedding Models)# “Good representations are the foundation of AI.” —— 优秀的表示层是人工智能的基石。本章将从零开始，深入探讨如何构建用于语义检索（Semantic Search）和 RAG 的高性能嵌入模型。
目录# 第一节：嵌入模型的本质与架构 1.1 为什么需要嵌入模型？（语义鸿沟） 1.2 嵌入模型核心架构 (Bi vs Cross) 1.3 嵌入空间的数学本质 1.4 SOTA嵌入模型对比 1.5 本节小结 第二节：对比学习与InfoNCE损失 2.1 对比学习的核心思想 2.2 InfoNCE损失函数详解 2.3 In-Batch Negatives 高效训练策略 2.4 关键超参：温度系数 ($\tau$) 的影响 2.5 实战：实现对比学习训练器 2.6 本节小结 第三节：数据工程：难负样本挖掘 3.1 为什么需要难负样本？ 3.2 静态挖掘：BM25策略 3.3 动态挖掘：ANCE算法 3.4 合成数据：LLM蒸馏 (Data Distillation) 3.5 本节小结 第四节：多任务联合训练与嵌套表示 4.1 为什么需要多任务训练？ 4.2 多任务训练框架 4.3 Matryoshka嵌套嵌入 (MRL) 4.4 本节小结 第五节：从零实战：训练与部署 5.1 完整训练流程代码 5.2 模型评估：MTEB基准 5.3 生产环境部署建议 第4章小结 思考练习 参考资料 第一节：嵌入模型的本质与架构# 1.1 为什么需要嵌入模型？（语义鸿沟）# 在深入技术细节前，我们先回答一个根本问题：为什么传统的关键词搜索（如 ElasticSearch 的默认设置）在 AI 时代不够用了？"><meta itemprop=wordCount content="953"><title>第4章 创建更优的嵌入模型 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle checked>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/ class=active>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第4章 创建更优的嵌入模型</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节嵌入模型的本质与架构>第一节：嵌入模型的本质与架构</a><ul><li><a href=#11-为什么需要嵌入模型语义鸿沟>1.1 为什么需要嵌入模型？（语义鸿沟）</a></li><li><a href=#12-嵌入模型核心架构-bi-vs-cross>1.2 嵌入模型核心架构 (Bi vs Cross)</a><ul><li><a href=#1-bi-encoder-双编码器--召回层-retrieval>1. Bi-Encoder (双编码器) —— 召回层 (Retrieval)</a></li><li><a href=#2-cross-encoder-交叉编码器--精排层-reranking>2. Cross-Encoder (交叉编码器) —— 精排层 (Reranking)</a></li></ul></li><li><a href=#13-嵌入空间的数学本质>1.3 嵌入空间的数学本质</a></li><li><a href=#14-sota嵌入模型对比>1.4 SOTA嵌入模型对比</a></li><li><a href=#15-本节小结>1.5 本节小结</a></li></ul></li><li><a href=#第二节对比学习与infonce损失>第二节：对比学习与InfoNCE损失</a><ul><li><a href=#21-对比学习的核心思想>2.1 对比学习的核心思想</a></li><li><a href=#22-infonce-损失函数详解>2.2 InfoNCE 损失函数详解</a></li><li><a href=#23-in-batch-negatives-高效训练策略>2.3 In-Batch Negatives 高效训练策略</a></li><li><a href=#24-关键超参温度系数-tau-的影响>2.4 关键超参：温度系数 ($\tau$) 的影响</a></li><li><a href=#25-实战实现对比学习训练器>2.5 实战：实现对比学习训练器</a></li><li><a href=#26-本节小结>2.6 本节小结</a></li></ul></li><li><a href=#第三节数据工程难负样本挖掘>第三节：数据工程：难负样本挖掘</a><ul><li><a href=#31-为什么需要难负样本>3.1 为什么需要难负样本？</a></li><li><a href=#32-静态挖掘bm25策略>3.2 静态挖掘：BM25策略</a></li><li><a href=#33-动态挖掘ance算法>3.3 动态挖掘：ANCE算法</a></li><li><a href=#34-合成数据llm蒸馏-data-distillation>3.4 合成数据：LLM蒸馏 (Data Distillation)</a></li><li><a href=#35-本节小结>3.5 本节小结</a></li></ul></li><li><a href=#第四节多任务联合训练与嵌套表示>第四节：多任务联合训练与嵌套表示</a><ul><li><a href=#41-为什么需要多任务训练>4.1 为什么需要多任务训练？</a></li><li><a href=#42-多任务训练框架>4.2 多任务训练框架</a></li><li><a href=#43-matryoshka嵌套嵌入-mrl>4.3 Matryoshka嵌套嵌入 (MRL)</a></li><li><a href=#44-本节小结>4.4 本节小结</a></li></ul></li><li><a href=#第五节从零实战训练与部署>第五节：从零实战：训练与部署</a><ul><li><a href=#51-完整训练流程代码>5.1 完整训练流程代码</a></li><li><a href=#52-模型评估mteb基准>5.2 模型评估：MTEB基准</a></li><li><a href=#53-生产环境部署建议>5.3 生产环境部署建议</a></li></ul></li><li><a href=#第4章小结>第4章小结</a><ul><li><a href=#决策指南何时训练自己的嵌入模型>决策指南：何时训练自己的嵌入模型？</a></li><li><a href=#核心技术回顾>核心技术回顾</a><ul><li><a href=#1-嵌入模型架构>1. 嵌入模型架构</a></li><li><a href=#2-训练关键技术>2. 训练关键技术</a></li><li><a href=#3-前沿技术趋势>3. 前沿技术趋势</a></li></ul></li><li><a href=#延伸阅读资源>延伸阅读资源</a></li><li><a href=#思考练习>思考练习</a><ul><li><a href=#基础练习>基础练习</a></li><li><a href=#高级练习>高级练习</a></li><li><a href=#实战项目>实战项目</a></li></ul></li><li><a href=#参考资料>参考资料</a><ul><li><a href=#核心论文>核心论文</a></li><li><a href=#开源项目>开源项目</a></li><li><a href=#数据集>数据集</a></li></ul></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第4章高性能嵌入模型实战-embedding-models>第4章：高性能嵌入模型实战 (Embedding Models)<a class=anchor href=#%e7%ac%ac4%e7%ab%a0%e9%ab%98%e6%80%a7%e8%83%bd%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%ae%9e%e6%88%98-embedding-models>#</a></h1><blockquote class=book-hint><p>&ldquo;Good representations are the foundation of AI.&rdquo; —— 优秀的表示层是人工智能的基石。本章将从零开始，深入探讨如何构建用于语义检索（Semantic Search）和 RAG 的高性能嵌入模型。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%b8%8e%e6%9e%b6%e6%9e%84>第一节：嵌入模型的本质与架构</a><ul><li><a href=#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e8%af%ad%e4%b9%89%e9%b8%bf%e6%b2%9f>1.1 为什么需要嵌入模型？（语义鸿沟）</a></li><li><a href=#12-%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e6%a0%b8%e5%bf%83%e6%9e%b6%e6%9e%84-bi-vs-cross>1.2 嵌入模型核心架构 (Bi vs Cross)</a></li><li><a href=#13-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8>1.3 嵌入空间的数学本质</a></li><li><a href=#14-sota%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%af%b9%e6%af%94>1.4 SOTA嵌入模型对比</a></li><li><a href=#15-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>1.5 本节小结</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e8%8a%82%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e4%b8%8einfonce%e6%8d%9f%e5%a4%b1>第二节：对比学习与InfoNCE损失</a><ul><li><a href=#21-%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>2.1 对比学习的核心思想</a></li><li><a href=#22-infonce%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e8%af%a6%e8%a7%a3>2.2 InfoNCE损失函数详解</a></li><li><a href=#23-in-batch-negatives-%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5>2.3 In-Batch Negatives 高效训练策略</a></li><li><a href=#24-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82%e6%b8%a9%e5%ba%a6%e7%b3%bb%e6%95%b0-tau-%e7%9a%84%e5%bd%b1%e5%93%8d>2.4 关键超参：温度系数 ($\tau$) 的影响</a></li><li><a href=#25-%e5%ae%9e%e6%88%98%e5%ae%9e%e7%8e%b0%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e8%ae%ad%e7%bb%83%e5%99%a8>2.5 实战：实现对比学习训练器</a></li><li><a href=#26-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>2.6 本节小结</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac%e6%8c%96%e6%8e%98>第三节：数据工程：难负样本挖掘</a><ul><li><a href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac>3.1 为什么需要难负样本？</a></li><li><a href=#32-%e9%9d%99%e6%80%81%e6%8c%96%e6%8e%98bm25%e7%ad%96%e7%95%a5>3.2 静态挖掘：BM25策略</a></li><li><a href=#33-%e5%8a%a8%e6%80%81%e6%8c%96%e6%8e%98ance%e7%ae%97%e6%b3%95>3.3 动态挖掘：ANCE算法</a></li><li><a href=#34-%e5%90%88%e6%88%90%e6%95%b0%e6%8d%aellm%e8%92%b8%e9%a6%8f-data-distillation>3.4 合成数据：LLM蒸馏 (Data Distillation)</a></li><li><a href=#35-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>3.5 本节小结</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e8%8a%82%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%81%94%e5%90%88%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%b5%8c%e5%a5%97%e8%a1%a8%e7%a4%ba>第四节：多任务联合训练与嵌套表示</a><ul><li><a href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83>4.1 为什么需要多任务训练？</a></li><li><a href=#42-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6>4.2 多任务训练框架</a></li><li><a href=#43-matryoshka%e5%b5%8c%e5%a5%97%e5%b5%8c%e5%85%a5-mrl>4.3 Matryoshka嵌套嵌入 (MRL)</a></li><li><a href=#44-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>4.4 本节小结</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e8%8a%82%e4%bb%8e%e9%9b%b6%e5%ae%9e%e6%88%98%e8%ae%ad%e7%bb%83%e4%b8%8e%e9%83%a8%e7%bd%b2>第五节：从零实战：训练与部署</a><ul><li><a href=#51-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b%e4%bb%a3%e7%a0%81>5.1 完整训练流程代码</a></li><li><a href=#52-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0mteb%e5%9f%ba%e5%87%86>5.2 模型评估：MTEB基准</a></li><li><a href=#53-%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2%e5%bb%ba%e8%ae%ae>5.3 生产环境部署建议</a></li></ul></li><li><a href=#%e7%ac%ac4%e7%ab%a0%e5%b0%8f%e7%bb%93>第4章小结</a></li><li><a href=#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0>思考练习</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99>参考资料</a></li></ul><hr><h2 id=第一节嵌入模型的本质与架构>第一节：嵌入模型的本质与架构<a class=anchor href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%b8%8e%e6%9e%b6%e6%9e%84>#</a></h2><h3 id=11-为什么需要嵌入模型语义鸿沟>1.1 为什么需要嵌入模型？（语义鸿沟）<a class=anchor href=#11-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e8%af%ad%e4%b9%89%e9%b8%bf%e6%b2%9f>#</a></h3><p>在深入技术细节前，我们先回答一个根本问题：为什么传统的关键词搜索（如 ElasticSearch 的默认设置）在 AI 时代不够用了？</p><p>核心痛点在于 <strong>Lexical Gap（词汇鸿沟）</strong>。</p><ul><li><strong>关键词检索 (BM25/TF-IDF)</strong>：基于<strong>字面匹配</strong>。<ul><li><em>Query</em>: &ldquo;如何训练深度学习模型？&rdquo;</li><li><em>Doc</em>: &ldquo;神经网络的反向传播算法详解&mldr;&rdquo;</li><li><em>Result</em>: <strong>匹配失败</strong>。因为 Doc 中没有出现“深度学习”这个词，尽管它就是标准答案。</li></ul></li><li><strong>语义检索 (Embedding)</strong>：基于<strong>语义向量距离</strong>。<ul><li>模型将“深度学习”和“神经网络”映射到向量空间中相近的位置。</li><li><em>Result</em>: <strong>成功召回</strong>。即使没有词汇重叠，也能理解其内在的语义关联。</li></ul></li></ul><p><strong>核心价值</strong>：嵌入模型将非结构化的文本（Text）转化为计算机可计算的向量（Vector），使得“计算语义相似度”成为可能。</p><h3 id=12-嵌入模型核心架构-bi-vs-cross>1.2 嵌入模型核心架构 (Bi vs Cross)<a class=anchor href=#12-%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e6%a0%b8%e5%bf%83%e6%9e%b6%e6%9e%84-bi-vs-cross>#</a></h3><p>工业界最经典的检索架构是**“漏斗模式” (Retrieval Funnel)**，由两种不同架构的模型组成：</p><h4 id=1-bi-encoder-双编码器--召回层-retrieval>1. Bi-Encoder (双编码器) —— 召回层 (Retrieval)<a class=anchor href=#1-bi-encoder-%e5%8f%8c%e7%bc%96%e7%a0%81%e5%99%a8--%e5%8f%ac%e5%9b%9e%e5%b1%82-retrieval>#</a></h4><ul><li><strong>架构</strong>：两个独立的 BERT 编码器（通常共享参数）。</li><li><strong>公式</strong>：$sim(q, d) = \cos(\text{Enc}(q), \text{Enc}(d))$</li><li><strong>特性</strong>：<ul><li><strong>速度极快</strong>：文档向量可以<strong>预先计算</strong>并存入向量数据库（Milvus/Faiss）。</li><li><strong>精度中等</strong>：Query 和 Doc 缺乏深层交互。</li></ul></li><li><strong>作用</strong>：从海量数据（100万+）中快速筛选 Top-100。</li></ul><h4 id=2-cross-encoder-交叉编码器--精排层-reranking>2. Cross-Encoder (交叉编码器) —— 精排层 (Reranking)<a class=anchor href=#2-cross-encoder-%e4%ba%a4%e5%8f%89%e7%bc%96%e7%a0%81%e5%99%a8--%e7%b2%be%e6%8e%92%e5%b1%82-reranking>#</a></h4><ul><li><strong>架构</strong>：Query 和 Doc 拼接后输入同一个 BERT。</li><li><strong>公式</strong>：$score = \text{Enc}([CLS] \ q \ [SEP] \ d \ [SEP])$</li><li><strong>特性</strong>：<ul><li><strong>精度极高</strong>：利用 Self-Attention 全连接，能捕获“否定词”、“定语”等细微语义。</li><li><strong>速度慢</strong>：无法预计算，适合处理少量数据。</li></ul></li><li><strong>作用</strong>：对 Top-100 进行精细打分，输出最终 Top-10。</li></ul><h3 id=13-嵌入空间的数学本质>1.3 嵌入空间的数学本质<a class=anchor href=#13-%e5%b5%8c%e5%85%a5%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8>#</a></h3><p>一个健康的嵌入空间应该具备良好的几何性质，而不是一团混乱。</p><p><strong>1. 避免表示坍塌 (Representation Collapse)</strong></p><ul><li>如果模型训练失败，所有文本的向量可能会挤在空间的一个小角落，导致任意两个文本的相似度都高达 0.99。</li><li><strong>理想状态</strong>：无关文本的相似度应接近 0，相关文本接近 1。</li></ul><p><strong>2. 各向同性 (Isotropy)</strong></p><ul><li><strong>各向异性 (由差的模型产生)</strong>：向量分布呈圆锥形 (Cone)，所有向量都指向同一个大方向，占据很小的空间体积。</li><li><strong>各向同性 (由好的模型产生)</strong>：向量均匀分布在整个高维球面上，最大化空间利用率。</li><li><strong>手段</strong>：通过对比学习（Contrastive Learning）和规范化（Normalization）来校正空间分布。</li></ul><h3 id=14-sota嵌入模型对比>1.4 SOTA嵌入模型对比<a class=anchor href=#14-sota%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e5%af%b9%e6%af%94>#</a></h3><p>根据 MTEB (Massive Text Embedding Benchmark) 数据，主流模型已全面转向 LLM 基座。</p><table><thead><tr><th>模型</th><th>基座架构</th><th>维度</th><th>最大长度</th><th>备注</th></tr></thead><tbody><tr><td><strong>Voyage-Large</strong></td><td>Transformer (闭源)</td><td>2048</td><td>32k</td><td>商业闭源 SOTA，针对 RAG 优化</td></tr><tr><td><strong>GTE-Qwen2-7B</strong></td><td>Qwen2 (Decoder)</td><td>3584</td><td>32k</td><td>开源 SOTA，利用 LLM 强语义</td></tr><tr><td><strong>OpenAI v3</strong></td><td>Undisclosed</td><td>3072</td><td>8k</td><td>工业标准基线</td></tr><tr><td><strong>BGE-M3</strong></td><td>XLM-RoBERTa</td><td>1024</td><td>8k</td><td>支持多语言、稀疏检索 (Sparse)</td></tr><tr><td><strong>E5-Mistral</strong></td><td>Mistral-7B</td><td>4096</td><td>32k</td><td>首个证明 Decoder 优于 Encoder 的工作</td></tr></tbody></table><h3 id=15-本节小结>1.5 本节小结<a class=anchor href=#15-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h3><ul><li>嵌入模型解决了关键词匹配的语义鸿沟问题。</li><li><strong>Bi-Encoder</strong> 负责快（召回），<strong>Cross-Encoder</strong> 负责准（精排）。</li><li>优秀的嵌入空间应通过训练达到<strong>各向同性</strong>，避免坍塌。</li></ul><hr><h2 id=第二节对比学习与infonce损失>第二节：对比学习与InfoNCE损失<a class=anchor href=#%e7%ac%ac%e4%ba%8c%e8%8a%82%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e4%b8%8einfonce%e6%8d%9f%e5%a4%b1>#</a></h2><h3 id=21-对比学习的核心思想>2.1 对比学习的核心思想<a class=anchor href=#21-%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p>训练嵌入模型不再是预测下一个词，而是<strong>辨别</strong>（Discrimination）。</p><ul><li><strong>拉近 (Pull)</strong>：让 Query 和相关的 Document（正样本）在向量空间靠得更近。</li><li><strong>推远 (Push)</strong>：让 Query 和无关的 Document（负样本）在向量空间离得更远。</li></ul><h3 id=22-infonce-损失函数详解>2.2 InfoNCE 损失函数详解<a class=anchor href=#22-infonce-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e8%af%a6%e8%a7%a3>#</a></h3><p>InfoNCE 是对比学习的灵魂公式：</p><p>$$
\mathcal{L} = - \log \frac{e^{sim(q, d^+) / \tau}}{e^{sim(q, d^+) / \tau} + \sum_{i=1}^{K} e^{sim(q, d^-_i) / \tau}}
$$</p><ul><li><strong>分子</strong>：正样本的相似度得分（指数化）。</li><li><strong>分母</strong>：所有候选样本（正+负）的得分总和。</li><li><strong>目标</strong>：最大化正样本在分布中的概率占比（类似于分类问题中的 Softmax）。</li></ul><h3 id=23-in-batch-negatives-高效训练策略>2.3 In-Batch Negatives 高效训练策略<a class=anchor href=#23-in-batch-negatives-%e9%ab%98%e6%95%88%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5>#</a></h3><p>为了计算分母中的负样本，我们不需要显式采样。利用 GPU 矩阵运算的特性，我们可以<strong>复用 Batch 内的其他样本</strong>。</p><p><strong>原理</strong>：
假设 Batch Size = $N$。
对于第 $i$ 个 Query，除了它对应的第 $i$ 个 Doc 是正样本，Batch 内剩余的 $N-1$ 个 Doc 全都可以视为负样本。</p><p><strong>这意味着</strong>：Batch Size 越大 $\rightarrow$ 负样本越多 $\rightarrow$ 训练越难 $\rightarrow$ 模型效果越好。SOTA 训练通常需要 Batch Size > 1024。</p><h3 id=24-关键超参温度系数-tau-的影响>2.4 关键超参：温度系数 ($\tau$) 的影响<a class=anchor href=#24-%e5%85%b3%e9%94%ae%e8%b6%85%e5%8f%82%e6%b8%a9%e5%ba%a6%e7%b3%bb%e6%95%b0-tau-%e7%9a%84%e5%bd%b1%e5%93%8d>#</a></h3><p>公式中的 $\tau$ (Temperature) 是一个极其关键的超参数。</p><ul><li><strong>$\tau$ 较大 (如 0.1, 1.0)</strong>：<ul><li>分布平滑。模型对所有负样本“一视同仁”，梯度也比较平缓。</li><li><em>缺点</em>：模型可能学不到很难的细微差别。</li></ul></li><li><strong>$\tau$ 较小 (如 0.01, 0.05)</strong>：<ul><li>分布尖锐。模型会<strong>极端关注</strong>那些得分很高但实际是错误的<strong>困难负样本</strong>（Hard Negatives），忽略简单的负样本。</li><li><em>建议</em>：通常设为 <strong>0.02 - 0.05</strong>，这能迫使模型学习具有区分力的特征。</li></ul></li></ul><h3 id=25-实战实现对比学习训练器>2.5 实战：实现对比学习训练器<a class=anchor href=#25-%e5%ae%9e%e6%88%98%e5%ae%9e%e7%8e%b0%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e8%ae%ad%e7%bb%83%e5%99%a8>#</a></h3><p>这是工业级 InfoNCE Loss 的标准实现：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>InfoNCELoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>0.05</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>=</span> <span class=n>temperature</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query_embs</span><span class=p>,</span> <span class=n>doc_embs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 归一化 (Cosine Similarity)</span>
</span></span><span class=line><span class=cl>        <span class=n>query_embs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>normalize</span><span class=p>(</span><span class=n>query_embs</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>doc_embs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>normalize</span><span class=p>(</span><span class=n>doc_embs</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 矩阵乘法计算所有两两相似度</span>
</span></span><span class=line><span class=cl>        <span class=c1># [batch, batch]</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query_embs</span><span class=p>,</span> <span class=n>doc_embs</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 标签：对角线是正样本</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>device</span><span class=o>=</span><span class=n>scores</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. 交叉熵损失</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span></span></span></code></pre></div><h3 id=26-本节小结>2.6 本节小结<a class=anchor href=#26-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h3><ul><li>对比学习通过 InfoNCE Loss 优化向量空间的相对距离。</li><li><strong>In-Batch Negatives</strong> 使得大 Batch 训练成为提升性能的关键。</li><li><strong>温度参数 $\tau$</strong> 越小，模型挖掘难负样本的能力越强。</li></ul><hr><h2 id=第三节数据工程难负样本挖掘>第三节：数据工程：难负样本挖掘<a class=anchor href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%95%b0%e6%8d%ae%e5%b7%a5%e7%a8%8b%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac%e6%8c%96%e6%8e%98>#</a></h2><h3 id=31-为什么需要难负样本>3.1 为什么需要难负样本？<a class=anchor href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9a%be%e8%b4%9f%e6%a0%b7%e6%9c%ac>#</a></h3><ul><li><strong>简单负样本 (Simple Negatives)</strong>：随机抽样的文档。如 Query=&ldquo;苹果手机&rdquo;，Neg=&ldquo;今天天气不错&rdquo;。模型一眼就能识别，Loss 接近 0，训练效率低。</li><li><strong>难负样本 (Hard Negatives)</strong>：看起来像但由于细微差别而不相关的文档。如 Query=&ldquo;苹果手机&rdquo;，Neg=&ldquo;苹果怎么种？"（包含“苹果”关键词但语义不同）。<strong>这才是提升模型的关键养料</strong>。</li></ul><h3 id=32-静态挖掘bm25策略>3.2 静态挖掘：BM25策略<a class=anchor href=#32-%e9%9d%99%e6%80%81%e6%8c%96%e6%8e%98bm25%e7%ad%96%e7%95%a5>#</a></h3><p>利用 BM25 的“字面匹配”特性来攻击向量模型。</p><ul><li><strong>方法</strong>：用 Query 去 BM25 检索 Top-100。</li><li><strong>筛选</strong>：排除真正的正样本，剩下的那些排名很高（字面重叠多）但不是答案的文档，就是绝佳的难负样本。</li><li><strong>效果</strong>：迫使模型不再仅仅依赖关键词重叠，而是理解语义。</li></ul><h3 id=33-动态挖掘ance算法>3.3 动态挖掘：ANCE算法<a class=anchor href=#33-%e5%8a%a8%e6%80%81%e6%8c%96%e6%8e%98ance%e7%ae%97%e6%b3%95>#</a></h3><p><strong>ANCE (Approximate Nearest Neighbor Negative Contrastive Estimation)</strong> 是一种进阶策略。</p><ul><li><strong>问题</strong>：训练初期觉得难的样本，后期可能变简单了。</li><li><strong>方法</strong>：在训练过程中，<strong>每隔 N 步</strong>用当前的模型 checkpoint 对全库进行一次索引，找出当前模型最容易混淆的样本作为下一阶段的负样本。</li><li><strong>特点</strong>：实现了“课程学习”，难度动态跟随模型能力提升。</li></ul><h3 id=34-合成数据llm蒸馏-data-distillation>3.4 合成数据：LLM蒸馏 (Data Distillation)<a class=anchor href=#34-%e5%90%88%e6%88%90%e6%95%b0%e6%8d%aellm%e8%92%b8%e9%a6%8f-data-distillation>#</a></h3><p>2024 年以来的 SOTA 模型（E5-Mistral, GTE-Qwen）普遍采用 LLM 合成数据。</p><p><strong>核心逻辑</strong>：</p><ol><li>收集海量无标注文本片段。</li><li>让 GPT-4/Claude 生成对应的查询问题（Prompt: &ldquo;Generate a question that this document answers&rdquo;）。</li><li>利用这些高质量的 <code>(生成的Query, 原始Doc)</code> 对模型进行微调。</li></ol><p>这一步打破了人工标注数据的规模瓶颈，是垂直领域模型训练的必经之路。</p><h3 id=35-本节小结>3.5 本节小结<a class=anchor href=#35-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h3><ul><li>不仅要正样本，更要高质量的负样本。</li><li><strong>BM25</strong> 挖掘字面相似负样本，<strong>ANCE</strong> 挖掘语义混淆负样本。</li><li><strong>LLM 合成</strong>是低成本获取海量领域数据的最佳途径。</li></ul><hr><h2 id=第四节多任务联合训练与嵌套表示>第四节：多任务联合训练与嵌套表示<a class=anchor href=#%e7%ac%ac%e5%9b%9b%e8%8a%82%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%81%94%e5%90%88%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%b5%8c%e5%a5%97%e8%a1%a8%e7%a4%ba>#</a></h2><h3 id=41-为什么需要多任务训练>4.1 为什么需要多任务训练？<a class=anchor href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83>#</a></h3><p>单一的检索训练可能导致模型过拟合，或在其他任务（如文本分类、聚类）上表现不佳。通用的嵌入模型（如 BGE, GTE）通常采用多任务联合训练：</p><ul><li><strong>Retrieval</strong>：非对称搜索 (Query != Doc)。</li><li><strong>STS (Semantic Textual Similarity)</strong>：对称相似度。</li><li><strong>Classification</strong>：分类任务（通过 [CLS] 向量）。</li></ul><h3 id=42-多任务训练框架>4.2 多任务训练框架<a class=anchor href=#42-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6>#</a></h3><p>在代码实现上，我们通常构建一个 <code>MultiTaskDataSampler</code>，按比例混合不同任务的 Batch。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码逻辑</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 按概率采样一个任务 (如 80% 概率选 Retrieval, 20% 选 STS)</span>
</span></span><span class=line><span class=cl>    <span class=n>task</span> <span class=o>=</span> <span class=n>sample_task</span><span class=p>(</span><span class=n>tasks</span><span class=p>,</span> <span class=n>weights</span><span class=o>=</span><span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>batch</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=n>task</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>task</span><span class=o>.</span><span class=n>type</span> <span class=o>==</span> <span class=s2>&#34;Retrieval&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>InfoNCELoss</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>task</span><span class=o>.</span><span class=n>type</span> <span class=o>==</span> <span class=s2>&#34;STS&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>MSELoss</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>  <span class=c1># 均方误差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span></span></span></code></pre></div><h3 id=43-matryoshka嵌套嵌入-mrl>4.3 Matryoshka嵌套嵌入 (MRL)<a class=anchor href=#43-matryoshka%e5%b5%8c%e5%a5%97%e5%b5%8c%e5%85%a5-mrl>#</a></h3><p><strong>Matryoshka Representation Learning (MRL)</strong> 是提升模型部署灵活性的关键技术。</p><p><strong>问题</strong>：768 维或 3072 维的向量存储成本太高。
<strong>方案</strong>：训练时强制要求向量的<strong>前 k 维</strong>（如前 64, 128 维）也能独立具备高语义能力。</p><p><strong>实现</strong>：
$$ \mathcal{L}<em>{total} = \sum</em>{k \in {64, 128, &mldr;}} w_k \cdot \mathcal{L}_{InfoNCE}(dim=k) $$</p><p>通过这种方式，同一个模型可以“像洋葱一样”被剥开使用：</p><ul><li>内存受限端侧：只用前 64 维。</li><li>云端高精检索：用完整 768 维。</li></ul><h3 id=44-本节小结>4.4 本节小结<a class=anchor href=#44-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h3><ul><li>多任务训练提升了模型的通用性（Generalization）。</li><li><strong>MRL</strong> 技术让模型具备了弹性维度的能力，大幅降低了推理和存储成本。</li></ul><hr><h2 id=第五节从零实战训练与部署>第五节：从零实战：训练与部署<a class=anchor href=#%e7%ac%ac%e4%ba%94%e8%8a%82%e4%bb%8e%e9%9b%b6%e5%ae%9e%e6%88%98%e8%ae%ad%e7%bb%83%e4%b8%8e%e9%83%a8%e7%bd%b2>#</a></h2><h3 id=51-完整训练流程代码>5.1 完整训练流程代码<a class=anchor href=#51-%e5%ae%8c%e6%95%b4%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b%e4%bb%a3%e7%a0%81>#</a></h3><p>基于 <code>sentence-transformers</code> 的极简 SOTA 训练脚本：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span><span class=p>,</span> <span class=n>SentenceTransformerTrainer</span><span class=p>,</span> <span class=n>SentenceTransformerTrainingArguments</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers.losses</span> <span class=kn>import</span> <span class=n>MultipleNegativesRankingLoss</span><span class=p>,</span> <span class=n>MatryoshkaLoss</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s2>&#34;microsoft/mpnet-base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;json&#34;</span><span class=p>,</span> <span class=n>data_files</span><span class=o>=</span><span class=s2>&#34;train_data.jsonl&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 组合 Loss: InfoNCE + MRL</span>
</span></span><span class=line><span class=cl><span class=n>train_loss</span> <span class=o>=</span> <span class=n>MatryoshkaLoss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>=</span><span class=n>MultipleNegativesRankingLoss</span><span class=p>(</span><span class=n>model</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>matryoshka_dims</span><span class=o>=</span><span class=p>[</span><span class=mi>768</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>args</span> <span class=o>=</span> <span class=n>SentenceTransformerTrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;output/model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>SentenceTransformerTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>args</span><span class=o>=</span><span class=n>args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=n>train_loss</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span></span></span></code></pre></div><h3 id=52-模型评估mteb基准>5.2 模型评估：MTEB基准<a class=anchor href=#52-%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0mteb%e5%9f%ba%e5%87%86>#</a></h3><p>训练后，务必使用 MTEB 库进行自动化评测，不要盲目自信。</p><h3 id=53-生产环境部署建议>5.3 生产环境部署建议<a class=anchor href=#53-%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2%e5%bb%ba%e8%ae%ae>#</a></h3><ol><li><strong>导出格式</strong>：将 PyTorch 模型导出为 <strong>ONNX</strong> 或 <strong>TensorRT</strong>，可获得 3-5 倍推理加速。</li><li><strong>量化</strong>：使用 int8 或 binary 量化，可减少 4-32 倍显存占用，精度损失通常在可接受范围。</li><li><strong>向量数据库</strong>：推荐使用支持 SIMD 加速的数据库（Milvus, Qdrant, Weaviate）。</li></ol><hr><h2 id=第4章小结>第4章小结<a class=anchor href=#%e7%ac%ac4%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=决策指南何时训练自己的嵌入模型>决策指南：何时训练自己的嵌入模型？<a class=anchor href=#%e5%86%b3%e7%ad%96%e6%8c%87%e5%8d%97%e4%bd%95%e6%97%b6%e8%ae%ad%e7%bb%83%e8%87%aa%e5%b7%b1%e7%9a%84%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b>#</a></h3><table><thead><tr><th>场景</th><th>推荐方案</th><th>理由</th></tr></thead><tbody><tr><td><strong>通用领域</strong></td><td>直接使用SOTA模型</td><td>OpenAI/Cohere模型已在万亿级token上训练，难以超越</td></tr><tr><td><strong>垂直领域</strong></td><td><strong>微调开源模型</strong></td><td>医疗、法律、金融等领域有大量专有术语，通用模型理解不深</td></tr><tr><td><strong>数据隐私</strong></td><td><strong>私有化部署</strong></td><td>敏感数据不能上传云端，必须自建</td></tr><tr><td><strong>超长文本</strong></td><td><strong>长上下文模型</strong></td><td>如法律合同、技术文档，需要8k+长度支持</td></tr></tbody></table><h3 id=核心技术回顾>核心技术回顾<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e5%9b%9e%e9%a1%be>#</a></h3><h4 id=1-嵌入模型架构>1. 嵌入模型架构<a class=anchor href=#1-%e5%b5%8c%e5%85%a5%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84>#</a></h4><ul><li>✅ <strong>Bi-Encoder</strong>：召回阶段首选，速度快，可预计算</li><li>✅ <strong>Cross-Encoder</strong>：精排阶段首选，精度高，但计算昂贵</li></ul><h4 id=2-训练关键技术>2. 训练关键技术<a class=anchor href=#2-%e8%ae%ad%e7%bb%83%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af>#</a></h4><ul><li>🔥 <strong>对比学习</strong>：InfoNCE是标准损失，In-batch negatives是效率关键</li><li>🔥 <strong>难负样本</strong>：决定了模型的上限，必须挖掘"看着像但不是"的负样本</li><li>🔥 <strong>多任务学习</strong>：提升泛化能力，避免过拟合单一任务</li></ul><h4 id=3-前沿技术趋势>3. 前沿技术趋势<a class=anchor href=#3-%e5%89%8d%e6%b2%bf%e6%8a%80%e6%9c%af%e8%b6%8b%e5%8a%bf>#</a></h4><ul><li>⭐ <strong>LLM做基座</strong>：使用Qwen/Mistral等7B模型做基座，性能显著超越BERT</li><li>⭐ <strong>Matryoshka嵌入</strong>：一次训练，任意维度部署，弹性极佳</li><li>⭐ <strong>生成式嵌入</strong>：利用LLM生成伪数据进行增强训练</li></ul><h3 id=延伸阅读资源>延伸阅读资源<a class=anchor href=#%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb%e8%b5%84%e6%ba%90>#</a></h3><ul><li><a href=https://www.sbert.net/>Sentence-Transformers官方文档</a> - 必读</li><li><a href=https://huggingface.co/spaces/mteb/leaderboard>MTEB排行榜</a> - 关注最新SOTA</li><li><a href=https://github.com/FlagOpen/FlagEmbedding>FlagEmbedding</a> - BGE模型官方仓库</li></ul><hr><h3 id=思考练习>思考练习<a class=anchor href=#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0>#</a></h3><h4 id=基础练习>基础练习<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%bb%83%e4%b9%a0>#</a></h4><p><strong>练习1</strong>：实现简单的Bi-Encoder</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 实现一个Bi-Encoder，包含：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. BERT编码器</span>
</span></span><span class=line><span class=cl><span class=c1># 2. Mean pooling</span>
</span></span><span class=line><span class=cl><span class=c1># 3. L2归一化</span>
</span></span><span class=line><span class=cl><span class=c1># 提示：参考第一节代码</span></span></span></code></pre></div><p><strong>练习2</strong>：计算InfoNCE损失</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 给定anchor, positive, negatives，计算InfoNCE损失</span>
</span></span><span class=line><span class=cl><span class=c1># anchor: [batch, dim]</span>
</span></span><span class=line><span class=cl><span class=c1># positive: [batch, dim]</span>
</span></span><span class=line><span class=cl><span class=c1># negatives: [batch, num_neg, dim]</span>
</span></span><span class=line><span class=cl><span class=c1># temperature: 0.07</span></span></span></code></pre></div><p><strong>练习3</strong>：BM25检索</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 实现BM25算法，检索Top-K文档</span>
</span></span><span class=line><span class=cl><span class=c1># 输入：query, corpus</span>
</span></span><span class=line><span class=cl><span class=c1># 输出：Top-K文档索引</span></span></span></code></pre></div><hr><h4 id=高级练习>高级练习<a class=anchor href=#%e9%ab%98%e7%ba%a7%e7%bb%83%e4%b9%a0>#</a></h4><p><strong>练习4</strong>：动态难负样本挖掘</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 实现一个难负样本挖掘器</span>
</span></span><span class=line><span class=cl><span class=c1># 要求：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 预计算文档嵌入</span>
</span></span><span class=line><span class=cl><span class=c1># 2. 给定query，检索Top-K最相似但错误的文档</span>
</span></span><span class=line><span class=cl><span class=c1># 3. 支持定期更新嵌入</span></span></span></code></pre></div><p><strong>练习5</strong>：多任务训练器</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 实现一个多任务训练器</span>
</span></span><span class=line><span class=cl><span class=c1># 要求：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 支持至少3种任务（检索、STS、分类）</span>
</span></span><span class=line><span class=cl><span class=c1># 2. 按权重采样任务</span>
</span></span><span class=line><span class=cl><span class=c1># 3. 计算每个任务的损失并加权求和</span></span></span></code></pre></div><p><strong>练习6</strong>：Matryoshka嵌入</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># TODO: 实现Matryoshka嵌入损失</span>
</span></span><span class=line><span class=cl><span class=c1># 要求：</span>
</span></span><span class=line><span class=cl><span class=c1># 1. 在多个维度（64, 128, 256, 512, 768）上计算损失</span>
</span></span><span class=line><span class=cl><span class=c1># 2. 返回平均损失</span>
</span></span><span class=line><span class=cl><span class=c1># 3. 测试不同维度的性能</span></span></span></code></pre></div><hr><h4 id=实战项目>实战项目<a class=anchor href=#%e5%ae%9e%e6%88%98%e9%a1%b9%e7%9b%ae>#</a></h4><p><strong>项目1</strong>：训练中文嵌入模型</p><ul><li>数据：DuReader检索数据 + STS-B中文版</li><li>模型：chinese-roberta-wwm-ext</li><li>目标：在中文MTEB上超越BGE-base</li></ul><p><strong>项目2</strong>：领域特定嵌入模型</p><ul><li>选择一个领域（如医疗、法律、金融）</li><li>收集领域数据</li><li>训练领域嵌入模型</li><li>对比通用模型性能</li></ul><p><strong>项目3</strong>：部署嵌入服务</p><ul><li>实现FastAPI嵌入服务</li><li>集成FAISS向量库</li><li>支持语义检索API</li><li>压测并优化性能</li></ul><hr><h3 id=参考资料>参考资料<a class=anchor href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99>#</a></h3><h4 id=核心论文>核心论文<a class=anchor href=#%e6%a0%b8%e5%bf%83%e8%ae%ba%e6%96%87>#</a></h4><ol><li><p><strong>Sentence-BERT</strong> (Reimers & Gurevych, 2019)</p><ul><li>开创性工作，提出Bi-Encoder架构</li><li><a href=https://arxiv.org/abs/1908.10084>https://arxiv.org/abs/1908.10084</a></li></ul></li><li><p><strong>DPR</strong> (Karpukhin et al., 2020)</p><ul><li>难负样本挖掘，双编码器检索</li><li><a href=https://arxiv.org/abs/2004.04906>https://arxiv.org/abs/2004.04906</a></li></ul></li><li><p><strong>SimCSE</strong> (Gao et al., 2021)</p><ul><li>Dropout作为数据增强</li><li><a href=https://arxiv.org/abs/2104.08821>https://arxiv.org/abs/2104.08821</a></li></ul></li><li><p><strong>E5</strong> (Wang et al., 2022)</p><ul><li>多任务训练，对比学习</li><li><a href=https://arxiv.org/abs/2212.03533>https://arxiv.org/abs/2212.03533</a></li></ul></li><li><p><strong>Matryoshka Representation Learning</strong> (Kusupati et al., 2022)</p><ul><li>可变维度嵌入</li><li><a href=https://arxiv.org/abs/2205.13147>https://arxiv.org/abs/2205.13147</a></li></ul></li></ol><h4 id=开源项目>开源项目<a class=anchor href=#%e5%bc%80%e6%ba%90%e9%a1%b9%e7%9b%ae>#</a></h4><ol><li><p><strong>Sentence-Transformers</strong></p><ul><li>最流行的嵌入模型库</li><li><a href=https://www.sbert.net>https://www.sbert.net</a></li></ul></li><li><p><strong>MTEB</strong></p><ul><li>嵌入模型评估基准</li><li><a href=https://github.com/embeddings-benchmark/mteb>https://github.com/embeddings-benchmark/mteb</a></li></ul></li><li><p><strong>FAISS</strong></p><ul><li>高效向量检索库</li><li><a href=https://github.com/facebookresearch/faiss>https://github.com/facebookresearch/faiss</a></li></ul></li></ol><h4 id=数据集>数据集<a class=anchor href=#%e6%95%b0%e6%8d%ae%e9%9b%86>#</a></h4><ol><li><p><strong>MS MARCO</strong></p><ul><li>大规模检索数据集</li><li><a href=https://microsoft.github.io/msmarco/>https://microsoft.github.io/msmarco/</a></li></ul></li><li><p><strong>BEIR</strong></p><ul><li>零样本检索评估</li><li><a href=https://github.com/beir-cellar/beir>https://github.com/beir-cellar/beir</a></li></ul></li><li><p><strong>STS Benchmark</strong></p><ul><li>语义相似度评估</li><li><a href=https://ixa2.si.ehu.eus/stswiki>https://ixa2.si.ehu.eus/stswiki</a></li></ul></li></ol><hr><h3 id=下一章预告>下一章预告<a class=anchor href=#%e4%b8%8b%e4%b8%80%e7%ab%a0%e9%a2%84%e5%91%8a>#</a></h3><p>第四部分第一章《提示工程与上下文学习》将深入讲解：</p><ul><li>Prompt Engineering最佳实践</li><li>Few-shot Learning策略</li><li>Chain-of-Thought推理</li><li>RAG系统设计模式</li><li>实战：从零构建智能对话系统</li></ul><p><strong>核心问题</strong>：如何不重新训练模型，就能让它理解复杂任务？</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第3章 与人类对齐：偏好优化</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/ class="flex align-center"><span>第1章 提示工程与上下文学习</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节嵌入模型的本质与架构>第一节：嵌入模型的本质与架构</a><ul><li><a href=#11-为什么需要嵌入模型语义鸿沟>1.1 为什么需要嵌入模型？（语义鸿沟）</a></li><li><a href=#12-嵌入模型核心架构-bi-vs-cross>1.2 嵌入模型核心架构 (Bi vs Cross)</a><ul><li><a href=#1-bi-encoder-双编码器--召回层-retrieval>1. Bi-Encoder (双编码器) —— 召回层 (Retrieval)</a></li><li><a href=#2-cross-encoder-交叉编码器--精排层-reranking>2. Cross-Encoder (交叉编码器) —— 精排层 (Reranking)</a></li></ul></li><li><a href=#13-嵌入空间的数学本质>1.3 嵌入空间的数学本质</a></li><li><a href=#14-sota嵌入模型对比>1.4 SOTA嵌入模型对比</a></li><li><a href=#15-本节小结>1.5 本节小结</a></li></ul></li><li><a href=#第二节对比学习与infonce损失>第二节：对比学习与InfoNCE损失</a><ul><li><a href=#21-对比学习的核心思想>2.1 对比学习的核心思想</a></li><li><a href=#22-infonce-损失函数详解>2.2 InfoNCE 损失函数详解</a></li><li><a href=#23-in-batch-negatives-高效训练策略>2.3 In-Batch Negatives 高效训练策略</a></li><li><a href=#24-关键超参温度系数-tau-的影响>2.4 关键超参：温度系数 ($\tau$) 的影响</a></li><li><a href=#25-实战实现对比学习训练器>2.5 实战：实现对比学习训练器</a></li><li><a href=#26-本节小结>2.6 本节小结</a></li></ul></li><li><a href=#第三节数据工程难负样本挖掘>第三节：数据工程：难负样本挖掘</a><ul><li><a href=#31-为什么需要难负样本>3.1 为什么需要难负样本？</a></li><li><a href=#32-静态挖掘bm25策略>3.2 静态挖掘：BM25策略</a></li><li><a href=#33-动态挖掘ance算法>3.3 动态挖掘：ANCE算法</a></li><li><a href=#34-合成数据llm蒸馏-data-distillation>3.4 合成数据：LLM蒸馏 (Data Distillation)</a></li><li><a href=#35-本节小结>3.5 本节小结</a></li></ul></li><li><a href=#第四节多任务联合训练与嵌套表示>第四节：多任务联合训练与嵌套表示</a><ul><li><a href=#41-为什么需要多任务训练>4.1 为什么需要多任务训练？</a></li><li><a href=#42-多任务训练框架>4.2 多任务训练框架</a></li><li><a href=#43-matryoshka嵌套嵌入-mrl>4.3 Matryoshka嵌套嵌入 (MRL)</a></li><li><a href=#44-本节小结>4.4 本节小结</a></li></ul></li><li><a href=#第五节从零实战训练与部署>第五节：从零实战：训练与部署</a><ul><li><a href=#51-完整训练流程代码>5.1 完整训练流程代码</a></li><li><a href=#52-模型评估mteb基准>5.2 模型评估：MTEB基准</a></li><li><a href=#53-生产环境部署建议>5.3 生产环境部署建议</a></li></ul></li><li><a href=#第4章小结>第4章小结</a><ul><li><a href=#决策指南何时训练自己的嵌入模型>决策指南：何时训练自己的嵌入模型？</a></li><li><a href=#核心技术回顾>核心技术回顾</a><ul><li><a href=#1-嵌入模型架构>1. 嵌入模型架构</a></li><li><a href=#2-训练关键技术>2. 训练关键技术</a></li><li><a href=#3-前沿技术趋势>3. 前沿技术趋势</a></li></ul></li><li><a href=#延伸阅读资源>延伸阅读资源</a></li><li><a href=#思考练习>思考练习</a><ul><li><a href=#基础练习>基础练习</a></li><li><a href=#高级练习>高级练习</a></li><li><a href=#实战项目>实战项目</a></li></ul></li><li><a href=#参考资料>参考资料</a><ul><li><a href=#核心论文>核心论文</a></li><li><a href=#开源项目>开源项目</a></li><li><a href=#数据集>数据集</a></li></ul></li><li><a href=#下一章预告>下一章预告</a></li></ul></li></ul></nav></div></aside></main></body></html>