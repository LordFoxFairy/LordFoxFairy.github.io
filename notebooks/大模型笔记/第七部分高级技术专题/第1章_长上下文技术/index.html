<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第1章：长上下文技术 (Long Context)# 如何让模型拥有一目十行的"过目不忘"能力？从 RoPE 到 FlashAttention。
目录# 一、长上下文的挑战 二、位置编码的进化：RoPE (Rotary Positional Embeddings) 1. 绝对位置 vs 相对位置 2. RoPE 核心原理 3. PyTorch 实现 RoPE 三、外推技术：打破长度限制 1. 线性内插 (Linear Interpolation) 2. NTK-Aware Scaled RoPE 3. YaRN (Yet another RoPE for Transformers) 四、工程优化：FlashAttention 1. 显存带宽瓶颈 (Memory Bound) 2. FlashAttention V1: Tiling & Recomputation 3. FlashAttention V2: 并行优化 五、显存优化技术 1. PagedAttention (vLLM) 2. KV Cache Quantization 3. Grouped-Query Attention (GQA) 六、代码实战：手写一个支持 32k 上下文的 Mini-Llama 七、本章小结 一、长上下文的挑战# 在 RAG 和 Agent 应用中，处理长文本（如 100k tokens 甚至 1M tokens）已成为刚需。但 Transformer 在处理长文本时面临三个核心物理瓶颈：
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 长上下文技术"><meta property="og:description" content='第1章：长上下文技术 (Long Context)# 如何让模型拥有一目十行的"过目不忘"能力？从 RoPE 到 FlashAttention。
目录# 一、长上下文的挑战 二、位置编码的进化：RoPE (Rotary Positional Embeddings) 1. 绝对位置 vs 相对位置 2. RoPE 核心原理 3. PyTorch 实现 RoPE 三、外推技术：打破长度限制 1. 线性内插 (Linear Interpolation) 2. NTK-Aware Scaled RoPE 3. YaRN (Yet another RoPE for Transformers) 四、工程优化：FlashAttention 1. 显存带宽瓶颈 (Memory Bound) 2. FlashAttention V1: Tiling & Recomputation 3. FlashAttention V2: 并行优化 五、显存优化技术 1. PagedAttention (vLLM) 2. KV Cache Quantization 3. Grouped-Query Attention (GQA) 六、代码实战：手写一个支持 32k 上下文的 Mini-Llama 七、本章小结 一、长上下文的挑战# 在 RAG 和 Agent 应用中，处理长文本（如 100k tokens 甚至 1M tokens）已成为刚需。但 Transformer 在处理长文本时面临三个核心物理瓶颈：'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 长上下文技术"><meta itemprop=description content='第1章：长上下文技术 (Long Context)# 如何让模型拥有一目十行的"过目不忘"能力？从 RoPE 到 FlashAttention。
目录# 一、长上下文的挑战 二、位置编码的进化：RoPE (Rotary Positional Embeddings) 1. 绝对位置 vs 相对位置 2. RoPE 核心原理 3. PyTorch 实现 RoPE 三、外推技术：打破长度限制 1. 线性内插 (Linear Interpolation) 2. NTK-Aware Scaled RoPE 3. YaRN (Yet another RoPE for Transformers) 四、工程优化：FlashAttention 1. 显存带宽瓶颈 (Memory Bound) 2. FlashAttention V1: Tiling & Recomputation 3. FlashAttention V2: 并行优化 五、显存优化技术 1. PagedAttention (vLLM) 2. KV Cache Quantization 3. Grouped-Query Attention (GQA) 六、代码实战：手写一个支持 32k 上下文的 Mini-Llama 七、本章小结 一、长上下文的挑战# 在 RAG 和 Agent 应用中，处理长文本（如 100k tokens 甚至 1M tokens）已成为刚需。但 Transformer 在处理长文本时面临三个核心物理瓶颈：'><meta itemprop=wordCount content="1424"><title>第1章 长上下文技术 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle checked>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/ class=active>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 长上下文技术</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一长上下文的挑战>一、长上下文的挑战</a></li><li><a href=#二位置编码的进化rope-rotary-positional-embeddings>二、位置编码的进化：RoPE (Rotary Positional Embeddings)</a><ul><li><a href=#1-绝对位置-vs-相对位置>1. 绝对位置 vs 相对位置</a></li><li><a href=#2-rope-核心原理>2. RoPE 核心原理</a></li><li><a href=#3-pytorch-实现-rope>3. PyTorch 实现 RoPE</a></li></ul></li><li><a href=#三外推技术打破长度限制>三、外推技术：打破长度限制</a><ul><li><a href=#1-线性内插-linear-interpolation>1. 线性内插 (Linear Interpolation)</a></li><li><a href=#2-ntk-aware-scaled-rope>2. NTK-Aware Scaled RoPE</a></li><li><a href=#3-yarn-yet-another-rope-for-transformers>3. YaRN (Yet another RoPE for Transformers)</a></li></ul></li><li><a href=#四工程优化flashattention>四、工程优化：FlashAttention</a><ul><li><a href=#1-显存带宽瓶颈-memory-bound>1. 显存带宽瓶颈 (Memory Bound)</a></li><li><a href=#2-flashattention-v1-tiling--recomputation>2. FlashAttention V1: Tiling & Recomputation</a></li><li><a href=#3-flashattention-v2-并行优化>3. FlashAttention V2: 并行优化</a></li></ul></li><li><a href=#五显存优化技术>五、显存优化技术</a><ul><li><a href=#1-pagedattention-vllm>1. PagedAttention (vLLM)</a></li><li><a href=#2-kv-cache-quantization>2. KV Cache Quantization</a></li><li><a href=#3-grouped-query-attention-gqa>3. Grouped-Query Attention (GQA)</a></li></ul></li><li><a href=#六代码实战手写一个支持-32k-上下文的-mini-llama>六、代码实战：手写一个支持 32k 上下文的 Mini-Llama</a></li><li><a href=#七本章小结>七、本章小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第1章长上下文技术-long-context>第1章：长上下文技术 (Long Context)<a class=anchor href=#%e7%ac%ac1%e7%ab%a0%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e6%8a%80%e6%9c%af-long-context>#</a></h1><blockquote class=book-hint><p>如何让模型拥有一目十行的"过目不忘"能力？从 RoPE 到 FlashAttention。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e6%8c%91%e6%88%98>一、长上下文的挑战</a></li><li><a href=#%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e7%9a%84%e8%bf%9b%e5%8c%96rope-rotary-positional-embeddings>二、位置编码的进化：RoPE (Rotary Positional Embeddings)</a><ul><li><a href=#1-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae-vs-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae>1. 绝对位置 vs 相对位置</a></li><li><a href=#2-rope-%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86>2. RoPE 核心原理</a></li><li><a href=#3-pytorch-%e5%ae%9e%e7%8e%b0-rope>3. PyTorch 实现 RoPE</a></li></ul></li><li><a href=#%e4%b8%89%e5%a4%96%e6%8e%a8%e6%8a%80%e6%9c%af%e6%89%93%e7%a0%b4%e9%95%bf%e5%ba%a6%e9%99%90%e5%88%b6>三、外推技术：打破长度限制</a><ul><li><a href=#1-%e7%ba%bf%e6%80%a7%e5%86%85%e6%8f%92-linear-interpolation>1. 线性内插 (Linear Interpolation)</a></li><li><a href=#2-ntk-aware-scaled-rope>2. NTK-Aware Scaled RoPE</a></li><li><a href=#3-yarn-yet-another-rope-for-transformers>3. YaRN (Yet another RoPE for Transformers)</a></li></ul></li><li><a href=#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96flashattention>四、工程优化：FlashAttention</a><ul><li><a href=#1-%e6%98%be%e5%ad%98%e5%b8%a6%e5%ae%bd%e7%93%b6%e9%a2%88-memory-bound>1. 显存带宽瓶颈 (Memory Bound)</a></li><li><a href=#2-flashattention-v1-tiling--recomputation>2. FlashAttention V1: Tiling & Recomputation</a></li><li><a href=#3-flashattention-v2-%e5%b9%b6%e8%a1%8c%e4%bc%98%e5%8c%96>3. FlashAttention V2: 并行优化</a></li></ul></li><li><a href=#%e4%ba%94%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e6%9c%af>五、显存优化技术</a><ul><li><a href=#1-pagedattention-vllm>1. PagedAttention (vLLM)</a></li><li><a href=#2-kv-cache-quantization>2. KV Cache Quantization</a></li><li><a href=#3-grouped-query-attention-gqa>3. Grouped-Query Attention (GQA)</a></li></ul></li><li><a href=#%e5%85%ad%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa%e6%94%af%e6%8c%81-32k-%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84-mini-llama>六、代码实战：手写一个支持 32k 上下文的 Mini-Llama</a></li><li><a href=#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>七、本章小结</a></li></ul><hr><h2 id=一长上下文的挑战>一、长上下文的挑战<a class=anchor href=#%e4%b8%80%e9%95%bf%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e6%8c%91%e6%88%98>#</a></h2><p>在 RAG 和 Agent 应用中，处理长文本（如 100k tokens 甚至 1M tokens）已成为刚需。但 Transformer 在处理长文本时面临三个核心物理瓶颈：</p><ol><li><p><strong>计算复杂度 $O(N^2)$</strong>：Attention 的计算矩阵是 $N \times N$。序列长度翻倍，计算量增加 4 倍。</p><ul><li>4k -> 8k: 计算量增加 4 倍</li><li>4k -> 100k: 计算量增加 625 倍！</li></ul></li><li><p><strong>KV Cache 显存爆炸</strong>：推理时需要存储所有历史 Token 的 KV 状态。</p><ul><li>LLaMA-2-7B (fp16), 4k context: ~2GB KV Cache</li><li>LLaMA-2-7B (fp16), 100k context: ~50GB KV Cache (单卡 A100 80G 直接撑爆)</li></ul></li><li><p><strong>位置编码的外推性 (Extrapolation)</strong>：训练时只见过 4k 长度，测试时给它 100k，位置编码会"乱套"。模型在超出训练长度后，PPL（困惑度）会急剧上升，开始胡言乱语。</p></li></ol><hr><h2 id=二位置编码的进化rope-rotary-positional-embeddings>二、位置编码的进化：RoPE (Rotary Positional Embeddings)<a class=anchor href=#%e4%ba%8c%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81%e7%9a%84%e8%bf%9b%e5%8c%96rope-rotary-positional-embeddings>#</a></h2><h3 id=1-绝对位置-vs-相对位置>1. 绝对位置 vs 相对位置<a class=anchor href=#1-%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae-vs-%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae>#</a></h3><p>在 Transformer 早期，使用的是绝对位置编码（Absolute PE）：</p><ul><li><strong>Sinusoidal (Attention is All You Need)</strong>: $\sin(pos/10000^{2i/d})$</li><li><strong>Learnable (BERT/GPT)</strong>: 学习一个 Embedding 矩阵 $P \in \mathbb{R}^{seq \times dim}$</li></ul><p><strong>问题</strong>：绝对位置编码无法捕捉 token 之间的<strong>相对距离</strong>。对于 &ldquo;Cat eats fish&rdquo;，&ldquo;Cat&rdquo; 和 &ldquo;fish&rdquo; 距离是 2。如果句子变成 &ldquo;The Cat eats fish&rdquo;，距离还是 2，但绝对位置变了（从 1,3 变成了 2,4）。模型需要重新学习这种情况。</p><p><strong>相对位置编码 (Relative PE)</strong>：直接在 Attention Score 计算中加入相对距离 $i-j$ 的信息。但实现复杂，且不容易缓存。</p><h3 id=2-rope-核心原理>2. RoPE 核心原理<a class=anchor href=#2-rope-%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86>#</a></h3><p>RoPE (Su et al., 2021) 通过将向量在复平面上旋转，巧妙地融合了绝对位置信息，但内积结果却只与相对位置有关。</p><p><strong>核心公式：</strong></p><p>$$
f(x, m) = x e^{i m \theta}
$$</p><p>当计算两个位置 $m$ 和 $n$ 的 Query 和 Key 的内积时：</p><p>$$
\langle f(q, m), f(k, n) \rangle = \text{Re}(q e^{i m \theta} \cdot \overline{k e^{i n \theta}}) = \text{Re}(q \bar{k} e^{i(m-n)\theta})
$$</p><p><strong>神奇之处</strong>：结果只包含 $(m-n)$，即<strong>相对距离</strong>！</p><p>这使得 RoPE 具有两个极佳特性：</p><ol><li><strong>平移不变性</strong>：无论 token 出现在句子的哪个位置，只要相对距离一样，Attention 分数就一样。</li><li><strong>远程衰减</strong>：随着相对距离增加，内积值自然衰减（关注近处多于远处）。</li></ol><h3 id=3-pytorch-实现-rope>3. PyTorch 实现 RoPE<a class=anchor href=#3-pytorch-%e5%ae%9e%e7%8e%b0-rope>#</a></h3><p>这是 LLaMA 官方实现的核心代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>precompute_freqs_cis</span><span class=p>(</span><span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>end</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>theta</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>10000.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    预计算旋转角度（复数形式）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        dim: head_dim (注意不是 hidden_size)
</span></span></span><span class=line><span class=cl><span class=s2>        end: 最大序列长度 max_seq_len
</span></span></span><span class=line><span class=cl><span class=s2>        theta: 基频 (LLaMA 1用10000, LLaMA 3用500000)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>theta</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)[:</span> <span class=p>(</span><span class=n>dim</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)]</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>end</span><span class=p>)</span>  <span class=c1># 位置索引 [0, 1, ..., end-1]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 外积计算所有位置的所有频率</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>freqs</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>  <span class=c1># [end, dim//2]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 转为复数 e^{i*freqs} = cos(freqs) + i*sin(freqs)</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs_cis</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>polar</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>freqs</span><span class=p>),</span> <span class=n>freqs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>freqs_cis</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>reshape_for_broadcast</span><span class=p>(</span><span class=n>freqs_cis</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ndim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>ndim</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=mi>0</span> <span class=o>&lt;=</span> <span class=mi>1</span> <span class=o>&lt;</span> <span class=n>ndim</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>freqs_cis</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>shape</span> <span class=o>=</span> <span class=p>[</span><span class=n>d</span> <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>i</span> <span class=o>==</span> <span class=n>ndim</span> <span class=o>-</span> <span class=mi>1</span> <span class=k>else</span> <span class=mi>1</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>d</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>freqs_cis</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>*</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_rotary_emb</span><span class=p>(</span><span class=n>xq</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>xk</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>freqs_cis</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    应用 RoPE 旋转
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        xq: Query [batch, seq_len, n_heads, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>        xk: Key   [batch, seq_len, n_kv_heads, head_dim]
</span></span></span><span class=line><span class=cl><span class=s2>        freqs_cis: 预计算的复数频率
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将 Q, K 重塑为复数张量 (把最后一维 dim 拆成 dim/2 个复数)</span>
</span></span><span class=line><span class=cl>    <span class=n>xq_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>xq</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>xq</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>xk_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>xk</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>xk</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 广播形状以匹配</span>
</span></span><span class=line><span class=cl>    <span class=n>freqs_cis</span> <span class=o>=</span> <span class=n>reshape_for_broadcast</span><span class=p>(</span><span class=n>freqs_cis</span><span class=p>,</span> <span class=n>xq_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 复数乘法（即旋转）: (a+bi)(c+di) = (ac-bd) + i(ad+bc)</span>
</span></span><span class=line><span class=cl>    <span class=n>xq_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>xq_</span> <span class=o>*</span> <span class=n>freqs_cis</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>xk_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>xk_</span> <span class=o>*</span> <span class=n>freqs_cis</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>xq_out</span><span class=o>.</span><span class=n>type_as</span><span class=p>(</span><span class=n>xq</span><span class=p>),</span> <span class=n>xk_out</span><span class=o>.</span><span class=n>type_as</span><span class=p>(</span><span class=n>xk</span><span class=p>)</span></span></span></code></pre></div><p><strong>实战 Tip</strong>：</p><ul><li><code>theta</code> 的选择至关重要。<code>theta</code> 越大，波长越长，能表示的相对距离越远。</li><li>LLaMA-1 (2k context): theta = 10000</li><li>LLaMA-2 (4k context): theta = 10000</li><li>CodeLLaMA (100k context): theta = 1000000</li><li>LLaMA-3 (8k context): theta = 500000</li></ul><hr><h2 id=三外推技术打破长度限制>三、外推技术：打破长度限制<a class=anchor href=#%e4%b8%89%e5%a4%96%e6%8e%a8%e6%8a%80%e6%9c%af%e6%89%93%e7%a0%b4%e9%95%bf%e5%ba%a6%e9%99%90%e5%88%b6>#</a></h2><p>如果模型训练时最大长度是 4096 (4k)，如何让它在推理时处理 32k 甚至 100k 的文本？</p><h3 id=1-线性内插-linear-interpolation>1. 线性内插 (Linear Interpolation)<a class=anchor href=#1-%e7%ba%bf%e6%80%a7%e5%86%85%e6%8f%92-linear-interpolation>#</a></h3><p><strong>问题</strong>：直接外推（Extrapolation）效果很差。因为高频位置编码旋转太快，超出训练分布。</p><p><strong>思路</strong>：把 32k 的长度"压缩"回 4k 的范围内。即<strong>欺骗模型</strong>。</p><p>$$
m&rsquo; = m \times \frac{L_{train}}{L_{test}}
$$</p><p>例如要扩展 8 倍，就让位置 0, 1, 2, &mldr;, 32 变成 0, 0.125, 0.25, &mldr;, 4。</p><p><strong>优点</strong>：非常稳定，不用重新训练模型就能跑起来（虽然效果会打折，但比直接崩了强）。
<strong>缺点</strong>：对于高频特征（关注局部信息的 Attention Head），距离被强行压缩了，导致分辨率下降（&ldquo;近视眼&rdquo;）。</p><p><strong>代码实现</strong>：
只需要在计算 <code>freqs</code> 时除以 scale 因子。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Linear Scaling</span>
</span></span><span class=line><span class=cl><span class=n>scale</span> <span class=o>=</span> <span class=mf>8.0</span>  <span class=c1># 4k -&gt; 32k</span>
</span></span><span class=line><span class=cl><span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>end</span><span class=p>)</span> <span class=o>/</span> <span class=n>scale</span></span></span></code></pre></div><h3 id=2-ntk-aware-scaled-rope>2. NTK-Aware Scaled RoPE<a class=anchor href=#2-ntk-aware-scaled-rope>#</a></h3><p>这是著名的"Reddit 网友"发现的改进方案。后来被证实与神经正切核 (Neural Tangent Kernel) 理论有关。</p><p><strong>核心思想</strong>：
低频分量和高频分量应该区别对待。</p><ul><li><strong>高频分量</strong>（捕捉局部关系）：保持不变，<strong>不进行插值</strong>。因为局部关系（&ldquo;of the&rdquo;, &ldquo;in a&rdquo;）在长文中也不会变。</li><li><strong>低频分量</strong>（捕捉长程关系）：<strong>进行插值</strong>，适应更长距离。</li></ul><p><strong>公式实现</strong>：
不修改位置索引 $t$，而是修改基频 $base$ (theta)。</p><p>$$
\text{Base}&rsquo; = \text{Base} \times \alpha^{\frac{dim}{dim-2}}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_ntk_base</span><span class=p>(</span><span class=n>scale</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>base</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>10000.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    计算 NTK 修正后的 Base
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        scale: 扩展倍数 (e.g., 8)
</span></span></span><span class=line><span class=cl><span class=s2>        dim: head_dim
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 核心公式：base = base * scale ^ (dim / (dim-2))</span>
</span></span><span class=line><span class=cl>    <span class=n>new_base</span> <span class=o>=</span> <span class=n>base</span> <span class=o>*</span> <span class=p>(</span><span class=n>scale</span> <span class=o>**</span> <span class=p>(</span><span class=n>dim</span> <span class=o>/</span> <span class=p>(</span><span class=n>dim</span> <span class=o>-</span> <span class=mi>2</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>new_base</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用新的 base 计算 freqs</span>
</span></span><span class=line><span class=cl><span class=n>freqs</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>new_base</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)[:</span> <span class=p>(</span><span class=n>dim</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)]</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span></span></span></code></pre></div><p><strong>效果</strong>：不微调的情况下，NTK 插值的 PPL 显著优于线性插值。</p><h3 id=3-yarn-yet-another-rope-for-transformers>3. YaRN (Yet another RoPE for Transformers)<a class=anchor href=#3-yarn-yet-another-rope-for-transformers>#</a></h3><p>YaRN 是目前最先进的外推方法之一（DeepSeek-V2, LLaMA-3 都在用类似思想）。</p><p>它结合了：</p><ol><li><p><strong>NTK-aware 插值</strong>：分频段处理。</p></li><li><p><strong>Attention Logit 修正</strong>：
当上下文变长时，Attention 分布会变得更平滑（Entropy 增加），导致模型注意力涣散。
YaRN 引入一个温度系数 $\sqrt{t}$ 来锐化 Attention：</p><p>$$
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d} \cdot t})V
$$</p></li></ol><hr><h2 id=四工程优化flashattention>四、工程优化：FlashAttention<a class=anchor href=#%e5%9b%9b%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96flashattention>#</a></h2><p>算法层面解决了位置编码，计算层面还得靠 FlashAttention。它是大模型训练和推理的<strong>基础设施</strong>。</p><h3 id=1-显存带宽瓶颈-memory-bound>1. 显存带宽瓶颈 (Memory Bound)<a class=anchor href=#1-%e6%98%be%e5%ad%98%e5%b8%a6%e5%ae%bd%e7%93%b6%e9%a2%88-memory-bound>#</a></h3><p>在 GPU 中：</p><ul><li><strong>HBM (High Bandwidth Memory)</strong>: 显存，大但慢 (80GB, 2TB/s)</li><li><strong>SRAM</strong>: 类似 L1/L2 Cache，极快但极小 (20MB, 19TB/s)</li></ul><p>标准的 Attention 计算：
$$S = QK^T \rightarrow P = \text{Softmax}(S) \rightarrow O = PV$$</p><p>需要反复将巨大的 $N \times N$ 矩阵在 HBM 和 SRAM 之间搬运：</p><ol><li>读 Q, K -> 算 $S$ -> 写回 HBM</li><li>读 $S$ -> 算 Softmax -> 写回 HBM</li><li>读 $P, V$ -> 算 $O$ -> 写回 HBM</li></ol><p><strong>痛点</strong>：$N \times N$ 矩阵太大了，根本塞不进 SRAM。而且大部分时间 GPU 核心在等 HBM 搬数据（IO 瓶颈）。</p><h3 id=2-flashattention-v1-tiling--recomputation>2. FlashAttention V1: Tiling & Recomputation<a class=anchor href=#2-flashattention-v1-tiling--recomputation>#</a></h3><p>FlashAttention (Dao et al., 2022) 的核心魔法是 <strong>Tiling (分块)</strong>。</p><p><strong>算法流程</strong>：</p><ol><li><p>将 $Q, K, V$ 切分成小块（Block），比如 $128 \times 128$。</p></li><li><p>每次只加载一部分块到 SRAM。</p></li><li><p>在 SRAM 中计算局部的 Attention Score。</p></li><li><p><strong>Online Softmax</strong>：利用数学技巧，不需要一次性看到所有分数就能计算 Softmax 的归一化因子。</p><p>$$
m_{new} = \max(m_{old}, \max(x_{new}))
$$
$$
l_{new} = l_{old} \cdot e^{m_{old}-m_{new}} + \sum e^{x_{new}-m_{new}}
$$</p></li><li><p>直接在 SRAM 中算完 $O$ 的一部分，只把最终结果写回 HBM。</p></li></ol><p><strong>收益</strong>：</p><ul><li><strong>显存占用</strong>：从 $O(N^2)$ 降为 $O(N)$（线性！）。不再需要存储 $N \times N$ 的 Attention Map。</li><li><strong>速度</strong>：加速 2-4 倍（减少了 HBM 访问次数）。</li></ul><h3 id=3-flashattention-v2-并行优化>3. FlashAttention V2: 并行优化<a class=anchor href=#3-flashattention-v2-%e5%b9%b6%e8%a1%8c%e4%bc%98%e5%8c%96>#</a></h3><p>FlashAttention V2 (Dao, 2023) 进一步优化：</p><ol><li><strong>减少非矩阵运算</strong>：把 Softmax 等操作尽量融入矩阵各乘法 (MatMul) 中。</li><li><strong>更好的并行化</strong>：<ul><li>V1 主要是按 Batch 和 Head 并行。</li><li>V2 增加了按 Sequence Length 并行（即使 batch size=1 也能占满 GPU）。</li></ul></li></ol><p><strong>实战代码 (使用 PyTorch 2.0+)</strong>：</p><p>现在 PyTorch 2.0 已经内置了 FlashAttention（称为 Scaled Dot Product Attention, SDPA）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 启用 FlashAttention</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>backends</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>sdp_kernel</span><span class=p>(</span><span class=n>enable_flash</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>enable_math</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>enable_mem_efficient</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dropout_p</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>is_causal</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span></span></span></code></pre></div><hr><h2 id=五显存优化技术>五、显存优化技术<a class=anchor href=#%e4%ba%94%e6%98%be%e5%ad%98%e4%bc%98%e5%8c%96%e6%8a%80%e6%9c%af>#</a></h2><h3 id=1-pagedattention-vllm>1. PagedAttention (vLLM)<a class=anchor href=#1-pagedattention-vllm>#</a></h3><p>随着 Context 变长，KV Cache 成为显存杀手。
传统的 KV Cache 是预分配连续显存的。如果 max_len=2048，即使用户只输入 5 个字，系统也会预留 2048 的槽位（或者产生大量碎片）。</p><p><strong>PagedAttention</strong> 灵感来自操作系统的 <strong>虚拟内存 (Virtual Memory)</strong>：</p><ul><li>把 KV Cache 切分成固定大小的 <strong>Block</strong> (e.g., 16 tokens/block)。</li><li>逻辑上连续的 token，在显存物理上可以不连续。</li><li>通过 <strong>Block Table</strong> 记录映射关系。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>零浪费</strong>：显存利用率接近 100%。</li><li><strong>动态分配</strong>：生成多少用多少。</li><li><strong>Copy-on-Write</strong>：多个请求共享 Prompt 的 KV Cache（如 System Prompt）。</li></ul><h3 id=2-kv-cache-quantization>2. KV Cache Quantization<a class=anchor href=#2-kv-cache-quantization>#</a></h3><p>将 KV Cache 从 FP16 (2 bytes) 压缩到 INT8 (1 byte) 甚至 INT4。</p><ul><li><strong>FP16</strong>: 2 * 2 * L * H * D bytes</li><li><strong>INT8</strong>: 1 * 2 * L * H * D bytes (省一半显存)</li></ul><p><strong>KIVI (2024)</strong> 等算法证明，KV Cache 即使量化到 2-bit，对精度影响也很小。</p><h3 id=3-grouped-query-attention-gqa>3. Grouped-Query Attention (GQA)<a class=anchor href=#3-grouped-query-attention-gqa>#</a></h3><p>LLaMA-2 和 LLaMA-3 都使用了 GQA。</p><ul><li><strong>MHA (Multi-Head)</strong>: Query heads = KV heads (1:1)。KV Cache 最大。</li><li><strong>MQA (Multi-Query)</strong>: 只有 1 个 KV head，所有 Query 共享。KV Cache 最小，但掉点明显。</li><li><strong>GQA (Grouped-Query)</strong>: 折中方案。比如 32 个 Query head，8 个 KV head (4:1)。</li></ul><p>GQA 在保持高性能的同时，将 KV Cache 显存降低了 4-8 倍。</p><hr><h2 id=六代码实战手写一个支持-32k-上下文的-mini-llama>六、代码实战：手写一个支持 32k 上下文的 Mini-Llama<a class=anchor href=#%e5%85%ad%e4%bb%a3%e7%a0%81%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa%e6%94%af%e6%8c%81-32k-%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84-mini-llama>#</a></h2><p>为了彻底理解，我们实现一个带有 <strong>RoPE</strong> 和 <strong>FlashAttention</strong> 的注意力层。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LlamaRotaryEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=n>max_position_embeddings</span><span class=o>=</span><span class=mi>32768</span><span class=p>,</span> <span class=n>base</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dim</span> <span class=o>=</span> <span class=n>dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_position_embeddings</span> <span class=o>=</span> <span class=n>max_position_embeddings</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>base</span> <span class=o>=</span> <span class=n>base</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 预计算 cos/sin</span>
</span></span><span class=line><span class=cl>        <span class=n>inv_freq</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>base</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;inv_freq&#34;</span><span class=p>,</span> <span class=n>inv_freq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>update_freqs</span><span class=p>(</span><span class=n>max_position_embeddings</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update_freqs</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>device</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>inv_freq</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>freqs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s2>&#34;i,j-&gt;ij&#34;</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>inv_freq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>emb</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>freqs</span><span class=p>,</span> <span class=n>freqs</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cos_cached</span> <span class=o>=</span> <span class=n>emb</span><span class=o>.</span><span class=n>cos</span><span class=p>()[</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sin_cached</span> <span class=o>=</span> <span class=n>emb</span><span class=o>.</span><span class=n>sin</span><span class=p>()[</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>seq_len</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># x: [bs, num_attention_heads, seq_len, head_size]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>seq_len</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 动态扩展（简单线性外推）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>update_freqs</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>cos_cached</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=n>seq_len</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>dtype</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>sin_cached</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=n>seq_len</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>dtype</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rotate_half</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Rotates half the hidden dims of the input.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>x1</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>x2</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=o>-</span><span class=n>x2</span><span class=p>,</span> <span class=n>x1</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_rotary_pos_emb</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>cos</span><span class=p>,</span> <span class=n>sin</span><span class=p>,</span> <span class=n>position_ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 简单的实现版本，没有使用复数</span>
</span></span><span class=line><span class=cl>    <span class=c1># q, k: [bs, num_heads, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>    <span class=n>q_embed</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>cos</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>rotate_half</span><span class=p>(</span><span class=n>q</span><span class=p>)</span> <span class=o>*</span> <span class=n>sin</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>k_embed</span> <span class=o>=</span> <span class=p>(</span><span class=n>k</span> <span class=o>*</span> <span class=n>cos</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>rotate_half</span><span class=p>(</span><span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>sin</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q_embed</span><span class=p>,</span> <span class=n>k_embed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LongContextAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>o_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rotary_emb</span> <span class=o>=</span> <span class=n>LlamaRotaryEmbedding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 投影</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 应用 RoPE</span>
</span></span><span class=line><span class=cl>        <span class=n>cos</span><span class=p>,</span> <span class=n>sin</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rotary_emb</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=n>seq_len</span><span class=o>=</span><span class=n>seq_len</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span><span class=p>,</span> <span class=n>k</span> <span class=o>=</span> <span class=n>apply_rotary_pos_emb</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>cos</span><span class=p>,</span> <span class=n>sin</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. FlashAttention</span>
</span></span><span class=line><span class=cl>        <span class=c1># 自动选择最优实现（FlashV2 &gt; MemEfficient &gt; Math）</span>
</span></span><span class=line><span class=cl>        <span class=n>context_layer</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dropout_p</span><span class=o>=</span><span class=mf>0.0</span> <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=k>else</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>is_causal</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. 输出</span>
</span></span><span class=line><span class=cl>        <span class=n>context_layer</span> <span class=o>=</span> <span class=n>context_layer</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>o_proj</span><span class=p>(</span><span class=n>context_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试代码</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=nb>type</span><span class=p>(</span><span class=s1>&#39;Config&#39;</span><span class=p>,</span> <span class=p>(),</span> <span class=p>{</span><span class=s1>&#39;hidden_size&#39;</span><span class=p>:</span> <span class=mi>4096</span><span class=p>,</span> <span class=s1>&#39;num_heads&#39;</span><span class=p>:</span> <span class=mi>32</span><span class=p>})()</span>
</span></span><span class=line><span class=cl><span class=n>attn</span> <span class=o>=</span> <span class=n>LongContextAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1024</span><span class=p>,</span> <span class=mi>4096</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输入形状: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;输出形状: </span><span class=si>{</span><span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># 应该是 [1, 1024, 4096]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;长上下文 Attention 计算成功！&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=七本章小结>七、本章小结<a class=anchor href=#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>长上下文技术是构建 Agent 记忆系统和大型 RAG 知识库的基础。</p><ol><li><strong>RoPE</strong>: 完美的相对位置编码，是 LLaMA 家族的标配。</li><li><strong>NTK/YaRN</strong>: &ldquo;不重新训练模型"就能把 Context 窗口拉长 4-8 倍的魔法。</li><li><strong>FlashAttention</strong>: 打破 IO 瓶颈，让 Attention 计算速度跟上 GPU 算力。</li><li><strong>PagedAttention</strong>: 像管理内存一样管理显存，解决碎片化问题。</li></ol><p>掌握这些技术，你就不再会被 &ldquo;Context Window exceeded&rdquo; 报错所困扰。</p><hr><p><strong>下一章预告：</strong> 第2章 - 新型架构探索</p><p>在下一章中，我们将拆解 Mixtral 8x7B 和 DeepSeek-MoE 背后的稀疏激活机制，以及 DeepSeek-V3 的 MLA 架构。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第3章 模型评估体系</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/ class="flex align-center"><span>第2章 新型架构探索</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一长上下文的挑战>一、长上下文的挑战</a></li><li><a href=#二位置编码的进化rope-rotary-positional-embeddings>二、位置编码的进化：RoPE (Rotary Positional Embeddings)</a><ul><li><a href=#1-绝对位置-vs-相对位置>1. 绝对位置 vs 相对位置</a></li><li><a href=#2-rope-核心原理>2. RoPE 核心原理</a></li><li><a href=#3-pytorch-实现-rope>3. PyTorch 实现 RoPE</a></li></ul></li><li><a href=#三外推技术打破长度限制>三、外推技术：打破长度限制</a><ul><li><a href=#1-线性内插-linear-interpolation>1. 线性内插 (Linear Interpolation)</a></li><li><a href=#2-ntk-aware-scaled-rope>2. NTK-Aware Scaled RoPE</a></li><li><a href=#3-yarn-yet-another-rope-for-transformers>3. YaRN (Yet another RoPE for Transformers)</a></li></ul></li><li><a href=#四工程优化flashattention>四、工程优化：FlashAttention</a><ul><li><a href=#1-显存带宽瓶颈-memory-bound>1. 显存带宽瓶颈 (Memory Bound)</a></li><li><a href=#2-flashattention-v1-tiling--recomputation>2. FlashAttention V1: Tiling & Recomputation</a></li><li><a href=#3-flashattention-v2-并行优化>3. FlashAttention V2: 并行优化</a></li></ul></li><li><a href=#五显存优化技术>五、显存优化技术</a><ul><li><a href=#1-pagedattention-vllm>1. PagedAttention (vLLM)</a></li><li><a href=#2-kv-cache-quantization>2. KV Cache Quantization</a></li><li><a href=#3-grouped-query-attention-gqa>3. Grouped-Query Attention (GQA)</a></li></ul></li><li><a href=#六代码实战手写一个支持-32k-上下文的-mini-llama>六、代码实战：手写一个支持 32k 上下文的 Mini-Llama</a></li><li><a href=#七本章小结>七、本章小结</a></li></ul></nav></div></aside></main></body></html>