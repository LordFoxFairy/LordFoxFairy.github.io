<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第2章：新型架构探索 (New Architectures)# 本章定位：打破 Transformer 的垄断。我们将深入 DeepSeek 和 Mixtral 及其背后的 MoE (混合专家) 技术，并探索挑战 Attention 机制的 SSM (Mamba) 架构。这也是 DeepSeek-V3 能在极低成本下训练出来的核心秘密。
目录# 1. 混合专家模型 (MoE) 深度解析 1.1 稀疏激活：从 Dense 到 Sparse 1.2 核心组件：Router (Gate) 原理 1.3 负载均衡与辅助损失 (Aux Loss) 1.4 实战：手写一个 MoE Layer 2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention) 2.1 KV Cache 的显存瓶颈 2.2 MLA 原理：低秩压缩 2.3 显存节省计算案例 3. 状态空间模型 (SSM) 与 Mamba 3.1 线性复杂度：O(N) vs O(N^2) 3.2 选择性机制 (Selection Mechanism) 3.3 Mamba 代码实现 本章小结 1. 混合专家模型 (MoE) 深度解析# 1.1 稀疏激活：从 Dense 到 Sparse# 传统 Transformer 是 Dense (稠密) 的：每个 Token 都要经过模型的所有参数计算。 MoE 是 Sparse (稀疏) 的：
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第2章 新型架构探索"><meta property="og:description" content="第2章：新型架构探索 (New Architectures)# 本章定位：打破 Transformer 的垄断。我们将深入 DeepSeek 和 Mixtral 及其背后的 MoE (混合专家) 技术，并探索挑战 Attention 机制的 SSM (Mamba) 架构。这也是 DeepSeek-V3 能在极低成本下训练出来的核心秘密。
目录# 1. 混合专家模型 (MoE) 深度解析 1.1 稀疏激活：从 Dense 到 Sparse 1.2 核心组件：Router (Gate) 原理 1.3 负载均衡与辅助损失 (Aux Loss) 1.4 实战：手写一个 MoE Layer 2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention) 2.1 KV Cache 的显存瓶颈 2.2 MLA 原理：低秩压缩 2.3 显存节省计算案例 3. 状态空间模型 (SSM) 与 Mamba 3.1 线性复杂度：O(N) vs O(N^2) 3.2 选择性机制 (Selection Mechanism) 3.3 Mamba 代码实现 本章小结 1. 混合专家模型 (MoE) 深度解析# 1.1 稀疏激活：从 Dense 到 Sparse# 传统 Transformer 是 Dense (稠密) 的：每个 Token 都要经过模型的所有参数计算。 MoE 是 Sparse (稀疏) 的："><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第2章 新型架构探索"><meta itemprop=description content="第2章：新型架构探索 (New Architectures)# 本章定位：打破 Transformer 的垄断。我们将深入 DeepSeek 和 Mixtral 及其背后的 MoE (混合专家) 技术，并探索挑战 Attention 机制的 SSM (Mamba) 架构。这也是 DeepSeek-V3 能在极低成本下训练出来的核心秘密。
目录# 1. 混合专家模型 (MoE) 深度解析 1.1 稀疏激活：从 Dense 到 Sparse 1.2 核心组件：Router (Gate) 原理 1.3 负载均衡与辅助损失 (Aux Loss) 1.4 实战：手写一个 MoE Layer 2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention) 2.1 KV Cache 的显存瓶颈 2.2 MLA 原理：低秩压缩 2.3 显存节省计算案例 3. 状态空间模型 (SSM) 与 Mamba 3.1 线性复杂度：O(N) vs O(N^2) 3.2 选择性机制 (Selection Mechanism) 3.3 Mamba 代码实现 本章小结 1. 混合专家模型 (MoE) 深度解析# 1.1 稀疏激活：从 Dense 到 Sparse# 传统 Transformer 是 Dense (稠密) 的：每个 Token 都要经过模型的所有参数计算。 MoE 是 Sparse (稀疏) 的："><meta itemprop=wordCount content="743"><title>第2章 新型架构探索 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle checked>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/ class=active>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第2章 新型架构探索</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-混合专家模型-moe-深度解析>1. 混合专家模型 (MoE) 深度解析</a><ul><li><a href=#11-稀疏激活从-dense-到-sparse>1.1 稀疏激活：从 Dense 到 Sparse</a></li><li><a href=#12-核心组件router-gate-原理>1.2 核心组件：Router (Gate) 原理</a></li><li><a href=#13-负载均衡与辅助损失-aux-loss>1.3 负载均衡与辅助损失 (Aux Loss)</a></li><li><a href=#14-实战手写一个-moe-layer>1.4 实战：手写一个 MoE Layer</a></li></ul></li><li><a href=#2-deepseek-v3-核心mla-multi-head-latent-attention>2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention)</a><ul><li><a href=#21-kv-cache-的显存瓶颈>2.1 KV Cache 的显存瓶颈</a></li><li><a href=#22-mla-原理低秩压缩>2.2 MLA 原理：低秩压缩</a></li><li><a href=#23-显存节省计算案例>2.3 显存节省计算案例</a></li></ul></li><li><a href=#3-状态空间模型-ssm-与-mamba>3. 状态空间模型 (SSM) 与 Mamba</a><ul><li><a href=#31-线性复杂度on-vs-on2>3.1 线性复杂度：O(N) vs O(N^2)</a></li><li><a href=#32-选择性机制-selection-mechanism>3.2 选择性机制 (Selection Mechanism)</a></li><li><a href=#33-mamba-代码实现>3.3 Mamba 代码实现</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第2章新型架构探索-new-architectures>第2章：新型架构探索 (New Architectures)<a class=anchor href=#%e7%ac%ac2%e7%ab%a0%e6%96%b0%e5%9e%8b%e6%9e%b6%e6%9e%84%e6%8e%a2%e7%b4%a2-new-architectures>#</a></h1><blockquote class=book-hint><p><strong>本章定位</strong>：打破 Transformer 的垄断。我们将深入 DeepSeek 和 Mixtral 及其背后的 <strong>MoE (混合专家)</strong> 技术，并探索挑战 Attention 机制的 <strong>SSM (Mamba)</strong> 架构。这也是 DeepSeek-V3 能在极低成本下训练出来的核心秘密。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#1-%e6%b7%b7%e5%90%88%e4%b8%93%e5%ae%b6%e6%a8%a1%e5%9e%8b-moe-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90>1. 混合专家模型 (MoE) 深度解析</a><ul><li><a href=#11-%e7%a8%80%e7%96%8f%e6%bf%80%e6%b4%bb%e4%bb%8e-dense-%e5%88%b0-sparse>1.1 稀疏激活：从 Dense 到 Sparse</a></li><li><a href=#12-%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6router-gate-%e5%8e%9f%e7%90%86>1.2 核心组件：Router (Gate) 原理</a></li><li><a href=#13-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e4%b8%8e%e8%be%85%e5%8a%a9%e6%8d%9f%e5%a4%b1-aux-loss>1.3 负载均衡与辅助损失 (Aux Loss)</a></li><li><a href=#14-%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa-moe-layer>1.4 实战：手写一个 MoE Layer</a></li></ul></li><li><a href=#2-deepseek-v3-%e6%a0%b8%e5%bf%83mla-multi-head-latent-attention>2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention)</a><ul><li><a href=#21-kv-cache-%e7%9a%84%e6%98%be%e5%ad%98%e7%93%b6%e9%a2%88>2.1 KV Cache 的显存瓶颈</a></li><li><a href=#22-mla-%e5%8e%9f%e7%90%86%e4%bd%8e%e7%a7%a9%e5%8e%8b%e7%bc%a9>2.2 MLA 原理：低秩压缩</a></li><li><a href=#23-%e6%98%be%e5%ad%98%e8%8a%82%e7%9c%81%e8%ae%a1%e7%ae%97%e6%a1%88%e4%be%8b>2.3 显存节省计算案例</a></li></ul></li><li><a href=#3-%e7%8a%b6%e6%80%81%e7%a9%ba%e9%97%b4%e6%a8%a1%e5%9e%8b-ssm-%e4%b8%8e-mamba>3. 状态空间模型 (SSM) 与 Mamba</a><ul><li><a href=#31-%e7%ba%bf%e6%80%a7%e5%a4%8d%e6%9d%82%e5%ba%a6on-vs-on2>3.1 线性复杂度：O(N) vs O(N^2)</a></li><li><a href=#32-%e9%80%89%e6%8b%a9%e6%80%a7%e6%9c%ba%e5%88%b6-selection-mechanism>3.2 选择性机制 (Selection Mechanism)</a></li><li><a href=#33-mamba-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>3.3 Mamba 代码实现</a></li></ul></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li></ul><hr><h2 id=1-混合专家模型-moe-深度解析>1. 混合专家模型 (MoE) 深度解析<a class=anchor href=#1-%e6%b7%b7%e5%90%88%e4%b8%93%e5%ae%b6%e6%a8%a1%e5%9e%8b-moe-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90>#</a></h2><h3 id=11-稀疏激活从-dense-到-sparse>1.1 稀疏激活：从 Dense 到 Sparse<a class=anchor href=#11-%e7%a8%80%e7%96%8f%e6%bf%80%e6%b4%bb%e4%bb%8e-dense-%e5%88%b0-sparse>#</a></h3><p>传统 Transformer 是 <strong>Dense (稠密)</strong> 的：每个 Token 都要经过模型的所有参数计算。
MoE 是 <strong>Sparse (稀疏)</strong> 的：</p><ul><li>模型包含 $N$ 个专家（Experts，通常是 FFN 层）。</li><li>每个 Token 只激活其中的 $k$ 个专家（例如 8 个专家里选 2 个）。</li></ul><p><strong>收益</strong>：</p><ul><li><strong>训练/推理成本</strong>：只取决于激活专家的参数量（Active Params）。</li><li><strong>模型容量</strong>：取决于总参数量（Total Params）。</li><li><strong>结论</strong>：用极低的计算量（如 7B 级别）享受到极大模型（如 50B 级别）的知识容量。</li></ul><h3 id=12-核心组件router-gate-原理>1.2 核心组件：Router (Gate) 原理<a class=anchor href=#12-%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6router-gate-%e5%8e%9f%e7%90%86>#</a></h3><p>Router 决定每个 Token 去哪个专家。最常用的是 <strong>Top-K Gating</strong>。</p><p>$$
h(x) = \text{Softmax}(x \cdot W_g)
$$
$$
\text{Gate}(x) = \text{TopK}(h(x), k)
$$</p><p>Token $x$ 最终的输出是选定专家的加权和：
$$
y = \sum_{i \in \text{TopK}} h(x)_i \cdot E_i(x)
$$</p><h3 id=13-负载均衡与辅助损失-aux-loss>1.3 负载均衡与辅助损失 (Aux Loss)<a class=anchor href=#13-%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e4%b8%8e%e8%be%85%e5%8a%a9%e6%8d%9f%e5%a4%b1-aux-loss>#</a></h3><p><strong>问题</strong>：Router 可能会“偷懒”，把所有 Token 都发给同一个专家（导致该专家过载，其他专家闲置）。这叫 <strong>Router Collapse</strong>。</p><p><strong>解决方案</strong>：引入辅助损失 (Auxiliary Loss) 惩罚负载不均。
$$ \mathcal{L}<em>{aux} = \alpha \cdot N \cdot \sum</em>{i=1}^N f_i \cdot P_i $$</p><ul><li>$f_i$: 分配给专家 $i$ 的 Token 比例。</li><li>$P_i$: 专家 $i$ 被选中的平均概率。</li><li>目标是让每个专家处理的数据量尽量均匀。</li></ul><h3 id=14-实战手写一个-moe-layer>1.4 实战：手写一个 MoE Layer<a class=anchor href=#14-%e5%ae%9e%e6%88%98%e6%89%8b%e5%86%99%e4%b8%80%e4%b8%aa-moe-layer>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MoELayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>num_experts</span><span class=p>,</span> <span class=n>top_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_experts</span> <span class=o>=</span> <span class=n>num_experts</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>top_k</span> <span class=o>=</span> <span class=n>top_k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 门控网络 (Router)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gate</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>num_experts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 专家网络 (这里用简单的 Linear 模拟 MLP)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>experts</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_experts</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># x: [batch, seq, dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span><span class=p>,</span> <span class=n>seq</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>x_flat</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 计算路由概率</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gate</span><span class=p>(</span><span class=n>x_flat</span><span class=p>)</span>  <span class=c1># [total_tokens, num_experts]</span>
</span></span><span class=line><span class=cl>        <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. 选出 Top-K 专家</span>
</span></span><span class=line><span class=cl>        <span class=c1># indices: [total_tokens, top_k] (专家的 ID)</span>
</span></span><span class=line><span class=cl>        <span class=c1># weights: [total_tokens, top_k] (路由权重)</span>
</span></span><span class=line><span class=cl>        <span class=n>weights</span><span class=p>,</span> <span class=n>indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>top_k</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 归一化权重 (让选中的 k 个权重之和为 1)</span>
</span></span><span class=line><span class=cl>        <span class=n>weights</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>/</span> <span class=n>weights</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 专家计算 (这里用循环模拟，实际用 CUDA Kernel 优化)</span>
</span></span><span class=line><span class=cl>        <span class=n>final_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x_flat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_experts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 找到分配给专家 i 的 token</span>
</span></span><span class=line><span class=cl>            <span class=c1># mask: [total_tokens, top_k]</span>
</span></span><span class=line><span class=cl>            <span class=n>mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>indices</span> <span class=o>==</span> <span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># batch_idx, k_idx: 哪些 token 的第几个选择是专家 i</span>
</span></span><span class=line><span class=cl>            <span class=n>batch_idx</span><span class=p>,</span> <span class=n>k_idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>batch_idx</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 提取输入</span>
</span></span><span class=line><span class=cl>            <span class=n>expert_input</span> <span class=o>=</span> <span class=n>x_flat</span><span class=p>[</span><span class=n>batch_idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 专家前向传播</span>
</span></span><span class=line><span class=cl>            <span class=n>expert_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>experts</span><span class=p>[</span><span class=n>i</span><span class=p>](</span><span class=n>expert_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 加权累加回输出</span>
</span></span><span class=line><span class=cl>            <span class=c1># weights[batch_idx, k_idx]: 对应的路由权重</span>
</span></span><span class=line><span class=cl>            <span class=n>scale</span> <span class=o>=</span> <span class=n>weights</span><span class=p>[</span><span class=n>batch_idx</span><span class=p>,</span> <span class=n>k_idx</span><span class=p>]</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>final_output</span><span class=o>.</span><span class=n>index_add_</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=n>expert_output</span> <span class=o>*</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>final_output</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>seq</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试</span>
</span></span><span class=line><span class=cl><span class=n>moe</span> <span class=o>=</span> <span class=n>MoELayer</span><span class=p>(</span><span class=n>hidden_dim</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>num_experts</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>moe</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Input: </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>, Output: </span><span class=si>{</span><span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=2-deepseek-v3-核心mla-multi-head-latent-attention>2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention)<a class=anchor href=#2-deepseek-v3-%e6%a0%b8%e5%bf%83mla-multi-head-latent-attention>#</a></h2><blockquote class=book-hint><p><strong>核心价值</strong>：DeepSeek-V3 相比传统 Llama 架构，节省了 <strong>98.4% 的 KV Cache 显存</strong>。这是它能由极其“廉价”的成本提供高性能服务的关键。</p></blockquote><h3 id=21-kv-cache-的显存瓶颈>2.1 KV Cache 的显存瓶颈<a class=anchor href=#21-kv-cache-%e7%9a%84%e6%98%be%e5%ad%98%e7%93%b6%e9%a2%88>#</a></h3><p>在传统 <strong>MHA (Multi-Head Attention)</strong> 中，每个 Token 都需要存储完整的 K 和 V 矩阵：
$$ \text{Cache} = 2 \times \text{Layers} \times \text{Heads} \times \text{HeadDim} $$
对于一个 70B 模型，128k 上下文，仅 KV Cache 就要占用 <strong>100GB+ 显存</strong>！这导致必须堆显卡。</p><h3 id=22-mla-原理低秩压缩>2.2 MLA 原理：低秩压缩<a class=anchor href=#22-mla-%e5%8e%9f%e7%90%86%e4%bd%8e%e7%a7%a9%e5%8e%8b%e7%bc%a9>#</a></h3><p>MLA 认为 K 和 V 矩阵存在大量冗余（Low Rank）。它不直接存储 K 和 V，而是存储一个<strong>压缩的潜在向量 (Latent Vector)</strong> $C_{KV}$。</p><p><strong>公式推导</strong>：</p><ol><li><strong>压缩 (Down-projection)</strong>:
$$ C_{KV} = X \cdot W_{Down} $$
$C_{KV}$ 的维度 $d_c$ (如 512) 远小于原始 KV 维度 (如 128头 $\times$ 128维 = 16384)。</li><li><strong>存储</strong>: KV Cache 只存 $C_{KV}$。</li><li><strong>解压 (Up-projection)</strong>:
推理计算 Attention 时，临时恢复成 K 和 V：
$$ K = C_{KV} \cdot W_{UpK} $$
$$ V = C_{KV} \cdot W_{UpV} $$</li></ol><h3 id=23-显存节省计算案例>2.3 显存节省计算案例<a class=anchor href=#23-%e6%98%be%e5%ad%98%e8%8a%82%e7%9c%81%e8%ae%a1%e7%ae%97%e6%a1%88%e4%be%8b>#</a></h3><p>以 DeepSeek-V3 配置为例：</p><ul><li>Heads = 128, HeadDim = 128</li><li>MHA 存储维度 = $128 \times 128 = 16384$</li><li>MLA 压缩维度 $d_c = 512$</li></ul><p>$$ \text{压缩比} = \frac{512}{16384} \approx 3% $$</p><p><strong>结论</strong>：MLA 只需要传统 MHA <strong>3% 的显存</strong>。这使得单机可以跑超长上下文，或者支持极大并发 (Batch Size)。</p><hr><h2 id=3-状态空间模型-ssm-与-mamba>3. 状态空间模型 (SSM) 与 Mamba<a class=anchor href=#3-%e7%8a%b6%e6%80%81%e7%a9%ba%e9%97%b4%e6%a8%a1%e5%9e%8b-ssm-%e4%b8%8e-mamba>#</a></h2><h3 id=31-线性复杂度on-vs-on2>3.1 线性复杂度：O(N) vs O(N^2)<a class=anchor href=#31-%e7%ba%bf%e6%80%a7%e5%a4%8d%e6%9d%82%e5%ba%a6on-vs-on2>#</a></h3><ul><li><strong>Transformer</strong>: Attention 机制需要两两计算相似度，复杂度是序列长度的平方 $O(N^2)$。长文训练极慢。</li><li><strong>RNN</strong>: 只能看上一步，无法并行训练。</li><li><strong>Mamba (SSM)</strong>: 结合了 RNN 的推理效率 (O(1)) 和 Transformer 的并行训练能力。</li></ul><h3 id=32-选择性机制-selection-mechanism>3.2 选择性机制 (Selection Mechanism)<a class=anchor href=#32-%e9%80%89%e6%8b%a9%e6%80%a7%e6%9c%ba%e5%88%b6-selection-mechanism>#</a></h3><p>Mamba 的核心创新是<strong>选择性地遗忘和记忆</strong>。
它引入了随输入 $x_t$ 变化的参数 $\Delta, B, C$，让模型能根据当前内容动态决定：</p><ul><li>这是一个重要的 Token -> <strong>记入状态</strong>。</li><li>这是一个噪音 Token -> <strong>忽略/遗忘</strong>。</li></ul><h3 id=33-mamba-代码实现>3.3 Mamba 代码实现<a class=anchor href=#33-mamba-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MambaBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_state</span><span class=o>=</span><span class=mi>16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 线性投影</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>x_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>+</span> <span class=n>d_state</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span> <span class=c1># 生成 delta, B, C</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. SSM 参数 (A 是固定的/缓慢变化的)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>A_log</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>d_state</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 简化版前向传播 (非并行优化的 Scan 实现)</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span><span class=p>,</span> <span class=n>seq</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_state</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>seq</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>xt</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span> <span class=n>t</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 动态计算参数 (Selective Scan)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 这里省略了复杂的离散化 (Discretization) 步骤</span>
</span></span><span class=line><span class=cl>            <span class=c1># h_t = A * h_{t-1} + B * x_t</span>
</span></span><span class=line><span class=cl>            <span class=c1># y_t = C * h_t</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># ... (完整实现需 CUDA Kernel)</span>
</span></span><span class=line><span class=cl>            <span class=k>pass</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=c1># Placeholder</span></span></span></code></pre></div><p><em>(注：真实 Mamba 训练依赖 Triton 编写的 Parallel Scan Kernel，Python 循环无法训练)</em></p><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><ol><li><strong>MoE (DeepSeek/Mixtral)</strong>：通过稀疏激活解决了<strong>参数量与计算量的矛盾</strong>。核心是 Router 和 Load Balancing。</li><li><strong>MLA (DeepSeek-V3)</strong>：通过低秩压缩解决了<strong>KV Cache 显存瓶颈</strong>，是长文本推理的神器。</li><li><strong>SSM (Mamba)</strong>：通过线性复杂度挑战了 Transformer 的统治地位，特别适合超长序列 (如基因组、长视频)。</li></ol><p>掌握了这些，你就看懂了当前 LLM 架构演进的主战场。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第1章 长上下文技术</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/ class="flex align-center"><span>第3章 推理加速黑科技</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-混合专家模型-moe-深度解析>1. 混合专家模型 (MoE) 深度解析</a><ul><li><a href=#11-稀疏激活从-dense-到-sparse>1.1 稀疏激活：从 Dense 到 Sparse</a></li><li><a href=#12-核心组件router-gate-原理>1.2 核心组件：Router (Gate) 原理</a></li><li><a href=#13-负载均衡与辅助损失-aux-loss>1.3 负载均衡与辅助损失 (Aux Loss)</a></li><li><a href=#14-实战手写一个-moe-layer>1.4 实战：手写一个 MoE Layer</a></li></ul></li><li><a href=#2-deepseek-v3-核心mla-multi-head-latent-attention>2. DeepSeek-V3 核心：MLA (Multi-Head Latent Attention)</a><ul><li><a href=#21-kv-cache-的显存瓶颈>2.1 KV Cache 的显存瓶颈</a></li><li><a href=#22-mla-原理低秩压缩>2.2 MLA 原理：低秩压缩</a></li><li><a href=#23-显存节省计算案例>2.3 显存节省计算案例</a></li></ul></li><li><a href=#3-状态空间模型-ssm-与-mamba>3. 状态空间模型 (SSM) 与 Mamba</a><ul><li><a href=#31-线性复杂度on-vs-on2>3.1 线性复杂度：O(N) vs O(N^2)</a></li><li><a href=#32-选择性机制-selection-mechanism>3.2 选择性机制 (Selection Mechanism)</a></li><li><a href=#33-mamba-代码实现>3.3 Mamba 代码实现</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></div></aside></main></body></html>