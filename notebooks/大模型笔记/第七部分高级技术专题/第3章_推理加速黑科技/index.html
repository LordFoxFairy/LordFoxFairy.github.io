<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第3章：推理加速黑科技 (Inference Acceleration)# 本章定位：在不改变模型权重的前提下,让推理速度提升 2-3 倍。核心技术:投机解码(Speculative Decoding)、Medusa 多头预测、Lookahead 前瞻解码。这些技术已被集成到 vLLM/TGI/SGLang 等生产系统中。
目录# 1. 自回归解码的性能瓶颈 1.1 为什么 Transformer 推理这么慢? 1.2 Batch Size=1 的GPU利用率灾难 2. 投机解码 (Speculative Decoding) 2.1 核心思想：草稿模型 + 并行验证 2.2 数学原理：无损加速的保证 2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B 3. Medusa：多头并行预测 3.1 架构：在 LM Head 之上增加多个预测头 3.2 训练：自监督蒸馏 3.3 Tree Attention 优化 4. Lookahead Decoding：前瞻解码 4.1 N-gram 缓存原理 4.2 Jacobi 迭代并行化 4.3 适用场景分析 5. 其他前沿技术 5.1 Eagle：基于特征的推测 5.2 Cascade Speculation：层级推测 本章小结 1. 自回归解码的性能瓶颈# 1.1 为什么 Transformer 推理这么慢?# LLM 的生成是自回归 (Autoregressive) 的:每次只能生成一个 Token,必须等上一个 Token 出来才能生成下一个。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第3章 推理加速黑科技"><meta property="og:description" content="第3章：推理加速黑科技 (Inference Acceleration)# 本章定位：在不改变模型权重的前提下,让推理速度提升 2-3 倍。核心技术:投机解码(Speculative Decoding)、Medusa 多头预测、Lookahead 前瞻解码。这些技术已被集成到 vLLM/TGI/SGLang 等生产系统中。
目录# 1. 自回归解码的性能瓶颈 1.1 为什么 Transformer 推理这么慢? 1.2 Batch Size=1 的GPU利用率灾难 2. 投机解码 (Speculative Decoding) 2.1 核心思想：草稿模型 + 并行验证 2.2 数学原理：无损加速的保证 2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B 3. Medusa：多头并行预测 3.1 架构：在 LM Head 之上增加多个预测头 3.2 训练：自监督蒸馏 3.3 Tree Attention 优化 4. Lookahead Decoding：前瞻解码 4.1 N-gram 缓存原理 4.2 Jacobi 迭代并行化 4.3 适用场景分析 5. 其他前沿技术 5.1 Eagle：基于特征的推测 5.2 Cascade Speculation：层级推测 本章小结 1. 自回归解码的性能瓶颈# 1.1 为什么 Transformer 推理这么慢?# LLM 的生成是自回归 (Autoregressive) 的:每次只能生成一个 Token,必须等上一个 Token 出来才能生成下一个。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第3章 推理加速黑科技"><meta itemprop=description content="第3章：推理加速黑科技 (Inference Acceleration)# 本章定位：在不改变模型权重的前提下,让推理速度提升 2-3 倍。核心技术:投机解码(Speculative Decoding)、Medusa 多头预测、Lookahead 前瞻解码。这些技术已被集成到 vLLM/TGI/SGLang 等生产系统中。
目录# 1. 自回归解码的性能瓶颈 1.1 为什么 Transformer 推理这么慢? 1.2 Batch Size=1 的GPU利用率灾难 2. 投机解码 (Speculative Decoding) 2.1 核心思想：草稿模型 + 并行验证 2.2 数学原理：无损加速的保证 2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B 3. Medusa：多头并行预测 3.1 架构：在 LM Head 之上增加多个预测头 3.2 训练：自监督蒸馏 3.3 Tree Attention 优化 4. Lookahead Decoding：前瞻解码 4.1 N-gram 缓存原理 4.2 Jacobi 迭代并行化 4.3 适用场景分析 5. 其他前沿技术 5.1 Eagle：基于特征的推测 5.2 Cascade Speculation：层级推测 本章小结 1. 自回归解码的性能瓶颈# 1.1 为什么 Transformer 推理这么慢?# LLM 的生成是自回归 (Autoregressive) 的:每次只能生成一个 Token,必须等上一个 Token 出来才能生成下一个。"><meta itemprop=wordCount content="1213"><title>第3章 推理加速黑科技 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle checked>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/ class=active>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第3章 推理加速黑科技</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-自回归解码的性能瓶颈>1. 自回归解码的性能瓶颈</a><ul><li><a href=#11-为什么-transformer-推理这么慢>1.1 为什么 Transformer 推理这么慢?</a></li><li><a href=#12-batch-size1-的gpu利用率灾难>1.2 Batch Size=1 的GPU利用率灾难</a></li></ul></li><li><a href=#2-投机解码-speculative-decoding>2. 投机解码 (Speculative Decoding)</a><ul><li><a href=#21-核心思想草稿模型--并行验证>2.1 核心思想：草稿模型 + 并行验证</a></li><li><a href=#22-数学原理无损加速的保证>2.2 数学原理：无损加速的保证</a></li><li><a href=#23-实战用-qwen2-05b-加速-qwen2-7b>2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B</a></li></ul></li><li><a href=#3-medusa多头并行预测>3. Medusa：多头并行预测</a><ul><li><a href=#31-架构在-lm-head-之上增加多个预测头>3.1 架构：在 LM Head 之上增加多个预测头</a></li><li><a href=#32-训练自监督蒸馏>3.2 训练：自监督蒸馏</a></li><li><a href=#33-tree-attention-优化>3.3 Tree Attention 优化</a></li></ul></li><li><a href=#4-lookahead-decoding前瞻解码>4. Lookahead Decoding：前瞻解码</a><ul><li><a href=#41-n-gram-缓存原理>4.1 N-gram 缓存原理</a></li><li><a href=#42-jacobi-迭代并行化>4.2 Jacobi 迭代并行化</a></li><li><a href=#43-适用场景分析>4.3 适用场景分析</a></li></ul></li><li><a href=#5-其他前沿技术>5. 其他前沿技术</a><ul><li><a href=#51-eagle基于特征的推测>5.1 Eagle：基于特征的推测</a></li><li><a href=#52-cascade-speculation层级推测>5.2 Cascade Speculation：层级推测</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第3章推理加速黑科技-inference-acceleration>第3章：推理加速黑科技 (Inference Acceleration)<a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e6%8e%a8%e7%90%86%e5%8a%a0%e9%80%9f%e9%bb%91%e7%a7%91%e6%8a%80-inference-acceleration>#</a></h1><blockquote class=book-hint><p><strong>本章定位</strong>：在不改变模型权重的前提下,让推理速度提升 2-3 倍。核心技术:<strong>投机解码</strong>(Speculative Decoding)、<strong>Medusa 多头预测</strong>、<strong>Lookahead 前瞻解码</strong>。这些技术已被集成到 vLLM/TGI/SGLang 等生产系统中。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#1-%e8%87%aa%e5%9b%9e%e5%bd%92%e8%a7%a3%e7%a0%81%e7%9a%84%e6%80%a7%e8%83%bd%e7%93%b6%e9%a2%88>1. 自回归解码的性能瓶颈</a><ul><li><a href=#11-%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e6%8e%a8%e7%90%86%e8%bf%99%e4%b9%88%e6%85%a2>1.1 为什么 Transformer 推理这么慢?</a></li><li><a href=#12-batch-size1-%e7%9a%84gpu%e5%88%a9%e7%94%a8%e7%8e%87%e7%81%be%e9%9a%be>1.2 Batch Size=1 的GPU利用率灾难</a></li></ul></li><li><a href=#2-%e6%8a%95%e6%9c%ba%e8%a7%a3%e7%a0%81-speculative-decoding>2. 投机解码 (Speculative Decoding)</a><ul><li><a href=#21-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%8d%89%e7%a8%bf%e6%a8%a1%e5%9e%8b--%e5%b9%b6%e8%a1%8c%e9%aa%8c%e8%af%81>2.1 核心思想：草稿模型 + 并行验证</a></li><li><a href=#22-%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e6%97%a0%e6%8d%9f%e5%8a%a0%e9%80%9f%e7%9a%84%e4%bf%9d%e8%af%81>2.2 数学原理：无损加速的保证</a></li><li><a href=#23-%e5%ae%9e%e6%88%98%e7%94%a8-qwen2-05b-%e5%8a%a0%e9%80%9f-qwen2-7b>2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B</a></li></ul></li><li><a href=#3-medusa%e5%a4%9a%e5%a4%b4%e5%b9%b6%e8%a1%8c%e9%a2%84%e6%b5%8b>3. Medusa：多头并行预测</a><ul><li><a href=#31-%e6%9e%b6%e6%9e%84%e5%9c%a8-lm-head-%e4%b9%8b%e4%b8%8a%e5%a2%9e%e5%8a%a0%e5%a4%9a%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%a4%b4>3.1 架构：在 LM Head 之上增加多个预测头</a></li><li><a href=#32-%e8%ae%ad%e7%bb%83%e8%87%aa%e7%9b%91%e7%9d%a3%e8%92%b8%e9%a6%8f>3.2 训练：自监督蒸馏</a></li><li><a href=#33-tree-attention-%e4%bc%98%e5%8c%96>3.3 Tree Attention 优化</a></li></ul></li><li><a href=#4-lookahead-decoding%e5%89%8d%e7%9e%bb%e8%a7%a3%e7%a0%81>4. Lookahead Decoding：前瞻解码</a><ul><li><a href=#41-n-gram-%e7%bc%93%e5%ad%98%e5%8e%9f%e7%90%86>4.1 N-gram 缓存原理</a></li><li><a href=#42-jacobi-%e8%bf%ad%e4%bb%a3%e5%b9%b6%e8%a1%8c%e5%8c%96>4.2 Jacobi 迭代并行化</a></li><li><a href=#43-%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90>4.3 适用场景分析</a></li></ul></li><li><a href=#5-%e5%85%b6%e4%bb%96%e5%89%8d%e6%b2%bf%e6%8a%80%e6%9c%af>5. 其他前沿技术</a><ul><li><a href=#51-eagle%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e6%8e%a8%e6%b5%8b>5.1 Eagle：基于特征的推测</a></li><li><a href=#52-cascade-speculation%e5%b1%82%e7%ba%a7%e6%8e%a8%e6%b5%8b>5.2 Cascade Speculation：层级推测</a></li></ul></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li></ul><hr><h2 id=1-自回归解码的性能瓶颈>1. 自回归解码的性能瓶颈<a class=anchor href=#1-%e8%87%aa%e5%9b%9e%e5%bd%92%e8%a7%a3%e7%a0%81%e7%9a%84%e6%80%a7%e8%83%bd%e7%93%b6%e9%a2%88>#</a></h2><h3 id=11-为什么-transformer-推理这么慢>1.1 为什么 Transformer 推理这么慢?<a class=anchor href=#11-%e4%b8%ba%e4%bb%80%e4%b9%88-transformer-%e6%8e%a8%e7%90%86%e8%bf%99%e4%b9%88%e6%85%a2>#</a></h3><p>LLM 的生成是<strong>自回归 (Autoregressive)</strong> 的:每次只能生成一个 Token,必须等上一个 Token 出来才能生成下一个。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>生成 &#34;Hello, how are you today?&#34;
</span></span><span class=line><span class=cl>Step 1: [] → &#34;Hello&#34;
</span></span><span class=line><span class=cl>Step 2: [&#34;Hello&#34;] → &#34;,&#34;
</span></span><span class=line><span class=cl>Step 3: [&#34;Hello&#34;, &#34;,&#34;] → &#34;how&#34;
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>Step N: [&#34;Hello&#34;, &#34;,&#34;, &#34;how&#34;, ..., &#34;you&#34;] → &#34;today&#34;</span></span></code></pre></div><p><strong>关键问题</strong>：</p><ul><li>每一步都要做一次完整的前向传播 (Forward Pass)。</li><li>如果生成 100 个 Token,就要跑 100 次模型。</li><li>即使使用了 KV Cache,仍然是<strong>顺序依赖</strong>。</li></ul><h3 id=12-batch-size1-的gpu利用率灾难>1.2 Batch Size=1 的GPU利用率灾难<a class=anchor href=#12-batch-size1-%e7%9a%84gpu%e5%88%a9%e7%94%a8%e7%8e%87%e7%81%be%e9%9a%be>#</a></h3><p><strong>Prefill 阶段</strong> (处理输入 Prompt):</p><ul><li>输入长度 N,一次性计算所有 Token 的 Attention。</li><li>矩阵乘法维度: <code>[N, d_model] × [d_model, d_model]</code>。</li><li>GPU 利用率:<strong>高</strong> (80%+)。</li></ul><p><strong>Decode 阶段</strong> (生成输出):</p><ul><li>每次只处理 1 个 Token。</li><li>矩阵乘法维度: <code>[1, d_model] × [d_model, d_model]</code>。</li><li>GPU 利用率:<strong>极低</strong> (5-15%)。</li></ul><p>这意味着在生成阶段,GPU 的大量计算核心都在<strong>空转</strong>。</p><p><strong>结论</strong>：如果能在一次前向传播中生成多个 Token,就能大幅提升效率。</p><hr><h2 id=2-投机解码-speculative-decoding>2. 投机解码 (Speculative Decoding)<a class=anchor href=#2-%e6%8a%95%e6%9c%ba%e8%a7%a3%e7%a0%81-speculative-decoding>#</a></h2><blockquote class=book-hint><p><strong>论文</strong>: <em>Fast Inference from Transformers via Speculative Decoding</em> (DeepMind, 2023)</p></blockquote><h3 id=21-核心思想草稿模型--并行验证>2.1 核心思想：草稿模型 + 并行验证<a class=anchor href=#21-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%8d%89%e7%a8%bf%e6%a8%a1%e5%9e%8b--%e5%b9%b6%e8%a1%8c%e9%aa%8c%e8%af%81>#</a></h3><p>投机解码使用<strong>两个模型</strong>:</p><ol><li><strong>Draft Model (草稿模型)</strong>: 小而快 (如 Qwen2-0.5B)。</li><li><strong>Target Model (目标模型)</strong>: 大而准 (如 Qwen2-7B)。</li></ol><p><strong>流程</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>1. 草稿模型快速生成 K 个候选 Token (K=4-5)
</span></span><span class=line><span class=cl>   Draft: [&#34;Hello&#34;, &#34;there&#34;, &#34;!&#34;, &#34;How&#34;]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. 目标模型一次性验证这 K 个 Token
</span></span><span class=line><span class=cl>   - 并行计算 logits(Token_1), logits(Token_2), ...
</span></span><span class=line><span class=cl>   - 接受或拒绝每个 Token
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. 如果全部接受 → 跳过 K 步
</span></span><span class=line><span class=cl>   如果部分拒绝 → 从拒绝点重新生成</span></span></code></pre></div><p><strong>关键优势</strong>:</p><ul><li>目标模型只需跑 <strong>1 次</strong>前向传播,而不是 K 次。</li><li>即使草稿模型有时猜错,也不影响最终输出质量。</li></ul><h3 id=22-数学原理无损加速的保证>2.2 数学原理：无损加速的保证<a class=anchor href=#22-%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e6%97%a0%e6%8d%9f%e5%8a%a0%e9%80%9f%e7%9a%84%e4%bf%9d%e8%af%81>#</a></h3><p>投机解码的核心是<strong>概率修正 (Rejection Sampling)</strong>,确保输出分布与目标模型完全一致。</p><p>设:</p><ul><li>$p(x)$: 目标模型的概率分布</li><li>$q(x)$: 草稿模型的概率分布</li><li>$\gamma = \frac{p(x)}{q(x)}$: 重要性权重</li></ul><p><strong>接受准则</strong>:
对于草稿模型生成的 Token $x_i$,以概率 $\min(1, \gamma)$ 接受。</p><p>$$
\text{Accept}(x_i) =
\begin{cases}
\text{True}, & \text{if } \mathcal{U}(0,1) \leq \min\left(1, \frac{p(x_i \mid x_{&lt;i})}{q(x_i \mid x_{&lt;i})}\right) \
\text{False}, & \text{otherwise}
\end{cases}
$$</p><p><strong>为什么是无损的?</strong></p><p>这是经典的拒绝采样 (Rejection Sampling)。可以严格证明:最终采样出的 Token 序列分布 = $p(x)$。</p><p><strong>直觉解释</strong>:</p><ul><li>如果草稿模型的预测 $q(x_i)$ 与目标模型 $p(x_i)$ 接近 → $\gamma \approx 1$ → 高概率接受。</li><li>如果草稿模型严重偏离 → $\gamma \ll 1$ → 拒绝,并从目标模型重新采样。</li></ul><h3 id=23-实战用-qwen2-05b-加速-qwen2-7b>2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B<a class=anchor href=#23-%e5%ae%9e%e6%88%98%e7%94%a8-qwen2-05b-%e5%8a%a0%e9%80%9f-qwen2-7b>#</a></h3><p>以下是基于 Transformers 的简化实现 (生产环境推荐用 vLLM 的原生支持):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SpeculativeDecoder</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>draft_model_id</span><span class=p>,</span> <span class=n>target_model_id</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>draft</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>draft_model_id</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>target</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>target_model_id</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>target_model_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        投机解码生成
</span></span></span><span class=line><span class=cl><span class=s2>        k: 每次草稿模型生成的 Token 数量 (lookahead 长度)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>generated</span> <span class=o>=</span> <span class=n>input_ids</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=n>generated</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>max_new_tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Step 1: 草稿模型快速生成 k 个 Token</span>
</span></span><span class=line><span class=cl>            <span class=n>draft_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>            <span class=n>draft_input</span> <span class=o>=</span> <span class=n>generated</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>draft</span><span class=p>(</span><span class=n>draft_input</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>                <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>draft_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>next_token</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>draft_input</span><span class=p>,</span> <span class=n>next_token</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Step 2: 目标模型并行验证</span>
</span></span><span class=line><span class=cl>            <span class=c1># 关键: 一次前向传播计算所有 k 个位置的 logits</span>
</span></span><span class=line><span class=cl>            <span class=n>target_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>target</span><span class=p>(</span><span class=n>draft_input</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Step 3: 逐 Token 验证</span>
</span></span><span class=line><span class=cl>            <span class=n>accepted</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=c1># 取出第 i 个 Token 位置的目标模型概率</span>
</span></span><span class=line><span class=cl>                <span class=n>target_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>target_logits</span><span class=p>[:,</span> <span class=o>-</span><span class=p>(</span><span class=n>k</span><span class=o>-</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=p>:],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_token_id</span> <span class=o>=</span> <span class=n>draft_tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>p_target</span> <span class=o>=</span> <span class=n>target_probs</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>draft_token_id</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 草稿模型的概率</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_logits_i</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>draft</span><span class=p>(</span><span class=n>generated</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>                <span class=n>draft_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>draft_logits_i</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>p_draft</span> <span class=o>=</span> <span class=n>draft_probs</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>draft_token_id</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 接受准则: min(1, p_target / p_draft)</span>
</span></span><span class=line><span class=cl>                <span class=n>gamma</span> <span class=o>=</span> <span class=n>p_target</span> <span class=o>/</span> <span class=p>(</span><span class=n>p_draft</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>&lt;</span> <span class=nb>min</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>gamma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                    <span class=c1># 接受</span>
</span></span><span class=line><span class=cl>                    <span class=n>generated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>generated</span><span class=p>,</span> <span class=n>draft_tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>accepted</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=c1># 拒绝,从目标模型重新采样</span>
</span></span><span class=line><span class=cl>                    <span class=n>new_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>target_probs</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>generated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>generated</span><span class=p>,</span> <span class=n>new_token</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=k>break</span>  <span class=c1># 停止后续验证</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>accepted</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=c1># 如果一个都没接受,至少生成一个 Token</span>
</span></span><span class=line><span class=cl>                <span class=n>target_logits_final</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>target</span><span class=p>(</span><span class=n>generated</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>                <span class=n>new_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>target_logits_final</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>generated</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>generated</span><span class=p>,</span> <span class=n>new_token</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>generated</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>decoder</span> <span class=o>=</span> <span class=n>SpeculativeDecoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>draft_model_id</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen2-0.5B-Instruct&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>target_model_id</span><span class=o>=</span><span class=s2>&#34;Qwen/Qwen2-7B-Instruct&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>decoder</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=s2>&#34;Explain quantum computing in simple terms:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span></span></span></code></pre></div><p><strong>实际加速效果</strong>:</p><ul><li>理想情况 (Draft 模型准确率高): <strong>2.5-3x</strong> 加速。</li><li>一般情况: <strong>1.8-2x</strong> 加速。</li><li>最坏情况 (Draft 模型完全随机): 无加速甚至变慢 (因为验证开销)。</li></ul><p><strong>生产环境集成 (vLLM)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -m vllm.entrypoints.openai.api_server <span class=se>\
</span></span></span><span class=line><span class=cl>    --model Qwen/Qwen2-7B-Instruct <span class=se>\
</span></span></span><span class=line><span class=cl>    --speculative-model Qwen/Qwen2-0.5B-Instruct <span class=se>\
</span></span></span><span class=line><span class=cl>    --num-speculative-tokens <span class=m>5</span></span></span></code></pre></div><hr><h2 id=3-medusa多头并行预测>3. Medusa：多头并行预测<a class=anchor href=#3-medusa%e5%a4%9a%e5%a4%b4%e5%b9%b6%e8%a1%8c%e9%a2%84%e6%b5%8b>#</a></h2><blockquote class=book-hint><p><strong>论文</strong>: <em>Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</em> (2024)</p></blockquote><p>Speculative Decoding 需要额外的草稿模型,而 <strong>Medusa</strong> 只需要在原模型基础上增加几个轻量级的预测头。</p><h3 id=31-架构在-lm-head-之上增加多个预测头>3.1 架构：在 LM Head 之上增加多个预测头<a class=anchor href=#31-%e6%9e%b6%e6%9e%84%e5%9c%a8-lm-head-%e4%b9%8b%e4%b8%8a%e5%a2%9e%e5%8a%a0%e5%a4%9a%e4%b8%aa%e9%a2%84%e6%b5%8b%e5%a4%b4>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>                      ┌─→ Head_1 (预测下1个Token)
</span></span><span class=line><span class=cl>                      │
</span></span><span class=line><span class=cl>Hidden States ────────┼─→ Head_2 (预测下2个Token)
</span></span><span class=line><span class=cl>(from last layer)     │
</span></span><span class=line><span class=cl>                      ├─→ Head_3 (预测下3个Token)
</span></span><span class=line><span class=cl>                      │
</span></span><span class=line><span class=cl>                      └─→ ... (最多预测 K=5 个)</span></span></code></pre></div><p><strong>关键特点</strong>:</p><ul><li>每个 Medusa Head 都是一个轻量级的 MLP (如 1-2 层,维度 2048)。</li><li>Head_k 负责预测"未来第 k 个 Token"。</li><li>所有 Head <strong>并行</strong>输出,一次前向传播得到多个候选。</li></ul><h3 id=32-训练自监督蒸馏>3.2 训练：自监督蒸馏<a class=anchor href=#32-%e8%ae%ad%e7%bb%83%e8%87%aa%e7%9b%91%e7%9d%a3%e8%92%b8%e9%a6%8f>#</a></h3><p>Medusa Head 的训练<strong>不需要额外标注</strong>,直接从原模型的生成结果中学习。</p><p><strong>训练数据构造</strong>:
给定序列 <code>[x1, x2, x3, x4, x5]</code>,对于位置 <code>x3</code>:</p><ul><li>Head_1 的标签 = <code>x4</code></li><li>Head_2 的标签 = <code>x5</code></li><li>Head_3 的标签 = <code>x6</code> (如果存在)</li></ul><p><strong>损失函数</strong>:
$$
\mathcal{L} = \sum_{k=1}^{K} \text{CE}(\text{Head}<em>k(h_t), x</em>{t+k})
$$</p><p>其中 $h_t$ 是位置 $t$ 的隐藏状态,$x_{t+k}$ 是未来第 $k$ 个 Token。</p><p><strong>训练流程</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MedusaHead</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_states</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        hidden_states: [batch, seq, hidden]
</span></span></span><span class=line><span class=cl><span class=s2>        返回: list of [batch, seq, vocab_size]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span><span class=n>head</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练伪代码</span>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;Qwen/Qwen2-7B&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>medusa</span> <span class=o>=</span> <span class=n>MedusaHead</span><span class=p>(</span><span class=n>hidden_size</span><span class=o>=</span><span class=mi>3584</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=mi>151936</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 冻结 base_model,只训练 Medusa Head</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>base_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ... 标准的监督学习训练循环 ...</span></span></span></code></pre></div><p><strong>训练成本</strong>:</p><ul><li>数据量: 约 10-50M Tokens (远小于预训练)。</li><li>时间: 单卡 A100 训练 1-2 天。</li><li>显存: 与原模型推理相当 (因为 base_model 冻结)。</li></ul><h3 id=33-tree-attention-优化>3.3 Tree Attention 优化<a class=anchor href=#33-tree-attention-%e4%bc%98%e5%8c%96>#</a></h3><p>Medusa 的核心挑战:<strong>如何高效地验证多个候选路径</strong>?</p><p>假设每个 Head 输出 Top-2 候选,K=3 个 Head,则可能的路径数 = $2^3 = 8$ 条。</p><p><strong>朴素方案</strong>: 逐条验证 → 8 次前向传播 → <strong>没有加速</strong>。</p><p><strong>Tree Attention</strong>:
将所有候选路径组织成一棵树,用<strong>一次</strong>前向传播并行验证。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>        Root (当前Token)
</span></span><span class=line><span class=cl>        /            \
</span></span><span class=line><span class=cl>    Cand_1_A      Cand_1_B  (Head 1 的 Top-2)
</span></span><span class=line><span class=cl>    /    \         /    \
</span></span><span class=line><span class=cl>Cand_2_A Cand_2_B ...   ... (Head 2 的候选)</span></span></code></pre></div><p><strong>实现</strong>:
使用特殊的 Attention Mask,让每个节点只能看到其祖先:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 构造 Tree Attention Mask</span>
</span></span><span class=line><span class=cl><span class=c1># 示例: 8 个候选路径</span>
</span></span><span class=line><span class=cl><span class=c1>#   Mask[i, j] = 1 表示 Token i 可以 attend to Token j</span>
</span></span><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>  <span class=c1># Root</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>  <span class=c1># Path 1-A</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>  <span class=c1># Path 1-B</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>  <span class=c1># Path 1-A -&gt; 2-A</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl><span class=p>])</span></span></span></code></pre></div><p><strong>加速效果</strong>:</p><ul><li>开源榜单 (MT-Bench): <strong>2.2-2.8x</strong> 加速。</li><li>代码生成 (HumanEval): <strong>1.5-2x</strong> 加速 (因为代码的可预测性更强)。</li></ul><hr><h2 id=4-lookahead-decoding前瞻解码>4. Lookahead Decoding：前瞻解码<a class=anchor href=#4-lookahead-decoding%e5%89%8d%e7%9e%bb%e8%a7%a3%e7%a0%81>#</a></h2><blockquote class=book-hint><p><strong>论文</strong>: <em>Break the Sequential Dependency of LLM Inference Using Lookahead Decoding</em> (ICLR 2024)</p></blockquote><h3 id=41-n-gram-缓存原理>4.1 N-gram 缓存原理<a class=anchor href=#41-n-gram-%e7%bc%93%e5%ad%98%e5%8e%9f%e7%90%86>#</a></h3><p>Lookahead Decoding 利用一个观察:<strong>LLM 的输出存在大量重复模式</strong>。</p><p>例如在代码生成中:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 之后很可能再次出现</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>j</span><span class=p>)</span></span></span></code></pre></div><p>如果我们缓存了 <code>"for i in range(10):\n print(i)"</code> 的生成结果,那么生成 <code>for j</code> 时可以直接复用部分计算。</p><p><strong>核心数据结构: N-gram Cache</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ngram_cache</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;for&#34;</span><span class=p>,</span> <span class=s2>&#34;i&#34;</span><span class=p>,</span> <span class=s2>&#34;in&#34;</span><span class=p>):</span> <span class=s2>&#34;range&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;range&#34;</span><span class=p>,</span> <span class=s2>&#34;(&#34;</span><span class=p>,</span> <span class=s2>&#34;10&#34;</span><span class=p>):</span> <span class=s2>&#34;)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><h3 id=42-jacobi-迭代并行化>4.2 Jacobi 迭代并行化<a class=anchor href=#42-jacobi-%e8%bf%ad%e4%bb%a3%e5%b9%b6%e8%a1%8c%e5%8c%96>#</a></h3><p>Lookahead 的第二个技巧:<strong>Jacobi Decoding</strong>。</p><p>传统解码:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x_1 = f(x_0)
</span></span><span class=line><span class=cl>x_2 = f(x_1)
</span></span><span class=line><span class=cl>x_3 = f(x_2)  # 顺序依赖</span></span></code></pre></div><p>Jacobi 迭代:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl># 并行猜测
</span></span><span class=line><span class=cl>x&#39;_1, x&#39;_2, x&#39;_3 = 随机初始化
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># 迭代优化
</span></span><span class=line><span class=cl>for iter in range(T):
</span></span><span class=line><span class=cl>    x&#39;_1 = f(x_0)         # 基于真实前缀
</span></span><span class=line><span class=cl>    x&#39;_2 = f(x&#39;_1)        # 基于猜测
</span></span><span class=line><span class=cl>    x&#39;_3 = f(x&#39;_2)
</span></span><span class=line><span class=cl>    # 如果收敛 → 提前退出</span></span></code></pre></div><p><strong>关键洞察</strong>: 如果猜测合理,Jacobi 迭代通常 2-3 步即可收敛,而不是线性的 N 步。</p><h3 id=43-适用场景分析>4.3 适用场景分析<a class=anchor href=#43-%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af%e5%88%86%e6%9e%90>#</a></h3><p>Lookahead Decoding 的加速效果<strong>高度依赖任务特性</strong>:</p><table><thead><tr><th>任务类型</th><th>加速效果</th><th>原因</th></tr></thead><tbody><tr><td>代码生成</td><td><strong>2-3x</strong></td><td>高度结构化,重复模式多</td></tr><tr><td>数学推导</td><td><strong>1.8-2.5x</strong></td><td>公式有固定格式</td></tr><tr><td>闲聊对话</td><td><strong>1.2-1.5x</strong></td><td>随机性强,难以预测</td></tr><tr><td>翻译</td><td><strong>1.5-2x</strong></td><td>句式有规律</td></tr></tbody></table><p><strong>生产环境启用 (SGLang)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -m sglang.launch_server <span class=se>\
</span></span></span><span class=line><span class=cl>    --model Qwen/Qwen2-7B-Instruct <span class=se>\
</span></span></span><span class=line><span class=cl>    --enable-lookahead <span class=se>\
</span></span></span><span class=line><span class=cl>    --lookahead-window <span class=m>4</span></span></span></code></pre></div><hr><h2 id=5-其他前沿技术>5. 其他前沿技术<a class=anchor href=#5-%e5%85%b6%e4%bb%96%e5%89%8d%e6%b2%bf%e6%8a%80%e6%9c%af>#</a></h2><h3 id=51-eagle基于特征的推测>5.1 Eagle：基于特征的推测<a class=anchor href=#51-eagle%e5%9f%ba%e4%ba%8e%e7%89%b9%e5%be%81%e7%9a%84%e6%8e%a8%e6%b5%8b>#</a></h3><blockquote class=book-hint><p><strong>论文</strong>: <em>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</em> (2024)</p></blockquote><p><strong>创新点</strong>:
传统 Speculative Decoding 在 Token 空间做推测。EAGLE 在<strong>隐藏状态 (Feature)</strong> 空间做推测。</p><p><strong>优势</strong>:</p><ul><li>隐藏状态的维度 (如 4096) 远小于词表 (如 100k)。</li><li>特征空间更平滑,更容易预测。</li></ul><p><strong>架构</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Draft Model: 轻量级 Transformer
</span></span><span class=line><span class=cl>输入: 目标模型的最后一层隐藏状态 h_t
</span></span><span class=line><span class=cl>输出: 预测未来的隐藏状态 h_{t+1}, h_{t+2}, ...</span></span></code></pre></div><p><strong>加速效果</strong>: 比传统 Speculative Decoding 再提升 <strong>20-30%</strong>。</p><h3 id=52-cascade-speculation层级推测>5.2 Cascade Speculation：层级推测<a class=anchor href=#52-cascade-speculation%e5%b1%82%e7%ba%a7%e6%8e%a8%e6%b5%8b>#</a></h3><p><strong>核心思想</strong>: 不同的 Token 难度不同。</p><ul><li>简单 Token (如标点、连词): 用极小模型 (0.1B) 生成。</li><li>中等 Token (如常见名词): 用小模型 (0.5B) 生成。</li><li>困难 Token (如专业术语): 用大模型 (7B) 生成。</li></ul><p><strong>实现</strong>:
构建一个"模型金字塔",从小到大逐级验证。</p><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><table><thead><tr><th>方法</th><th>核心思想</th><th>加速比</th><th>是否需要额外训练</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>Speculative Decoding</strong></td><td>小模型草稿 + 大模型验证</td><td>2-3x</td><td>否</td><td>通用</td></tr><tr><td><strong>Medusa</strong></td><td>多头并行预测</td><td>2-2.8x</td><td>是 (轻量)</td><td>通用</td></tr><tr><td><strong>Lookahead</strong></td><td>N-gram 缓存 + Jacobi 迭代</td><td>1.5-3x</td><td>否</td><td>结构化任务</td></tr><tr><td><strong>Eagle</strong></td><td>特征空间推测</td><td>2.5-3.5x</td><td>是 (中等)</td><td>通用</td></tr></tbody></table><p><strong>工程建议</strong>:</p><ol><li><strong>快速试验</strong>: 先用 Speculative Decoding (vLLM 原生支持,无需训练)。</li><li><strong>极致性能</strong>: 训练 Medusa Head (1-2 天成本,长期收益高)。</li><li><strong>特定任务</strong>: 代码/数学任务开启 Lookahead。</li></ol><p><strong>终极组合</strong>: Speculative Decoding + Medusa</p><ul><li>草稿模型用 Medusa-enhanced 0.5B。</li><li>目标模型用标准 7B。</li><li>理论加速: <strong>3-5x</strong>。</li></ul><p><strong>下一章预告</strong>: 第4章 - 推理模型专题 (DeepSeek-R1 / OpenAI o1)</p><p>在下一章,我们将探讨<strong>慢推理</strong>(Slow Inference)——如何通过增加推理时计算(Test-Time Compute)来换取更高的准确性,这与本章的加速技术形成互补。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第2章 新型架构探索</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/ class="flex align-center"><span>第4章 推理模型专题</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-自回归解码的性能瓶颈>1. 自回归解码的性能瓶颈</a><ul><li><a href=#11-为什么-transformer-推理这么慢>1.1 为什么 Transformer 推理这么慢?</a></li><li><a href=#12-batch-size1-的gpu利用率灾难>1.2 Batch Size=1 的GPU利用率灾难</a></li></ul></li><li><a href=#2-投机解码-speculative-decoding>2. 投机解码 (Speculative Decoding)</a><ul><li><a href=#21-核心思想草稿模型--并行验证>2.1 核心思想：草稿模型 + 并行验证</a></li><li><a href=#22-数学原理无损加速的保证>2.2 数学原理：无损加速的保证</a></li><li><a href=#23-实战用-qwen2-05b-加速-qwen2-7b>2.3 实战：用 Qwen2-0.5B 加速 Qwen2-7B</a></li></ul></li><li><a href=#3-medusa多头并行预测>3. Medusa：多头并行预测</a><ul><li><a href=#31-架构在-lm-head-之上增加多个预测头>3.1 架构：在 LM Head 之上增加多个预测头</a></li><li><a href=#32-训练自监督蒸馏>3.2 训练：自监督蒸馏</a></li><li><a href=#33-tree-attention-优化>3.3 Tree Attention 优化</a></li></ul></li><li><a href=#4-lookahead-decoding前瞻解码>4. Lookahead Decoding：前瞻解码</a><ul><li><a href=#41-n-gram-缓存原理>4.1 N-gram 缓存原理</a></li><li><a href=#42-jacobi-迭代并行化>4.2 Jacobi 迭代并行化</a></li><li><a href=#43-适用场景分析>4.3 适用场景分析</a></li></ul></li><li><a href=#5-其他前沿技术>5. 其他前沿技术</a><ul><li><a href=#51-eagle基于特征的推测>5.1 Eagle：基于特征的推测</a></li><li><a href=#52-cascade-speculation层级推测>5.2 Cascade Speculation：层级推测</a></li></ul></li><li><a href=#本章小结>本章小结</a></li></ul></nav></div></aside></main></body></html>