<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第1章：提示工程与上下文学习 (Prompt Engineering & ICL)# “In-Context Learning is meta-learning without gradient descent.” —— 上下文学习本质上是一种无需梯度更新的元学习。本章将深入探讨如何在不更新模型参数的情况下，通过提示工程（Prompt Engineering）和上下文学习（In-Context Learning）激发大模型的潜能，构建复杂的应用系统。
目录# 第一节：提示工程最佳实践 1.1 结构化提示词 (Structured Prompt) 1.2 角色与约束 (Role & Constraints) 1.3 输出控制 (Output Format) 第二节：上下文学习 (In-Context Learning) 2.1 Few-Shot Learning 原理 2.2 动态示例选择 (Dynamic Few-Shot) 2.3 实战：构建 Few-Shot 文本分类器 第三节：思维链推理 (Chain-of-Thought) 3.1 Zero-Shot CoT 3.2 Manual CoT 3.3 Least-to-Most Prompting 第四节：RAG 系统设计模式预览 4.1 为什么需要 RAG？ 4.2 基础 RAG 流程 4.3 模块化 RAG 架构 第五节：实战：从零构建智能对话系统 5.1 系统架构设计 5.2 核心 Prompt 编排 5.3 完整代码实现 第六节：进阶应用：SetFit 与 语义聚类 6.1 SetFit：少样本分类微调 6.2 BERTopic：语义主题建模 本章小结 思考练习 参考资料 第一节：提示工程最佳实践# 提示工程（Prompt Engineering）并非玄学，而是与模型沟通的编程语言。SOTA 的提示词设计通常遵循清晰的结构化原则。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 提示工程与上下文学习"><meta property="og:description" content="第1章：提示工程与上下文学习 (Prompt Engineering & ICL)# “In-Context Learning is meta-learning without gradient descent.” —— 上下文学习本质上是一种无需梯度更新的元学习。本章将深入探讨如何在不更新模型参数的情况下，通过提示工程（Prompt Engineering）和上下文学习（In-Context Learning）激发大模型的潜能，构建复杂的应用系统。
目录# 第一节：提示工程最佳实践 1.1 结构化提示词 (Structured Prompt) 1.2 角色与约束 (Role & Constraints) 1.3 输出控制 (Output Format) 第二节：上下文学习 (In-Context Learning) 2.1 Few-Shot Learning 原理 2.2 动态示例选择 (Dynamic Few-Shot) 2.3 实战：构建 Few-Shot 文本分类器 第三节：思维链推理 (Chain-of-Thought) 3.1 Zero-Shot CoT 3.2 Manual CoT 3.3 Least-to-Most Prompting 第四节：RAG 系统设计模式预览 4.1 为什么需要 RAG？ 4.2 基础 RAG 流程 4.3 模块化 RAG 架构 第五节：实战：从零构建智能对话系统 5.1 系统架构设计 5.2 核心 Prompt 编排 5.3 完整代码实现 第六节：进阶应用：SetFit 与 语义聚类 6.1 SetFit：少样本分类微调 6.2 BERTopic：语义主题建模 本章小结 思考练习 参考资料 第一节：提示工程最佳实践# 提示工程（Prompt Engineering）并非玄学，而是与模型沟通的编程语言。SOTA 的提示词设计通常遵循清晰的结构化原则。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 提示工程与上下文学习"><meta itemprop=description content="第1章：提示工程与上下文学习 (Prompt Engineering & ICL)# “In-Context Learning is meta-learning without gradient descent.” —— 上下文学习本质上是一种无需梯度更新的元学习。本章将深入探讨如何在不更新模型参数的情况下，通过提示工程（Prompt Engineering）和上下文学习（In-Context Learning）激发大模型的潜能，构建复杂的应用系统。
目录# 第一节：提示工程最佳实践 1.1 结构化提示词 (Structured Prompt) 1.2 角色与约束 (Role & Constraints) 1.3 输出控制 (Output Format) 第二节：上下文学习 (In-Context Learning) 2.1 Few-Shot Learning 原理 2.2 动态示例选择 (Dynamic Few-Shot) 2.3 实战：构建 Few-Shot 文本分类器 第三节：思维链推理 (Chain-of-Thought) 3.1 Zero-Shot CoT 3.2 Manual CoT 3.3 Least-to-Most Prompting 第四节：RAG 系统设计模式预览 4.1 为什么需要 RAG？ 4.2 基础 RAG 流程 4.3 模块化 RAG 架构 第五节：实战：从零构建智能对话系统 5.1 系统架构设计 5.2 核心 Prompt 编排 5.3 完整代码实现 第六节：进阶应用：SetFit 与 语义聚类 6.1 SetFit：少样本分类微调 6.2 BERTopic：语义主题建模 本章小结 思考练习 参考资料 第一节：提示工程最佳实践# 提示工程（Prompt Engineering）并非玄学，而是与模型沟通的编程语言。SOTA 的提示词设计通常遵循清晰的结构化原则。"><meta itemprop=wordCount content="988"><title>第1章 提示工程与上下文学习 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle checked>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/ class=active>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 提示工程与上下文学习</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节提示工程最佳实践>第一节：提示工程最佳实践</a><ul><li><a href=#11-结构化提示词-structured-prompt>1.1 结构化提示词 (Structured Prompt)</a></li><li><a href=#12-角色与约束-role--constraints>1.2 角色与约束 (Role & Constraints)</a></li><li><a href=#13-输出控制-output-format>1.3 输出控制 (Output Format)</a></li></ul></li><li><a href=#第二节上下文学习-in-context-learning>第二节：上下文学习 (In-Context Learning)</a><ul><li><a href=#21-few-shot-learning-原理>2.1 Few-Shot Learning 原理</a></li><li><a href=#22-动态示例选择-dynamic-few-shot>2.2 动态示例选择 (Dynamic Few-Shot)</a></li><li><a href=#23-实战构建-few-shot-文本分类器>2.3 实战：构建 Few-Shot 文本分类器</a></li></ul></li><li><a href=#第三节思维链推理-chain-of-thought>第三节：思维链推理 (Chain-of-Thought)</a><ul><li><a href=#31-为什么需要-cot数学视角>3.1 为什么需要 CoT？（数学视角）</a></li><li><a href=#32-经典-cot-模式>3.2 经典 CoT 模式</a><ul><li><a href=#1-zero-shot-cot>(1) Zero-Shot CoT</a></li><li><a href=#2-manual-cot-few-shot>(2) Manual CoT (Few-Shot)</a></li></ul></li><li><a href=#33-cot-的缺陷与改进>3.3 CoT 的缺陷与改进</a></li><li><a href=#34-least-to-most-prompting>3.4 Least-to-Most Prompting</a></li></ul></li><li><a href=#第四节rag-系统设计模式预览>第四节：RAG 系统设计模式预览</a><ul><li><a href=#41-为什么需要-rag>4.1 为什么需要 RAG？</a></li><li><a href=#42-基础-rag-流程与代码>4.2 基础 RAG 流程与代码</a></li><li><a href=#43-模块化-rag-架构>4.3 模块化 RAG 架构</a></li></ul></li><li><a href=#第五节实战从零构建智能对话系统>第五节：实战：从零构建智能对话系统</a><ul><li><a href=#51-系统架构设计>5.1 系统架构设计</a></li><li><a href=#52-核心-prompt-编排>5.2 核心 Prompt 编排</a></li><li><a href=#53-完整代码实现>5.3 完整代码实现</a></li></ul></li><li><a href=#第六节进阶应用setfit-与-语义聚类>第六节：进阶应用：SetFit 与 语义聚类</a><ul><li><a href=#61-setfit少样本分类微调>6.1 SetFit：少样本分类微调</a></li><li><a href=#62-bertopic语义主题建模>6.2 BERTopic：语义主题建模</a></li></ul></li><li><a href=#本章小结>本章小结</a></li><li><a href=#思考练习>思考练习</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第1章提示工程与上下文学习-prompt-engineering--icl>第1章：提示工程与上下文学习 (Prompt Engineering & ICL)<a class=anchor href=#%e7%ac%ac1%e7%ab%a0%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e4%b8%8e%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0-prompt-engineering--icl>#</a></h1><blockquote class=book-hint><p>&ldquo;In-Context Learning is meta-learning without gradient descent.&rdquo; —— 上下文学习本质上是一种无需梯度更新的元学习。本章将深入探讨如何在不更新模型参数的情况下，通过提示工程（Prompt Engineering）和上下文学习（In-Context Learning）激发大模型的潜能，构建复杂的应用系统。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>第一节：提示工程最佳实践</a><ul><li><a href=#11-%e7%bb%93%e6%9e%84%e5%8c%96%e6%8f%90%e7%a4%ba%e8%af%8d-structured-prompt>1.1 结构化提示词 (Structured Prompt)</a></li><li><a href=#12-%e8%a7%92%e8%89%b2%e4%b8%8e%e7%ba%a6%e6%9d%9f-role--constraints>1.2 角色与约束 (Role & Constraints)</a></li><li><a href=#13-%e8%be%93%e5%87%ba%e6%8e%a7%e5%88%b6-output-format>1.3 输出控制 (Output Format)</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%8c%e8%8a%82%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0-in-context-learning>第二节：上下文学习 (In-Context Learning)</a><ul><li><a href=#21-few-shot-learning-%e5%8e%9f%e7%90%86>2.1 Few-Shot Learning 原理</a></li><li><a href=#22-%e5%8a%a8%e6%80%81%e7%a4%ba%e4%be%8b%e9%80%89%e6%8b%a9-dynamic-few-shot>2.2 动态示例选择 (Dynamic Few-Shot)</a></li><li><a href=#23-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba-few-shot-%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e5%99%a8>2.3 实战：构建 Few-Shot 文本分类器</a></li></ul></li><li><a href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%80%9d%e7%bb%b4%e9%93%be%e6%8e%a8%e7%90%86-chain-of-thought>第三节：思维链推理 (Chain-of-Thought)</a><ul><li><a href=#31-zero-shot-cot>3.1 Zero-Shot CoT</a></li><li><a href=#32-manual-cot>3.2 Manual CoT</a></li><li><a href=#33-least-to-most-prompting>3.3 Least-to-Most Prompting</a></li></ul></li><li><a href=#%e7%ac%ac%e5%9b%9b%e8%8a%82rag-%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f%e9%a2%84%e8%a7%88>第四节：RAG 系统设计模式预览</a><ul><li><a href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-rag>4.1 为什么需要 RAG？</a></li><li><a href=#42-%e5%9f%ba%e7%a1%80-rag-%e6%b5%81%e7%a8%8b>4.2 基础 RAG 流程</a></li><li><a href=#43-%e6%a8%a1%e5%9d%97%e5%8c%96-rag-%e6%9e%b6%e6%9e%84>4.3 模块化 RAG 架构</a></li></ul></li><li><a href=#%e7%ac%ac%e4%ba%94%e8%8a%82%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e6%99%ba%e8%83%bd%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f>第五节：实战：从零构建智能对话系统</a><ul><li><a href=#51-%e7%b3%bb%e7%bb%9f%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1>5.1 系统架构设计</a></li><li><a href=#52-%e6%a0%b8%e5%bf%83-prompt-%e7%bc%96%e6%8e%92>5.2 核心 Prompt 编排</a></li><li><a href=#53-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>5.3 完整代码实现</a></li></ul></li><li><a href=#%e7%ac%ac%e5%85%ad%e8%8a%82%e8%bf%9b%e9%98%b6%e5%ba%94%e7%94%a8setfit-%e4%b8%8e-%e8%af%ad%e4%b9%89%e8%81%9a%e7%b1%bb>第六节：进阶应用：SetFit 与 语义聚类</a><ul><li><a href=#61-setfit%e5%b0%91%e6%a0%b7%e6%9c%ac%e5%88%86%e7%b1%bb%e5%be%ae%e8%b0%83>6.1 SetFit：少样本分类微调</a></li><li><a href=#62-bertopic%e8%af%ad%e4%b9%89%e4%b8%bb%e9%a2%98%e5%bb%ba%e6%a8%a1>6.2 BERTopic：语义主题建模</a></li></ul></li><li><a href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>本章小结</a></li><li><a href=#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0>思考练习</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99>参考资料</a></li></ul><hr><h2 id=第一节提示工程最佳实践>第一节：提示工程最佳实践<a class=anchor href=#%e7%ac%ac%e4%b8%80%e8%8a%82%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>#</a></h2><p>提示工程（Prompt Engineering）并非玄学，而是与模型沟通的<strong>编程语言</strong>。SOTA 的提示词设计通常遵循清晰的结构化原则。</p><h3 id=11-结构化提示词-structured-prompt>1.1 结构化提示词 (Structured Prompt)<a class=anchor href=#11-%e7%bb%93%e6%9e%84%e5%8c%96%e6%8f%90%e7%a4%ba%e8%af%8d-structured-prompt>#</a></h3><p>一个优秀的 Prompt 应该像代码一样具备模块化结构，通常包含以下要素：</p><ol><li><strong>Role (角色)</strong>：定义 AI 的身份和能力边界。</li><li><strong>Context (背景)</strong>：提供任务背景信息。</li><li><strong>Instruction (指令)</strong>：清晰、动词导向的任务描述。</li><li><strong>Data (数据)</strong>：输入的数据内容。</li><li><strong>Output Indicator (输出指引)</strong>：期望的输出格式。</li></ol><p><strong>代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>PROMPT_TEMPLATE</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>### Role
</span></span></span><span class=line><span class=cl><span class=s2>你是一位资深的数据分析师，擅长从非结构化文本中提取关键商业洞察。
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Context
</span></span></span><span class=line><span class=cl><span class=s2>我们收到了一批用户关于&#34;智能咖啡机&#34;的产品反馈，需要整理用户的核心痛点。
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Instruction
</span></span></span><span class=line><span class=cl><span class=s2>请分析以下用户评论，提取出：
</span></span></span><span class=line><span class=cl><span class=s2>1. 情感倾向 (Positive/Negative/Neutral)
</span></span></span><span class=line><span class=cl><span class=s2>2. 核心关键词 (最多3个)
</span></span></span><span class=line><span class=cl><span class=s2>3. 问题摘要 (一句话)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Data
</span></span></span><span class=line><span class=cl><span class=s2>用户评论：&#34;</span><span class=si>{user_review}</span><span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Output Format
</span></span></span><span class=line><span class=cl><span class=s2>请仅输出 JSON 格式，不要包含Markdown标记：
</span></span></span><span class=line><span class=cl><span class=s2>{{
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;sentiment&#34;: &#34;...&#34;,
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;keywords&#34;: [&#34;...&#34;, &#34;...&#34;],
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;summary&#34;: &#34;...&#34;
</span></span></span><span class=line><span class=cl><span class=s2>}}
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><h3 id=12-角色与约束-role--constraints>1.2 角色与约束 (Role & Constraints)<a class=anchor href=#12-%e8%a7%92%e8%89%b2%e4%b8%8e%e7%ba%a6%e6%9d%9f-role--constraints>#</a></h3><p><strong>角色设定</strong>不仅是“扮演游戏”，它实际上是在潜在空间（Latent Space）中锁定模型的生成模式。</p><ul><li><strong>弱角色</strong>：&ldquo;帮我写个代码。&rdquo;</li><li><strong>强角色</strong>：&ldquo;你是一位 Google L5 级别的 Python 工程师，遵循 PEP8 规范，代码需包含类型提示（Type Hints）和 Google 风格的 Docstring。&rdquo;</li></ul><p><strong>约束技巧</strong>：</p><ul><li><strong>负向约束</strong> (Negative Constraints)：明确告诉模型<strong>不要</strong>做什么（例如：&ldquo;不要使用礼貌用语&rdquo;、&ldquo;不要解释代码&rdquo;）。</li><li><strong>长度约束</strong>：指定字数或段落数。</li></ul><h3 id=13-输出控制-output-format>1.3 输出控制 (Output Format)<a class=anchor href=#13-%e8%be%93%e5%87%ba%e6%8e%a7%e5%88%b6-output-format>#</a></h3><p>在工程化应用中，稳定的输出格式至关重要。</p><ul><li><strong>JSON Mode</strong>：现代 LLM（如 GPT-4o, DeepSeek-V3）通常支持 <code>response_format={"type": "json_object"}</code>。</li><li><strong>Structure Prompting</strong>：在 Prompt 末尾给出明确的 Schema 定义。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用 Pydantic 定义输出结构 (配合 Instructor 库)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic</span> <span class=kn>import</span> <span class=n>BaseModel</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AnalysisResult</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sentiment</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>keywords</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>summary</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 这种方式能确保 100% 的格式稳定性</span></span></span></code></pre></div><hr><h2 id=第二节上下文学习-in-context-learning>第二节：上下文学习 (In-Context Learning)<a class=anchor href=#%e7%ac%ac%e4%ba%8c%e8%8a%82%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0-in-context-learning>#</a></h2><p><strong>核心问题</strong>：如何不重新训练模型，就能让它理解复杂任务？
<strong>答案</strong>：In-Context Learning (ICL)。利用模型强大的短期记忆（Context Window），直接在 Prompt 中提供示例。</p><h3 id=21-few-shot-learning-原理>2.1 Few-Shot Learning 原理<a class=anchor href=#21-few-shot-learning-%e5%8e%9f%e7%90%86>#</a></h3><p>Few-Shot Learning（少样本学习）是指在 Prompt 中提供 <strong>Input-Output 对</strong> 作为示例。</p><p><strong>为什么有效？</strong>
模型通过注意力机制（Self-Attention）&ldquo;读取"这些示例，捕捉输入与输出之间的映射关系，并在推理时模仿这种模式。这本质上是一种<strong>无需梯度更新的元学习</strong>。</p><p><strong>示例对比</strong>：</p><ul><li><strong>Zero-Shot</strong>:<div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>这句评论是正面的还是负面的？
</span></span><span class=line><span class=cl>&#34;快递慢得像乌龟。&#34;</span></span></code></pre></div></li><li><strong>Few-Shot</strong>:<div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>判断评论情感：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输入：&#34;屏幕清晰度很高。&#34;
</span></span><span class=line><span class=cl>输出：正面
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输入：&#34;电池用了半天就没电了。&#34;
</span></span><span class=line><span class=cl>输出：负面
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>输入：&#34;快递慢得像乌龟。&#34;
</span></span><span class=line><span class=cl>输出：</span></span></code></pre></div></li></ul><h3 id=22-动态示例选择-dynamic-few-shot>2.2 动态示例选择 (Dynamic Few-Shot)<a class=anchor href=#22-%e5%8a%a8%e6%80%81%e7%a4%ba%e4%be%8b%e9%80%89%e6%8b%a9-dynamic-few-shot>#</a></h3><p>当任务复杂且示例池很大时，固定示例效果不佳。最佳实践是<strong>根据 Query 动态检索最相似的示例</strong>。</p><p><strong>架构设计</strong>：</p><ol><li>建立一个 <strong>示例库 (Example Store)</strong> (Input-Output Pairs)。</li><li>为示例库中的 Input 计算 Embeddings，存入向量库。</li><li>用户输入 Query 时，先检索 Top-K 最相似的 Input 示例。</li><li>将这 K 个示例组装进 Prompt。</li></ol><p><strong>代码逻辑</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span><span class=p>,</span> <span class=n>util</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embedder</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;BAAI/bge-large-zh-v1.5&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例库: 输入-输出对</span>
</span></span><span class=line><span class=cl><span class=n>example_corpus</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;屏幕清晰度很高。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;电池用了半天就没电了。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;物流速度超快!&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>example_labels</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;正面&#34;</span><span class=p>,</span> <span class=s2>&#34;负面&#34;</span><span class=p>,</span> <span class=s2>&#34;正面&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>example_embeddings</span> <span class=o>=</span> <span class=n>embedder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>example_corpus</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_dynamic_prompt</span><span class=p>(</span><span class=n>query</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>query_emb</span> <span class=o>=</span> <span class=n>embedder</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 检索最相似的 3 个示例</span>
</span></span><span class=line><span class=cl>    <span class=n>hits</span> <span class=o>=</span> <span class=n>util</span><span class=o>.</span><span class=n>semantic_search</span><span class=p>(</span><span class=n>query_emb</span><span class=p>,</span> <span class=n>example_embeddings</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;参考以下相似案例进行回答：</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>hit</span> <span class=ow>in</span> <span class=n>hits</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=n>idx</span> <span class=o>=</span> <span class=n>hit</span><span class=p>[</span><span class=s1>&#39;corpus_id&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;示例输入：</span><span class=si>{</span><span class=n>example_corpus</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>示例输出：</span><span class=si>{</span><span class=n>example_labels</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;当前输入：</span><span class=si>{</span><span class=n>query</span><span class=si>}</span><span class=se>\n</span><span class=s2>输出：&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>prompt</span></span></span></code></pre></div><h3 id=23-实战构建-few-shot-文本分类器>2.3 实战：构建 Few-Shot 文本分类器<a class=anchor href=#23-%e5%ae%9e%e6%88%98%e6%9e%84%e5%bb%ba-few-shot-%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e5%99%a8>#</a></h3><p>利用 ICL，我们可以快速构建一个高精度的意图分类器，无需任何训练。</p><p><strong>(此部分整合了原章节的分类实战内容，但侧重于 Prompt 实现)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 核心 Prompt 模板</span>
</span></span><span class=line><span class=cl><span class=n>CLASSIFICATION_PROMPT</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>你是一个智能客服意图识别助手。请参考以下示例，确定用户问题的类别。
</span></span></span><span class=line><span class=cl><span class=s2>类别列表：[账号问题, 支付失败, 物流查询, 售后退换]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>示例 1:
</span></span></span><span class=line><span class=cl><span class=s2>用户: &#34;怎么还没发货？都三天了&#34;
</span></span></span><span class=line><span class=cl><span class=s2>类别: 物流查询
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>示例 2:
</span></span></span><span class=line><span class=cl><span class=s2>用户: &#34;充值成功了但是余额没变&#34;
</span></span></span><span class=line><span class=cl><span class=s2>类别: 支付失败
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>示例 3:
</span></span></span><span class=line><span class=cl><span class=s2>用户: &#34;我想修改绑定的手机号&#34;
</span></span></span><span class=line><span class=cl><span class=s2>类别: 账号问题
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>用户: &#34;</span><span class=si>{query}</span><span class=s2>&#34;
</span></span></span><span class=line><span class=cl><span class=s2>类别:
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><hr><h2 id=第三节思维链推理-chain-of-thought>第三节：思维链推理 (Chain-of-Thought)<a class=anchor href=#%e7%ac%ac%e4%b8%89%e8%8a%82%e6%80%9d%e7%bb%b4%e9%93%be%e6%8e%a8%e7%90%86-chain-of-thought>#</a></h2><h3 id=31-为什么需要-cot数学视角>3.1 为什么需要 CoT？（数学视角）<a class=anchor href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-cot%e6%95%b0%e5%ad%a6%e8%a7%86%e8%a7%92>#</a></h3><p>在直觉上，<strong>CoT (Chain-of-Thought)</strong> 是让模型"慢下来思考&rdquo;。但在数学上，它的本质是引入了<strong>隐变量 (Latent Variable)</strong> 来解构复杂概率分布。</p><p><strong>1. 贝叶斯视角</strong>：
对于复杂问题（如数学题），直接建模 $P(answer|question)$ 是非常困难的，因为输入空间到输出空间的映射极其非线性。
CoT 引入了中间推理步骤 $z$ (rationale)：
$$ P(a|q) = \sum_{z} P(a|z, q) \cdot P(z|q) $$
其中：</p><ul><li>$P(z|q)$：给定问题，生成推理步骤的概率（这一步往往更符合自然语言逻辑，容易建模）。</li><li>$P(a|z, q)$：给定推理步骤，生成答案的概率（这一步通常是确定性的）。</li></ul><p><strong>2. 计算复杂度视角</strong>：
Transformer 模型的计算深度（层数）是固定的。对于需要 $N$ 步逻辑推理的问题，如果只输出一个 token 的答案，模型必须在有限的层数内完成所有计算。
CoT 允许模型生成 $T$ 个 token 的推理过程，这相当于将<strong>计算时间线性扩展</strong>，用更多的 FLOPs (浮点运算) 换取更高的准确率。</p><h3 id=32-经典-cot-模式>3.2 经典 CoT 模式<a class=anchor href=#32-%e7%bb%8f%e5%85%b8-cot-%e6%a8%a1%e5%bc%8f>#</a></h3><h4 id=1-zero-shot-cot>(1) Zero-Shot CoT<a class=anchor href=#1-zero-shot-cot>#</a></h4><p>最著名的"魔法咒语"：</p><blockquote class=book-hint><p><strong>&ldquo;Let&rsquo;s think step by step.&rdquo; (让我们一步步思考)</strong></p></blockquote><p>这句话会显著改变模型的生成概率分布，使其倾向于输出逻辑连接词（如 &ldquo;First&rdquo;, &ldquo;Therefore&rdquo;），从而触发内部的推理电路。</p><h4 id=2-manual-cot-few-shot>(2) Manual CoT (Few-Shot)<a class=anchor href=#2-manual-cot-few-shot>#</a></h4><p>在 Few-Shot 示例中，显式写出推理过程。</p><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>问题：Roger 有 5 个网球，他又买了两罐网球，每罐有 3 个。他现在一共有多少个网球？
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>思考过程：
</span></span><span class=line><span class=cl>1. Roger 起始有 5 个球。
</span></span><span class=line><span class=cl>2. 2 罐网球，每罐 3 个，共 2 * 3 = 6 个球。
</span></span><span class=line><span class=cl>3. 总数为 5 + 6 = 11 个。
</span></span><span class=line><span class=cl>答案：11</span></span></code></pre></div><h3 id=33-cot-的缺陷与改进>3.3 CoT 的缺陷与改进<a class=anchor href=#33-cot-%e7%9a%84%e7%bc%ba%e9%99%b7%e4%b8%8e%e6%94%b9%e8%bf%9b>#</a></h3><p><strong>1. 缺陷 A：错误级联 (Error Cascading)</strong>
由于 $P(a|z)$ 强依赖于 $z$，如果推理链中的某一步 $z_t$ 出现幻觉，后续的所有推理都会基于这个错误前提。</p><p><strong>2. 缺陷 B：事后合理化 (Post-hoc Rationalization)</strong>
这是一种更隐蔽的缺陷，揭示了<strong>理论理想与实践现实的差距</strong>。</p><ul><li><strong>理论模型</strong> (贝叶斯视角)：CoT 应该遵循 $P(a|z,q) \cdot P(z|q)$ 的链式推理，即先生成推理步骤 $z$，再基于 $z$ 推导答案 $a$。</li><li><strong>实践现实</strong> (Post-hoc)：模型可能因为训练数据中存在某种"捷径"，直接通过 $P(a|q)$ 先"蒙"出了正确答案，然后为了满足 CoT 格式的要求，事后编造一段看似合理的推理过程。</li></ul><p><strong>现象</strong>：推理过程 $z$ 充满错误逻辑，但最终答案 $a$ 竟然是对的。此时 CoT 的数学分解失效，变成了"马后炮"。这说明贝叶斯分解是 CoT 的理想工作机制，但在实际应用中，模型并不总是严格遵循这一机制。</p><p><strong>3. 改进方案：Self-Consistency (自洽性)</strong>
利用温度采样（Temperature > 0）生成 $K$ 条不同的推理路径，然后对最终答案进行<strong>投票 (Majority Vote)</strong>。
$$ \hat{a} = \arg\max_{a} \sum_{k=1}^K \mathbb{I}(a_k = a) $$
这利用了<strong>大数定律</strong>消除了单条推理路径的随机噪声。</p><p><strong>3. 进阶结构：Tree of Thoughts (ToT)</strong>
将线性的 CoT 扩展为树状结构，允许模型在推理过程中：</p><ul><li><strong>分支</strong>：探索多种可能性。</li><li><strong>回溯</strong>：如果当前路径不可行，退回上一步。</li><li><strong>评估</strong>：对每一步的状态进行自我打分。</li></ul><h3 id=34-least-to-most-prompting>3.4 Least-to-Most Prompting<a class=anchor href=#34-least-to-most-prompting>#</a></h3><p>对于极度复杂的问题，采用**“拆解-解决”**策略：</p><ol><li><strong>Decomposition</strong>: 先让模型把大问题拆解为子问题列表。</li><li><strong>Sequential Solving</strong>: 逐个解决子问题，把上一步的答案作为下一步的输入。</li></ol><hr><p><strong>从推理到检索：CoT 的局限性</strong></p><p>CoT 强化了模型的推理能力，但它依然无法解决一个根本问题：<strong>知识的边界</strong>。无论推理链多么完善，如果模型的参数中没有存储相关知识（例如最新的市场数据、企业内部文档），它只能基于"幻觉"进行推理。</p><p>这就引出了下一个核心问题：<strong>如何让模型访问外部知识？</strong> 这正是 RAG (Retrieval-Augmented Generation) 的使命——将模型的生成能力与外部知识库的检索能力相结合。</p><h2 id=第四节rag-系统设计模式预览>第四节：RAG 系统设计模式预览<a class=anchor href=#%e7%ac%ac%e5%9b%9b%e8%8a%82rag-%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1%e6%a8%a1%e5%bc%8f%e9%a2%84%e8%a7%88>#</a></h2><p>虽然 RAG (Retrieval-Augmented Generation) 也是提示工程的一种延伸（将检索结果作为 Context），但它已发展为独立领域。本节简要预览，详细内容见下一章。</p><h3 id=41-为什么需要-rag>4.1 为什么需要 RAG？<a class=anchor href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81-rag>#</a></h3><ul><li><strong>幻觉 (Hallucination)</strong>：模型会一本正经地胡说八道。</li><li><strong>时效性 (Cutoff Date)</strong>：模型知识有截止日期（如 2023 年）。</li><li><strong>私有数据 (Private Data)</strong>：模型不知道企业内部文档。</li></ul><h3 id=42-基础-rag-流程与代码>4.2 基础 RAG 流程与代码<a class=anchor href=#42-%e5%9f%ba%e7%a1%80-rag-%e6%b5%81%e7%a8%8b%e4%b8%8e%e4%bb%a3%e7%a0%81>#</a></h3><p><code>Query -> Search(Vector DB) -> Context -> Augmented Prompt -> LLM -> Answer</code></p><p><strong>极简代码示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 检索 (Retrieve)</span>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=n>vector_db</span><span class=o>.</span><span class=n>similarity_search</span><span class=p>(</span><span class=s2>&#34;公司Q3营收&#34;</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=n>d</span><span class=o>.</span><span class=n>page_content</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 增强 (Augment)</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;基于以下上下文回答问题：</span><span class=se>\n</span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>问题：公司Q3营收是多少？&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 生成 (Generate)</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}]</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><h3 id=43-模块化-rag-架构>4.3 模块化 RAG 架构<a class=anchor href=#43-%e6%a8%a1%e5%9d%97%e5%8c%96-rag-%e6%9e%b6%e6%9e%84>#</a></h3><ul><li><strong>RRR 模式</strong>：Rewrite (改写问题) -> Retrieve (检索) -> Read (阅读回答)。</li><li><strong>HyDE</strong>：先假设一个答案，用假设答案去检索，再生成真实答案。</li></ul><hr><h2 id=第五节实战从零构建智能对话系统>第五节：实战：从零构建智能对话系统<a class=anchor href=#%e7%ac%ac%e4%ba%94%e8%8a%82%e5%ae%9e%e6%88%98%e4%bb%8e%e9%9b%b6%e6%9e%84%e5%bb%ba%e6%99%ba%e8%83%bd%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f>#</a></h2><p>本节我们将综合运用 Role, Few-Shot, CoT 技术，构建一个<strong>基于文档的智能问答助手</strong>。</p><h3 id=51-系统架构设计>5.1 系统架构设计<a class=anchor href=#51-%e7%b3%bb%e7%bb%9f%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1>#</a></h3><p>系统分为三层：</p><ol><li><strong>输入处理层</strong>：Prompt 优化、意图识别。</li><li><strong>上下文层</strong>：管理对话历史 (Memory)、检索知识库。</li><li><strong>生成层</strong>：调用 LLM API，结构化输出。</li></ol><h3 id=52-核心-prompt-编排>5.2 核心 Prompt 编排<a class=anchor href=#52-%e6%a0%b8%e5%bf%83-prompt-%e7%bc%96%e6%8e%92>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>SYSTEM_PROMPT</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Role:
</span></span></span><span class=line><span class=cl><span class=s2>你是一个专业的金融文档助手。你的任务是依据提供的上下文（Context）回答用户关于财报的问题。
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>Constraints:
</span></span></span><span class=line><span class=cl><span class=s2>1. 只能基于 Context 回答，不要使用你的外部知识。
</span></span></span><span class=line><span class=cl><span class=s2>2. 如果 Context 中没有答案，请直接回答&#34;根据当前文档无法回答&#34;，不要编造。
</span></span></span><span class=line><span class=cl><span class=s2>3. 保持客观、专业，引用 Context 中的数据时要保留 2 位小数。
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>Thinking Process (CoT):
</span></span></span><span class=line><span class=cl><span class=s2>请先分析用户的意图，然后在 Context 中寻找相关段落，最后整合成答案。
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div><h3 id=53-完整代码实现>5.3 完整代码实现<a class=anchor href=#53-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>chat_bot</span><span class=p>(</span><span class=n>user_query</span><span class=p>,</span> <span class=n>context_chunks</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 构建 Context 字符串</span>
</span></span><span class=line><span class=cl>    <span class=n>context_str</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>---</span><span class=se>\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>context_chunks</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 组装 Prompt</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>SYSTEM_PROMPT</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;Context:</span><span class=se>\n</span><span class=si>{</span><span class=n>context_str</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>Question: </span><span class=si>{</span><span class=n>user_query</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 调用 LLM</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>=</span><span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>temperature</span><span class=o>=</span><span class=mf>0.3</span> <span class=c1># 降低随机性，提高事实准确度</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模拟运行</span>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Q3财报显示，公司营收达到 10.52 亿元，同比增长 15.3%。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;净利润为 2.1 亿元，主要得益于云服务业务的扩展。&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>chat_bot</span><span class=p>(</span><span class=s2>&#34;公司Q3营收表现如何？&#34;</span><span class=p>,</span> <span class=n>docs</span><span class=p>))</span></span></span></code></pre></div><hr><h2 id=第六节进阶应用setfit-与-语义聚类>第六节：进阶应用：SetFit 与 语义聚类<a class=anchor href=#%e7%ac%ac%e5%85%ad%e8%8a%82%e8%bf%9b%e9%98%b6%e5%ba%94%e7%94%a8setfit-%e4%b8%8e-%e8%af%ad%e4%b9%89%e8%81%9a%e7%b1%bb>#</a></h2><p>为了弥补传统 Prompt 在<strong>小样本高精度</strong>场景下的不足，我们可以引入更重的工程方案。这是连接 Prompt Engineering 与 Fine-tuning 的桥梁。</p><p><em>(本节保留了原“文本分类”章节的精华内容，作为高级实战案例)</em></p><h3 id=61-setfit少样本分类微调>6.1 SetFit：少样本分类微调<a class=anchor href=#61-setfit%e5%b0%91%e6%a0%b7%e6%9c%ac%e5%88%86%e7%b1%bb%e5%be%ae%e8%b0%83>#</a></h3><p><strong>SetFit (Sentence Transformer Fine-tuning)</strong> 是一种无需大规模标注数据的高效分类框架。</p><ul><li><strong>原理</strong>：先对 Embedding 模型进行对比学习微调（Contrastive Learning），再训练一个分类头（Classification Head）。</li><li><strong>优势</strong>：在只有 8 个样本/类的情况下，性能可媲美全量微调的 BERT。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>setfit</span> <span class=kn>import</span> <span class=n>SetFitModel</span><span class=p>,</span> <span class=n>SetFitTrainer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载少样本数据 (每类仅需8条)</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;SetFit/emotion&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train[:32]&#34;</span><span class=p>)</span> <span class=c1># 示例</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 初始化模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SetFitModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 训练 (极快, CPU上几分钟)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>SetFitTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_class</span><span class=o>=</span><span class=s2>&#34;CosFaceLoss&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>metric</span><span class=o>=</span><span class=s2>&#34;accuracy&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_iterations</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=c1># 对比学习迭代次数</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 推理</span>
</span></span><span class=line><span class=cl><span class=n>preds</span> <span class=o>=</span> <span class=n>model</span><span class=p>([</span><span class=s2>&#34;This movie is so boring...&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>preds</span><span class=p>)</span></span></span></code></pre></div><h3 id=62-bertopic语义主题建模>6.2 BERTopic：语义主题建模<a class=anchor href=#62-bertopic%e8%af%ad%e4%b9%89%e4%b8%bb%e9%a2%98%e5%bb%ba%e6%a8%a1>#</a></h3><p>当没有标签时，如何理解大规模文本数据？Prompt Engineering 很难处理全量数据聚类，这时需要 <strong>BERTopic</strong>。</p><p><strong>代码实战</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bertopic</span> <span class=kn>import</span> <span class=n>BERTopic</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>fetch_20newsgroups</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载数据</span>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=n>fetch_20newsgroups</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=s1>&#39;all&#39;</span><span class=p>)[</span><span class=s1>&#39;data&#39;</span><span class=p>][:</span><span class=mi>1000</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 训练模型 (Embed -&gt; UMAP -&gt; HDBSCAN -&gt; c-TF-IDF)</span>
</span></span><span class=line><span class=cl><span class=n>topic_model</span> <span class=o>=</span> <span class=n>BERTopic</span><span class=p>(</span><span class=n>language</span><span class=o>=</span><span class=s2>&#34;english&#34;</span><span class=p>,</span> <span class=n>calculate_probabilities</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>topics</span><span class=p>,</span> <span class=n>probs</span> <span class=o>=</span> <span class=n>topic_model</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 查看主题</span>
</span></span><span class=line><span class=cl><span class=n>topic_model</span><span class=o>.</span><span class=n>get_topic_info</span><span class=p>()</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: Topic 0: [game, team, ball...], Topic 1: [key, chip, encryption...]</span></span></span></code></pre></div><p><strong>对比</strong>:</p><ul><li><strong>Prompt</strong>: 适合处理单条数据的精细理解。</li><li><strong>BERTopic</strong>: 适合对百万级数据进行宏观鸟瞰。</li></ul><hr><h2 id=本章小结>本章小结<a class=anchor href=#%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>本章是应用开发的起点，我们完成了从<strong>指令设计</strong>到<strong>系统构建</strong>的跨越：</p><ol><li><strong>Prompt Engineering</strong>：不只是写句子，而是<strong>结构化编程</strong>（Role, Context, Constraints）。</li><li><strong>In-Context Learning</strong>：利用<strong>Few-Shot</strong>和<strong>动态示例检索</strong>，无需训练即可通过图灵测试。</li><li><strong>CoT</strong>：通过显式思维链，解锁了模型的复杂推理能力。</li><li><strong>实战落地</strong>：通过<strong>SetFit</strong>等工具，我们将 Prompt 的思想延伸到了轻量级训练领域。</li></ol><p><strong>核心心法</strong>：</p><blockquote class=book-hint><p>&ldquo;不要试图让模型&rsquo;猜&rsquo;你的意图，要像写代码一样，给它清晰、明确、结构化的指令。&rdquo;</p></blockquote><hr><h2 id=思考练习>思考练习<a class=anchor href=#%e6%80%9d%e8%80%83%e7%bb%83%e4%b9%a0>#</a></h2><ol><li><strong>Prompt 逆向工程</strong>：找一个 ChatGPT 的优秀回答，尝试反推它的 System Prompt 是怎么写的？</li><li><strong>CoT 陷阱</strong>：在什么情况下，使用 CoT 反而会降低模型的效果？（提示：简单任务或知识检索任务）</li><li><strong>Few-Shot 鲁棒性</strong>：如果示例中的标签是错误的（Label Noise），LLM 还能正确分类吗？这说明了什么？</li></ol><hr><h2 id=参考资料>参考资料<a class=anchor href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99>#</a></h2><ol><li><strong>OpenAI Prompt Engineering Guide</strong>: <a href=https://platform.openai.com/docs/guides/prompt-engineering>platform.openai.com/docs/guides/prompt-engineering</a></li><li><strong>Chain-of-Thought Paper</strong>: &ldquo;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&rdquo; (Wei et al., 2022)</li><li><strong>SetFit</strong>: <a href=https://github.com/huggingface/setfit>https://github.com/huggingface/setfit</a></li><li><strong>Lilian Weng Blog</strong>: &ldquo;Prompt Engineering&rdquo; (lilianweng.github.io)</li></ol></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第4章 创建更优的嵌入模型</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/ class="flex align-center"><span>第2章 检索增强生成（RAG）原理</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#第一节提示工程最佳实践>第一节：提示工程最佳实践</a><ul><li><a href=#11-结构化提示词-structured-prompt>1.1 结构化提示词 (Structured Prompt)</a></li><li><a href=#12-角色与约束-role--constraints>1.2 角色与约束 (Role & Constraints)</a></li><li><a href=#13-输出控制-output-format>1.3 输出控制 (Output Format)</a></li></ul></li><li><a href=#第二节上下文学习-in-context-learning>第二节：上下文学习 (In-Context Learning)</a><ul><li><a href=#21-few-shot-learning-原理>2.1 Few-Shot Learning 原理</a></li><li><a href=#22-动态示例选择-dynamic-few-shot>2.2 动态示例选择 (Dynamic Few-Shot)</a></li><li><a href=#23-实战构建-few-shot-文本分类器>2.3 实战：构建 Few-Shot 文本分类器</a></li></ul></li><li><a href=#第三节思维链推理-chain-of-thought>第三节：思维链推理 (Chain-of-Thought)</a><ul><li><a href=#31-为什么需要-cot数学视角>3.1 为什么需要 CoT？（数学视角）</a></li><li><a href=#32-经典-cot-模式>3.2 经典 CoT 模式</a><ul><li><a href=#1-zero-shot-cot>(1) Zero-Shot CoT</a></li><li><a href=#2-manual-cot-few-shot>(2) Manual CoT (Few-Shot)</a></li></ul></li><li><a href=#33-cot-的缺陷与改进>3.3 CoT 的缺陷与改进</a></li><li><a href=#34-least-to-most-prompting>3.4 Least-to-Most Prompting</a></li></ul></li><li><a href=#第四节rag-系统设计模式预览>第四节：RAG 系统设计模式预览</a><ul><li><a href=#41-为什么需要-rag>4.1 为什么需要 RAG？</a></li><li><a href=#42-基础-rag-流程与代码>4.2 基础 RAG 流程与代码</a></li><li><a href=#43-模块化-rag-架构>4.3 模块化 RAG 架构</a></li></ul></li><li><a href=#第五节实战从零构建智能对话系统>第五节：实战：从零构建智能对话系统</a><ul><li><a href=#51-系统架构设计>5.1 系统架构设计</a></li><li><a href=#52-核心-prompt-编排>5.2 核心 Prompt 编排</a></li><li><a href=#53-完整代码实现>5.3 完整代码实现</a></li></ul></li><li><a href=#第六节进阶应用setfit-与-语义聚类>第六节：进阶应用：SetFit 与 语义聚类</a><ul><li><a href=#61-setfit少样本分类微调>6.1 SetFit：少样本分类微调</a></li><li><a href=#62-bertopic语义主题建模>6.2 BERTopic：语义主题建模</a></li></ul></li><li><a href=#本章小结>本章小结</a></li><li><a href=#思考练习>思考练习</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></div></aside></main></body></html>