<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第2章 检索增强生成（RAG）原理# 本章定位: 构建基于外部知识库的增强生成系统
核心内容: RAG标准架构 → Chunking策略 → 检索技术 → 重排序 → 高级RAG变体
前置知识: Part 1 第3章（Embedding）、Part 3 第4章（Embedding模型训练）
目录# 一、RAG：为什么需要外部知识？ 二、RAG 标准架构详解 三、核心技术：Chunking 与 Indexing 四、核心技术：检索 (Retrieval) 五、核心技术：重排序 (Reranking) 六、高级 RAG 变体 七、本章小结 一、RAG：为什么需要外部知识？# 1.1 大模型的知识困境# 问题1：知识过时
# 向GPT-4提问（假设训练数据截止2023年10月） question = "最新的诺贝尔物理学奖获得者是谁？" response = llm.generate(question) # 输出: "我的知识截止到2023年，无法回答..."问题2：私域知识缺失
企业内部文档、财报、技术规范 实时更新的法律法规、医疗指南 个人笔记、代码库 问题3：幻觉（Hallucination）
模型"编造"看似合理但实际错误的信息 在知识密集型任务中尤为严重 1.2 RAG的核心思想# Retrieval-Augmented Generation（检索增强生成）：
传统LLM: 问题 ──> LLM ──> 答案（基于参数化知识） ↓ 可能过时/缺失/幻觉 RAG流程: 问题 ──> 检索器 ──> 相关文档 ↓ LLM + 文档上下文 ──> 答案（有依据）核心优势:
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第2章 检索增强生成（RAG）原理"><meta property="og:description" content='第2章 检索增强生成（RAG）原理# 本章定位: 构建基于外部知识库的增强生成系统
核心内容: RAG标准架构 → Chunking策略 → 检索技术 → 重排序 → 高级RAG变体
前置知识: Part 1 第3章（Embedding）、Part 3 第4章（Embedding模型训练）
目录# 一、RAG：为什么需要外部知识？ 二、RAG 标准架构详解 三、核心技术：Chunking 与 Indexing 四、核心技术：检索 (Retrieval) 五、核心技术：重排序 (Reranking) 六、高级 RAG 变体 七、本章小结 一、RAG：为什么需要外部知识？# 1.1 大模型的知识困境# 问题1：知识过时
# 向GPT-4提问（假设训练数据截止2023年10月） question = "最新的诺贝尔物理学奖获得者是谁？" response = llm.generate(question) # 输出: "我的知识截止到2023年，无法回答..."问题2：私域知识缺失
企业内部文档、财报、技术规范 实时更新的法律法规、医疗指南 个人笔记、代码库 问题3：幻觉（Hallucination）
模型"编造"看似合理但实际错误的信息 在知识密集型任务中尤为严重 1.2 RAG的核心思想# Retrieval-Augmented Generation（检索增强生成）：
传统LLM: 问题 ──> LLM ──> 答案（基于参数化知识） ↓ 可能过时/缺失/幻觉 RAG流程: 问题 ──> 检索器 ──> 相关文档 ↓ LLM + 文档上下文 ──> 答案（有依据）核心优势:'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第2章 检索增强生成（RAG）原理"><meta itemprop=description content='第2章 检索增强生成（RAG）原理# 本章定位: 构建基于外部知识库的增强生成系统
核心内容: RAG标准架构 → Chunking策略 → 检索技术 → 重排序 → 高级RAG变体
前置知识: Part 1 第3章（Embedding）、Part 3 第4章（Embedding模型训练）
目录# 一、RAG：为什么需要外部知识？ 二、RAG 标准架构详解 三、核心技术：Chunking 与 Indexing 四、核心技术：检索 (Retrieval) 五、核心技术：重排序 (Reranking) 六、高级 RAG 变体 七、本章小结 一、RAG：为什么需要外部知识？# 1.1 大模型的知识困境# 问题1：知识过时
# 向GPT-4提问（假设训练数据截止2023年10月） question = "最新的诺贝尔物理学奖获得者是谁？" response = llm.generate(question) # 输出: "我的知识截止到2023年，无法回答..."问题2：私域知识缺失
企业内部文档、财报、技术规范 实时更新的法律法规、医疗指南 个人笔记、代码库 问题3：幻觉（Hallucination）
模型"编造"看似合理但实际错误的信息 在知识密集型任务中尤为严重 1.2 RAG的核心思想# Retrieval-Augmented Generation（检索增强生成）：
传统LLM: 问题 ──> LLM ──> 答案（基于参数化知识） ↓ 可能过时/缺失/幻觉 RAG流程: 问题 ──> 检索器 ──> 相关文档 ↓ LLM + 文档上下文 ──> 答案（有依据）核心优势:'><meta itemprop=wordCount content="3082"><title>第2章 检索增强生成（RAG）原理 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle checked>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle checked>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/ class=active>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第2章 检索增强生成（RAG）原理</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一rag为什么需要外部知识>一、RAG：为什么需要外部知识？</a><ul><li><a href=#11-大模型的知识困境>1.1 大模型的知识困境</a></li><li><a href=#12-rag的核心思想>1.2 RAG的核心思想</a></li></ul></li><li><a href=#二rag标准架构四大核心环节>二、RAG标准架构：四大核心环节</a><ul><li><a href=#21-完整pipeline流程>2.1 完整Pipeline流程</a></li></ul></li><li><a href=#三文档分块chunking策略>三、文档分块（Chunking）策略</a><ul><li><a href=#31-为什么需要分块>3.1 为什么需要分块？</a></li><li><a href=#32-固定长度分块fixed-size-chunking>3.2 固定长度分块（Fixed-size Chunking）</a></li><li><a href=#33-递归分块recursive-chunking>3.3 递归分块（Recursive Chunking）</a></li><li><a href=#34-语义分块semantic-chunking>3.4 语义分块（Semantic Chunking）</a></li><li><a href=#35-结构化分块markdownhtml-splitter>3.5 结构化分块（Markdown/HTML Splitter）</a></li><li><a href=#36-分块策略对比>3.6 分块策略对比</a></li></ul></li><li><a href=#四检索技术retrieval>四、检索技术（Retrieval）</a><ul><li><a href=#41-稠密检索dense-retrieval>4.1 稠密检索（Dense Retrieval）</a></li><li><a href=#42-稀疏检索sparse-retrieval---bm25>4.2 稀疏检索（Sparse Retrieval - BM25）</a></li><li><a href=#43-混合检索hybrid-retrieval>4.3 混合检索（Hybrid Retrieval）</a></li></ul></li><li><a href=#五重排序reranking>五、重排序（Reranking）</a><ul><li><a href=#51-为什么需要重排序>5.1 为什么需要重排序？</a></li><li><a href=#52-cross-encoder重排序>5.2 Cross-Encoder重排序</a></li><li><a href=#53-rerank效果提升>5.3 Rerank效果提升</a></li></ul></li><li><a href=#六高级rag技术>六、高级RAG技术</a><ul><li><a href=#61-hyde假设性文档嵌入>6.1 HyDE（假设性文档嵌入）</a></li><li><a href=#62-self-rag自我反思检索增强生成>6.2 Self-RAG（自我反思检索增强生成）</a><ul><li><a href=#621-reflection-tokens自我反思的核心机制>6.2.1 Reflection Tokens：自我反思的核心机制</a></li><li><a href=#622-self-rag训练方法>6.2.2 Self-RAG训练方法</a></li></ul></li><li><a href=#63-crag纠错性rag>6.3 CRAG（纠错性RAG）</a></li></ul></li><li><a href=#七graphrag知识图谱增强rag>七、GraphRAG（知识图谱增强RAG）</a><ul><li><a href=#71-核心思想>7.1 核心思想</a></li><li><a href=#72-架构流程>7.2 架构流程</a></li><li><a href=#73-核心步骤>7.3 核心步骤</a><ul><li><a href=#731-community-detectionleiden算法详解>7.3.1 Community Detection：Leiden算法详解</a></li></ul></li><li><a href=#74-代码实现基于llamaindex>7.4 代码实现（基于LlamaIndex）</a></li><li><a href=#75-graphrag优势>7.5 GraphRAG优势</a></li></ul></li><li><a href=#八完整rag-pipeline实战>八、完整RAG Pipeline实战</a><ul><li><a href=#81-使用langchain实现>8.1 使用LangChain实现</a></li><li><a href=#82-使用llamaindex实现>8.2 使用LlamaIndex实现</a></li></ul></li><li><a href=#九rag评估指标>九、RAG评估指标</a><ul><li><a href=#91-检索质量指标>9.1 检索质量指标</a></li><li><a href=#92-生成质量指标>9.2 生成质量指标</a></li><li><a href=#93-使用ragas评估>9.3 使用RAGAS评估</a></li></ul></li><li><a href=#十rag优化最佳实践>十、RAG优化最佳实践</a><ul><li><a href=#101-优化checklist>10.1 优化Checklist</a></li><li><a href=#102-成本优化>10.2 成本优化</a></li><li><a href=#103-延迟优化>10.3 延迟优化</a></li></ul></li><li><a href=#十一long-context-vs-rag>十一、Long Context vs RAG</a><ul><li><a href=#111-技术演进探讨rag-vs-long-context>11.1 技术演进探讨：RAG vs Long Context</a></li><li><a href=#112-混合架构设计>11.2 混合架构设计</a></li></ul></li><li><a href=#十二本章小结>十二、本章小结</a><ul><li><a href=#121-核心要点>12.1 核心要点</a></li><li><a href=#122-技术选型决策树>12.2 技术选型决策树</a></li><li><a href=#123-延伸阅读>12.3 延伸阅读</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第2章-检索增强生成rag原理>第2章 检索增强生成（RAG）原理<a class=anchor href=#%e7%ac%ac2%e7%ab%a0-%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90rag%e5%8e%9f%e7%90%86>#</a></h1><blockquote class=book-hint><p><strong>本章定位</strong>: 构建基于外部知识库的增强生成系统</p><p><strong>核心内容</strong>: RAG标准架构 → Chunking策略 → 检索技术 → 重排序 → 高级RAG变体</p><p><strong>前置知识</strong>: Part 1 第3章（Embedding）、Part 3 第4章（Embedding模型训练）</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80rag%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%96%e9%83%a8%e7%9f%a5%e8%af%86>一、RAG：为什么需要外部知识？</a></li><li><a href=#%e4%ba%8crag-%e6%a0%87%e5%87%86%e6%9e%b6%e6%9e%84%e8%af%a6%e8%a7%a3>二、RAG 标准架构详解</a></li><li><a href=#%e4%b8%89%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%afchunking-%e4%b8%8e-indexing>三、核心技术：Chunking 与 Indexing</a></li><li><a href=#%e5%9b%9b%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e6%a3%80%e7%b4%a2-retrieval>四、核心技术：检索 (Retrieval)</a></li><li><a href=#%e4%ba%94%e6%a0%b8%e5%bf%83%e6%8a%80%e6%9c%af%e9%87%8d%e6%8e%92%e5%ba%8f-reranking>五、核心技术：重排序 (Reranking)</a></li><li><a href=#%e5%85%ad%e9%ab%98%e7%ba%a7-rag-%e5%8f%98%e4%bd%93>六、高级 RAG 变体</a></li><li><a href=#%e4%b8%83%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>七、本章小结</a></li></ul><hr><h2 id=一rag为什么需要外部知识>一、RAG：为什么需要外部知识？<a class=anchor href=#%e4%b8%80rag%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%a4%96%e9%83%a8%e7%9f%a5%e8%af%86>#</a></h2><h3 id=11-大模型的知识困境>1.1 大模型的知识困境<a class=anchor href=#11-%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9a%84%e7%9f%a5%e8%af%86%e5%9b%b0%e5%a2%83>#</a></h3><p><strong>问题1：知识过时</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 向GPT-4提问（假设训练数据截止2023年10月）</span>
</span></span><span class=line><span class=cl><span class=n>question</span> <span class=o>=</span> <span class=s2>&#34;最新的诺贝尔物理学奖获得者是谁？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>question</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: &#34;我的知识截止到2023年，无法回答...&#34;</span></span></span></code></pre></div><p><strong>问题2：私域知识缺失</strong></p><ul><li>企业内部文档、财报、技术规范</li><li>实时更新的法律法规、医疗指南</li><li>个人笔记、代码库</li></ul><p><strong>问题3：幻觉（Hallucination）</strong></p><ul><li>模型"编造"看似合理但实际错误的信息</li><li>在知识密集型任务中尤为严重</li></ul><h3 id=12-rag的核心思想>1.2 RAG的核心思想<a class=anchor href=#12-rag%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p><strong>Retrieval-Augmented Generation（检索增强生成）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>传统LLM:   问题 ──&gt; LLM ──&gt; 答案（基于参数化知识）
</span></span><span class=line><span class=cl>           ↓
</span></span><span class=line><span class=cl>        可能过时/缺失/幻觉
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>RAG流程:   问题 ──&gt; 检索器 ──&gt; 相关文档
</span></span><span class=line><span class=cl>                      ↓
</span></span><span class=line><span class=cl>                    LLM + 文档上下文 ──&gt; 答案（有依据）</span></span></code></pre></div><p><strong>核心优势</strong>:</p><ol><li><strong>知识可更新</strong>: 无需重新训练模型，只需更新知识库</li><li><strong>可溯源</strong>: 答案可标注来源文档</li><li><strong>成本低</strong>: 避免昂贵的全量微调</li><li><strong>专业性强</strong>: 适合垂直领域（法律、医疗、金融）</li></ol><hr><h2 id=二rag标准架构四大核心环节>二、RAG标准架构：四大核心环节<a class=anchor href=#%e4%ba%8crag%e6%a0%87%e5%87%86%e6%9e%b6%e6%9e%84%e5%9b%9b%e5%a4%a7%e6%a0%b8%e5%bf%83%e7%8e%af%e8%8a%82>#</a></h2><pre class=mermaid>graph LR
    A[原始文档] --&gt; B[1. Chunking&lt;br/&gt;文档分块]
    B --&gt; C[2. Embedding&lt;br/&gt;向量化]
    C --&gt; D[3. VectorDB&lt;br/&gt;向量存储]

    Q[用户查询] --&gt; E[4. Retrieval&lt;br/&gt;检索]
    D --&gt; E
    E --&gt; F[5. Rerank&lt;br/&gt;重排序]
    F --&gt; G[6. Generation&lt;br/&gt;生成答案]
    G --&gt; H[最终答案]

    style B fill:#e1f5ff
    style E fill:#fff4e1
    style F fill:#ffe1f5
    style G fill:#e1ffe1</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><h3 id=21-完整pipeline流程>2.1 完整Pipeline流程<a class=anchor href=#21-%e5%ae%8c%e6%95%b4pipeline%e6%b5%81%e7%a8%8b>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>RAG标准流程 (使用LlamaIndex伪代码示例)
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core</span> <span class=kn>import</span> <span class=n>VectorStoreIndex</span><span class=p>,</span> <span class=n>SimpleDirectoryReader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.embeddings.openai</span> <span class=kn>import</span> <span class=n>OpenAIEmbedding</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.llms.openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ============ 离线阶段：构建索引 ============</span>
</span></span><span class=line><span class=cl><span class=c1># 步骤1: 加载文档</span>
</span></span><span class=line><span class=cl><span class=n>documents</span> <span class=o>=</span> <span class=n>SimpleDirectoryReader</span><span class=p>(</span><span class=s2>&#34;./data&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤2: 文档分块 (Chunking)</span>
</span></span><span class=line><span class=cl><span class=c1># LlamaIndex默认使用SentenceSplitter，chunk_size=1024</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.node_parser</span> <span class=kn>import</span> <span class=n>SentenceSplitter</span>
</span></span><span class=line><span class=cl><span class=n>parser</span> <span class=o>=</span> <span class=n>SentenceSplitter</span><span class=p>(</span><span class=n>chunk_size</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>nodes</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>get_nodes_from_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤3: Embedding + 向量存储</span>
</span></span><span class=line><span class=cl><span class=n>embed_model</span> <span class=o>=</span> <span class=n>OpenAIEmbedding</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>index</span> <span class=o>=</span> <span class=n>VectorStoreIndex</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nodes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>embed_model</span><span class=o>=</span><span class=n>embed_model</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ============ 在线阶段：查询回答 ============</span>
</span></span><span class=line><span class=cl><span class=c1># 步骤4: 检索 (Retrieval)</span>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>similarity_top_k</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>  <span class=c1># 检索Top-5</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤5: 检索 + 生成</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>query_engine</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=s2>&#34;Transformer架构的核心是什么？&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 步骤6: 查看来源</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>response</span><span class=o>.</span><span class=n>source_nodes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;来源: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>metadata</span><span class=p>[</span><span class=s1>&#39;file_name&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>, 相似度: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>score</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;内容片段: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>text</span><span class=p>[:</span><span class=mi>100</span><span class=p>]</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>输出示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>答案: Transformer的核心是Self-Attention机制，它允许模型在处理每个token时
</span></span><span class=line><span class=cl>      关注到整个序列的信息...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>来源: transformer_paper.pdf, 相似度: 0.892
</span></span><span class=line><span class=cl>内容片段: The Transformer architecture relies entirely on attention mechanisms...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>来源: attention_tutorial.md, 相似度: 0.856
</span></span><span class=line><span class=cl>内容片段: Self-Attention computes Q, K, V matrices from input embeddings...</span></span></code></pre></div><hr><h2 id=三文档分块chunking策略>三、文档分块（Chunking）策略<a class=anchor href=#%e4%b8%89%e6%96%87%e6%a1%a3%e5%88%86%e5%9d%97chunking%e7%ad%96%e7%95%a5>#</a></h2><h3 id=31-为什么需要分块>3.1 为什么需要分块？<a class=anchor href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%88%86%e5%9d%97>#</a></h3><p><strong>问题</strong>: 整篇文档太长，无法直接作为检索单元。</p><p><strong>目标</strong>:</p><ul><li><strong>粒度适中</strong>: 既包含完整语义，又不超过Embedding模型限制（通常512-2048 tokens）</li><li><strong>检索精准</strong>: 避免无关信息干扰</li><li><strong>上下文完整</strong>: 保持语义连贯性</li></ul><h3 id=32-固定长度分块fixed-size-chunking>3.2 固定长度分块（Fixed-size Chunking）<a class=anchor href=#32-%e5%9b%ba%e5%ae%9a%e9%95%bf%e5%ba%a6%e5%88%86%e5%9d%97fixed-size-chunking>#</a></h3><p><strong>原理</strong>: 按字符数或Token数固定切分。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fixed_size_chunk</span><span class=p>(</span><span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>chunk_size</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>512</span><span class=p>,</span> <span class=n>overlap</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>50</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    固定长度分块
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        text: 原始文本
</span></span></span><span class=line><span class=cl><span class=s2>        chunk_size: 每块的字符数
</span></span></span><span class=line><span class=cl><span class=s2>        overlap: 重叠字符数（保证语义连续性）
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>start</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span> <span class=o>=</span> <span class=n>start</span> <span class=o>+</span> <span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>text</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span> <span class=o>+=</span> <span class=p>(</span><span class=n>chunk_size</span> <span class=o>-</span> <span class=n>overlap</span><span class=p>)</span>  <span class=c1># 步长 = chunk_size - overlap</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Transformer是Google在2017年提出的...（5000字）&#34;</span>
</span></span><span class=line><span class=cl><span class=n>chunks</span> <span class=o>=</span> <span class=n>fixed_size_chunk</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>chunk_size</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>overlap</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 结果: [&#39;Transformer是Google...&#39;, &#39;...在2017年提出的深度学习...&#39;, ...]</span></span></span></code></pre></div><p><strong>优点</strong>: 简单、快速、可控
<strong>缺点</strong>: 可能切断句子/段落，破坏语义</p><h3 id=33-递归分块recursive-chunking>3.3 递归分块（Recursive Chunking）<a class=anchor href=#33-%e9%80%92%e5%bd%92%e5%88%86%e5%9d%97recursive-chunking>#</a></h3><p><strong>原理</strong>: 按语义单位（段落→句子→词）递归分割。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.node_parser</span> <span class=kn>import</span> <span class=n>SentenceSplitter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LlamaIndex的SentenceSplitter实现递归逻辑</span>
</span></span><span class=line><span class=cl><span class=n>splitter</span> <span class=o>=</span> <span class=n>SentenceSplitter</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>chunk_size</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>        <span class=c1># 目标块大小</span>
</span></span><span class=line><span class=cl>    <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>      <span class=c1># 重叠量</span>
</span></span><span class=line><span class=cl>    <span class=n>separator</span><span class=o>=</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>,</span>      <span class=c1># 优先按段落分</span>
</span></span><span class=line><span class=cl>    <span class=n>secondary_separator</span><span class=o>=</span><span class=s2>&#34;. &#34;</span> <span class=c1># 其次按句子分</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nodes</span> <span class=o>=</span> <span class=n>splitter</span><span class=o>.</span><span class=n>get_nodes_from_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span></span></span></code></pre></div><p><strong>分割优先级</strong>:</p><ol><li>按段落分（<code>\n\n</code>）</li><li>如果段落过长，按句子分（<code>. </code>）</li><li>如果句子仍过长，按固定长度分</li></ol><p><strong>优点</strong>: 保持语义完整性
<strong>缺点</strong>: 块大小不完全均匀</p><h3 id=34-语义分块semantic-chunking>3.4 语义分块（Semantic Chunking）<a class=anchor href=#34-%e8%af%ad%e4%b9%89%e5%88%86%e5%9d%97semantic-chunking>#</a></h3><p><strong>原理</strong>: 基于Embedding相似度动态分块。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.node_parser</span> <span class=kn>import</span> <span class=n>SemanticSplitterNodeParser</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.embeddings.openai</span> <span class=kn>import</span> <span class=n>OpenAIEmbedding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用Embedding模型计算句子间相似度</span>
</span></span><span class=line><span class=cl><span class=n>embed_model</span> <span class=o>=</span> <span class=n>OpenAIEmbedding</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>splitter</span> <span class=o>=</span> <span class=n>SemanticSplitterNodeParser</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>          <span class=c1># 滑动窗口大小</span>
</span></span><span class=line><span class=cl>    <span class=n>breakpoint_percentile_threshold</span><span class=o>=</span><span class=mi>95</span><span class=p>,</span>  <span class=c1># 相似度阈值</span>
</span></span><span class=line><span class=cl>    <span class=n>embed_model</span><span class=o>=</span><span class=n>embed_model</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nodes</span> <span class=o>=</span> <span class=n>splitter</span><span class=o>.</span><span class=n>get_nodes_from_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span></span></span></code></pre></div><p><strong>工作流程</strong>:</p><ol><li>将文档按句子分割</li><li>计算相邻句子的Embedding余弦相似度</li><li>当相似度低于阈值时，插入分块边界</li></ol><p><strong>示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>句子1: &#34;Transformer使用Self-Attention机制&#34;
</span></span><span class=line><span class=cl>句子2: &#34;Attention可以捕捉长距离依赖&#34;       --&gt; 相似度 0.89 (高) → 合并
</span></span><span class=line><span class=cl>句子3: &#34;Python是一门编程语言&#34;              --&gt; 相似度 0.23 (低) → 分块边界</span></span></code></pre></div><p><strong>优点</strong>: 语义连贯性最强
<strong>缺点</strong>: 计算成本高（需要对每个句子做Embedding）</p><h3 id=35-结构化分块markdownhtml-splitter>3.5 结构化分块（Markdown/HTML Splitter）<a class=anchor href=#35-%e7%bb%93%e6%9e%84%e5%8c%96%e5%88%86%e5%9d%97markdownhtml-splitter>#</a></h3><p><strong>原理</strong>: 利用文档本身的结构（标题、段落）来确定边界。这对于技术文档、法律条文尤为有效，能保证“一个标题下的内容不被切断”。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.text_splitter</span> <span class=kn>import</span> <span class=n>MarkdownHeaderTextSplitter</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>markdown_document</span> <span class=o>=</span> <span class=s2>&#34;# Title</span><span class=se>\n\n</span><span class=s2>## Section 1</span><span class=se>\n</span><span class=s2>content...</span><span class=se>\n\n</span><span class=s2>## Section 2</span><span class=se>\n</span><span class=s2>content...&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>headers_to_split_on</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;#&#34;</span><span class=p>,</span> <span class=s2>&#34;Header 1&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;##&#34;</span><span class=p>,</span> <span class=s2>&#34;Header 2&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>splitter</span> <span class=o>=</span> <span class=n>MarkdownHeaderTextSplitter</span><span class=p>(</span><span class=n>headers_to_split_on</span><span class=o>=</span><span class=n>headers_to_split_on</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>splits</span> <span class=o>=</span> <span class=n>splitter</span><span class=o>.</span><span class=n>split_text</span><span class=p>(</span><span class=n>markdown_document</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 结果会保留标题元数据：</span>
</span></span><span class=line><span class=cl><span class=c1># Split(page_content=&#34;content...&#34;, metadata={&#39;Header 1&#39;: &#39;Title&#39;, &#39;Header 2&#39;: &#39;Section 1&#39;})</span></span></span></code></pre></div><p><strong>优点</strong>: 极高语义完整性，天然带有元数据（Metadata）
<strong>缺点</strong>: 依赖文档格式规范</p><h3 id=36-分块策略对比>3.6 分块策略对比<a class=anchor href=#36-%e5%88%86%e5%9d%97%e7%ad%96%e7%95%a5%e5%af%b9%e6%af%94>#</a></h3><table><thead><tr><th>策略</th><th>计算成本</th><th>语义完整性</th><th>块大小均匀性</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>Fixed-size</strong></td><td>极低</td><td>低</td><td>高</td><td>快速原型、日志分析</td></tr><tr><td><strong>Recursive</strong></td><td>低</td><td>中</td><td>中</td><td><strong>通用场景（推荐）</strong></td></tr><tr><td><strong>Semantic</strong></td><td>高</td><td>高</td><td>低</td><td>学术论文、法律文档</td></tr></tbody></table><p><strong>最佳实践</strong>:</p><ul><li>起步使用 <strong>Recursive Chunking</strong>（兼顾效率和质量）</li><li>对专业文档（法律、学术）考虑 <strong>Semantic Chunking</strong></li><li>添加 <strong>chunk_overlap</strong>（通常50-100 tokens）避免语义断裂</li></ul><hr><h2 id=四检索技术retrieval>四、检索技术（Retrieval）<a class=anchor href=#%e5%9b%9b%e6%a3%80%e7%b4%a2%e6%8a%80%e6%9c%afretrieval>#</a></h2><h3 id=41-稠密检索dense-retrieval>4.1 稠密检索（Dense Retrieval）<a class=anchor href=#41-%e7%a8%a0%e5%af%86%e6%a3%80%e7%b4%a2dense-retrieval>#</a></h3><p><strong>原理</strong>: 将查询和文档都转为稠密向量（Embedding），计算余弦相似度。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics.pairwise</span> <span class=kn>import</span> <span class=n>cosine_similarity</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 离线：文档Embedding</span>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;Transformer是深度学习架构&#34;</span><span class=p>,</span> <span class=s2>&#34;Python是编程语言&#34;</span><span class=p>,</span> <span class=s2>&#34;北京是中国首都&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>doc_embeddings</span> <span class=o>=</span> <span class=n>embed_model</span><span class=o>.</span><span class=n>embed_documents</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>  <span class=c1># shape: (3, 1536)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 在线：查询Embedding</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;什么是Transformer？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=n>embed_model</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>   <span class=c1># shape: (1536,)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 计算相似度</span>
</span></span><span class=line><span class=cl><span class=n>similarities</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=n>query_embedding</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>doc_embeddings</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 结果: [0.92, 0.15, 0.08]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 排序并返回Top-K</span>
</span></span><span class=line><span class=cl><span class=n>top_k_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>similarities</span><span class=p>)[::</span><span class=o>-</span><span class=mi>1</span><span class=p>][:</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[(</span><span class=n>docs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>similarities</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>top_k_indices</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># [(&#39;Transformer是深度学习架构&#39;, 0.92), (&#39;Python是编程语言&#39;, 0.15)]</span></span></span></code></pre></div><p><strong>优点</strong>: 语义理解强，可处理同义词/改写
<strong>缺点</strong>: 对罕见词/专有名词召回率低</p><p><strong>常用Embedding模型</strong>:</p><ul><li><strong>OpenAI</strong>: <code>text-embedding-3-small</code> (1536维), <code>text-embedding-3-large</code> (3072维)</li><li><strong>开源</strong>: <code>bge-large-zh-v1.5</code>, <code>gte-large</code>, <code>m3e-base</code></li></ul><h3 id=42-稀疏检索sparse-retrieval---bm25>4.2 稀疏检索（Sparse Retrieval - BM25）<a class=anchor href=#42-%e7%a8%80%e7%96%8f%e6%a3%80%e7%b4%a2sparse-retrieval---bm25>#</a></h3><p><strong>原理</strong>: 基于词频统计的关键词匹配（TF-IDF升级版）。</p><p><strong>BM25公式</strong>:
$$
\text{Score}(Q, D) = \sum_{t \in Q} \text{IDF}(t) \cdot \frac{f(t, D) \cdot (k_1 + 1)}{f(t, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$</p><p>其中：</p><ul><li>$f(t, D)$: 词 $t$ 在文档 $D$ 中的频率</li><li>$|D|$: 文档长度</li><li>$k_1=1.5, b=0.75$: 调节参数</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>rank_bm25</span> <span class=kn>import</span> <span class=n>BM25Okapi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 离线：构建BM25索引</span>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Transformer uses self-attention mechanism&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Python is a programming language&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Beijing is the capital of China&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>tokenized_docs</span> <span class=o>=</span> <span class=p>[</span><span class=n>doc</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=o>.</span><span class=n>split</span><span class=p>()</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bm25</span> <span class=o>=</span> <span class=n>BM25Okapi</span><span class=p>(</span><span class=n>tokenized_docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 在线：查询</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;self-attention&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenized_query</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 计算BM25得分</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>bm25</span><span class=o>.</span><span class=n>get_scores</span><span class=p>(</span><span class=n>tokenized_query</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [0.93, 0.0, 0.0]  # 第1篇文档完全匹配</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 返回Top-K</span>
</span></span><span class=line><span class=cl><span class=n>top_k_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>scores</span><span class=p>)[::</span><span class=o>-</span><span class=mi>1</span><span class=p>][:</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[(</span><span class=n>docs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>scores</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>top_k_indices</span><span class=p>]</span></span></span></code></pre></div><p><strong>优点</strong>:</p><ul><li>精确关键词匹配</li><li>对专有名词、缩写敏感</li><li>计算速度快</li></ul><p><strong>缺点</strong>: 无法理解同义词（如"汽车"和"轿车"）</p><h3 id=43-混合检索hybrid-retrieval>4.3 混合检索（Hybrid Retrieval）<a class=anchor href=#43-%e6%b7%b7%e5%90%88%e6%a3%80%e7%b4%a2hybrid-retrieval>#</a></h3><p><strong>核心思想</strong>: 结合稠密检索（语义）+ 稀疏检索（关键词）。</p><p><strong>融合策略：倒数排名融合（RRF, Reciprocal Rank Fusion）</strong></p><p>$$
\text{Score}<em>{\text{RRF}}(d) = \sum</em>{r \in \text{rankings}} \frac{1}{k + r(d)}
$$</p><p>其中：</p><ul><li>$r(d)$: 文档 $d$ 在某个排名列表中的位置</li><li>$k=60$: 常数（降低高排名的权重，平滑分数差异）</li></ul><p><strong>RRF 核心优势</strong>：</p><ul><li><strong>无需归一化</strong>：不同检索器的分数可能在不同量纲（BM25是0-∞，余弦相似度是0-1），RRF直接使用排名避免归一化问题</li><li><strong>抗噪声</strong>：单个检索器的错误排名影响较小</li><li><strong>简单高效</strong>：无需调参，$k=60$ 是经验最优值</li></ul><p><strong>完整代码实现</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>reciprocal_rank_fusion</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25_results</span><span class=p>:</span> <span class=nb>list</span><span class=p>,</span>  <span class=c1># [(doc_id, score), ...]</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_results</span><span class=p>:</span> <span class=nb>list</span><span class=p>,</span> <span class=c1># [(doc_id, score), ...]</span>
</span></span><span class=line><span class=cl>    <span class=n>k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>60</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    倒数排名融合（RRF）算法实现
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        bm25_results: BM25检索结果列表
</span></span></span><span class=line><span class=cl><span class=s2>        dense_results: 稠密检索结果列表
</span></span></span><span class=line><span class=cl><span class=s2>        k: RRF常数，通常设为60（经验值）
</span></span></span><span class=line><span class=cl><span class=s2>        top_k: 返回的Top-K结果数
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        融合后的排序结果 [(doc_id, rrf_score), ...]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    示例:
</span></span></span><span class=line><span class=cl><span class=s2>        BM25排名: [doc1(rank=1), doc2(rank=2), doc3(rank=3)]
</span></span></span><span class=line><span class=cl><span class=s2>        Dense排名: [doc2(rank=1), doc1(rank=3), doc4(rank=2)]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        RRF分数计算:
</span></span></span><span class=line><span class=cl><span class=s2>        doc1: 1/(60+1) + 1/(60+3) = 0.0164 + 0.0159 = 0.0323
</span></span></span><span class=line><span class=cl><span class=s2>        doc2: 1/(60+2) + 1/(60+1) = 0.0161 + 0.0164 = 0.0325  &lt;- 最高
</span></span></span><span class=line><span class=cl><span class=s2>        doc3: 1/(60+3) + 0 = 0.0159
</span></span></span><span class=line><span class=cl><span class=s2>        doc4: 0 + 1/(60+2) = 0.0161
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>doc_scores</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 处理BM25结果</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>rank</span><span class=p>,</span> <span class=p>(</span><span class=n>doc_id</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>bm25_results</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>doc_scores</span><span class=p>[</span><span class=n>doc_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>doc_scores</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>doc_id</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>k</span> <span class=o>+</span> <span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 处理Dense检索结果</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>rank</span><span class=p>,</span> <span class=p>(</span><span class=n>doc_id</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dense_results</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>doc_scores</span><span class=p>[</span><span class=n>doc_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>doc_scores</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>doc_id</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>k</span> <span class=o>+</span> <span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 按RRF分数排序</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_docs</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>doc_scores</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>sorted_docs</span><span class=p>[:</span><span class=n>top_k</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ===== 完整混合检索示例 =====</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>rank_bm25</span> <span class=kn>import</span> <span class=n>BM25Okapi</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>hybrid_search_example</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;混合检索完整示例&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 准备文档库</span>
</span></span><span class=line><span class=cl>    <span class=n>docs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Transformer uses self-attention mechanism for sequence modeling&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;BERT is a pre-trained language model based on Transformer&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Python is a popular programming language for machine learning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Self-attention allows the model to weigh different parts of the input&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 构建BM25索引</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenized_docs</span> <span class=o>=</span> <span class=p>[</span><span class=n>doc</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=o>.</span><span class=n>split</span><span class=p>()</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25</span> <span class=o>=</span> <span class=n>BM25Okapi</span><span class=p>(</span><span class=n>tokenized_docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 构建Dense检索（使用预训练模型）</span>
</span></span><span class=line><span class=cl>    <span class=n>embed_model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;all-MiniLM-L6-v2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>doc_embeddings</span> <span class=o>=</span> <span class=n>embed_model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 4. 查询</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;what is self-attention?&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenized_query</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 5. BM25检索</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25_scores</span> <span class=o>=</span> <span class=n>bm25</span><span class=o>.</span><span class=n>get_scores</span><span class=p>(</span><span class=n>tokenized_query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25_results</span> <span class=o>=</span> <span class=p>[(</span><span class=n>i</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>bm25_scores</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>bm25_results</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>bm25_results</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 6. Dense检索（余弦相似度）</span>
</span></span><span class=line><span class=cl>    <span class=n>query_embedding</span> <span class=o>=</span> <span class=n>embed_model</span><span class=o>.</span><span class=n>encode</span><span class=p>([</span><span class=n>query</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_scores</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>doc_embeddings</span><span class=p>,</span> <span class=n>query_embedding</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_results</span> <span class=o>=</span> <span class=p>[(</span><span class=n>i</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dense_scores</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_results</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>dense_results</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 7. RRF融合</span>
</span></span><span class=line><span class=cl>    <span class=n>final_results</span> <span class=o>=</span> <span class=n>reciprocal_rank_fusion</span><span class=p>(</span><span class=n>bm25_results</span><span class=p>,</span> <span class=n>dense_results</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 8. 输出结果</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;混合检索Top-3结果:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>doc_id</span><span class=p>,</span> <span class=n>rrf_score</span> <span class=ow>in</span> <span class=n>final_results</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;RRF分数: </span><span class=si>{</span><span class=n>rrf_score</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> | </span><span class=si>{</span><span class=n>docs</span><span class=p>[</span><span class=n>doc_id</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 输出:</span>
</span></span><span class=line><span class=cl>    <span class=c1># RRF分数: 0.0328 | Self-attention allows the model to weigh different parts of the input</span>
</span></span><span class=line><span class=cl>    <span class=c1># RRF分数: 0.0325 | Transformer uses self-attention mechanism for sequence modeling</span>
</span></span><span class=line><span class=cl>    <span class=c1># RRF分数: 0.0164 | BERT is a pre-trained language model based on Transformer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>hybrid_search_example</span><span class=p>()</span></span></span></code></pre></div><p><strong>LlamaIndex实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.retrievers</span> <span class=kn>import</span> <span class=n>QueryFusionRetriever</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>QueryFusionRetriever</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>retrievers</span><span class=o>=</span><span class=p>[</span><span class=n>bm25_retriever</span><span class=p>,</span> <span class=n>vector_retriever</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;reciprocal_rerank&#34;</span><span class=p>,</span>  <span class=c1># RRF模式</span>
</span></span><span class=line><span class=cl>    <span class=n>num_queries</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nodes</span> <span class=o>=</span> <span class=n>retriever</span><span class=o>.</span><span class=n>retrieve</span><span class=p>(</span><span class=s2>&#34;Transformer的核心是什么？&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>效果对比</strong>（BEIR基准测试平均值）:</p><ul><li>BM25: NDCG@10 = 0.42</li><li>Dense (bge-large): NDCG@10 = 0.54</li><li><strong>Hybrid (RRF)</strong>: NDCG@10 = <strong>0.61</strong> ✅</li></ul><hr><h2 id=五重排序reranking>五、重排序（Reranking）<a class=anchor href=#%e4%ba%94%e9%87%8d%e6%8e%92%e5%ba%8freranking>#</a></h2><h3 id=51-为什么需要重排序>5.1 为什么需要重排序？<a class=anchor href=#51-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%87%8d%e6%8e%92%e5%ba%8f>#</a></h3><p><strong>问题</strong>:</p><ul><li>向量检索是<strong>单塔模型</strong>（Query和Doc分别编码），无法捕捉交互信息</li><li>可能召回语义相似但实际不相关的文档</li></ul><p><strong>例子</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;如何训练Transformer？&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>检索结果Top-5:
</span></span><span class=line><span class=cl>1. &#34;Transformer的训练需要大量数据...&#34; ✅ 相关
</span></span><span class=line><span class=cl>2. &#34;BERT是基于Transformer的模型...&#34;  ❌ 提到Transformer但不相关
</span></span><span class=line><span class=cl>3. &#34;训练深度学习模型的技巧...&#34;      ⚠️  泛化答案
</span></span><span class=line><span class=cl>4. &#34;Transformer架构的核心组件...&#34;   ❌ 架构而非训练
</span></span><span class=line><span class=cl>5. &#34;PyTorch训练神经网络教程...&#34;     ⚠️  工具教程</span></span></code></pre></div><h3 id=52-cross-encoder重排序>5.2 Cross-Encoder重排序<a class=anchor href=#52-cross-encoder%e9%87%8d%e6%8e%92%e5%ba%8f>#</a></h3><p><strong>原理</strong>: 将Query和Doc拼接后输入BERT，直接输出相关性分数。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>单塔模型 (Bi-Encoder):
</span></span><span class=line><span class=cl>    Query → Encoder1 → Vec1  ┐
</span></span><span class=line><span class=cl>                               ├─&gt; Cosine(Vec1, Vec2)
</span></span><span class=line><span class=cl>    Doc   → Encoder2 → Vec2  ┘
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>双塔模型 (Cross-Encoder):
</span></span><span class=line><span class=cl>    [CLS] Query [SEP] Doc [SEP] → BERT → [CLS] Embedding → 相关性分数</span></span></code></pre></div><p><strong>代码实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>CrossEncoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载Cross-Encoder模型</span>
</span></span><span class=line><span class=cl><span class=n>reranker</span> <span class=o>=</span> <span class=n>CrossEncoder</span><span class=p>(</span><span class=s1>&#39;BAAI/bge-reranker-large&#39;</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 初步检索（召回Top-20）</span>
</span></span><span class=line><span class=cl><span class=n>candidates</span> <span class=o>=</span> <span class=n>vector_retriever</span><span class=o>.</span><span class=n>retrieve</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 重排序</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;如何训练Transformer？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>pairs</span> <span class=o>=</span> <span class=p>[(</span><span class=n>query</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>text</span><span class=p>)</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>candidates</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>reranker</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>pairs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 按新分数排序</span>
</span></span><span class=line><span class=cl><span class=n>reranked_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>scores</span><span class=p>)[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>final_results</span> <span class=o>=</span> <span class=p>[</span><span class=n>candidates</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>reranked_indices</span><span class=p>[:</span><span class=mi>5</span><span class=p>]]</span></span></span></code></pre></div><p><strong>LlamaIndex集成</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.postprocessor.flag_embedding_reranker</span> <span class=kn>import</span> <span class=n>FlagEmbeddingReranker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>reranker</span> <span class=o>=</span> <span class=n>FlagEmbeddingReranker</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;BAAI/bge-reranker-large&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_n</span><span class=o>=</span><span class=mi>5</span>  <span class=c1># 重排后返回Top-5</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>similarity_top_k</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>      <span class=c1># 初步召回20个</span>
</span></span><span class=line><span class=cl>    <span class=n>node_postprocessors</span><span class=o>=</span><span class=p>[</span><span class=n>reranker</span><span class=p>]</span>  <span class=c1># 重排序为5个</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><h3 id=53-rerank效果提升>5.3 Rerank效果提升<a class=anchor href=#53-rerank%e6%95%88%e6%9e%9c%e6%8f%90%e5%8d%87>#</a></h3><p><strong>实验数据</strong>（MS MARCO数据集）:</p><table><thead><tr><th>方法</th><th>MRR@10</th><th>Recall@10</th></tr></thead><tbody><tr><td>Dense Retrieval (bge-large)</td><td>0.38</td><td>0.68</td></tr><tr><td>+ Rerank (bge-reranker-large)</td><td><strong>0.48</strong> (+26%)</td><td>0.68</td></tr></tbody></table><p><strong>性能开销</strong>:</p><ul><li>Dense检索：~5ms (GPU)</li><li>Rerank 20个候选：~50ms (GPU)</li><li><strong>建议</strong>: 初步召回20-50个，重排为5-10个</li></ul><hr><h2 id=六高级rag技术>六、高级RAG技术<a class=anchor href=#%e5%85%ad%e9%ab%98%e7%ba%a7rag%e6%8a%80%e6%9c%af>#</a></h2><h3 id=61-hyde假设性文档嵌入>6.1 HyDE（假设性文档嵌入）<a class=anchor href=#61-hyde%e5%81%87%e8%ae%be%e6%80%a7%e6%96%87%e6%a1%a3%e5%b5%8c%e5%85%a5>#</a></h3><p><strong>问题</strong>: 用户查询通常是简短问题，与文档风格不匹配。</p><p><strong>示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;Transformer是什么？&#34;
</span></span><span class=line><span class=cl>文档: &#34;Transformer是由Vaswani等人在2017年提出的深度学习架构，
</span></span><span class=line><span class=cl>       它完全基于Self-Attention机制，摒弃了传统的CNN和RNN结构...&#34;</span></span></code></pre></div><p><strong>HyDE解决方案</strong>（2022年提出）:</p><ol><li>用LLM生成<strong>假设性答案</strong>（Hypothetical Document）</li><li>对假设性答案做Embedding</li><li>用这个Embedding去检索</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.indices.query.query_transform</span> <span class=kn>import</span> <span class=n>HyDEQueryTransform</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 定义HyDE转换</span>
</span></span><span class=line><span class=cl><span class=n>hyde</span> <span class=o>=</span> <span class=n>HyDEQueryTransform</span><span class=p>(</span><span class=n>include_original</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 集成到查询引擎</span>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>query_transform</span><span class=o>=</span><span class=n>hyde</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 查询</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>query_engine</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=s2>&#34;Transformer是什么？&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>内部流程</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>用户查询: &#34;Transformer是什么？&#34;
</span></span><span class=line><span class=cl>          ↓
</span></span><span class=line><span class=cl>HyDE生成: &#34;Transformer是一种深度学习架构，由Google在2017年提出，
</span></span><span class=line><span class=cl>          其核心是Self-Attention机制...&#34; (假设性文档)
</span></span><span class=line><span class=cl>          ↓
</span></span><span class=line><span class=cl>Embedding: [0.12, -0.34, 0.56, ...]  (对假设性文档编码)
</span></span><span class=line><span class=cl>          ↓
</span></span><span class=line><span class=cl>向量检索: 找到真实文档</span></span></code></pre></div><p><strong>效果提升</strong>:</p><ul><li>BEIR基准测试：平均提升 <strong>5-10% nDCG@10</strong></li><li>尤其在复杂查询（如"对比类"问题）上效果显著</li></ul><h3 id=62-self-rag自我反思检索增强生成>6.2 Self-RAG（自我反思检索增强生成）<a class=anchor href=#62-self-rag%e8%87%aa%e6%88%91%e5%8f%8d%e6%80%9d%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e7%94%9f%e6%88%90>#</a></h3><p><strong>核心思想</strong>（2023年提出）: 让模型<strong>自主决定</strong>何时检索、如何使用检索内容。</p><p><strong>流程</strong>:</p><pre class=mermaid>graph TD
    A[用户查询] --&gt; B{需要检索?}
    B --&gt;|是| C[检索文档]
    B --&gt;|否| D[直接生成]
    C --&gt; E{文档相关?}
    E --&gt;|相关| F[基于文档生成]
    E --&gt;|不相关| D
    F --&gt; G{答案有依据?}
    G --&gt;|有| H[输出答案]
    G --&gt;|无| I[标记不确定性]</pre><h4 id=621-reflection-tokens自我反思的核心机制>6.2.1 Reflection Tokens：自我反思的核心机制<a class=anchor href=#621-reflection-tokens%e8%87%aa%e6%88%91%e5%8f%8d%e6%80%9d%e7%9a%84%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6>#</a></h4><p>Self-RAG 的核心创新是引入 <strong>特殊反思 token（Reflection Tokens）</strong>，让模型在生成过程中进行自我评估。共有三类反思 token：</p><p><strong>1. Retrieve Token（检索决策）</strong></p><ul><li><strong>作用</strong>：判断是否需要检索外部知识</li><li><strong>生成时机</strong>：在生成答案<strong>之前</strong></li><li><strong>可能输出</strong>：<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;[Retrieve]&#34;</span>     <span class=c1># 需要检索（如专业知识、实时信息）</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;[No Retrieve]&#34;</span>  <span class=c1># 无需检索（如常识性问题、数学计算）</span></span></span></code></pre></div></li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;最新的诺贝尔物理学奖获得者是谁？&#34;
</span></span><span class=line><span class=cl>模型输出: &#34;[Retrieve]&#34;  → 触发检索
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>查询: &#34;1 + 1 等于几？&#34;
</span></span><span class=line><span class=cl>模型输出: &#34;[No Retrieve]&#34;  → 直接生成答案</span></span></code></pre></div><p><strong>2. IsREL Token（相关性评估）</strong></p><ul><li><strong>作用</strong>：评估检索到的文档是否与问题相关</li><li><strong>生成时机</strong>：在检索<strong>之后</strong>、生成答案<strong>之前</strong></li><li><strong>可能输出</strong>：<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;[Relevant]&#34;</span>     <span class=c1># 文档相关，使用该文档生成答案</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;[Irrelevant]&#34;</span>   <span class=c1># 文档不相关，忽略该文档</span></span></span></code></pre></div></li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;Transformer的核心机制是什么？&#34;
</span></span><span class=line><span class=cl>检索到的文档: &#34;Transformer使用Self-Attention机制...&#34;
</span></span><span class=line><span class=cl>模型输出: &#34;[Relevant]&#34;  → 基于该文档生成
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>检索到的文档: &#34;Python是一门编程语言...&#34;
</span></span><span class=line><span class=cl>模型输出: &#34;[Irrelevant]&#34;  → 忽略该文档，继续检索或直接生成</span></span></code></pre></div><p><strong>3. Support Token（答案可信度）</strong></p><ul><li><strong>作用</strong>：评估生成的答案是否有文档支撑</li><li><strong>生成时机</strong>：在生成答案<strong>之后</strong></li><li><strong>可能输出</strong>：<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;[Fully supported]&#34;</span>     <span class=c1># 答案完全有文档依据</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;[Partially supported]&#34;</span> <span class=c1># 答案部分有依据</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;[No support]&#34;</span>          <span class=c1># 答案无文档依据（可能是幻觉）</span></span></span></code></pre></div></li></ul><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;Transformer在哪一年提出？&#34;
</span></span><span class=line><span class=cl>检索文档: &#34;Transformer由Vaswani等人在2017年提出&#34;
</span></span><span class=line><span class=cl>生成答案: &#34;Transformer在2017年提出&#34;
</span></span><span class=line><span class=cl>Support评估: &#34;[Fully supported]&#34;  → 输出答案
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>查询: &#34;Transformer有多少层？&#34;
</span></span><span class=line><span class=cl>检索文档: &#34;Transformer使用Self-Attention机制&#34;
</span></span><span class=line><span class=cl>生成答案: &#34;Transformer通常有12层&#34;
</span></span><span class=line><span class=cl>Support评估: &#34;[No support]&#34;  → 标记不确定性或重新检索</span></span></code></pre></div><h4 id=622-self-rag训练方法>6.2.2 Self-RAG训练方法<a class=anchor href=#622-self-rag%e8%ae%ad%e7%bb%83%e6%96%b9%e6%b3%95>#</a></h4><p><strong>数据构建</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练样本示例</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;query&#34;</span><span class=p>:</span> <span class=s2>&#34;What is the capital of France?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;retrieve_token&#34;</span><span class=p>:</span> <span class=s2>&#34;[No Retrieve]&#34;</span><span class=p>,</span>  <span class=c1># 常识不需要检索</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;answer&#34;</span><span class=p>:</span> <span class=s2>&#34;Paris&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;support_token&#34;</span><span class=p>:</span> <span class=s2>&#34;[Fully supported]&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;query&#34;</span><span class=p>:</span> <span class=s2>&#34;What is the latest research on quantum computing?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;retrieve_token&#34;</span><span class=p>:</span> <span class=s2>&#34;[Retrieve]&#34;</span><span class=p>,</span>     <span class=c1># 需要最新信息</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;documents&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;Recent studies show...&#34;</span><span class=p>,</span> <span class=s2>&#34;Quantum computing...&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;isrel_tokens&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;[Relevant]&#34;</span><span class=p>,</span> <span class=s2>&#34;[Irrelevant]&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;answer&#34;</span><span class=p>:</span> <span class=s2>&#34;Recent studies show that...&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;support_token&#34;</span><span class=p>:</span> <span class=s2>&#34;[Fully supported]&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></div><p><strong>训练流程</strong>：</p><ol><li><strong>监督微调（SFT）</strong>：使用标注数据训练模型生成反思 token</li><li><strong>强化学习（RL）</strong>：用奖励模型优化检索时机和答案质量<ul><li>奖励：答案正确性 + 检索效率（减少不必要检索）</li></ul></li></ol><p><strong>代码框架（伪代码）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SelfRAGModel</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤1: 判断是否需要检索</span>
</span></span><span class=line><span class=cl>        <span class=n>retrieve_token</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_retrieve_token</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>retrieve_token</span> <span class=o>==</span> <span class=s2>&#34;[Retrieve]&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 步骤2: 检索文档</span>
</span></span><span class=line><span class=cl>            <span class=n>documents</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>retriever</span><span class=o>.</span><span class=n>retrieve</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 步骤3: 评估文档相关性</span>
</span></span><span class=line><span class=cl>            <span class=n>relevant_docs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>documents</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>isrel_token</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_isrel_token</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>isrel_token</span> <span class=o>==</span> <span class=s2>&#34;[Relevant]&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>relevant_docs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 步骤4: 基于相关文档生成答案</span>
</span></span><span class=line><span class=cl>            <span class=n>answer</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>generate_answer</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>relevant_docs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 无需检索，直接生成</span>
</span></span><span class=line><span class=cl>            <span class=n>answer</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>generate_answer</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=p>[])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 步骤5: 评估答案可信度</span>
</span></span><span class=line><span class=cl>        <span class=n>support_token</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_support_token</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=p>,</span> <span class=n>answer</span><span class=p>,</span> <span class=n>relevant_docs</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;answer&#34;</span><span class=p>:</span> <span class=n>answer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;retrieve_decision&#34;</span><span class=p>:</span> <span class=n>retrieve_token</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;support_level&#34;</span><span class=p>:</span> <span class=n>support_token</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SelfRAGModel</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=s2>&#34;What is Transformer?&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;答案: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;answer&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;检索决策: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;retrieve_decision&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;支撑程度: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=s1>&#39;support_level&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出:</span>
</span></span><span class=line><span class=cl><span class=c1># 答案: Transformer is a deep learning architecture...</span>
</span></span><span class=line><span class=cl><span class=c1># 检索决策: [Retrieve]</span>
</span></span><span class=line><span class=cl><span class=c1># 支撑程度: [Fully supported]</span></span></span></code></pre></div><p><strong>效果提升</strong>:</p><ul><li><strong>准确率</strong>: 在PopQA数据集上比标准RAG提升 <strong>7-12%</strong></li><li><strong>成本优化</strong>: 减少不必要的检索调用（成本降低30%）</li><li><strong>可解释性</strong>: 反思token提供了决策透明度</li></ul><h3 id=63-crag纠错性rag>6.3 CRAG（纠错性RAG）<a class=anchor href=#63-crag%e7%ba%a0%e9%94%99%e6%80%a7rag>#</a></h3><p><strong>问题</strong>: 检索到的文档可能包含噪音或过时信息。</p><p><strong>CRAG方案</strong>:</p><ol><li><p><strong>置信度评估</strong>: 判断检索结果质量</p><ul><li>高置信度 → 直接使用</li><li>中等 → 结合Web搜索</li><li>低置信度 → 仅用LLM参数化知识</li></ul></li><li><p><strong>知识提炼</strong>: 从文档中提取关键语句（而非使用全文）</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.response_synthesizers</span> <span class=kn>import</span> <span class=n>TreeSummarize</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 知识提炼：提取关键句</span>
</span></span><span class=line><span class=cl><span class=n>refine_synthesizer</span> <span class=o>=</span> <span class=n>TreeSummarize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>response_synthesizer</span><span class=o>=</span><span class=n>refine_synthesizer</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><hr><h2 id=七graphrag知识图谱增强rag>七、GraphRAG（知识图谱增强RAG）<a class=anchor href=#%e4%b8%83graphrag%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1%e5%a2%9e%e5%bc%barag>#</a></h2><h3 id=71-核心思想>7.1 核心思想<a class=anchor href=#71-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p><strong>2024年4月微软提出</strong>: 传统RAG在处理跨文档关系、全局性问题总结时表现不佳。</p><p><strong>问题示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>查询: &#34;文档集中所有涉及到AI安全的部分有哪些共同点？&#34;
</span></span><span class=line><span class=cl>传统RAG: 只能检索局部片段，无法构建全局关联。</span></span></code></pre></div><p><strong>GraphRAG解决方案</strong>:
构建<strong>知识图谱(Knowledge Graph) + 社区摘要(Community Summary)</strong>。</p><h3 id=72-架构流程>7.2 架构流程<a class=anchor href=#72-%e6%9e%b6%e6%9e%84%e6%b5%81%e7%a8%8b>#</a></h3><pre class=mermaid>graph TB
    D[原始文档] --&gt; E[实体抽取&lt;br/&gt;LLM]
    E --&gt; F[构建图谱]
    F --&gt; G[社区检测&lt;br/&gt;Leiden算法]
    G --&gt; H[生成社区摘要&lt;br/&gt;LLM]
    H --&gt; I[全局查询]
    I --&gt; J[Map-Reduce生成]</pre><h3 id=73-核心步骤>7.3 核心步骤<a class=anchor href=#73-%e6%a0%b8%e5%bf%83%e6%ad%a5%e9%aa%a4>#</a></h3><ol><li><strong>源文档索引</strong>: 文本分块。</li><li><strong>知识提取</strong>: 使用LLM提取实体、关系、声明(claims)。</li><li><strong>图谱构建</strong>: 用NetworkX构建图结构。</li><li><strong>社区检测</strong>: 将图划分为不同层级的社区。</li><li><strong>社区摘要</strong>: 为每个社区生成摘要。</li><li><strong>查询处理</strong>:<ul><li><strong>全局查询</strong>: 使用社区摘要直接回答。</li><li><strong>局部查询</strong>: 结合图谱路径和向量检索。</li></ul></li></ol><h4 id=731-community-detectionleiden算法详解>7.3.1 Community Detection：Leiden算法详解<a class=anchor href=#731-community-detectionleiden%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3>#</a></h4><p><strong>为什么需要社区检测？</strong></p><p>在知识图谱中，实体和关系会形成复杂的网络结构。社区检测的目标是将密切相关的节点划分为<strong>社群（Community）</strong>，每个社群代表一个主题或概念集群。</p><p><strong>示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>文档集: 关于深度学习的100篇论文
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>构建的知识图谱:
</span></span><span class=line><span class=cl>- 节点: Transformer, BERT, GPT, Attention, RNN, LSTM, CNN...
</span></span><span class=line><span class=cl>- 边: (Transformer, 基于, Attention), (BERT, 使用, Transformer)...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>社区检测后:
</span></span><span class=line><span class=cl>社群1: {Transformer, Attention, Multi-Head Attention} → 主题: 注意力机制
</span></span><span class=line><span class=cl>社群2: {BERT, GPT, RoBERTa} → 主题: 预训练语言模型
</span></span><span class=line><span class=cl>社群3: {RNN, LSTM, GRU} → 主题: 循环神经网络</span></span></code></pre></div><p><strong>Leiden算法：优于Louvain的社区检测</strong></p><p>Leiden算法（2019年提出）是GraphRAG中使用的核心算法，它解决了经典Louvain算法的<strong>不连通社区</strong>问题。</p><p><strong>核心原理</strong>：</p><ol><li><strong>模块度优化（Modularity Optimization）</strong></li></ol><p>模块度 $Q$ 衡量社区划分的质量：
$$
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
$$</p><p>其中：</p><ul><li>$A_{ij}$: 节点 $i$ 和 $j$ 之间的边权重</li><li>$k_i$: 节点 $i$ 的度数</li><li>$m$: 图中边的总数</li><li>$\delta(c_i, c_j)$: 节点 $i$ 和 $j$ 是否在同一社区（是为1，否为0）</li></ul><p><strong>目标</strong>: 最大化 $Q$ 值（范围-1到1，越高越好）</p><ol start=2><li><strong>Leiden算法的三个阶段</strong></li></ol><p><strong>阶段1: 局部移动（Local Moving）</strong></p><ul><li>遍历每个节点，尝试将其移动到相邻社区</li><li>如果移动能提升模块度，则执行移动</li><li>重复直到没有节点可移动</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>graph</span><span class=o>.</span><span class=n>nodes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>best_community</span> <span class=o>=</span> <span class=n>node</span><span class=o>.</span><span class=n>current_community</span>
</span></span><span class=line><span class=cl>    <span class=n>best_delta_Q</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>neighbor_community</span> <span class=ow>in</span> <span class=n>node</span><span class=o>.</span><span class=n>neighbor_communities</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>delta_Q</span> <span class=o>=</span> <span class=n>calculate_modularity_gain</span><span class=p>(</span><span class=n>node</span><span class=p>,</span> <span class=n>neighbor_community</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>delta_Q</span> <span class=o>&gt;</span> <span class=n>best_delta_Q</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>best_community</span> <span class=o>=</span> <span class=n>neighbor_community</span>
</span></span><span class=line><span class=cl>            <span class=n>best_delta_Q</span> <span class=o>=</span> <span class=n>delta_Q</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>best_community</span> <span class=o>!=</span> <span class=n>node</span><span class=o>.</span><span class=n>current_community</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>move_node</span><span class=p>(</span><span class=n>node</span><span class=p>,</span> <span class=n>best_community</span><span class=p>)</span></span></span></code></pre></div><p><strong>阶段2: 社区精炼（Refinement）</strong></p><ul><li><strong>Leiden的核心创新</strong>：检测社区内的松散连接子集</li><li>将不良连接的节点分离，形成新的子社区</li><li><strong>解决问题</strong>：Louvain可能产生内部断开的社区</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 伪代码</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>community</span> <span class=ow>in</span> <span class=n>communities</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># 检测社区内的连通分量</span>
</span></span><span class=line><span class=cl>    <span class=n>subgraphs</span> <span class=o>=</span> <span class=n>find_connected_components_within</span><span class=p>(</span><span class=n>community</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>subgraphs</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 社区内部不连通，需要分裂</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>subgraph</span> <span class=ow>in</span> <span class=n>subgraphs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 尝试将子图重新分配到最佳社区</span>
</span></span><span class=line><span class=cl>            <span class=n>best_merge_community</span> <span class=o>=</span> <span class=n>find_best_community_for_subgraph</span><span class=p>(</span><span class=n>subgraph</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>best_merge_community</span> <span class=o>!=</span> <span class=n>community</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>move_subgraph</span><span class=p>(</span><span class=n>subgraph</span><span class=p>,</span> <span class=n>best_merge_community</span><span class=p>)</span></span></span></code></pre></div><p><strong>阶段3: 社区聚合（Aggregation）</strong></p><ul><li>将每个社区压缩为单个"超节点"</li><li>社区之间的边权重 = 原始节点之间的边权重总和</li><li>在新的聚合图上重复阶段1-2</li></ul><p><strong>3. Leiden vs Louvain 对比</strong></p><table><thead><tr><th>维度</th><th>Louvain算法</th><th>Leiden算法</th></tr></thead><tbody><tr><td><strong>连通性</strong></td><td>可能产生断开的社区</td><td><strong>保证社区内部连通</strong></td></tr><tr><td><strong>质量</strong></td><td>模块度较高</td><td><strong>模块度更高</strong></td></tr><tr><td><strong>速度</strong></td><td>快</td><td>稍慢（多了精炼步骤）</td></tr><tr><td><strong>层级性</strong></td><td>支持</td><td>支持</td></tr></tbody></table><p><strong>4. GraphRAG中的应用</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>使用Leiden算法进行社区检测
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>networkx</span> <span class=k>as</span> <span class=nn>nx</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>cdlib</span> <span class=kn>import</span> <span class=n>algorithms</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 构建知识图谱（从LLM提取的三元组）</span>
</span></span><span class=line><span class=cl><span class=n>G</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>Graph</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>triplets</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Transformer&#34;</span><span class=p>,</span> <span class=s2>&#34;proposed_by&#34;</span><span class=p>,</span> <span class=s2>&#34;Google&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Transformer&#34;</span><span class=p>,</span> <span class=s2>&#34;uses&#34;</span><span class=p>,</span> <span class=s2>&#34;Self-Attention&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;BERT&#34;</span><span class=p>,</span> <span class=s2>&#34;based_on&#34;</span><span class=p>,</span> <span class=s2>&#34;Transformer&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;GPT&#34;</span><span class=p>,</span> <span class=s2>&#34;based_on&#34;</span><span class=p>,</span> <span class=s2>&#34;Transformer&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;Self-Attention&#34;</span><span class=p>,</span> <span class=s2>&#34;computes&#34;</span><span class=p>,</span> <span class=s2>&#34;Q_K_V&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>head</span><span class=p>,</span> <span class=n>relation</span><span class=p>,</span> <span class=n>tail</span> <span class=ow>in</span> <span class=n>triplets</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>G</span><span class=o>.</span><span class=n>add_edge</span><span class=p>(</span><span class=n>head</span><span class=p>,</span> <span class=n>tail</span><span class=p>,</span> <span class=n>relation</span><span class=o>=</span><span class=n>relation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 使用Leiden算法检测社区</span>
</span></span><span class=line><span class=cl><span class=n>communities</span> <span class=o>=</span> <span class=n>algorithms</span><span class=o>.</span><span class=n>leiden</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>resolution</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 输出社区结果</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;检测到 </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>communities</span><span class=o>.</span><span class=n>communities</span><span class=p>)</span><span class=si>}</span><span class=s2> 个社区:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>community</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>communities</span><span class=o>.</span><span class=n>communities</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;社群 </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>community</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出:</span>
</span></span><span class=line><span class=cl><span class=c1># 检测到 2 个社区:</span>
</span></span><span class=line><span class=cl><span class=c1># 社群 1: [&#39;Transformer&#39;, &#39;BERT&#39;, &#39;GPT&#39;, &#39;Google&#39;]</span>
</span></span><span class=line><span class=cl><span class=c1># 社群 2: [&#39;Self-Attention&#39;, &#39;Q_K_V&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 为每个社区生成摘要（使用LLM）</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>community</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>communities</span><span class=o>.</span><span class=n>communities</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 提取社区内的所有关系</span>
</span></span><span class=line><span class=cl>    <span class=n>community_edges</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>u</span><span class=p>,</span> <span class=n>v</span><span class=p>,</span> <span class=n>G</span><span class=p>[</span><span class=n>u</span><span class=p>][</span><span class=n>v</span><span class=p>][</span><span class=s1>&#39;relation&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>u</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>G</span><span class=o>.</span><span class=n>edges</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>u</span> <span class=ow>in</span> <span class=n>community</span> <span class=ow>and</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>community</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 构建Prompt</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    根据以下知识图谱片段，生成一个简洁的主题摘要：
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    实体: </span><span class=si>{</span><span class=s1>&#39;, &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>community</span><span class=p>)</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    关系: </span><span class=si>{</span><span class=n>community_edges</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    摘要（1-2句话）:
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>summary</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>message</span><span class=o>.</span><span class=n>content</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>社群 </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2> 摘要:</span><span class=se>\n</span><span class=si>{</span><span class=n>summary</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出:</span>
</span></span><span class=line><span class=cl><span class=c1># 社群 1 摘要:</span>
</span></span><span class=line><span class=cl><span class=c1># 这个社群主要讨论Transformer架构及其衍生模型BERT和GPT，由Google提出。</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 社群 2 摘要:</span>
</span></span><span class=line><span class=cl><span class=c1># 这个社群关注Self-Attention机制，包括Q、K、V矩阵的计算。</span></span></span></code></pre></div><p><strong>5. 层级社区检测</strong></p><p>GraphRAG支持<strong>多层级社区检测</strong>，用于处理不同粒度的问题：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>层级社区检测
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 第1层：细粒度社区（10-20个节点/社区）</span>
</span></span><span class=line><span class=cl><span class=n>level1_communities</span> <span class=o>=</span> <span class=n>algorithms</span><span class=o>.</span><span class=n>leiden</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>resolution</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第2层：中等粒度（50-100个节点/社区）</span>
</span></span><span class=line><span class=cl><span class=n>level2_communities</span> <span class=o>=</span> <span class=n>algorithms</span><span class=o>.</span><span class=n>leiden</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>resolution</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第3层：粗粒度（整个图的全局摘要）</span>
</span></span><span class=line><span class=cl><span class=n>level3_communities</span> <span class=o>=</span> <span class=n>algorithms</span><span class=o>.</span><span class=n>leiden</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>resolution</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查询时根据问题类型选择层级:</span>
</span></span><span class=line><span class=cl><span class=c1># - 细节问题 → 使用 level1</span>
</span></span><span class=line><span class=cl><span class=c1># - 关联问题 → 使用 level2</span>
</span></span><span class=line><span class=cl><span class=c1># - 全局总结 → 使用 level3</span></span></span></code></pre></div><p><strong>6. Leiden算法的优势在GraphRAG中的体现</strong></p><table><thead><tr><th>优势</th><th>在GraphRAG中的应用</th></tr></thead><tbody><tr><td><strong>高质量社区</strong></td><td>更准确的主题聚类，减少跨主题噪音</td></tr><tr><td><strong>连通性保证</strong></td><td>社区内的实体确实相关，不会误聚合</td></tr><tr><td><strong>层级支持</strong></td><td>支持不同粒度的查询（局部/全局）</td></tr><tr><td><strong>可扩展</strong></td><td>适用于大规模知识图谱（百万级节点）</td></tr></tbody></table><h3 id=74-代码实现基于llamaindex>7.4 代码实现（基于LlamaIndex）<a class=anchor href=#74-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e5%9f%ba%e4%ba%8ellamaindex>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>GraphRAG实现：结合知识图谱和向量检索
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core</span> <span class=kn>import</span> <span class=n>KnowledgeGraphIndex</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.graph_stores</span> <span class=kn>import</span> <span class=n>SimpleGraphStore</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.llms.openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 构建知识图谱索引</span>
</span></span><span class=line><span class=cl><span class=n>graph_store</span> <span class=o>=</span> <span class=n>SimpleGraphStore</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>kg_index</span> <span class=o>=</span> <span class=n>KnowledgeGraphIndex</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>documents</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_triplets_per_chunk</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>  <span class=c1># 每个chunk提取3个三元组</span>
</span></span><span class=line><span class=cl>    <span class=n>graph_store</span><span class=o>=</span><span class=n>graph_store</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 查询（自动结合图谱推理）</span>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>kg_index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>include_embeddings</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># 结合向量检索</span>
</span></span><span class=line><span class=cl>    <span class=n>response_mode</span><span class=o>=</span><span class=s2>&#34;tree_summarize&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>query_engine</span><span class=o>.</span><span class=n>query</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Transformer和BERT的关系是什么？&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 可视化知识图谱</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyvis.network</span> <span class=kn>import</span> <span class=n>Network</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>kg_index</span><span class=o>.</span><span class=n>get_networkx_graph</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>Network</span><span class=p>(</span><span class=n>notebook</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>net</span><span class=o>.</span><span class=n>from_nx</span><span class=p>(</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>net</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=s2>&#34;knowledge_graph.html&#34;</span><span class=p>)</span></span></span></code></pre></div><p><strong>知识提取示例</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>文档: &#34;Google在2017年提出了Transformer架构，后来BERT基于Transformer进行改进。&#34;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>提取的三元组:
</span></span><span class=line><span class=cl>1. (Google, 提出, Transformer)
</span></span><span class=line><span class=cl>2. (Transformer, 提出年份, 2017)
</span></span><span class=line><span class=cl>3. (BERT, 基于, Transformer)</span></span></code></pre></div><h3 id=75-graphrag优势>7.5 GraphRAG优势<a class=anchor href=#75-graphrag%e4%bc%98%e5%8a%bf>#</a></h3><p><strong>对比实验</strong>（微软内部数据集）:</p><table><thead><tr><th>方法</th><th>全局理解准确率</th><th>多跳推理成功率</th></tr></thead><tbody><tr><td>传统RAG</td><td>42%</td><td>31%</td></tr><tr><td><strong>GraphRAG</strong></td><td><strong>67%</strong></td><td><strong>58%</strong></td></tr></tbody></table><p><strong>适用场景</strong>:</p><ul><li>跨文档关系挖掘</li><li>全局性总结问题</li><li>复杂多跳推理</li></ul><hr><h2 id=八完整rag-pipeline实战>八、完整RAG Pipeline实战<a class=anchor href=#%e5%85%ab%e5%ae%8c%e6%95%b4rag-pipeline%e5%ae%9e%e6%88%98>#</a></h2><h3 id=81-使用langchain实现>8.1 使用LangChain实现<a class=anchor href=#81-%e4%bd%bf%e7%94%a8langchain%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>完整RAG系统：使用LangChain
</span></span></span><span class=line><span class=cl><span class=s2>包含：Chunking → Embedding → VectorDB → Retrieval → Rerank → Generation
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_community.document_loaders</span> <span class=kn>import</span> <span class=n>DirectoryLoader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.text_splitter</span> <span class=kn>import</span> <span class=n>RecursiveCharacterTextSplitter</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_openai</span> <span class=kn>import</span> <span class=n>OpenAIEmbeddings</span><span class=p>,</span> <span class=n>ChatOpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_community.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>RetrievalQA</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.retrievers</span> <span class=kn>import</span> <span class=n>ContextualCompressionRetriever</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.retrievers.document_compressors</span> <span class=kn>import</span> <span class=n>CohereRerank</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第1步：加载文档 ==========</span>
</span></span><span class=line><span class=cl><span class=n>loader</span> <span class=o>=</span> <span class=n>DirectoryLoader</span><span class=p>(</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>glob</span><span class=o>=</span><span class=s2>&#34;**/*.md&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>documents</span> <span class=o>=</span> <span class=n>loader</span><span class=o>.</span><span class=n>load</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第2步：文档分块 ==========</span>
</span></span><span class=line><span class=cl><span class=n>text_splitter</span> <span class=o>=</span> <span class=n>RecursiveCharacterTextSplitter</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>chunk_size</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>separators</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=s2>&#34;. &#34;</span><span class=p>,</span> <span class=s2>&#34; &#34;</span><span class=p>,</span> <span class=s2>&#34;&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>chunks</span> <span class=o>=</span> <span class=n>text_splitter</span><span class=o>.</span><span class=n>split_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第3步：Embedding + 向量存储 ==========</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>OpenAIEmbeddings</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vectorstore</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span><span class=n>chunks</span><span class=p>,</span> <span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第4步：创建检索器（混合检索） ==========</span>
</span></span><span class=line><span class=cl><span class=c1># 4.1 向量检索器</span>
</span></span><span class=line><span class=cl><span class=n>dense_retriever</span> <span class=o>=</span> <span class=n>vectorstore</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>search_type</span><span class=o>=</span><span class=s2>&#34;similarity&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>search_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;k&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>}</span>  <span class=c1># 召回20个</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4.2 添加重排序（Cohere Rerank）</span>
</span></span><span class=line><span class=cl><span class=n>compressor</span> <span class=o>=</span> <span class=n>CohereRerank</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;rerank-english-v2.0&#34;</span><span class=p>,</span> <span class=n>top_n</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>ContextualCompressionRetriever</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>base_compressor</span><span class=o>=</span><span class=n>compressor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>base_retriever</span><span class=o>=</span><span class=n>dense_retriever</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第5步：创建QA链 ==========</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatOpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>qa_chain</span> <span class=o>=</span> <span class=n>RetrievalQA</span><span class=o>.</span><span class=n>from_chain_type</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chain_type</span><span class=o>=</span><span class=s2>&#34;stuff&#34;</span><span class=p>,</span>  <span class=c1># 将所有文档拼接到一个Prompt</span>
</span></span><span class=line><span class=cl>    <span class=n>retriever</span><span class=o>=</span><span class=n>retriever</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_source_documents</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第6步：查询 ==========</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;Transformer的核心机制是什么？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>qa_chain</span><span class=p>({</span><span class=s2>&#34;query&#34;</span><span class=p>:</span> <span class=n>query</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;答案:&#34;</span><span class=p>,</span> <span class=n>result</span><span class=p>[</span><span class=s2>&#34;result&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>来源文档:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>result</span><span class=p>[</span><span class=s2>&#34;source_documents&#34;</span><span class=p>],</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>. </span><span class=si>{</span><span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>[</span><span class=s1>&#39;source&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;   内容: </span><span class=si>{</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>[:</span><span class=mi>100</span><span class=p>]</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=82-使用llamaindex实现>8.2 使用LlamaIndex实现<a class=anchor href=#82-%e4%bd%bf%e7%94%a8llamaindex%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>完整RAG系统：使用LlamaIndex
</span></span></span><span class=line><span class=cl><span class=s2>包含：HyDE + 混合检索 + Rerank
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core</span> <span class=kn>import</span> <span class=n>VectorStoreIndex</span><span class=p>,</span> <span class=n>SimpleDirectoryReader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.retrievers</span> <span class=kn>import</span> <span class=n>VectorIndexRetriever</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.query_engine</span> <span class=kn>import</span> <span class=n>RetrieverQueryEngine</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.postprocessor</span> <span class=kn>import</span> <span class=n>SentenceTransformerRerank</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.response_synthesizers</span> <span class=kn>import</span> <span class=n>get_response_synthesizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.embeddings.openai</span> <span class=kn>import</span> <span class=n>OpenAIEmbedding</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.llms.openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第1步：加载 + 分块 ==========</span>
</span></span><span class=line><span class=cl><span class=n>documents</span> <span class=o>=</span> <span class=n>SimpleDirectoryReader</span><span class=p>(</span><span class=s2>&#34;./data&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第2步：创建索引 ==========</span>
</span></span><span class=line><span class=cl><span class=n>embed_model</span> <span class=o>=</span> <span class=n>OpenAIEmbedding</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;text-embedding-3-small&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-4o&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>index</span> <span class=o>=</span> <span class=n>VectorStoreIndex</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>documents</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>embed_model</span><span class=o>=</span><span class=n>embed_model</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第3步：配置检索器 ==========</span>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>VectorIndexRetriever</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>index</span><span class=o>=</span><span class=n>index</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>similarity_top_k</span><span class=o>=</span><span class=mi>20</span>  <span class=c1># 召回20个</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第4步：配置重排序 ==========</span>
</span></span><span class=line><span class=cl><span class=n>reranker</span> <span class=o>=</span> <span class=n>SentenceTransformerRerank</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;BAAI/bge-reranker-large&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_n</span><span class=o>=</span><span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第5步：创建查询引擎 ==========</span>
</span></span><span class=line><span class=cl><span class=n>response_synthesizer</span> <span class=o>=</span> <span class=n>get_response_synthesizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>response_mode</span><span class=o>=</span><span class=s2>&#34;compact&#34;</span>  <span class=c1># 自动压缩上下文</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>RetrieverQueryEngine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>retriever</span><span class=o>=</span><span class=n>retriever</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>response_synthesizer</span><span class=o>=</span><span class=n>response_synthesizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>node_postprocessors</span><span class=o>=</span><span class=p>[</span><span class=n>reranker</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ========== 第6步：查询 ==========</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>query_engine</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=s2>&#34;Self-Attention的计算过程是什么？&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看来源</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>response</span><span class=o>.</span><span class=n>source_nodes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;来源: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>node</span><span class=o>.</span><span class=n>metadata</span><span class=p>[</span><span class=s1>&#39;file_name&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;相似度: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>score</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;内容: </span><span class=si>{</span><span class=n>node</span><span class=o>.</span><span class=n>node</span><span class=o>.</span><span class=n>text</span><span class=p>[:</span><span class=mi>100</span><span class=p>]</span><span class=si>}</span><span class=s2>...</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=九rag评估指标>九、RAG评估指标<a class=anchor href=#%e4%b9%9drag%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87>#</a></h2><h3 id=91-检索质量指标>9.1 检索质量指标<a class=anchor href=#91-%e6%a3%80%e7%b4%a2%e8%b4%a8%e9%87%8f%e6%8c%87%e6%a0%87>#</a></h3><p><strong>1. Recall@K（召回率）</strong>
$$
\text{Recall@K} = \frac{\text{检索到的相关文档数}}{\text{所有相关文档数}}
$$</p><p><strong>2. Precision@K（精确率）</strong>
$$
\text{Precision@K} = \frac{\text{检索到的相关文档数}}{K}
$$</p><p><strong>3. MRR（Mean Reciprocal Rank，平均倒数排名）</strong>
$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$</p><p><strong>4. NDCG@K（归一化折损累计增益）</strong>
$$
\text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}
$$</p><h3 id=92-生成质量指标>9.2 生成质量指标<a class=anchor href=#92-%e7%94%9f%e6%88%90%e8%b4%a8%e9%87%8f%e6%8c%87%e6%a0%87>#</a></h3><p><strong>1. Faithfulness（忠实度）</strong></p><ul><li>生成内容是否基于检索到的文档</li><li>评估方法：用LLM判断答案是否能从文档推导</li></ul><p><strong>2. Answer Relevancy（答案相关性）</strong></p><ul><li>答案是否回答了问题</li><li>评估方法：计算答案和问题的语义相似度</li></ul><p><strong>3. Context Relevancy（上下文相关性）</strong></p><ul><li>检索到的文档是否与问题相关</li><li>计算公式：
$$
\text{Context Relevancy} = \frac{\text{相关句子数}}{\text{总句子数}}
$$</li></ul><h3 id=93-使用ragas评估>9.3 使用RAGAS评估<a class=anchor href=#93-%e4%bd%bf%e7%94%a8ragas%e8%af%84%e4%bc%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ragas</span> <span class=kn>import</span> <span class=n>evaluate</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ragas.metrics</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>faithfulness</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>answer_relevancy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>context_recall</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>context_precision</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>Dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 准备评估数据</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;question&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;Transformer是什么？&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;answer&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;Transformer是一种深度学习架构...&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;contexts&#34;</span><span class=p>:</span> <span class=p>[[</span><span class=s2>&#34;Transformer由Google提出...&#34;</span><span class=p>,</span> <span class=s2>&#34;Self-Attention是核心...&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;ground_truth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;Transformer是Google在2017年提出的深度学习架构&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>Dataset</span><span class=o>.</span><span class=n>from_dict</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 评估</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>faithfulness</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>answer_relevancy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>context_recall</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>context_precision</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># {&#39;faithfulness&#39;: 0.95, &#39;answer_relevancy&#39;: 0.88,</span>
</span></span><span class=line><span class=cl><span class=c1>#  &#39;context_recall&#39;: 0.92, &#39;context_precision&#39;: 0.85}</span></span></span></code></pre></div><hr><h2 id=十rag优化最佳实践>十、RAG优化最佳实践<a class=anchor href=#%e5%8d%81rag%e4%bc%98%e5%8c%96%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5>#</a></h2><h3 id=101-优化checklist>10.1 优化Checklist<a class=anchor href=#101-%e4%bc%98%e5%8c%96checklist>#</a></h3><table><thead><tr><th>优化点</th><th>基础方案</th><th>进阶方案</th><th>提升幅度</th></tr></thead><tbody><tr><td><strong>Chunking</strong></td><td>Fixed-size (512)</td><td>Recursive + Overlap (50)</td><td>+5% Recall</td></tr><tr><td><strong>Embedding</strong></td><td>text-embedding-ada-002</td><td>text-embedding-3-large</td><td>+8% MRR</td></tr><tr><td><strong>检索</strong></td><td>Dense (Top-5)</td><td>Hybrid (BM25+Dense, Top-20)</td><td>+12% NDCG</td></tr><tr><td><strong>Rerank</strong></td><td>无</td><td>Cross-Encoder (Top-5)</td><td>+15% MRR</td></tr><tr><td><strong>查询优化</strong></td><td>原始查询</td><td>HyDE</td><td>+7% NDCG</td></tr><tr><td><strong>上下文压缩</strong></td><td>无</td><td>Context Compression</td><td>降低50%成本</td></tr></tbody></table><h3 id=102-成本优化>10.2 成本优化<a class=anchor href=#102-%e6%88%90%e6%9c%ac%e4%bc%98%e5%8c%96>#</a></h3><p><strong>问题</strong>: 长上下文导致高成本。</p><p><strong>解决方案</strong>:</p><ol><li><strong>上下文压缩</strong>: 提取关键句而非全文</li><li><strong>Prompt缓存</strong>: 复用相同上下文（Claude支持）</li><li><strong>小模型Rerank</strong>: 用小模型过滤，大模型生成</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 示例：上下文压缩</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama_index.core.postprocessor</span> <span class=kn>import</span> <span class=n>LongContextReorder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>compressor</span> <span class=o>=</span> <span class=n>LongContextReorder</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>node_postprocessors</span><span class=o>=</span><span class=p>[</span><span class=n>reranker</span><span class=p>,</span> <span class=n>compressor</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div><h3 id=103-延迟优化>10.3 延迟优化<a class=anchor href=#103-%e5%bb%b6%e8%bf%9f%e4%bc%98%e5%8c%96>#</a></h3><p><strong>目标</strong>: 降低查询响应时间。</p><p><strong>方法</strong>:</p><ol><li><strong>异步检索</strong>: 并行调用多个检索器</li><li><strong>缓存</strong>: Redis缓存热门查询</li><li><strong>流式响应</strong>: 边检索边生成</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 流式响应</span>
</span></span><span class=line><span class=cl><span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>(</span><span class=n>streaming</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>query_engine</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=s2>&#34;...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>response</span><span class=o>.</span><span class=n>response_gen</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=n>flush</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=十一long-context-vs-rag>十一、Long Context vs RAG<a class=anchor href=#%e5%8d%81%e4%b8%80long-context-vs-rag>#</a></h2><h3 id=111-技术演进探讨rag-vs-long-context>11.1 技术演进探讨：RAG vs Long Context<a class=anchor href=#111-%e6%8a%80%e6%9c%af%e6%bc%94%e8%bf%9b%e6%8e%a2%e8%ae%a8rag-vs-long-context>#</a></h3><p>随着Gemini 1.5 Pro (1M tokens)、Claude 3.5 Sonnet (200K tokens)的出现，&ldquo;RAG vs Long Context"成为热议话题。</p><p><strong>实验结论（Needle In A Haystack测试）</strong>:</p><ul><li><strong>Long Context</strong>: 在100K+长度下，也能精准检索</li><li><strong>但是</strong>: 成本极高、延迟极高</li></ul><h3 id=112-混合架构设计>11.2 混合架构设计<a class=anchor href=#112-%e6%b7%b7%e5%90%88%e6%9e%b6%e6%9e%84%e8%ae%be%e8%ae%a1>#</a></h3><p><strong>最佳实践</strong>: <strong>RAG + Long Context</strong></p><ol><li><strong>RAG作为初筛</strong>: 先检索Top-50文档（而非Top-5）</li><li><strong>Long Context作为精排</strong>: 将这50个文档全部放入Prompt</li><li><strong>LLM生成</strong>: 利用长窗口能力进行跨文档推理</li></ol><p><strong>对比</strong>:</p><table><thead><tr><th>维度</th><th>传统RAG (Top-5)</th><th>Long Context (全文档)</th><th>混合模式 (RAG+LongCtx)</th></tr></thead><tbody><tr><td><strong>成本</strong></td><td>低</td><td>极高</td><td>中</td></tr><tr><td><strong>延迟</strong></td><td>低</td><td>高</td><td>中</td></tr><tr><td><strong>准确率</strong></td><td>受限于检索</td><td>高</td><td>最高</td></tr><tr><td><strong>跨文档推理</strong></td><td>弱</td><td>强</td><td>强</td></tr></tbody></table><p><strong>成本对比</strong>（假设1M tokens文档库）:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>纯Long Context:
</span></span><span class=line><span class=cl>  输入: 1M tokens × $3/1M = $3 每次查询
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>RAG (Top-5):
</span></span><span class=line><span class=cl>  输入: 5 × 500 tokens = 2.5K tokens × $3/1M = $0.0075 每次查询
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>混合模式 (Top-50):
</span></span><span class=line><span class=cl>  输入: 50 × 500 tokens = 25K tokens × $3/1M = $0.075 每次查询
</span></span><span class=line><span class=cl>  (成本是纯RAG的10倍，但比Long Context便宜40倍)</span></span></code></pre></div><hr><h2 id=十二本章小结>十二、本章小结<a class=anchor href=#%e5%8d%81%e4%ba%8c%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=121-核心要点>12.1 核心要点<a class=anchor href=#121-%e6%a0%b8%e5%bf%83%e8%a6%81%e7%82%b9>#</a></h3><ol><li><p><strong>核心公式</strong>: RAG = 检索(Recall) + 生成(Generation)。不仅是技术，更是一种<strong>解耦知识与推理</strong>的架构思想。</p></li><li><p><strong>关键组件</strong>:</p><ul><li><strong>Chunking</strong>: 固定长度是基准，Recursive是通用推荐，Semantic适合专业文档</li><li><strong>Embedding</strong>: 选择高质量模型（<code>text-embedding-3</code>, <code>bge-large-zh-v1.5</code>）至关重要</li><li><strong>VectorDB</strong>: FAISS（原型）、Milvus（生产）、Chroma（轻量级）</li><li><strong>检索</strong>: Hybrid（BM25+Dense）是最佳实践</li><li><strong>Rerank</strong>: Cross-Encoder重排序可带来10-20%的MRR提升</li></ul></li><li><p><strong>进阶优化</strong>:</p><ul><li><strong>Pre</strong>: HyDE改善查询质量（+5-10% NDCG）</li><li><strong>Post</strong>: Rerank是提升精度的性价比之选</li><li><strong>成本</strong>: 上下文压缩可降低50%成本</li></ul></li><li><p><strong>前沿架构</strong>:</p><ul><li><strong>Self-RAG</strong>: 模型自主决定何时检索（准确率+7-12%）</li><li><strong>GraphRAG</strong>: 解决全局性与复杂推理问题（多跳推理成功率+27%）</li><li><strong>CRAG</strong>: 纠错性RAG，处理噪音文档</li></ul></li><li><p><strong>未来趋势</strong>:</p><ul><li><strong>Agentic RAG</strong>: 让智能体自主控制检索策略（详见Part 4 第3章）</li><li><strong>RAG + Long Context</strong>: 混合架构兼顾成本与性能</li></ul></li></ol><h3 id=122-技术选型决策树>12.2 技术选型决策树<a class=anchor href=#122-%e6%8a%80%e6%9c%af%e9%80%89%e5%9e%8b%e5%86%b3%e7%ad%96%e6%a0%91>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>需要外部知识 → 是 →
</span></span><span class=line><span class=cl>    ├─ 文档量 &lt; 10K → 使用FAISS
</span></span><span class=line><span class=cl>    ├─ 文档量 &gt; 100K → 使用Milvus
</span></span><span class=line><span class=cl>    └─ 需要全局推理 → 使用GraphRAG
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>检索不准 →
</span></span><span class=line><span class=cl>    ├─ 关键词匹配差 → 添加BM25混合检索
</span></span><span class=line><span class=cl>    ├─ 语义理解差 → 升级Embedding模型
</span></span><span class=line><span class=cl>    └─ 排序不准 → 添加Rerank
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>成本过高 →
</span></span><span class=line><span class=cl>    ├─ 上下文太长 → 使用Context Compression
</span></span><span class=line><span class=cl>    ├─ 调用频繁 → 添加缓存
</span></span><span class=line><span class=cl>    └─ 考虑混合Long Context架构</span></span></code></pre></div><h3 id=123-延伸阅读>12.3 延伸阅读<a class=anchor href=#123-%e5%bb%b6%e4%bc%b8%e9%98%85%e8%af%bb>#</a></h3><p><strong>必读论文</strong>:</p><ol><li><a href=https://arxiv.org/abs/2005.11401>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> (Lewis et al., 2020) - RAG开山之作</li><li><a href=https://arxiv.org/abs/2212.10496>Precise Zero-Shot Dense Retrieval without Relevance Labels</a> (HyDE, 2022)</li><li><a href=https://arxiv.org/abs/2310.11511>Self-RAG: Learning to Retrieve, Generate, and Critique</a> (2023)</li><li><a href=https://arxiv.org/abs/2404.16130>From Local to Global: A Graph RAG Approach</a> (Microsoft GraphRAG, 2024)</li></ol><p><strong>框架文档</strong>:</p><ul><li><a href=https://docs.llamaindex.ai/>LlamaIndex文档</a></li><li><a href=https://python.langchain.com/docs/use_cases/question_answering/>LangChain RAG教程</a></li><li><a href=https://github.com/explodinggradients/ragas>RAGAS评估框架</a></li></ul><p><strong>下一步学习</strong>:</p><ul><li>Part 4 第3章：智能体（Agent）核心机制 - 学习Agentic RAG</li><li>Part 6 第3章：生产部署最佳实践 - 学习RAG系统部署</li><li>Part 7 第1章：长上下文技术 - 深入理解Long Context原理</li></ul><hr><blockquote class=book-hint><p><strong>章节边界提醒</strong>:</p><ul><li>❌ Embedding模型训练 → 详见 Part 3 第4章</li><li>❌ Agent架构设计 → 详见 Part 4 第3章</li><li>❌ 生产部署优化 → 详见 Part 6 第3章</li><li>❌ 长上下文技术细节 → 详见 Part 7 第1章</li></ul></blockquote></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第1章 提示工程与上下文学习</span>
</a></span><span><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/ class="flex align-center"><span>第3章 智能体（Agent）核心机制</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一rag为什么需要外部知识>一、RAG：为什么需要外部知识？</a><ul><li><a href=#11-大模型的知识困境>1.1 大模型的知识困境</a></li><li><a href=#12-rag的核心思想>1.2 RAG的核心思想</a></li></ul></li><li><a href=#二rag标准架构四大核心环节>二、RAG标准架构：四大核心环节</a><ul><li><a href=#21-完整pipeline流程>2.1 完整Pipeline流程</a></li></ul></li><li><a href=#三文档分块chunking策略>三、文档分块（Chunking）策略</a><ul><li><a href=#31-为什么需要分块>3.1 为什么需要分块？</a></li><li><a href=#32-固定长度分块fixed-size-chunking>3.2 固定长度分块（Fixed-size Chunking）</a></li><li><a href=#33-递归分块recursive-chunking>3.3 递归分块（Recursive Chunking）</a></li><li><a href=#34-语义分块semantic-chunking>3.4 语义分块（Semantic Chunking）</a></li><li><a href=#35-结构化分块markdownhtml-splitter>3.5 结构化分块（Markdown/HTML Splitter）</a></li><li><a href=#36-分块策略对比>3.6 分块策略对比</a></li></ul></li><li><a href=#四检索技术retrieval>四、检索技术（Retrieval）</a><ul><li><a href=#41-稠密检索dense-retrieval>4.1 稠密检索（Dense Retrieval）</a></li><li><a href=#42-稀疏检索sparse-retrieval---bm25>4.2 稀疏检索（Sparse Retrieval - BM25）</a></li><li><a href=#43-混合检索hybrid-retrieval>4.3 混合检索（Hybrid Retrieval）</a></li></ul></li><li><a href=#五重排序reranking>五、重排序（Reranking）</a><ul><li><a href=#51-为什么需要重排序>5.1 为什么需要重排序？</a></li><li><a href=#52-cross-encoder重排序>5.2 Cross-Encoder重排序</a></li><li><a href=#53-rerank效果提升>5.3 Rerank效果提升</a></li></ul></li><li><a href=#六高级rag技术>六、高级RAG技术</a><ul><li><a href=#61-hyde假设性文档嵌入>6.1 HyDE（假设性文档嵌入）</a></li><li><a href=#62-self-rag自我反思检索增强生成>6.2 Self-RAG（自我反思检索增强生成）</a><ul><li><a href=#621-reflection-tokens自我反思的核心机制>6.2.1 Reflection Tokens：自我反思的核心机制</a></li><li><a href=#622-self-rag训练方法>6.2.2 Self-RAG训练方法</a></li></ul></li><li><a href=#63-crag纠错性rag>6.3 CRAG（纠错性RAG）</a></li></ul></li><li><a href=#七graphrag知识图谱增强rag>七、GraphRAG（知识图谱增强RAG）</a><ul><li><a href=#71-核心思想>7.1 核心思想</a></li><li><a href=#72-架构流程>7.2 架构流程</a></li><li><a href=#73-核心步骤>7.3 核心步骤</a><ul><li><a href=#731-community-detectionleiden算法详解>7.3.1 Community Detection：Leiden算法详解</a></li></ul></li><li><a href=#74-代码实现基于llamaindex>7.4 代码实现（基于LlamaIndex）</a></li><li><a href=#75-graphrag优势>7.5 GraphRAG优势</a></li></ul></li><li><a href=#八完整rag-pipeline实战>八、完整RAG Pipeline实战</a><ul><li><a href=#81-使用langchain实现>8.1 使用LangChain实现</a></li><li><a href=#82-使用llamaindex实现>8.2 使用LlamaIndex实现</a></li></ul></li><li><a href=#九rag评估指标>九、RAG评估指标</a><ul><li><a href=#91-检索质量指标>9.1 检索质量指标</a></li><li><a href=#92-生成质量指标>9.2 生成质量指标</a></li><li><a href=#93-使用ragas评估>9.3 使用RAGAS评估</a></li></ul></li><li><a href=#十rag优化最佳实践>十、RAG优化最佳实践</a><ul><li><a href=#101-优化checklist>10.1 优化Checklist</a></li><li><a href=#102-成本优化>10.2 成本优化</a></li><li><a href=#103-延迟优化>10.3 延迟优化</a></li></ul></li><li><a href=#十一long-context-vs-rag>十一、Long Context vs RAG</a><ul><li><a href=#111-技术演进探讨rag-vs-long-context>11.1 技术演进探讨：RAG vs Long Context</a></li><li><a href=#112-混合架构设计>11.2 混合架构设计</a></li></ul></li><li><a href=#十二本章小结>十二、本章小结</a><ul><li><a href=#121-核心要点>12.1 核心要点</a></li><li><a href=#122-技术选型决策树>12.2 技术选型决策树</a></li><li><a href=#123-延伸阅读>12.3 延伸阅读</a></li></ul></li></ul></nav></div></aside></main></body></html>