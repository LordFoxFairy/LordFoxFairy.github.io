<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第14章 概率图模型：推断# “Probabilistic inference is nothing but counting, in appropriate ways.” — Judea Pearl
引言# 在概率图模型中，推断（Inference） 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。
本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。
14.1 推断问题的分类# 14.1.1 推断任务的类型# 设概率图模型定义在变量集合 $\mathcal{V} = {X_1, X_2, \ldots, X_n}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：
查询变量（Query Variables）：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。 证据变量（Evidence Variables）：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。 隐变量（Hidden Variables）：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。 常见的推断任务包括：
(1) 边缘推断（Marginal Inference）# 计算查询变量的边缘概率分布：
$$ P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})} $$
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第14章 概率图模型 推断"><meta property="og:description" content="第14章 概率图模型：推断# “Probabilistic inference is nothing but counting, in appropriate ways.” — Judea Pearl
引言# 在概率图模型中，推断（Inference） 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。
本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。
14.1 推断问题的分类# 14.1.1 推断任务的类型# 设概率图模型定义在变量集合 $\mathcal{V} = {X_1, X_2, \ldots, X_n}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：
查询变量（Query Variables）：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。 证据变量（Evidence Variables）：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。 隐变量（Hidden Variables）：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。 常见的推断任务包括：
(1) 边缘推断（Marginal Inference）# 计算查询变量的边缘概率分布：
$$ P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})} $$"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第14章 概率图模型 推断"><meta itemprop=description content="第14章 概率图模型：推断# “Probabilistic inference is nothing but counting, in appropriate ways.” — Judea Pearl
引言# 在概率图模型中，推断（Inference） 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。
本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。
14.1 推断问题的分类# 14.1.1 推断任务的类型# 设概率图模型定义在变量集合 $\mathcal{V} = {X_1, X_2, \ldots, X_n}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：
查询变量（Query Variables）：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。 证据变量（Evidence Variables）：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。 隐变量（Hidden Variables）：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。 常见的推断任务包括：
(1) 边缘推断（Marginal Inference）# 计算查询变量的边缘概率分布：
$$ P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})} $$"><meta itemprop=wordCount content="1318"><title>第14章 概率图模型 推断 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/ class=active>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第14章 概率图模型 推断</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#141-推断问题的分类>14.1 推断问题的分类</a><ul><li><a href=#1411-推断任务的类型>14.1.1 推断任务的类型</a><ul><li><a href=#1-边缘推断marginal-inference>(1) 边缘推断（Marginal Inference）</a></li><li><a href=#2-最大后验概率推断map-inference>(2) 最大后验概率推断（MAP Inference）</a></li></ul></li><li><a href=#1412-精确推断-vs-近似推断>14.1.2 精确推断 vs 近似推断</a></li></ul></li><li><a href=#142-变量消除算法variable-elimination>14.2 变量消除算法（Variable Elimination）</a><ul><li><a href=#1421-核心思想>14.2.1 核心思想</a></li><li><a href=#1422-算法流程>14.2.2 算法流程</a></li><li><a href=#1423-算法示例>14.2.3 算法示例</a></li><li><a href=#1424-计算复杂度与树宽>14.2.4 计算复杂度与树宽</a></li></ul></li><li><a href=#143-信念传播belief-propagation>14.3 信念传播（Belief Propagation）</a><ul><li><a href=#1431-消息传递机制>14.3.1 消息传递机制</a></li><li><a href=#1432-sum-product-算法边缘推断>14.3.2 Sum-Product 算法（边缘推断）</a></li><li><a href=#1433-max-product-算法map推断>14.3.3 Max-Product 算法（MAP推断）</a></li><li><a href=#1434-树结构-vs-有环图>14.3.4 树结构 vs 有环图</a><ul><li><a href=#树结构的优势>树结构的优势</a></li><li><a href=#loopy-belief-propagation有环图>Loopy Belief Propagation（有环图）</a></li></ul></li></ul></li><li><a href=#144-junction-tree-算法汇合树算法>14.4 Junction Tree 算法（汇合树算法）</a><ul><li><a href=#1441-动机与思想>14.4.1 动机与思想</a></li><li><a href=#1442-算法流程>14.4.2 算法流程</a></li><li><a href=#1443-running-intersection-property-rip>14.4.3 Running Intersection Property (RIP)</a></li><li><a href=#1444-算法复杂度>14.4.4 算法复杂度</a></li></ul></li><li><a href=#145-推断算法的比较>14.5 推断算法的比较</a></li><li><a href=#146-推断问题的计算复杂度>14.6 推断问题的计算复杂度</a><ul><li><a href=#1461-理论结果>14.6.1 理论结果</a></li><li><a href=#1462-可处理的特殊情况>14.6.2 可处理的特殊情况</a></li></ul></li><li><a href=#147-案例分析隐马尔可夫模型的推断>14.7 案例分析：隐马尔可夫模型的推断</a><ul><li><a href=#1471-问题描述>14.7.1 问题描述</a></li><li><a href=#1472-前向算法>14.7.2 前向算法</a></li><li><a href=#1473-viterbi-算法>14.7.3 Viterbi 算法</a></li></ul></li><li><a href=#148-近似推断简介>14.8 近似推断简介</a><ul><li><a href=#1481-采样方法sampling-based-inference>14.8.1 采样方法（Sampling-based Inference）</a></li><li><a href=#1482-变分推断variational-inference>14.8.2 变分推断（Variational Inference）</a></li><li><a href=#1483-loopy-bp-的应用>14.8.3 Loopy BP 的应用</a></li></ul></li><li><a href=#149-本章小结>14.9 本章小结</a></li><li><a href=#习题>习题</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第14章-概率图模型推断>第14章 概率图模型：推断<a class=anchor href=#%e7%ac%ac14%e7%ab%a0-%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%e6%8e%a8%e6%96%ad>#</a></h1><blockquote class=book-hint><p>&ldquo;Probabilistic inference is nothing but counting, in appropriate ways.&rdquo; — Judea Pearl</p></blockquote><h2 id=引言>引言<a class=anchor href=#%e5%bc%95%e8%a8%80>#</a></h2><p>在概率图模型中，<strong>推断（Inference）</strong> 是指基于观测变量的取值，计算未观测变量的概率分布或最可能的取值。推断是概率图模型最核心的任务之一，广泛应用于模式识别、计算机视觉、自然语言处理、因果推理等领域。</p><p>本章将系统介绍概率图模型中的推断问题及其求解算法，包括精确推断（变量消除、信念传播、Junction Tree）和近似推断的基本思想。</p><hr><h2 id=141-推断问题的分类>14.1 推断问题的分类<a class=anchor href=#141-%e6%8e%a8%e6%96%ad%e9%97%ae%e9%a2%98%e7%9a%84%e5%88%86%e7%b1%bb>#</a></h2><h3 id=1411-推断任务的类型>14.1.1 推断任务的类型<a class=anchor href=#1411-%e6%8e%a8%e6%96%ad%e4%bb%bb%e5%8a%a1%e7%9a%84%e7%b1%bb%e5%9e%8b>#</a></h3><p>设概率图模型定义在变量集合 $\mathcal{V} = {X_1, X_2, \ldots, X_n}$ 上，联合概率分布为 $P(\mathcal{V})$。将变量分为：</p><ul><li><strong>查询变量（Query Variables）</strong>：$\mathcal{Q} \subseteq \mathcal{V}$，我们希望推断的变量。</li><li><strong>证据变量（Evidence Variables）</strong>：$\mathcal{E} \subseteq \mathcal{V}$，已观测到的变量，取值为 $\mathbf{e}$。</li><li><strong>隐变量（Hidden Variables）</strong>：$\mathcal{H} = \mathcal{V} \setminus (\mathcal{Q} \cup \mathcal{E})$，既非查询也非证据的变量。</li></ul><p>常见的推断任务包括：</p><h4 id=1-边缘推断marginal-inference>(1) 边缘推断（Marginal Inference）<a class=anchor href=#1-%e8%be%b9%e7%bc%98%e6%8e%a8%e6%96%admarginal-inference>#</a></h4><p>计算查询变量的<strong>边缘概率分布</strong>：</p><p>$$
P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \frac{P(\mathcal{Q}, \mathcal{E} = \mathbf{e})}{P(\mathcal{E} = \mathbf{e})} = \frac{\sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}{\sum_{\mathcal{Q}, \mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})}
$$</p><p><strong>示例</strong>：在医疗诊断中，给定症状（证据），计算患某种疾病的概率分布。</p><h4 id=2-最大后验概率推断map-inference>(2) 最大后验概率推断（MAP Inference）<a class=anchor href=#2-%e6%9c%80%e5%a4%a7%e5%90%8e%e9%aa%8c%e6%a6%82%e7%8e%87%e6%8e%a8%e6%96%admap-inference>#</a></h4><p>寻找使后验概率最大的变量赋值：</p><p>$$
\mathbf{q}^* = \arg\max_{\mathcal{Q}} P(\mathcal{Q} | \mathcal{E} = \mathbf{e}) = \arg\max_{\mathcal{Q}} \sum_{\mathcal{H}} P(\mathcal{Q}, \mathcal{H}, \mathcal{E} = \mathbf{e})
$$</p><p><strong>示例</strong>：在图像分割中，给定图像观测，找到最可能的像素标签配置。</p><p><strong>特例</strong>：当 $\mathcal{Q} = \mathcal{V} \setminus \mathcal{E}$（即对所有未观测变量求MAP）时，称为 <strong>最大概率解释（MPE, Most Probable Explanation）</strong>：</p><p>$$
\mathbf{v}^* = \arg\max_{\mathcal{V} \setminus \mathcal{E}} P(\mathcal{V} \setminus \mathcal{E} | \mathcal{E} = \mathbf{e})
$$</p><h3 id=1412-精确推断-vs-近似推断>14.1.2 精确推断 vs 近似推断<a class=anchor href=#1412-%e7%b2%be%e7%a1%ae%e6%8e%a8%e6%96%ad-vs-%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad>#</a></h3><ul><li><p><strong>精确推断（Exact Inference）</strong>：</p><ul><li>计算推断问题的精确解。</li><li>适用于树结构图或图结构简单（treewidth小）的情况。</li><li>代表算法：变量消除、信念传播、Junction Tree。</li></ul></li><li><p><strong>近似推断（Approximate Inference）</strong>：</p><ul><li>当精确推断计算复杂度过高时（NP-hard），使用近似方法。</li><li>代表方法：<ul><li><strong>采样方法</strong>：MCMC（马尔可夫链蒙特卡洛）、重要性采样。</li><li><strong>变分推断</strong>：将推断问题转化为优化问题。</li><li><strong>Loopy Belief Propagation</strong>：在有环图上运行信念传播（不保证收敛到精确解）。</li></ul></li></ul></li></ul><p><strong>复杂度说明</strong>：</p><ul><li>边缘推断和MAP推断在一般图结构上均为 <strong>NP-hard</strong>。</li><li>精确推断的复杂度依赖于图的 <strong>树宽（Treewidth）</strong>，复杂度为 $O(n \cdot k^{w+1})$，其中 $n$ 为变量数，$k$ 为变量状态数，$w$ 为树宽。</li></ul><hr><h2 id=142-变量消除算法variable-elimination>14.2 变量消除算法（Variable Elimination）<a class=anchor href=#142-%e5%8f%98%e9%87%8f%e6%b6%88%e9%99%a4%e7%ae%97%e6%b3%95variable-elimination>#</a></h2><h3 id=1421-核心思想>14.2.1 核心思想<a class=anchor href=#1421-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p><strong>变量消除（Variable Elimination, VE）</strong> 是一种利用概率分布的因子分解结构，通过动态规划思想高效计算边缘概率的算法。</p><p>其核心思想是：</p><ol><li>将联合概率分布表示为若干**因子（Factors）**的乘积。</li><li>按一定顺序逐个<strong>消除（边缘化）隐变量</strong>。</li><li>利用<strong>分配律</strong>，将求和操作"推入"乘积内部，避免对所有变量的联合状态求和。</li></ol><h3 id=1422-算法流程>14.2.2 算法流程<a class=anchor href=#1422-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>#</a></h3><p><strong>问题</strong>：计算 $P(X_Q | \mathbf{e})$，其中 $\mathbf{e}$ 为证据。</p><p><strong>步骤</strong>：</p><ol><li><p><strong>因子初始化</strong>：</p><ul><li>将联合分布分解为因子乘积：$P(\mathbf{X}) = \prod_{i} \phi_i(\mathbf{D}_i)$，其中 $\mathbf{D}_i$ 为因子 $\phi_i$ 的作用域。</li><li>固定证据变量：将证据 $\mathbf{e}$ 代入因子，得到简化因子 $\phi_i&rsquo;(\mathbf{D}_i \setminus \mathcal{E})$。</li></ul></li><li><p><strong>选择消除顺序</strong>：</p><ul><li>确定隐变量的消除顺序 $\pi = (H_1, H_2, \ldots, H_m)$。</li></ul></li><li><p><strong>逐个消除变量</strong>：</p><ul><li>对每个隐变量 $H_i$：<ul><li>收集所有包含 $H_i$ 的因子：$\Phi_i = {\phi_j : H_i \in \mathbf{D}_j}$。</li><li>计算中间因子：$\tau_i = \sum_{H_i} \prod_{\phi_j \in \Phi_i} \phi_j$。</li><li>用 $\tau_i$ 替换 $\Phi_i$ 中的因子。</li></ul></li></ul></li><li><p><strong>归一化</strong>：</p><ul><li>最终得到关于查询变量 $X_Q$ 的非归一化分布 $\tilde{P}(X_Q, \mathbf{e})$。</li><li>归一化：$P(X_Q | \mathbf{e}) = \frac{\tilde{P}(X_Q, \mathbf{e})}{\sum_{X_Q} \tilde{P}(X_Q, \mathbf{e})}$。</li></ul></li></ol><h3 id=1423-算法示例>14.2.3 算法示例<a class=anchor href=#1423-%e7%ae%97%e6%b3%95%e7%a4%ba%e4%be%8b>#</a></h3><p>考虑链式贝叶斯网络：$X_1 \to X_2 \to X_3 \to X_4$，联合分布为：</p><p>$$
P(X_1, X_2, X_3, X_4) = P(X_1) P(X_2|X_1) P(X_3|X_2) P(X_4|X_3)
$$</p><p><strong>任务</strong>：计算 $P(X_4)$。</p><p><strong>朴素方法</strong>（枚举）：</p><p>$$
P(X_4) = \sum_{X_1, X_2, X_3} P(X_1) P(X_2|X_1) P(X_3|X_2) P(X_4|X_3)
$$</p><p>需要枚举 $k^3$ 种状态组合（$k$ 为每个变量的状态数）。</p><p><strong>变量消除</strong>：</p><p>消除顺序为 $X_1, X_2, X_3$：</p><ol><li><p>消除 $X_1$：
$$
\tau_1(X_2) = \sum_{X_1} P(X_1) P(X_2|X_1)
$$</p></li><li><p>消除 $X_2$：
$$
\tau_2(X_3) = \sum_{X_2} \tau_1(X_2) P(X_3|X_2)
$$</p></li><li><p>消除 $X_3$：
$$
P(X_4) = \sum_{X_3} \tau_2(X_3) P(X_4|X_3)
$$</p></li></ol><p><strong>复杂度</strong>：每步仅需枚举 $O(k^2)$ 种状态，总复杂度为 $O(k^2)$，远小于朴素方法的 $O(k^3)$。</p><h3 id=1424-计算复杂度与树宽>14.2.4 计算复杂度与树宽<a class=anchor href=#1424-%e8%ae%a1%e7%ae%97%e5%a4%8d%e6%9d%82%e5%ba%a6%e4%b8%8e%e6%a0%91%e5%ae%bd>#</a></h3><p><strong>关键观察</strong>：变量消除的复杂度取决于<strong>中间因子的作用域大小</strong>。</p><ul><li>每次消除变量 $H_i$ 时，新因子 $\tau_i$ 的作用域为所有包含 $H_i$ 的因子作用域的并集（去掉 $H_i$）。</li><li>中间因子的最大作用域大小记为 <strong>induced width</strong>，与图的 <strong>树宽（Treewidth）</strong> 密切相关。</li></ul><p><strong>树宽定义</strong>：</p><p>图 $G$ 的树宽 $w$ 是最优三角化（triangulation）后，最大团的大小减1。</p><p><strong>复杂度定理</strong>：</p><p>变量消除算法的时间复杂度为：</p><p>$$
O(n \cdot k^{w+1})
$$</p><p>其中 $n$ 为变量数，$k$ 为变量状态数，$w$ 为树宽。</p><p><strong>消除顺序的影响</strong>：</p><ul><li>不同的消除顺序会产生不同的中间因子，影响计算复杂度。</li><li>寻找<strong>最优消除顺序</strong>（最小化induced width）是NP-hard问题。</li><li>实践中使用启发式方法（如最小填充、最小度数）选择消除顺序。</li></ul><hr><h2 id=143-信念传播belief-propagation>14.3 信念传播（Belief Propagation）<a class=anchor href=#143-%e4%bf%a1%e5%bf%b5%e4%bc%a0%e6%92%adbelief-propagation>#</a></h2><h3 id=1431-消息传递机制>14.3.1 消息传递机制<a class=anchor href=#1431-%e6%b6%88%e6%81%af%e4%bc%a0%e9%80%92%e6%9c%ba%e5%88%b6>#</a></h3><p><strong>信念传播（Belief Propagation, BP）</strong> 是一种基于 <strong>消息传递（Message Passing）</strong> 的推断算法，特别适用于树结构的图模型。</p><p><strong>核心思想</strong>：</p><ul><li>图中的节点通过传递**消息（Messages）**来交换信息。</li><li>每个节点根据接收到的消息计算其<strong>信念（Belief）</strong>，即边缘概率分布。</li></ul><p><strong>消息定义</strong>：</p><p>节点 $i$ 向邻居节点 $j$ 发送的消息 $m_{i \to j}(X_j)$ 表示：节点 $i$ 及其子树对 $X_j$ 的"看法"。</p><h3 id=1432-sum-product-算法边缘推断>14.3.2 Sum-Product 算法（边缘推断）<a class=anchor href=#1432-sum-product-%e7%ae%97%e6%b3%95%e8%be%b9%e7%bc%98%e6%8e%a8%e6%96%ad>#</a></h3><p><strong>目标</strong>：计算每个变量的边缘概率分布 $P(X_i)$。</p><p><strong>算法描述</strong>（针对树结构的无向图）：</p><p>设无向树 $T = (V, E)$，因子分解为：</p><p>$$
P(\mathbf{X}) = \frac{1}{Z} \prod_{(i,j) \in E} \psi_{ij}(X_i, X_j) \prod_{i \in V} \phi_i(X_i)
$$</p><p><strong>消息传递规则</strong>：</p><p>节点 $i$ 向邻居节点 $j$ 发送消息：</p><p>$$
m_{i \to j}(X_j) = \sum_{X_i} \psi_{ij}(X_i, X_j) \phi_i(X_i) \prod_{k \in \mathcal{N}(i) \setminus {j}} m_{k \to i}(X_i)
$$</p><p>其中 $\mathcal{N}(i)$ 为节点 $i$ 的邻居集合。</p><p><strong>信念计算</strong>：</p><p>节点 $i$ 的信念（边缘分布）为：</p><p>$$
b_i(X_i) \propto \phi_i(X_i) \prod_{k \in \mathcal{N}(i)} m_{k \to i}(X_i)
$$</p><p>归一化后得到 $P(X_i)$。</p><p><strong>执行流程</strong>：</p><ol><li><strong>选择根节点</strong>：在树上选择任意节点作为根。</li><li><strong>从叶到根传递消息</strong>：按拓扑顺序，从叶节点向根节点传递消息。</li><li><strong>从根到叶传递消息</strong>：从根节点向叶节点传递消息。</li><li><strong>计算信念</strong>：每个节点根据收到的消息计算边缘分布。</li></ol><p><strong>算法示意图（占位符）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-svg data-lang=svg><span class=line><span class=cl><span class=c>&lt;!-- SVG Placeholder: Sum-Product Algorithm Message Passing on Tree --&gt;</span>
</span></span><span class=line><span class=cl><span class=c>&lt;!-- 绘制树结构，标注消息方向（双向箭头），节点表示变量，边标注消息m_{i-&gt;j} --&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;svg</span> <span class=na>xmlns=</span><span class=s>&#34;http://www.w3.org/2000/svg&#34;</span> <span class=na>viewBox=</span><span class=s>&#34;0 0 400 300&#34;</span><span class=nt>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;text</span> <span class=na>x=</span><span class=s>&#34;200&#34;</span> <span class=na>y=</span><span class=s>&#34;150&#34;</span> <span class=na>text-anchor=</span><span class=s>&#34;middle&#34;</span> <span class=na>font-size=</span><span class=s>&#34;14&#34;</span><span class=nt>&gt;</span>Sum-Product Message Passing Diagram<span class=nt>&lt;/text&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;text</span> <span class=na>x=</span><span class=s>&#34;200&#34;</span> <span class=na>y=</span><span class=s>&#34;170&#34;</span> <span class=na>text-anchor=</span><span class=s>&#34;middle&#34;</span> <span class=na>font-size=</span><span class=s>&#34;12&#34;</span><span class=nt>&gt;</span>(Tree structure with bidirectional messages)<span class=nt>&lt;/text&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/svg&gt;</span></span></span></code></pre></div><p><strong>定理（Sum-Product 正确性）</strong>：</p><p>在<strong>树结构</strong>的图模型上，Sum-Product 算法经过两轮消息传递后，计算得到的信念 $b_i(X_i)$ 等于精确的边缘概率 $P(X_i)$。</p><h3 id=1433-max-product-算法map推断>14.3.3 Max-Product 算法（MAP推断）<a class=anchor href=#1433-max-product-%e7%ae%97%e6%b3%95map%e6%8e%a8%e6%96%ad>#</a></h3><p><strong>目标</strong>：寻找最大概率配置（MPE）：</p><p>$$
\mathbf{x}^* = \arg\max_{\mathbf{X}} P(\mathbf{X})
$$</p><p><strong>算法描述</strong>：</p><p>将 Sum-Product 算法中的 <strong>求和（$\sum$）</strong> 替换为 <strong>求最大值（$\max$）</strong>。</p><p><strong>消息传递规则</strong>：</p><p>$$
m_{i \to j}(X_j) = \max_{X_i} \left[ \psi_{ij}(X_i, X_j) \phi_i(X_i) \prod_{k \in \mathcal{N}(i) \setminus {j}} m_{k \to i}(X_i) \right]
$$</p><p><strong>最优配置计算</strong>：</p><ol><li>运行 Max-Product 算法传递消息。</li><li>在根节点找到最优取值：
$$
x_{\text{root}}^* = \arg\max_{X_{\text{root}}} \left[ \phi_{\text{root}}(X_{\text{root}}) \prod_{k \in \mathcal{N}(\text{root})} m_{k \to \text{root}}(X_{\text{root}}) \right]
$$</li><li>通过回溯（backtracking）得到其他节点的最优取值。</li></ol><p><strong>对数空间优化（Max-Sum）</strong>：</p><p>为避免数值下溢，通常在对数空间进行计算，将乘法转化为加法：</p><p>$$
\log m_{i \to j}(X_j) = \max_{X_i} \left[ \log \psi_{ij}(X_i, X_j) + \log \phi_i(X_i) + \sum_{k \in \mathcal{N}(i) \setminus {j}} \log m_{k \to i}(X_i) \right]
$$</p><h3 id=1434-树结构-vs-有环图>14.3.4 树结构 vs 有环图<a class=anchor href=#1434-%e6%a0%91%e7%bb%93%e6%9e%84-vs-%e6%9c%89%e7%8e%af%e5%9b%be>#</a></h3><h4 id=树结构的优势>树结构的优势<a class=anchor href=#%e6%a0%91%e7%bb%93%e6%9e%84%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h4><ul><li><strong>无环性保证收敛</strong>：消息传递在有限轮次内终止。</li><li><strong>计算精确</strong>：Sum-Product 和 Max-Product 算法在树上给出精确解。</li><li><strong>复杂度线性</strong>：时间复杂度为 $O(|E| \cdot k^2)$，其中 $|E|$ 为边数，$k$ 为变量状态数。</li></ul><h4 id=loopy-belief-propagation有环图>Loopy Belief Propagation（有环图）<a class=anchor href=#loopy-belief-propagation%e6%9c%89%e7%8e%af%e5%9b%be>#</a></h4><p>当图中存在环时，直接应用 BP 算法称为 <strong>Loopy BP</strong>：</p><ul><li><strong>迭代执行</strong>：反复传递消息直至收敛（或达到最大迭代次数）。</li><li><strong>不保证收敛</strong>：消息可能在环上震荡，不收敛。</li><li><strong>不保证精确</strong>：即使收敛，结果也可能是近似解。</li><li><strong>实践效果</strong>：在许多应用中（如LDPC译码、图像去噪），Loopy BP 效果良好。</li></ul><p><strong>Loopy BP 的理论理解</strong>：</p><ul><li>可以视为对 <strong>Bethe 自由能（Bethe Free Energy）</strong> 的优化。</li><li>在某些条件下（如单峰分布、弱耦合），Loopy BP 的不动点对应局部最优解。</li></ul><p><strong>算法流程</strong>：</p><ol><li>初始化所有消息为均匀分布。</li><li>重复以下步骤直至收敛：<ul><li>对每条边 $(i, j)$，按上述规则更新消息 $m_{i \to j}(X_j)$。</li><li>检查消息变化是否小于阈值。</li></ul></li><li>根据最终消息计算信念。</li></ol><p><strong>收敛性判据</strong>：</p><p>$$
\max_{i,j} | m_{i \to j}^{(t+1)} - m_{i \to j}^{(t)} | &lt; \epsilon
$$</p><hr><h2 id=144-junction-tree-算法汇合树算法>14.4 Junction Tree 算法（汇合树算法）<a class=anchor href=#144-junction-tree-%e7%ae%97%e6%b3%95%e6%b1%87%e5%90%88%e6%a0%91%e7%ae%97%e6%b3%95>#</a></h2><h3 id=1441-动机与思想>14.4.1 动机与思想<a class=anchor href=#1441-%e5%8a%a8%e6%9c%ba%e4%b8%8e%e6%80%9d%e6%83%b3>#</a></h3><p><strong>问题</strong>：变量消除和信念传播在有环图上效率低或不精确。</p><p><strong>Junction Tree 算法</strong> 将任意图转化为树结构（团树），然后在树上进行精确推断。</p><p><strong>核心思想</strong>：</p><ol><li><strong>构造团树（Clique Tree）</strong>：将原图中的变量分组为"团（Cliques）"，团之间形成树结构。</li><li><strong>满足 Running Intersection Property（RIP）</strong>：保证团树能够正确表示原图的联合分布。</li><li><strong>在团树上运行信念传播</strong>：团之间传递消息，计算边缘概率。</li></ol><h3 id=1442-算法流程>14.4.2 算法流程<a class=anchor href=#1442-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>#</a></h3><p><strong>步骤</strong>：</p><ol><li><p><strong>道德化（Moralization）</strong>（针对有向图）：</p><ul><li>将有向图转化为无向图：连接所有父节点，去掉边的方向。</li></ul></li><li><p><strong>三角化（Triangulation）</strong>：</p><ul><li>在无向图中添加边，使得图中不存在长度大于3的无弦环。</li><li>目标：最小化最大团的大小（树宽）。</li></ul></li><li><p><strong>构造团树</strong>：</p><ul><li>识别所有最大团（maximal cliques）。</li><li>构造团树，使得满足 <strong>Running Intersection Property</strong>：<ul><li>对于任意变量 $X$，所有包含 $X$ 的团在团树上形成连通子树。</li></ul></li></ul></li><li><p><strong>初始化团势函数（Clique Potentials）</strong>：</p><ul><li>将原图的因子分配到包含其作用域的团上。</li></ul></li><li><p><strong>消息传递</strong>：</p><ul><li>在团树上运行 Sum-Product 算法。</li><li>团之间通过**分隔符（Separator）**传递消息。</li></ul></li><li><p><strong>计算边缘概率</strong>：</p><ul><li>从团的势函数中提取查询变量的边缘分布。</li></ul></li></ol><h3 id=1443-running-intersection-property-rip>14.4.3 Running Intersection Property (RIP)<a class=anchor href=#1443-running-intersection-property-rip>#</a></h3><p><strong>定义</strong>：</p><p>团树 $T = (C, E)$ 满足 RIP，当且仅当：</p><p>$$
\forall X \in \mathcal{V}, \quad {C_i : X \in C_i} \text{ 在 } T \text{ 上形成连通子树}
$$</p><p><strong>意义</strong>：</p><ul><li>RIP 保证团树能够一致地表示原图的联合分布。</li><li>消息传递时，变量的信息能够正确传播到所有相关团。</li></ul><h3 id=1444-算法复杂度>14.4.4 算法复杂度<a class=anchor href=#1444-%e7%ae%97%e6%b3%95%e5%a4%8d%e6%9d%82%e5%ba%a6>#</a></h3><p><strong>时间复杂度</strong>：</p><p>$$
O(|C| \cdot k^{w+1})
$$</p><p>其中 $|C|$ 为团的数量，$w$ 为树宽（最大团大小减1），$k$ 为变量状态数。</p><p><strong>空间复杂度</strong>：</p><p>$$
O(|C| \cdot k^{w+1})
$$</p><p>需要存储每个团的势函数。</p><p><strong>优化</strong>：</p><ul><li>使用稀疏表示存储势函数。</li><li>采用启发式三角化算法（如最小填充、最小度数）降低树宽。</li></ul><p><strong>Junction Tree 算法示意图（占位符）</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-svg data-lang=svg><span class=line><span class=cl><span class=c>&lt;!-- SVG Placeholder: Junction Tree Construction --&gt;</span>
</span></span><span class=line><span class=cl><span class=c>&lt;!-- 绘制原图 -&gt; 道德化 -&gt; 三角化 -&gt; 团树 的转换过程 --&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;svg</span> <span class=na>xmlns=</span><span class=s>&#34;http://www.w3.org/2000/svg&#34;</span> <span class=na>viewBox=</span><span class=s>&#34;0 0 500 300&#34;</span><span class=nt>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;text</span> <span class=na>x=</span><span class=s>&#34;250&#34;</span> <span class=na>y=</span><span class=s>&#34;150&#34;</span> <span class=na>text-anchor=</span><span class=s>&#34;middle&#34;</span> <span class=na>font-size=</span><span class=s>&#34;14&#34;</span><span class=nt>&gt;</span>Junction Tree Algorithm Pipeline<span class=nt>&lt;/text&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;text</span> <span class=na>x=</span><span class=s>&#34;250&#34;</span> <span class=na>y=</span><span class=s>&#34;170&#34;</span> <span class=na>text-anchor=</span><span class=s>&#34;middle&#34;</span> <span class=na>font-size=</span><span class=s>&#34;12&#34;</span><span class=nt>&gt;</span>(Original Graph → Moralization → Triangulation → Clique Tree)<span class=nt>&lt;/text&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/svg&gt;</span></span></span></code></pre></div><hr><h2 id=145-推断算法的比较>14.5 推断算法的比较<a class=anchor href=#145-%e6%8e%a8%e6%96%ad%e7%ae%97%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83>#</a></h2><table><thead><tr><th>算法</th><th>适用图结构</th><th>精确性</th><th>复杂度</th><th>优缺点</th></tr></thead><tbody><tr><td><strong>变量消除</strong></td><td>任意图</td><td>精确</td><td>$O(n \cdot k^{w+1})$</td><td>简单直观；需选择消除顺序；每次查询需重新计算</td></tr><tr><td><strong>Sum-Product BP</strong></td><td>树结构</td><td>精确</td><td>$O(|E| \cdot k^2)$</td><td>高效；仅适用于树</td></tr><tr><td><strong>Loopy BP</strong></td><td>有环图</td><td>近似</td><td>$O(T \cdot |E| \cdot k^2)$</td><td>易实现；不保证收敛或精确；实践中效果较好</td></tr><tr><td><strong>Junction Tree</strong></td><td>任意图</td><td>精确</td><td>$O(|C| \cdot k^{w+1})$</td><td>通用；支持多次查询；构造团树有开销</td></tr></tbody></table><p><strong>选择建议</strong>：</p><ul><li><strong>树结构</strong>：首选信念传播（BP）。</li><li><strong>小树宽的图</strong>：Junction Tree 或变量消除。</li><li><strong>大树宽的图</strong>：使用近似推断（Loopy BP、MCMC、变分推断）。</li><li><strong>单次查询</strong>：变量消除。</li><li><strong>多次查询</strong>：Junction Tree（预计算团树）。</li></ul><hr><h2 id=146-推断问题的计算复杂度>14.6 推断问题的计算复杂度<a class=anchor href=#146-%e6%8e%a8%e6%96%ad%e9%97%ae%e9%a2%98%e7%9a%84%e8%ae%a1%e7%ae%97%e5%a4%8d%e6%9d%82%e5%ba%a6>#</a></h2><h3 id=1461-理论结果>14.6.1 理论结果<a class=anchor href=#1461-%e7%90%86%e8%ae%ba%e7%bb%93%e6%9e%9c>#</a></h3><p><strong>定理</strong>：</p><ul><li>**边缘推断（Marginal Inference）**在一般贝叶斯网络上是 <strong>#P-complete</strong>（计数复杂度类）。</li><li><strong>MAP 推断</strong>在一般贝叶斯网络上是 <strong>NP-hard</strong>。</li></ul><p><strong>含义</strong>：</p><ul><li>精确推断在最坏情况下不存在多项式时间算法（除非P=NP）。</li><li>实际应用中需根据图的结构选择合适算法。</li></ul><h3 id=1462-可处理的特殊情况>14.6.2 可处理的特殊情况<a class=anchor href=#1462-%e5%8f%af%e5%a4%84%e7%90%86%e7%9a%84%e7%89%b9%e6%ae%8a%e6%83%85%e5%86%b5>#</a></h3><p>以下图结构的推断问题可在多项式时间内求解：</p><ol><li><strong>树结构</strong>：$O(nk^2)$。</li><li><strong>固定树宽的图</strong>：$O(n \cdot k^{w+1})$，当 $w$ 为常数时为多项式。</li><li><strong>链式模型</strong>：$O(nk^2)$（隐马尔可夫模型）。</li><li><strong>二分图的完美匹配结构</strong>：某些特殊因子图。</li></ol><hr><h2 id=147-案例分析隐马尔可夫模型的推断>14.7 案例分析：隐马尔可夫模型的推断<a class=anchor href=#147-%e6%a1%88%e4%be%8b%e5%88%86%e6%9e%90%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%8e%a8%e6%96%ad>#</a></h2><h3 id=1471-问题描述>14.7.1 问题描述<a class=anchor href=#1471-%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0>#</a></h3><p><strong>隐马尔可夫模型（HMM）</strong>：</p><ul><li>隐状态序列：$Z_1, Z_2, \ldots, Z_T$（Markov链）。</li><li>观测序列：$X_1, X_2, \ldots, X_T$（每个 $X_t$ 仅依赖于 $Z_t$）。</li></ul><p>联合分布：</p><p>$$
P(Z_{1:T}, X_{1:T}) = P(Z_1) \prod_{t=2}^{T} P(Z_t | Z_{t-1}) \prod_{t=1}^{T} P(X_t | Z_t)
$$</p><p><strong>推断任务</strong>：</p><ol><li><strong>前向算法（Forward Algorithm）</strong>：计算 $P(Z_t | X_{1:t})$。</li><li><strong>后向算法（Backward Algorithm）</strong>：计算 $P(X_{t+1:T} | Z_t)$。</li><li><strong>前向-后向算法（Forward-Backward）</strong>：计算 $P(Z_t | X_{1:T})$（平滑）。</li><li><strong>Viterbi 算法</strong>：计算最可能的隐状态序列（MAP）。</li></ol><h3 id=1472-前向算法>14.7.2 前向算法<a class=anchor href=#1472-%e5%89%8d%e5%90%91%e7%ae%97%e6%b3%95>#</a></h3><p><strong>定义前向概率</strong>：</p><p>$$
\alpha_t(z_t) = P(Z_t = z_t, X_{1:t})
$$</p><p><strong>递推公式</strong>：</p><p>$$
\alpha_t(z_t) = P(X_t | Z_t = z_t) \sum_{z_{t-1}} \alpha_{t-1}(z_{t-1}) P(Z_t = z_t | Z_{t-1} = z_{t-1})
$$</p><p><strong>初始化</strong>：</p><p>$$
\alpha_1(z_1) = P(Z_1 = z_1) P(X_1 | Z_1 = z_1)
$$</p><p><strong>算法对应</strong>：前向算法是变量消除算法在HMM上的特例，消除顺序为 $Z_1, Z_2, \ldots$。</p><h3 id=1473-viterbi-算法>14.7.3 Viterbi 算法<a class=anchor href=#1473-viterbi-%e7%ae%97%e6%b3%95>#</a></h3><p><strong>定义</strong>：</p><p>$$
\delta_t(z_t) = \max_{z_{1:t-1}} P(Z_{1:t-1}, Z_t = z_t, X_{1:t})
$$</p><p><strong>递推公式</strong>：</p><p>$$
\delta_t(z_t) = P(X_t | Z_t = z_t) \max_{z_{t-1}} \left[ \delta_{t-1}(z_{t-1}) P(Z_t = z_t | Z_{t-1} = z_{t-1}) \right]
$$</p><p><strong>回溯</strong>：</p><p>记录每步的最优前驱状态 $\psi_t(z_t)$，最后从 $z_T^* = \arg\max_{z_T} \delta_T(z_T)$ 回溯得到完整路径。</p><p><strong>算法对应</strong>：Viterbi 算法是 Max-Product 算法在链式结构上的应用。</p><hr><h2 id=148-近似推断简介>14.8 近似推断简介<a class=anchor href=#148-%e8%bf%91%e4%bc%bc%e6%8e%a8%e6%96%ad%e7%ae%80%e4%bb%8b>#</a></h2><p>当精确推断不可行时，需采用近似方法。以下简要介绍几种常用技术。</p><h3 id=1481-采样方法sampling-based-inference>14.8.1 采样方法（Sampling-based Inference）<a class=anchor href=#1481-%e9%87%87%e6%a0%b7%e6%96%b9%e6%b3%95sampling-based-inference>#</a></h3><p><strong>基本思想</strong>：通过从分布中抽取样本，估计边缘概率或期望。</p><p><strong>常用方法</strong>：</p><ol><li><p><strong>前向采样（Forward Sampling）</strong>：</p><ul><li>按拓扑顺序依次采样变量。</li><li>适用于无证据或证据较少的情况。</li></ul></li><li><p><strong>拒绝采样（Rejection Sampling）</strong>：</p><ul><li>采样后丢弃不满足证据的样本。</li><li>当证据概率很小时效率低。</li></ul></li><li><p><strong>重要性采样（Importance Sampling）</strong>：</p><ul><li>从提议分布 $Q$ 中采样，用重要性权重修正。</li><li>权重：$w(\mathbf{x}) = \frac{P(\mathbf{x})}{Q(\mathbf{x})}$。</li></ul></li><li><p><strong>马尔可夫链蒙特卡洛（MCMC）</strong>：</p><ul><li><strong>Gibbs 采样</strong>：逐个变量条件采样。</li><li><strong>Metropolis-Hastings</strong>：接受-拒绝机制。</li><li>收敛到平稳分布（目标分布）。</li></ul></li></ol><h3 id=1482-变分推断variational-inference>14.8.2 变分推断（Variational Inference）<a class=anchor href=#1482-%e5%8f%98%e5%88%86%e6%8e%a8%e6%96%advariational-inference>#</a></h3><p><strong>基本思想</strong>：将推断问题转化为优化问题。</p><p><strong>方法</strong>：</p><ul><li>用简单分布 $Q(\mathbf{X})$ 近似真实后验 $P(\mathbf{X} | \mathbf{e})$。</li><li>最小化 KL 散度：
$$
Q^* = \arg\min_{Q} \text{KL}(Q | P) = \arg\min_{Q} \int Q(\mathbf{X}) \log \frac{Q(\mathbf{X})}{P(\mathbf{X} | \mathbf{e})} d\mathbf{X}
$$</li></ul><p><strong>平均场近似（Mean Field Approximation）</strong>：</p><p>假设 $Q$ 完全分解：</p><p>$$
Q(\mathbf{X}) = \prod_{i} Q_i(X_i)
$$</p><p><strong>优化</strong>：坐标上升法，逐个更新 $Q_i(X_i)$：</p><p>$$
\log Q_i^*(X_i) = \mathbb{E}<em>{Q</em>{-i}}[\log P(\mathbf{X}, \mathbf{e})] + \text{const}
$$</p><p><strong>优点</strong>：</p><ul><li>计算速度快。</li><li>可扩展到大规模问题。</li></ul><p><strong>缺点</strong>：</p><ul><li>近似质量依赖于 $Q$ 的选择。</li><li>可能低估后验方差。</li></ul><h3 id=1483-loopy-bp-的应用>14.8.3 Loopy BP 的应用<a class=anchor href=#1483-loopy-bp-%e7%9a%84%e5%ba%94%e7%94%a8>#</a></h3><p>如前所述，Loopy BP 在有环图上作为近似推断方法广泛应用，特别是在：</p><ul><li><strong>纠错码译码</strong>（Turbo 码、LDPC 码）。</li><li><strong>计算机视觉</strong>（立体匹配、图像分割）。</li><li><strong>社交网络分析</strong>。</li></ul><hr><h2 id=149-本章小结>14.9 本章小结<a class=anchor href=#149-%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>本章系统介绍了概率图模型中的推断问题及其求解算法：</p><ol><li><p><strong>推断任务</strong>：</p><ul><li>边缘推断（计算后验分布）。</li><li>MAP 推断（寻找最可能配置）。</li></ul></li><li><p><strong>精确推断算法</strong>：</p><ul><li><strong>变量消除</strong>：利用因子分解和动态规划，复杂度依赖于消除顺序和树宽。</li><li><strong>信念传播</strong>：在树结构上通过消息传递高效计算边缘概率和MAP。</li><li><strong>Junction Tree</strong>：将任意图转化为团树，进行精确推断。</li></ul></li><li><p><strong>近似推断</strong>：</p><ul><li>Loopy BP：在有环图上运行信念传播。</li><li>采样方法：MCMC、重要性采样。</li><li>变分推断：将推断转化为优化问题。</li></ul></li><li><p><strong>计算复杂度</strong>：</p><ul><li>一般图上的推断是NP-hard问题。</li><li>树结构和小树宽图可高效求解。</li></ul></li></ol><p><strong>关键要点</strong>：</p><ul><li>推断算法的选择取决于图的结构、精度要求和计算资源。</li><li>精确推断在树和小树宽图上可行，一般图需要近似方法。</li><li>信念传播是理解和实现推断算法的核心框架。</li></ul><hr><h2 id=习题>习题<a class=anchor href=#%e4%b9%a0%e9%a2%98>#</a></h2><ol><li><p><strong>变量消除</strong>：</p><ul><li>对于链式贝叶斯网络 $X_1 \to X_2 \to \cdots \to X_n$，分析不同消除顺序的复杂度差异。</li></ul></li><li><p><strong>Sum-Product 算法</strong>：</p><ul><li>手动执行 Sum-Product 算法，计算如下树状贝叶斯网络的边缘概率 $P(X_3)$：
$$
X_1 \to X_2 \to X_3 \leftarrow X_4
$$</li></ul></li><li><p><strong>Loopy BP 实验</strong>：</p><ul><li>在包含单个环的图上运行 Loopy BP，观察消息传递的收敛性。</li></ul></li><li><p><strong>Junction Tree 构造</strong>：</p><ul><li>对给定的无向图进行三角化，构造 Junction Tree，并验证 Running Intersection Property。</li></ul></li><li><p><strong>HMM 推断</strong>：</p><ul><li>实现 Viterbi 算法，对观测序列进行最优隐状态解码。</li></ul></li><li><p><strong>变分推断</strong>：</p><ul><li>用平均场近似推导高斯混合模型的变分EM算法。</li></ul></li></ol><hr><h2 id=参考文献>参考文献<a class=anchor href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae>#</a></h2><ol><li><p><strong>Koller, D., & Friedman, N.</strong> (2009). <em>Probabilistic Graphical Models: Principles and Techniques</em>. MIT Press.</p><ul><li>权威教材，详细介绍推断算法。</li></ul></li><li><p><strong>Pearl, J.</strong> (1988). <em>Probabilistic Reasoning in Intelligent Systems</em>. Morgan Kaufmann.</p><ul><li>信念传播算法的开创性著作。</li></ul></li><li><p><strong>Wainwright, M. J., & Jordan, M. I.</strong> (2008). <em>Graphical Models, Exponential Families, and Variational Inference</em>. Foundations and Trends in Machine Learning.</p><ul><li>变分推断的深入分析。</li></ul></li><li><p><strong>Murphy, K. P.</strong> (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.</p><ul><li>第20-22章：推断与学习算法。</li></ul></li><li><p><strong>Bishop, C. M.</strong> (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p><ul><li>第8章：图模型与推断。</li></ul></li><li><p><strong>Yedidia, J. S., Freeman, W. T., & Weiss, Y.</strong> (2003). &ldquo;Understanding Belief Propagation and Its Generalizations&rdquo;. <em>Exploring Artificial Intelligence in the New Millennium</em>.</p><ul><li>Loopy BP 的理论分析。</li></ul></li></ol><hr><blockquote class=book-hint><p>&ldquo;The art of inference is the art of message passing.&rdquo; — Anonymous</p></blockquote><p><strong>下一章预告</strong>：第15章将介绍概率图模型的<strong>学习（Learning）</strong>，包括参数学习（极大似然估计、EM算法）和结构学习（评分搜索、约束方法）。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第13章 概率图模型 表示</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/ class="flex align-center"><span>skills</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#141-推断问题的分类>14.1 推断问题的分类</a><ul><li><a href=#1411-推断任务的类型>14.1.1 推断任务的类型</a><ul><li><a href=#1-边缘推断marginal-inference>(1) 边缘推断（Marginal Inference）</a></li><li><a href=#2-最大后验概率推断map-inference>(2) 最大后验概率推断（MAP Inference）</a></li></ul></li><li><a href=#1412-精确推断-vs-近似推断>14.1.2 精确推断 vs 近似推断</a></li></ul></li><li><a href=#142-变量消除算法variable-elimination>14.2 变量消除算法（Variable Elimination）</a><ul><li><a href=#1421-核心思想>14.2.1 核心思想</a></li><li><a href=#1422-算法流程>14.2.2 算法流程</a></li><li><a href=#1423-算法示例>14.2.3 算法示例</a></li><li><a href=#1424-计算复杂度与树宽>14.2.4 计算复杂度与树宽</a></li></ul></li><li><a href=#143-信念传播belief-propagation>14.3 信念传播（Belief Propagation）</a><ul><li><a href=#1431-消息传递机制>14.3.1 消息传递机制</a></li><li><a href=#1432-sum-product-算法边缘推断>14.3.2 Sum-Product 算法（边缘推断）</a></li><li><a href=#1433-max-product-算法map推断>14.3.3 Max-Product 算法（MAP推断）</a></li><li><a href=#1434-树结构-vs-有环图>14.3.4 树结构 vs 有环图</a><ul><li><a href=#树结构的优势>树结构的优势</a></li><li><a href=#loopy-belief-propagation有环图>Loopy Belief Propagation（有环图）</a></li></ul></li></ul></li><li><a href=#144-junction-tree-算法汇合树算法>14.4 Junction Tree 算法（汇合树算法）</a><ul><li><a href=#1441-动机与思想>14.4.1 动机与思想</a></li><li><a href=#1442-算法流程>14.4.2 算法流程</a></li><li><a href=#1443-running-intersection-property-rip>14.4.3 Running Intersection Property (RIP)</a></li><li><a href=#1444-算法复杂度>14.4.4 算法复杂度</a></li></ul></li><li><a href=#145-推断算法的比较>14.5 推断算法的比较</a></li><li><a href=#146-推断问题的计算复杂度>14.6 推断问题的计算复杂度</a><ul><li><a href=#1461-理论结果>14.6.1 理论结果</a></li><li><a href=#1462-可处理的特殊情况>14.6.2 可处理的特殊情况</a></li></ul></li><li><a href=#147-案例分析隐马尔可夫模型的推断>14.7 案例分析：隐马尔可夫模型的推断</a><ul><li><a href=#1471-问题描述>14.7.1 问题描述</a></li><li><a href=#1472-前向算法>14.7.2 前向算法</a></li><li><a href=#1473-viterbi-算法>14.7.3 Viterbi 算法</a></li></ul></li><li><a href=#148-近似推断简介>14.8 近似推断简介</a><ul><li><a href=#1481-采样方法sampling-based-inference>14.8.1 采样方法（Sampling-based Inference）</a></li><li><a href=#1482-变分推断variational-inference>14.8.2 变分推断（Variational Inference）</a></li><li><a href=#1483-loopy-bp-的应用>14.8.3 Loopy BP 的应用</a></li></ul></li><li><a href=#149-本章小结>14.9 本章小结</a></li><li><a href=#习题>习题</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></aside></main></body></html>