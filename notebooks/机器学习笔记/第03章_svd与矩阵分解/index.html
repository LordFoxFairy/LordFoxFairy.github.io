<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第03章：SVD与矩阵分解# 核心思想：任何矩阵都可以看作"旋转-拉伸-旋转"的组合。SVD 是线性代数的终极武器。
前言# 如果说线性代数有皇冠,那么奇异值分解 (SVD) 就是皇冠上的明珠。Gilbert Strang 教授称其为"线性代数的顶峰"。
在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的万能钥匙。
本章我们将从几何变换的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为旋转、拉伸、再旋转。
目录# 引言:从圆到椭圆
1.1 矩阵变换的本质 1.2 特征值分解的局限 特征分解(EVD):对称矩阵的美学
2.1 谱定理(Spectral Theorem) 2.2 几何直觉 2.3 正定性:碗的形状 奇异值分解(SVD):万能钥匙
3.1 核心思想:让非方阵也能对角化 3.2 推导 SVD 3.3 SVD 的几何图景:旋转-拉伸-旋转 3.4 薄 SVD(Reduced SVD) 3.5 外积形式(Dyadic Expansion) 四个基本子空间的 SVD 视角
4.1 回顾:四个基本子空间 4.2 SVD 的完美切分 4.3 正交关系图 4.4 伪逆的几何意义 低秩近似:SVD 的杀手级应用
5.1 问题设定 5.2 Eckart-Young-Mirsky 定理 5.3 直觉:丢弃小奇异值 = 去噪 5.4 应用 1:图像压缩 5.5 应用 2:推荐系统与矩阵补全 5.6 应用 3:主成分分析(PCA) SVD 与 EVD 的联系
6.1 核心关系 6.2 特殊情况:对称矩阵 计算方法简述
7.1 直接方法(不推荐) 7.2 实际算法 总结
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第03章 SVD与矩阵分解"><meta property="og:description" content='第03章：SVD与矩阵分解# 核心思想：任何矩阵都可以看作"旋转-拉伸-旋转"的组合。SVD 是线性代数的终极武器。
前言# 如果说线性代数有皇冠,那么奇异值分解 (SVD) 就是皇冠上的明珠。Gilbert Strang 教授称其为"线性代数的顶峰"。
在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的万能钥匙。
本章我们将从几何变换的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为旋转、拉伸、再旋转。
目录# 引言:从圆到椭圆
1.1 矩阵变换的本质 1.2 特征值分解的局限 特征分解(EVD):对称矩阵的美学
2.1 谱定理(Spectral Theorem) 2.2 几何直觉 2.3 正定性:碗的形状 奇异值分解(SVD):万能钥匙
3.1 核心思想:让非方阵也能对角化 3.2 推导 SVD 3.3 SVD 的几何图景:旋转-拉伸-旋转 3.4 薄 SVD(Reduced SVD) 3.5 外积形式(Dyadic Expansion) 四个基本子空间的 SVD 视角
4.1 回顾:四个基本子空间 4.2 SVD 的完美切分 4.3 正交关系图 4.4 伪逆的几何意义 低秩近似:SVD 的杀手级应用
5.1 问题设定 5.2 Eckart-Young-Mirsky 定理 5.3 直觉:丢弃小奇异值 = 去噪 5.4 应用 1:图像压缩 5.5 应用 2:推荐系统与矩阵补全 5.6 应用 3:主成分分析(PCA) SVD 与 EVD 的联系
6.1 核心关系 6.2 特殊情况:对称矩阵 计算方法简述
7.1 直接方法(不推荐) 7.2 实际算法 总结'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第03章 SVD与矩阵分解"><meta itemprop=description content='第03章：SVD与矩阵分解# 核心思想：任何矩阵都可以看作"旋转-拉伸-旋转"的组合。SVD 是线性代数的终极武器。
前言# 如果说线性代数有皇冠,那么奇异值分解 (SVD) 就是皇冠上的明珠。Gilbert Strang 教授称其为"线性代数的顶峰"。
在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的万能钥匙。
本章我们将从几何变换的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为旋转、拉伸、再旋转。
目录# 引言:从圆到椭圆
1.1 矩阵变换的本质 1.2 特征值分解的局限 特征分解(EVD):对称矩阵的美学
2.1 谱定理(Spectral Theorem) 2.2 几何直觉 2.3 正定性:碗的形状 奇异值分解(SVD):万能钥匙
3.1 核心思想:让非方阵也能对角化 3.2 推导 SVD 3.3 SVD 的几何图景:旋转-拉伸-旋转 3.4 薄 SVD(Reduced SVD) 3.5 外积形式(Dyadic Expansion) 四个基本子空间的 SVD 视角
4.1 回顾:四个基本子空间 4.2 SVD 的完美切分 4.3 正交关系图 4.4 伪逆的几何意义 低秩近似:SVD 的杀手级应用
5.1 问题设定 5.2 Eckart-Young-Mirsky 定理 5.3 直觉:丢弃小奇异值 = 去噪 5.4 应用 1:图像压缩 5.5 应用 2:推荐系统与矩阵补全 5.6 应用 3:主成分分析(PCA) SVD 与 EVD 的联系
6.1 核心关系 6.2 特殊情况:对称矩阵 计算方法简述
7.1 直接方法(不推荐) 7.2 实际算法 总结'><meta itemprop=wordCount content="1498"><title>第03章 SVD与矩阵分解 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/ class=active>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第03章 SVD与矩阵分解</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#目录>目录</a></li><li><a href=#1-引言从圆到椭圆>1. 引言：从圆到椭圆</a><ul><li><a href=#11-矩阵变换的本质>1.1 矩阵变换的本质</a></li><li><a href=#12-特征值分解的局限>1.2 特征值分解的局限</a></li></ul></li><li><a href=#2-特征分解evd对称矩阵的美学>2. 特征分解（EVD）：对称矩阵的美学</a><ul><li><a href=#21-谱定理spectral-theorem>2.1 谱定理（Spectral Theorem）</a></li><li><a href=#22-几何直觉>2.2 几何直觉</a></li><li><a href=#23-正定性碗的形状>2.3 正定性：碗的形状</a></li></ul></li><li><a href=#3-奇异值分解svd万能钥匙>3. 奇异值分解（SVD）：万能钥匙</a><ul><li><a href=#31-核心思想让非方阵也能对角化>3.1 核心思想：让非方阵也能对角化</a></li><li><a href=#32-推导-svd>3.2 推导 SVD</a></li><li><a href=#33-svd-的几何图景旋转-拉伸-旋转>3.3 SVD 的几何图景：旋转-拉伸-旋转</a></li><li><a href=#34-薄-svdreduced-svd>3.4 薄 SVD（Reduced SVD）</a></li><li><a href=#35-外积形式dyadic-expansion>3.5 外积形式（Dyadic Expansion）</a></li></ul></li><li><a href=#4-四个基本子空间的-svd-视角>4. 四个基本子空间的 SVD 视角</a><ul><li><a href=#41-回顾四个基本子空间>4.1 回顾：四个基本子空间</a></li><li><a href=#42-svd-的完美切分>4.2 SVD 的完美切分</a></li><li><a href=#43-正交关系图>4.3 正交关系图</a></li><li><a href=#44-伪逆的几何意义>4.4 伪逆的几何意义</a></li></ul></li><li><a href=#5-低秩近似svd-的杀手级应用>5. 低秩近似：SVD 的杀手级应用</a><ul><li><a href=#51-问题设定>5.1 问题设定</a></li><li><a href=#52-eckart-young-mirsky-定理>5.2 Eckart-Young-Mirsky 定理</a></li><li><a href=#53-直觉丢弃小奇异值--去噪>5.3 直觉：丢弃小奇异值 = 去噪</a></li><li><a href=#54-应用-1图像压缩>5.4 应用 1：图像压缩</a></li><li><a href=#55-应用-2推荐系统与矩阵补全>5.5 应用 2：推荐系统与矩阵补全</a></li><li><a href=#56-应用-3主成分分析pca>5.6 应用 3：主成分分析（PCA）</a></li></ul></li><li><a href=#6-svd-与-evd-的联系>6. SVD 与 EVD 的联系</a><ul><li><a href=#61-核心关系>6.1 核心关系</a></li><li><a href=#62-特殊情况对称矩阵>6.2 特殊情况：对称矩阵</a></li></ul></li><li><a href=#7-计算方法简述>7. 计算方法简述</a><ul><li><a href=#71-直接方法不推荐>7.1 直接方法（不推荐）</a></li><li><a href=#72-实际算法>7.2 实际算法</a></li></ul></li><li><a href=#8-总结>8. 总结</a><ul><li><a href=#81-svd-的核心价值>8.1 SVD 的核心价值</a></li><li><a href=#82-svd-的应用场景>8.2 SVD 的应用场景</a></li><li><a href=#83-理解-svd-的三个层次>8.3 理解 SVD 的三个层次</a></li><li><a href=#84-最终洞察>8.4 最终洞察</a></li></ul></li><li><a href=#附录关键公式速查>附录：关键公式速查</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第03章svd与矩阵分解>第03章：SVD与矩阵分解<a class=anchor href=#%e7%ac%ac03%e7%ab%a0svd%e4%b8%8e%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3>#</a></h1><blockquote class=book-hint><p><strong>核心思想</strong>：任何矩阵都可以看作"旋转-拉伸-旋转"的组合。SVD 是线性代数的终极武器。</p></blockquote><hr><h2 id=前言>前言<a class=anchor href=#%e5%89%8d%e8%a8%80>#</a></h2><p>如果说线性代数有皇冠,那么<strong>奇异值分解 (SVD)</strong> 就是皇冠上的明珠。Gilbert Strang 教授称其为"线性代数的顶峰"。</p><p>在机器学习中,数据往往是矩阵,而 SVD 是理解数据结构(Data Structure)、降维(PCA)、去噪和推荐系统的<strong>万能钥匙</strong>。</p><p>本章我们将从<strong>几何变换</strong>的视角出发,一步步揭开 SVD 的面纱,并证明任何矩阵(无论方圆)都可以被分解为<strong>旋转、拉伸、再旋转</strong>。</p><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><p><a href=#1-%e5%bc%95%e8%a8%80%e4%bb%8e%e5%9c%86%e5%88%b0%e6%a4%ad%e5%9c%86>引言:从圆到椭圆</a></p><ul><li>1.1 <a href=#11-%e7%9f%a9%e9%98%b5%e5%8f%98%e6%8d%a2%e7%9a%84%e6%9c%ac%e8%b4%a8>矩阵变换的本质</a></li><li>1.2 <a href=#12-%e7%89%b9%e5%be%81%e5%80%bc%e5%88%86%e8%a7%a3%e7%9a%84%e5%b1%80%e9%99%90>特征值分解的局限</a></li></ul></li><li><p><a href=#2-%e7%89%b9%e5%be%81%e5%88%86%e8%a7%a3evd%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%e7%9a%84%e7%be%8e%e5%ad%a6>特征分解(EVD):对称矩阵的美学</a></p><ul><li>2.1 <a href=#21-%e8%b0%b1%e5%ae%9a%e7%90%86spectral-theorem>谱定理(Spectral Theorem)</a></li><li>2.2 <a href=#22-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>几何直觉</a></li><li>2.3 <a href=#23-%e6%ad%a3%e5%ae%9a%e6%80%a7%e7%a2%97%e7%9a%84%e5%bd%a2%e7%8a%b6>正定性:碗的形状</a></li></ul></li><li><p><a href=#3-%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3svd%e4%b8%87%e8%83%bd%e9%92%a5%e5%8c%99>奇异值分解(SVD):万能钥匙</a></p><ul><li>3.1 <a href=#31-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%ae%a9%e9%9d%9e%e6%96%b9%e9%98%b5%e4%b9%9f%e8%83%bd%e5%af%b9%e8%a7%92%e5%8c%96>核心思想:让非方阵也能对角化</a></li><li>3.2 <a href=#32-%e6%8e%a8%e5%af%bc-svd>推导 SVD</a></li><li>3.3 <a href=#33-svd-%e7%9a%84%e5%87%a0%e4%bd%95%e5%9b%be%e6%99%af%e6%97%8b%e8%bd%ac-%e6%8b%89%e4%bc%b8-%e6%97%8b%e8%bd%ac>SVD 的几何图景:旋转-拉伸-旋转</a></li><li>3.4 <a href=#34-%e8%96%84-svdreduced-svd>薄 SVD(Reduced SVD)</a></li><li>3.5 <a href=#35-%e5%a4%96%e7%a7%af%e5%bd%a2%e5%bc%8fdyadic-expansion>外积形式(Dyadic Expansion)</a></li></ul></li><li><p><a href=#4-%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4%e7%9a%84-svd-%e8%a7%86%e8%a7%92>四个基本子空间的 SVD 视角</a></p><ul><li>4.1 <a href=#41-%e5%9b%9e%e9%a1%be%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4>回顾:四个基本子空间</a></li><li>4.2 <a href=#42-svd-%e7%9a%84%e5%ae%8c%e7%be%8e%e5%88%87%e5%88%86>SVD 的完美切分</a></li><li>4.3 <a href=#43-%e6%ad%a3%e4%ba%a4%e5%85%b3%e7%b3%bb%e5%9b%be>正交关系图</a></li><li>4.4 <a href=#44-%e4%bc%aa%e9%80%86%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89>伪逆的几何意义</a></li></ul></li><li><p><a href=#5-%e4%bd%8e%e7%a7%a9%e8%bf%91%e4%bc%bcsvd-%e7%9a%84%e6%9d%80%e6%89%8b%e7%ba%a7%e5%ba%94%e7%94%a8>低秩近似:SVD 的杀手级应用</a></p><ul><li>5.1 <a href=#51-%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a>问题设定</a></li><li>5.2 <a href=#52-eckart-young-mirsky-%e5%ae%9a%e7%90%86>Eckart-Young-Mirsky 定理</a></li><li>5.3 <a href=#53-%e7%9b%b4%e8%a7%89%e4%b8%a2%e5%bc%83%e5%b0%8f%e5%a5%87%e5%bc%82%e5%80%bc--%e5%8e%bb%e5%99%aa>直觉:丢弃小奇异值 = 去噪</a></li><li>5.4 <a href=#54-%e5%ba%94%e7%94%a8-1%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9>应用 1:图像压缩</a></li><li>5.5 <a href=#55-%e5%ba%94%e7%94%a8-2%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f%e4%b8%8e%e7%9f%a9%e9%98%b5%e8%a1%a5%e5%85%a8>应用 2:推荐系统与矩阵补全</a></li><li>5.6 <a href=#56-%e5%ba%94%e7%94%a8-3%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90pca>应用 3:主成分分析(PCA)</a></li></ul></li><li><p><a href=#6-svd-%e4%b8%8e-evd-%e7%9a%84%e8%81%94%e7%b3%bb>SVD 与 EVD 的联系</a></p><ul><li>6.1 <a href=#61-%e6%a0%b8%e5%bf%83%e5%85%b3%e7%b3%bb>核心关系</a></li><li>6.2 <a href=#62-%e7%89%b9%e6%ae%8a%e6%83%85%e5%86%b5%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5>特殊情况:对称矩阵</a></li></ul></li><li><p><a href=#7-%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e7%ae%80%e8%bf%b0>计算方法简述</a></p><ul><li>7.1 <a href=#71-%e7%9b%b4%e6%8e%a5%e6%96%b9%e6%b3%95%e4%b8%8d%e6%8e%a8%e8%8d%90>直接方法(不推荐)</a></li><li>7.2 <a href=#72-%e5%ae%9e%e9%99%85%e7%ae%97%e6%b3%95>实际算法</a></li></ul></li><li><p><a href=#8-%e6%80%bb%e7%bb%93>总结</a></p><ul><li>8.1 <a href=#81-svd-%e7%9a%84%e6%a0%b8%e5%bf%83%e4%bb%b7%e5%80%bc>SVD 的核心价值</a></li><li>8.2 <a href=#82-svd-%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af>SVD 的应用场景</a></li><li>8.3 <a href=#83-%e7%90%86%e8%a7%a3-svd-%e7%9a%84%e4%b8%89%e4%b8%aa%e5%b1%82%e6%ac%a1>理解 SVD 的三个层次</a></li><li>8.4 <a href=#84-%e6%9c%80%e7%bb%88%e6%b4%9e%e5%af%9f>最终洞察</a></li></ul></li><li><p><a href=#%e9%99%84%e5%bd%95%e5%85%b3%e9%94%ae%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5>附录:关键公式速查</a></p></li></ol><hr><h2 id=1-引言从圆到椭圆>1. 引言：从圆到椭圆<a class=anchor href=#1-%e5%bc%95%e8%a8%80%e4%bb%8e%e5%9c%86%e5%88%b0%e6%a4%ad%e5%9c%86>#</a></h2><h3 id=11-矩阵变换的本质>1.1 矩阵变换的本质<a class=anchor href=#11-%e7%9f%a9%e9%98%b5%e5%8f%98%e6%8d%a2%e7%9a%84%e6%9c%ac%e8%b4%a8>#</a></h3><p>想象在二维平面上画一个单位圆：所有满足 $x^2 + y^2 = 1$ 的点。现在对这个圆施加一个矩阵变换 $A$：</p><p>$$
\begin{bmatrix} x&rsquo; \ y&rsquo; \end{bmatrix} = A \begin{bmatrix} x \ y \end{bmatrix}
$$</p><p><strong>奇妙的事情发生了</strong>：圆变成了椭圆！</p><ul><li>椭圆的长轴、短轴方向：矩阵 $A$ 的"主方向"</li><li>椭圆的长轴、短轴长度：矩阵 $A$ 的"拉伸程度"</li></ul><p><strong>深刻的问题</strong>：能否找到一组特殊的基，使得矩阵 $A$ 的作用变得简单（仅仅是沿着坐标轴拉伸）？</p><h3 id=12-特征值分解的局限>1.2 特征值分解的局限<a class=anchor href=#12-%e7%89%b9%e5%be%81%e5%80%bc%e5%88%86%e8%a7%a3%e7%9a%84%e5%b1%80%e9%99%90>#</a></h3><p>如果 $A$ 是<strong>方阵</strong>，我们有特征值分解：</p><p>$$
A v = \lambda v
$$</p><p><strong>物理意义</strong>：特征向量 $v$ 的方向在变换后保持不变，只是长度变为 $\lambda$ 倍。</p><p><strong>但是</strong>：</p><ul><li>特征值分解只适用于<strong>方阵</strong></li><li>即使是方阵，也不一定可以对角化（如果特征向量不够）</li><li>非对称矩阵的特征向量不正交，失去几何直观性</li></ul><p><strong>我们需要更强大的工具</strong>：适用于任意 $m \times n$ 矩阵，始终存在，且具有优美几何意义的分解。</p><p>这就是<strong>奇异值分解（SVD）</strong>。</p><hr><h2 id=2-特征分解evd对称矩阵的美学>2. 特征分解（EVD）：对称矩阵的美学<a class=anchor href=#2-%e7%89%b9%e5%be%81%e5%88%86%e8%a7%a3evd%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%e7%9a%84%e7%be%8e%e5%ad%a6>#</a></h2><p>在讨论 SVD 之前，先理解对称矩阵的特殊性质。</p><h3 id=21-谱定理spectral-theorem>2.1 谱定理（Spectral Theorem）<a class=anchor href=#21-%e8%b0%b1%e5%ae%9a%e7%90%86spectral-theorem>#</a></h3><p><strong>定理</strong>：设 $A \in \mathbb{R}^{n \times n}$ 是实对称矩阵（$A = A^T$），则：</p><p>$$
A = Q \Lambda Q^T
$$</p><p>其中：</p><ul><li>$Q$ 是<strong>正交矩阵</strong>（$Q^T Q = I$），列向量是 $A$ 的特征向量</li><li>$\Lambda$ 是<strong>对角矩阵</strong>，对角元素是 $A$ 的特征值</li><li>特征值都是<strong>实数</strong></li><li>特征向量<strong>相互正交</strong></li></ul><h3 id=22-几何直觉>2.2 几何直觉<a class=anchor href=#22-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>#</a></h3><p>对称矩阵有什么特殊性？</p><p><strong>关键洞察</strong>：对称矩阵表示的变换<strong>不产生切变</strong>，只有旋转和伸缩。</p><p>分解 $A = Q \Lambda Q^T$ 的三步曲：</p><ol><li><strong>$Q^T$</strong>：旋转到特征向量构成的坐标系</li><li><strong>$\Lambda$</strong>：沿着新坐标轴拉伸（特征值决定拉伸倍数）</li><li><strong>$Q$</strong>：旋转回原坐标系</li></ol><p><strong>例子</strong>：协方差矩阵</p><p>协方差矩阵 $\Sigma = \mathbb{E}[(X - \mu)(X - \mu)^T]$ 是对称的。特征分解告诉我们：</p><ul><li>特征向量：数据的主方向（PCA 的基础）</li><li>特征值：数据在各主方向上的方差</li></ul><h3 id=23-正定性碗的形状>2.3 正定性：碗的形状<a class=anchor href=#23-%e6%ad%a3%e5%ae%9a%e6%80%a7%e7%a2%97%e7%9a%84%e5%bd%a2%e7%8a%b6>#</a></h3><p>考虑二次型：</p><p>$$
f(x) = x^T A x
$$</p><p>如果 $A = Q \Lambda Q^T$，令 $y = Q^T x$，则：</p><p>$$
f(x) = y^T \Lambda y = \sum_{i=1}^{n} \lambda_i y_i^2
$$</p><p><strong>几何意义</strong>：</p><ul><li>所有 $\lambda_i > 0$（正定）：向上开口的碗，有唯一最小值</li><li>存在 $\lambda_i &lt; 0$（不定）：马鞍面，有鞍点</li><li>所有 $\lambda_i \geq 0$，某些为 0（半正定）：退化的碗</li></ul><p><strong>这在优化中至关重要</strong>：Hessian 矩阵的特征值决定了临界点的性质。</p><hr><h2 id=3-奇异值分解svd万能钥匙>3. 奇异值分解（SVD）：万能钥匙<a class=anchor href=#3-%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3svd%e4%b8%87%e8%83%bd%e9%92%a5%e5%8c%99>#</a></h2><h3 id=31-核心思想让非方阵也能对角化>3.1 核心思想：让非方阵也能对角化<a class=anchor href=#31-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%ae%a9%e9%9d%9e%e6%96%b9%e9%98%b5%e4%b9%9f%e8%83%bd%e5%af%b9%e8%a7%92%e5%8c%96>#</a></h3><p><strong>问题</strong>：对于一般的 $A \in \mathbb{R}^{m \times n}$（$m \neq n$），如何分解？</p><p><strong>关键洞察</strong>：虽然 $A$ 不是对称矩阵，但 $A^T A$ 和 $A A^T$ 是！</p><ul><li>$A^T A \in \mathbb{R}^{n \times n}$，对称半正定</li><li>$A A^T \in \mathbb{R}^{m \times m}$，对称半正定</li></ul><h3 id=32-推导-svd>3.2 推导 SVD<a class=anchor href=#32-%e6%8e%a8%e5%af%bc-svd>#</a></h3><p><strong>步骤 1</strong>：对 $A^T A$ 做特征分解</p><p>$$
A^T A = V \Lambda V^T
$$</p><p>其中 $V \in \mathbb{R}^{n \times n}$ 正交，$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$，$\lambda_i \geq 0$。</p><p><strong>步骤 2</strong>：定义奇异值</p><p>令 $\sigma_i = \sqrt{\lambda_i}$，这些 $\sigma_i$ 称为 $A$ 的<strong>奇异值</strong>。按降序排列：</p><p>$$
\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0 = \sigma_{r+1} = \cdots = \sigma_{\min(m,n)}
$$</p><p>其中 $r = \text{rank}(A)$。</p><p><strong>步骤 3</strong>：构造左奇异向量</p><p>对于前 $r$ 个特征向量 $v_i$（对应非零奇异值），定义：</p><p>$$
u_i = \frac{1}{\sigma_i} A v_i, \quad i = 1, \ldots, r
$$</p><p><strong>验证正交性</strong>：</p><p>$$
u_i^T u_j = \frac{1}{\sigma_i \sigma_j} v_i^T A^T A v_j = \frac{1}{\sigma_i \sigma_j} v_i^T (\lambda_j v_j) = \frac{\lambda_j}{\sigma_i \sigma_j} v_i^T v_j
$$</p><p>当 $i = j$ 时，$u_i^T u_i = \frac{\sigma_i^2}{\sigma_i^2} = 1$；当 $i \neq j$ 时，$u_i^T u_j = 0$。</p><p><strong>步骤 4</strong>：扩展为完整的正交基</p><p>将 ${u_1, \ldots, u_r}$ 扩展为 $\mathbb{R}^m$ 的标准正交基 ${u_1, \ldots, u_m}$（后面的向量在 $A$ 的左零空间中）。</p><p><strong>最终形式</strong>：</p><p>$$
A = U \Sigma V^T
$$</p><p>其中：</p><ul><li>$U \in \mathbb{R}^{m \times m}$：左奇异向量，正交矩阵</li><li>$\Sigma \in \mathbb{R}^{m \times n}$：对角矩阵（广义，矩形），对角线是奇异值</li><li>$V \in \mathbb{R}^{n \times n}$：右奇异向量，正交矩阵</li></ul><h3 id=33-svd-的几何图景旋转-拉伸-旋转>3.3 SVD 的几何图景：旋转-拉伸-旋转<a class=anchor href=#33-svd-%e7%9a%84%e5%87%a0%e4%bd%95%e5%9b%be%e6%99%af%e6%97%8b%e8%bd%ac-%e6%8b%89%e4%bc%b8-%e6%97%8b%e8%bd%ac>#</a></h3><p><strong>三步曲</strong>（这是理解 SVD 的最直观方式）：</p><p>对于任意向量 $x \in \mathbb{R}^n$，计算 $Ax$：</p><p>$$
A x = U \Sigma V^T x
$$</p><ol><li><p><strong>第一步：$V^T x$（旋转到行空间基）</strong></p><ul><li>$V^T$ 是正交变换，将 $x$ 旋转到由 $V$ 的列向量（$A^T A$ 的特征向量）张成的坐标系</li><li>物理意义：选择"最合适"的输入方向</li></ul></li><li><p><strong>第二步：$\Sigma (V^T x)$（沿主轴拉伸）</strong></p><ul><li>对角矩阵，沿着各坐标轴独立缩放</li><li>第 $i$ 个分量乘以 $\sigma_i$</li><li>物理意义：信息的放大/缩小</li></ul></li><li><p><strong>第三步：$U (\Sigma V^T x)$（旋转到列空间）</strong></p><ul><li>$U$ 是正交变换，将结果旋转到由 $U$ 的列向量（$AA^T$ 的特征向量）张成的坐标系</li><li>物理意义：映射到"最合适"的输出方向</li></ul></li></ol><p><strong>核心洞察</strong>：任何矩阵变换都可以分解为"选择方向 → 缩放 → 输出方向"。</p><h3 id=34-薄-svdreduced-svd>3.4 薄 SVD（Reduced SVD）<a class=anchor href=#34-%e8%96%84-svdreduced-svd>#</a></h3><p>当 $m > n$ 时，$\Sigma$ 的后面 $m - n$ 行全是零，对应的 $U$ 的列向量没有贡献。我们可以截断：</p><p>$$
A = U_r \Sigma_r V_r^T
$$</p><p>其中：</p><ul><li>$U_r \in \mathbb{R}^{m \times r}$：前 $r$ 个左奇异向量</li><li>$\Sigma_r \in \mathbb{R}^{r \times r}$：非零奇异值组成的对角矩阵</li><li>$V_r \in \mathbb{R}^{n \times r}$：前 $r$ 个右奇异向量</li></ul><p>这是最常用的形式，避免了冗余。</p><h3 id=35-外积形式dyadic-expansion>3.5 外积形式（Dyadic Expansion）<a class=anchor href=#35-%e5%a4%96%e7%a7%af%e5%bd%a2%e5%bc%8fdyadic-expansion>#</a></h3><p>SVD 还可以写成外积和的形式：</p><p>$$
A = \sum_{i=1}^{r} \sigma_i u_i v_i^T
$$</p><p><strong>物理意义</strong>：</p><ul><li>每个 $u_i v_i^T$ 是一个秩-1 矩阵</li><li>$A$ 是 $r$ 个秩-1 矩阵的加权和</li><li>$\sigma_i$ 是第 $i$ 个"成分"的重要性</li></ul><p><strong>这为低秩近似奠定了基础</strong>。</p><hr><h2 id=4-四个基本子空间的-svd-视角>4. 四个基本子空间的 SVD 视角<a class=anchor href=#4-%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4%e7%9a%84-svd-%e8%a7%86%e8%a7%92>#</a></h2><p>这是 SVD 最深刻的几何洞察之一。</p><h3 id=41-回顾四个基本子空间>4.1 回顾：四个基本子空间<a class=anchor href=#41-%e5%9b%9e%e9%a1%be%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4>#</a></h3><p>对于矩阵 $A \in \mathbb{R}^{m \times n}$，有四个基本子空间：</p><ol><li><strong>列空间</strong>（Column Space）：$\mathcal{C}(A) \subseteq \mathbb{R}^m$，维度 $r$</li><li><strong>零空间</strong>（Null Space）：$\mathcal{N}(A) \subseteq \mathbb{R}^n$，维度 $n - r$</li><li><strong>行空间</strong>（Row Space）：$\mathcal{C}(A^T) \subseteq \mathbb{R}^n$，维度 $r$</li><li><strong>左零空间</strong>（Left Null Space）：$\mathcal{N}(A^T) \subseteq \mathbb{R}^m$，维度 $m - r$</li></ol><h3 id=42-svd-的完美切分>4.2 SVD 的完美切分<a class=anchor href=#42-svd-%e7%9a%84%e5%ae%8c%e7%be%8e%e5%88%87%e5%88%86>#</a></h3><p>SVD 的 $U$ 和 $V$ 恰好给出了这四个子空间的标准正交基：</p><p>$$
V = \begin{bmatrix} \underbrace{v_1 \cdots v_r}<em>{\text{行空间}} & \underbrace{v</em>{r+1} \cdots v_n}_{\text{零空间}} \end{bmatrix}
$$</p><p>$$
U = \begin{bmatrix} \underbrace{u_1 \cdots u_r}<em>{\text{列空间}} & \underbrace{u</em>{r+1} \cdots u_m}_{\text{左零空间}} \end{bmatrix}
$$</p><p><strong>验证</strong>：</p><ul><li><strong>行空间</strong>：$A^T A v_i = \sigma_i^2 v_i$（$i \leq r$），所以 $A^T (A v_i) \neq 0$，即 $A v_i$ 在 $\mathcal{C}(A^T)$ 中</li><li><strong>零空间</strong>：$A^T A v_i = 0$（$i > r$），所以 $A v_i = 0$，即 $v_i \in \mathcal{N}(A)$</li><li><strong>列空间</strong>：$u_i = \frac{1}{\sigma_i} A v_i$（$i \leq r$），是 $A$ 的列向量的线性组合</li><li><strong>左零空间</strong>：$A^T u_i = 0$（$i > r$），由构造保证</li></ul><h3 id=43-正交关系图>4.3 正交关系图<a class=anchor href=#43-%e6%ad%a3%e4%ba%a4%e5%85%b3%e7%b3%bb%e5%9b%be>#</a></h3><p>用文字描述的几何图景：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入空间 ℝⁿ                    输出空间 ℝᵐ
</span></span><span class=line><span class=cl>┌─────────────────┐           ┌─────────────────┐
</span></span><span class=line><span class=cl>│                 │           │                 │
</span></span><span class=line><span class=cl>│   行空间        │    A      │   列空间        │
</span></span><span class=line><span class=cl>│   (v₁...vᵣ)    │ ────────&gt; │   (u₁...uᵣ)    │
</span></span><span class=line><span class=cl>│   维度 r        │           │   维度 r        │
</span></span><span class=line><span class=cl>│                 │           │                 │
</span></span><span class=line><span class=cl>├─────────────────┤           ├─────────────────┤
</span></span><span class=line><span class=cl>│                 │           │                 │
</span></span><span class=line><span class=cl>│   零空间        │    A      │   左零空间      │
</span></span><span class=line><span class=cl>│   (vᵣ₊₁...vₙ)  │ ────────&gt; │   (uᵣ₊₁...uₘ)  │
</span></span><span class=line><span class=cl>│   维度 n-r      │   ↓0      │   维度 m-r      │
</span></span><span class=line><span class=cl>│                 │           │                 │
</span></span><span class=line><span class=cl>└─────────────────┘           └─────────────────┘
</span></span><span class=line><span class=cl>      ⊥                              ⊥</span></span></code></pre></div><p><strong>关键性质</strong>：</p><ul><li>行空间 ⊥ 零空间（在 $\mathbb{R}^n$ 中）</li><li>列空间 ⊥ 左零空间（在 $\mathbb{R}^m$ 中）</li><li>$A$ 将行空间<strong>一一映射</strong>到列空间（可逆）</li><li>$A$ 将零空间<strong>全部映射</strong>到零向量</li></ul><h3 id=44-伪逆的几何意义>4.4 伪逆的几何意义<a class=anchor href=#44-%e4%bc%aa%e9%80%86%e7%9a%84%e5%87%a0%e4%bd%95%e6%84%8f%e4%b9%89>#</a></h3><p>基于 SVD，我们可以定义<strong>Moore-Penrose 伪逆</strong>：</p><p>$$
A^+ = V \Sigma^+ U^T
$$</p><p>其中 $\Sigma^+$ 是将非零奇异值取倒数：</p><p>$$
\Sigma^+ = \begin{bmatrix}
1/\sigma_1 & & & \
& \ddots & & \
& & 1/\sigma_r & \
& & & 0_{(n-r) \times (m-r)}
\end{bmatrix}
$$</p><p><strong>几何意义</strong>：</p><ul><li>在列空间中，$A^+$ 是 $A$ 的逆（$A^+ A = I$ 在行空间上）</li><li>在左零空间中，$A^+$ 映射到零</li><li>$A^+$ 给出线性方程组 $Ax = b$ 的<strong>最小范数解</strong></li></ul><hr><h2 id=5-低秩近似svd-的杀手级应用>5. 低秩近似：SVD 的杀手级应用<a class=anchor href=#5-%e4%bd%8e%e7%a7%a9%e8%bf%91%e4%bc%bcsvd-%e7%9a%84%e6%9d%80%e6%89%8b%e7%ba%a7%e5%ba%94%e7%94%a8>#</a></h2><h3 id=51-问题设定>5.1 问题设定<a class=anchor href=#51-%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a>#</a></h3><p><strong>问题</strong>：给定矩阵 $A \in \mathbb{R}^{m \times n}$，秩为 $r$。如何找到秩为 $k$ 的矩阵 $A_k$（$k &lt; r$），使得：</p><p>$$
\min_{\text{rank}(B) = k} |A - B|_F
$$</p><p>其中 $|M|<em>F = \sqrt{\sum</em>{i,j} M_{ij}^2}$ 是 Frobenius 范数（所有元素平方和的平方根）。</p><p><strong>直觉</strong>：用更少的信息（低秩）来近似原矩阵。</p><h3 id=52-eckart-young-mirsky-定理>5.2 Eckart-Young-Mirsky 定理<a class=anchor href=#52-eckart-young-mirsky-%e5%ae%9a%e7%90%86>#</a></h3><p><strong>定理</strong>：设 $A = U \Sigma V^T$ 是 SVD，定义截断 SVD：</p><p>$$
A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T = U_k \Sigma_k V_k^T
$$</p><p>则 $A_k$ 是所有秩为 $k$ 的矩阵中，Frobenius 范数下距离 $A$ 最近的矩阵：</p><p>$$
|A - A_k|<em>F = \sqrt{\sum</em>{i=k+1}^{r} \sigma_i^2} = \text{最小可能误差}
$$</p><p><strong>证明思路</strong>（不严格，但有启发性）：</p><p>由外积形式：</p><p>$$
A - A_k = \sum_{i=k+1}^{r} \sigma_i u_i v_i^T
$$</p><p>因为 $u_i$ 和 $v_i$ 都是标准正交的，所以：</p><p>$$
|A - A_k|<em>F^2 = \sum</em>{i=k+1}^{r} \sigma_i^2 |u_i v_i^T|<em>F^2 = \sum</em>{i=k+1}^{r} \sigma_i^2
$$</p><p>任何其他秩为 $k$ 的近似都无法做得更好（需要变分法严格证明）。</p><h3 id=53-直觉丢弃小奇异值--去噪>5.3 直觉：丢弃小奇异值 = 去噪<a class=anchor href=#53-%e7%9b%b4%e8%a7%89%e4%b8%a2%e5%bc%83%e5%b0%8f%e5%a5%87%e5%bc%82%e5%80%bc--%e5%8e%bb%e5%99%aa>#</a></h3><p><strong>信号 vs 噪声</strong>：</p><ul><li>大的奇异值：主要信息、结构化模式</li><li>小的奇异值：细节、噪声、随机性</li></ul><p>截断 SVD 相当于<strong>自动去噪</strong>：只保留最重要的 $k$ 个"模式"。</p><p><strong>能量视角</strong>：</p><p>矩阵的"总能量"：</p><p>$$
|A|<em>F^2 = \sum</em>{i=1}^{r} \sigma_i^2
$$</p><p>前 $k$ 个奇异值捕获的能量占比：</p><p>$$
\frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{i=1}^{r} \sigma_i^2}
$$</p><p>如果前几个奇异值远大于后面的（快速衰减），则低秩近似非常有效。</p><h3 id=54-应用-1图像压缩>5.4 应用 1：图像压缩<a class=anchor href=#54-%e5%ba%94%e7%94%a8-1%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9>#</a></h3><p><strong>设定</strong>：灰度图像是 $m \times n$ 的矩阵（每个元素是像素值）。</p><p><strong>原始存储</strong>：$mn$ 个数。</p><p><strong>SVD 压缩</strong>：只存储前 $k$ 个奇异值和对应的奇异向量：</p><ul><li>$\sigma_1, \ldots, \sigma_k$：$k$ 个数</li><li>$u_1, \ldots, u_k$：$mk$ 个数</li><li>$v_1, \ldots, v_k$：$nk$ 个数</li></ul><p><strong>总存储量</strong>：$k(m + n + 1)$。</p><p><strong>压缩比</strong>：</p><p>$$
\frac{k(m + n + 1)}{mn}
$$</p><p>例如，$m = n = 1000$，$k = 50$，压缩比约为 $10%$。</p><p><strong>效果</strong>：如果图像有结构（自然图像通常如此），前几个奇异值就能捕获主要轮廓，重建质量很好。</p><h3 id=55-应用-2推荐系统与矩阵补全>5.5 应用 2：推荐系统与矩阵补全<a class=anchor href=#55-%e5%ba%94%e7%94%a8-2%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f%e4%b8%8e%e7%9f%a9%e9%98%b5%e8%a1%a5%e5%85%a8>#</a></h3><p><strong>设定</strong>：用户-物品评分矩阵 $R \in \mathbb{R}^{m \times n}$：</p><ul><li>$R_{ij}$：用户 $i$ 对物品 $j$ 的评分</li><li>问题：大部分元素是缺失的（用户没有评价所有物品）</li></ul><p><strong>低秩假设</strong>：</p><ul><li>假设用户的偏好由少数几个"隐因子"决定（如电影的类型）</li><li>因此 $R$ 应该是<strong>低秩</strong>的（或近似低秩）</li></ul><p><strong>策略</strong>：</p><ol><li>对已观测的评分，用 SVD（或矩阵分解）找到低秩近似 $R_k$</li><li>用 $R_k$ 的对应元素来预测缺失的评分</li></ol><p><strong>Netflix Prize</strong>：这一思想的成功应用。</p><h3 id=56-应用-3主成分分析pca>5.6 应用 3：主成分分析（PCA）<a class=anchor href=#56-%e5%ba%94%e7%94%a8-3%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90pca>#</a></h3><p>PCA 本质上就是对数据的协方差矩阵（或数据矩阵本身）做 SVD。</p><p><strong>设定</strong>：数据矩阵 $X \in \mathbb{R}^{n \times d}$（$n$ 个样本，$d$ 个特征），已中心化（每列均值为 0）。</p><p><strong>目标</strong>：找到 $k$ 个方向，使得数据在这些方向上的投影方差最大。</p><p><strong>方法</strong>：对 $X$ 做 SVD：</p><p>$$
X = U \Sigma V^T
$$</p><ul><li>$V$ 的列向量：主成分方向（特征）</li><li>$\Sigma$ 的对角元素：对应方向上的标准差（奇异值）</li><li>$U \Sigma$：降维后的数据（前 $k$ 列）</li></ul><p><strong>降维</strong>：</p><p>$$
X_k = U_k \Sigma_k V_k^T
$$</p><p>保留最大的 $k$ 个奇异值，重构误差最小。</p><hr><h2 id=6-svd-与-evd-的联系>6. SVD 与 EVD 的联系<a class=anchor href=#6-svd-%e4%b8%8e-evd-%e7%9a%84%e8%81%94%e7%b3%bb>#</a></h2><h3 id=61-核心关系>6.1 核心关系<a class=anchor href=#61-%e6%a0%b8%e5%bf%83%e5%85%b3%e7%b3%bb>#</a></h3><p><strong>对于任意矩阵 $A$</strong>：</p><p>$$
A^T A = (V \Sigma U^T)(U \Sigma V^T) = V \Sigma^2 V^T
$$</p><p>$$
A A^T = (U \Sigma V^T)(V \Sigma U^T) = U \Sigma^2 U^T
$$</p><p><strong>结论</strong>：</p><ul><li>$V$ 是 $A^T A$ 的特征向量矩阵</li><li>$U$ 是 $A A^T$ 的特征向量矩阵</li><li>$A$ 的奇异值 $\sigma_i$ 是 $A^T A$（或 $A A^T$）的特征值 $\lambda_i$ 的平方根：</li></ul><p>$$
\sigma_i = \sqrt{\lambda_i}
$$</p><h3 id=62-特殊情况对称矩阵>6.2 特殊情况：对称矩阵<a class=anchor href=#62-%e7%89%b9%e6%ae%8a%e6%83%85%e5%86%b5%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5>#</a></h3><p>如果 $A = A^T$（对称矩阵），则：</p><p>$$
A^T A = A^2
$$</p><p>设 $A = Q \Lambda Q^T$ 是特征分解，则：</p><p>$$
A^2 = Q \Lambda^2 Q^T
$$</p><p>此时 SVD 退化为：</p><p>$$
A = Q |\Lambda| Q^T
$$</p><p>其中 $|\Lambda|$ 是特征值的绝对值组成的对角矩阵。</p><p><strong>注意</strong>：对称矩阵的特征值可以是负数，但奇异值始终非负。</p><p><strong>例子</strong>：</p><p>$$
A = \begin{bmatrix} 0 & 1 \ 1 & 0 \end{bmatrix}
$$</p><ul><li>特征值：$\lambda_1 = 1, \lambda_2 = -1$</li><li>奇异值：$\sigma_1 = 1, \sigma_2 = 1$</li></ul><hr><h2 id=7-计算方法简述>7. 计算方法简述<a class=anchor href=#7-%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e7%ae%80%e8%bf%b0>#</a></h2><h3 id=71-直接方法不推荐>7.1 直接方法（不推荐）<a class=anchor href=#71-%e7%9b%b4%e6%8e%a5%e6%96%b9%e6%b3%95%e4%b8%8d%e6%8e%a8%e8%8d%90>#</a></h3><p>理论上可以：</p><ol><li>计算 $A^T A$</li><li>求 $A^T A$ 的特征值和特征向量</li><li>计算奇异值和左奇异向量</li></ol><p><strong>问题</strong>：</p><ul><li>$A^T A$ 的条件数是 $A$ 的平方，数值不稳定</li><li>计算量大</li></ul><h3 id=72-实际算法>7.2 实际算法<a class=anchor href=#72-%e5%ae%9e%e9%99%85%e7%ae%97%e6%b3%95>#</a></h3><p>实际使用的是<strong>Golub-Kahan 双对角化</strong>或<strong>分而治之法</strong>：</p><ol><li>将 $A$ 约化为双对角矩阵（通过正交变换）</li><li>对双对角矩阵求 SVD（高效且数值稳定）</li></ol><p>现代库（NumPy、MATLAB、LAPACK）都实现了这些算法，直接调用即可。</p><p><strong>Python 示例</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>U</span><span class=p>,</span> <span class=n>s</span><span class=p>,</span> <span class=n>Vt</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svd</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>full_matrices</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 低秩近似</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>A_k</span> <span class=o>=</span> <span class=n>U</span><span class=p>[:,</span> <span class=p>:</span><span class=n>k</span><span class=p>]</span> <span class=o>@</span> <span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>s</span><span class=p>[:</span><span class=n>k</span><span class=p>])</span> <span class=o>@</span> <span class=n>Vt</span><span class=p>[:</span><span class=n>k</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl><span class=n>error</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>A</span> <span class=o>-</span> <span class=n>A_k</span><span class=p>,</span> <span class=s1>&#39;fro&#39;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=8-总结>8. 总结<a class=anchor href=#8-%e6%80%bb%e7%bb%93>#</a></h2><h3 id=81-svd-的核心价值>8.1 SVD 的核心价值<a class=anchor href=#81-svd-%e7%9a%84%e6%a0%b8%e5%bf%83%e4%bb%b7%e5%80%bc>#</a></h3><ol><li><strong>通用性</strong>：适用于任何矩阵（方阵、长矩阵、宽矩阵）</li><li><strong>存在性</strong>：始终存在，且数值稳定</li><li><strong>几何直观性</strong>：旋转-拉伸-旋转，清晰的物理意义</li><li><strong>正交性</strong>：$U$ 和 $V$ 都是正交矩阵，保持几何结构</li></ol><h3 id=82-svd-的应用场景>8.2 SVD 的应用场景<a class=anchor href=#82-svd-%e7%9a%84%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><table><thead><tr><th>应用</th><th>核心思想</th></tr></thead><tbody><tr><td><strong>低秩近似</strong></td><td>截断小奇异值，去噪/压缩</td></tr><tr><td><strong>PCA</strong></td><td>找到方差最大的方向</td></tr><tr><td><strong>推荐系统</strong></td><td>矩阵补全，隐因子模型</td></tr><tr><td><strong>图像处理</strong></td><td>压缩、去噪、特征提取</td></tr><tr><td><strong>伪逆计算</strong></td><td>求解欠定/超定方程组</td></tr><tr><td><strong>矩阵秩估计</strong></td><td>通过奇异值分布判断数值秩</td></tr><tr><td><strong>最小二乘</strong></td><td>$\min |Ax - b|_2$ 的稳定解法</td></tr></tbody></table><h3 id=83-理解-svd-的三个层次>8.3 理解 SVD 的三个层次<a class=anchor href=#83-%e7%90%86%e8%a7%a3-svd-%e7%9a%84%e4%b8%89%e4%b8%aa%e5%b1%82%e6%ac%a1>#</a></h3><ol><li><strong>代数层次</strong>：$A = U \Sigma V^T$，矩阵的分解</li><li><strong>几何层次</strong>：任何线性变换 = 旋转 + 拉伸 + 旋转</li><li><strong>语义层次</strong>：提取数据的"主要模式"，过滤噪声</li></ol><h3 id=84-最终洞察>8.4 最终洞察<a class=anchor href=#84-%e6%9c%80%e7%bb%88%e6%b4%9e%e5%af%9f>#</a></h3><p><strong>SVD 是线性代数的顶峰</strong>：</p><ul><li>它统一了特征值分解（对称矩阵的特例）</li><li>它揭示了矩阵的四个基本子空间的完美结构</li><li>它是现代数据科学的基石（PCA、推荐系统、自然语言处理中的 LSA 等）</li></ul><p><strong>记住这句话</strong>：</p><blockquote class=book-hint><p><em>&ldquo;Every matrix is a rotation, followed by a stretch, followed by another rotation.&rdquo;</em>
— Gilbert Strang</p></blockquote><p>SVD 将这个直觉变成了严格的数学定理，并赋予了它强大的计算能力。</p><hr><h2 id=附录关键公式速查>附录：关键公式速查<a class=anchor href=#%e9%99%84%e5%bd%95%e5%85%b3%e9%94%ae%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5>#</a></h2><table><thead><tr><th>概念</th><th>公式</th></tr></thead><tbody><tr><td><strong>SVD 完整形式</strong></td><td>$A = U \Sigma V^T$，$U^T U = I$，$V^T V = I$</td></tr><tr><td><strong>薄 SVD</strong></td><td>$A = U_r \Sigma_r V_r^T$，$r = \text{rank}(A)$</td></tr><tr><td><strong>外积形式</strong></td><td>$A = \sum_{i=1}^{r} \sigma_i u_i v_i^T$</td></tr><tr><td><strong>奇异值与特征值</strong></td><td>$\sigma_i = \sqrt{\lambda_i(A^T A)} = \sqrt{\lambda_i(A A^T)}$</td></tr><tr><td><strong>低秩近似</strong></td><td>$A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T$</td></tr><tr><td><strong>近似误差</strong></td><td>$|A - A_k|<em>F = \sqrt{\sum</em>{i=k+1}^{r} \sigma_i^2}$</td></tr><tr><td><strong>伪逆</strong></td><td>$A^+ = V \Sigma^+ U^T$，$\Sigma^+_{ii} = 1/\sigma_i$ （若 $\sigma_i \neq 0$）</td></tr><tr><td><strong>四个子空间</strong></td><td>$\mathcal{C}(A) = \text{span}(u_1, \ldots, u_r)$<br>$\mathcal{N}(A) = \text{span}(v_{r+1}, \ldots, v_n)$<br>$\mathcal{C}(A^T) = \text{span}(v_1, \ldots, v_r)$<br>$\mathcal{N}(A^T) = \text{span}(u_{r+1}, \ldots, u_m)$</td></tr></tbody></table><hr><p><strong>下一章预告</strong>：我们将把这些线性代数工具应用到概率论中，理解多元高斯分布的几何结构，以及协方差矩阵的特征分解如何揭示数据的内在结构。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第02章 矩阵运算与微积分</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/ class="flex align-center"><span>第04章 概率分布 指数族与共轭先验</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#目录>目录</a></li><li><a href=#1-引言从圆到椭圆>1. 引言：从圆到椭圆</a><ul><li><a href=#11-矩阵变换的本质>1.1 矩阵变换的本质</a></li><li><a href=#12-特征值分解的局限>1.2 特征值分解的局限</a></li></ul></li><li><a href=#2-特征分解evd对称矩阵的美学>2. 特征分解（EVD）：对称矩阵的美学</a><ul><li><a href=#21-谱定理spectral-theorem>2.1 谱定理（Spectral Theorem）</a></li><li><a href=#22-几何直觉>2.2 几何直觉</a></li><li><a href=#23-正定性碗的形状>2.3 正定性：碗的形状</a></li></ul></li><li><a href=#3-奇异值分解svd万能钥匙>3. 奇异值分解（SVD）：万能钥匙</a><ul><li><a href=#31-核心思想让非方阵也能对角化>3.1 核心思想：让非方阵也能对角化</a></li><li><a href=#32-推导-svd>3.2 推导 SVD</a></li><li><a href=#33-svd-的几何图景旋转-拉伸-旋转>3.3 SVD 的几何图景：旋转-拉伸-旋转</a></li><li><a href=#34-薄-svdreduced-svd>3.4 薄 SVD（Reduced SVD）</a></li><li><a href=#35-外积形式dyadic-expansion>3.5 外积形式（Dyadic Expansion）</a></li></ul></li><li><a href=#4-四个基本子空间的-svd-视角>4. 四个基本子空间的 SVD 视角</a><ul><li><a href=#41-回顾四个基本子空间>4.1 回顾：四个基本子空间</a></li><li><a href=#42-svd-的完美切分>4.2 SVD 的完美切分</a></li><li><a href=#43-正交关系图>4.3 正交关系图</a></li><li><a href=#44-伪逆的几何意义>4.4 伪逆的几何意义</a></li></ul></li><li><a href=#5-低秩近似svd-的杀手级应用>5. 低秩近似：SVD 的杀手级应用</a><ul><li><a href=#51-问题设定>5.1 问题设定</a></li><li><a href=#52-eckart-young-mirsky-定理>5.2 Eckart-Young-Mirsky 定理</a></li><li><a href=#53-直觉丢弃小奇异值--去噪>5.3 直觉：丢弃小奇异值 = 去噪</a></li><li><a href=#54-应用-1图像压缩>5.4 应用 1：图像压缩</a></li><li><a href=#55-应用-2推荐系统与矩阵补全>5.5 应用 2：推荐系统与矩阵补全</a></li><li><a href=#56-应用-3主成分分析pca>5.6 应用 3：主成分分析（PCA）</a></li></ul></li><li><a href=#6-svd-与-evd-的联系>6. SVD 与 EVD 的联系</a><ul><li><a href=#61-核心关系>6.1 核心关系</a></li><li><a href=#62-特殊情况对称矩阵>6.2 特殊情况：对称矩阵</a></li></ul></li><li><a href=#7-计算方法简述>7. 计算方法简述</a><ul><li><a href=#71-直接方法不推荐>7.1 直接方法（不推荐）</a></li><li><a href=#72-实际算法>7.2 实际算法</a></li></ul></li><li><a href=#8-总结>8. 总结</a><ul><li><a href=#81-svd-的核心价值>8.1 SVD 的核心价值</a></li><li><a href=#82-svd-的应用场景>8.2 SVD 的应用场景</a></li><li><a href=#83-理解-svd-的三个层次>8.3 理解 SVD 的三个层次</a></li><li><a href=#84-最终洞察>8.4 最终洞察</a></li></ul></li><li><a href=#附录关键公式速查>附录：关键公式速查</a></li></ul></nav></div></aside></main></body></html>