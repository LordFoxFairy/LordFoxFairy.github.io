<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第09章 决策树与集成学习# “三个臭皮匠，顶个诸葛亮。” —— 中国谚语
“The whole is greater than the sum of its parts.” —— Aristotle
重要提示：集成学习是机器学习中最优雅的哲学之一。
当你意识到一群弱小的模型，通过恰当的组织方式，能够超越任何单一的强大模型时，你触碰到了群体智慧的数学本质。这不仅仅是工程技巧，更是对**“涌现” (emergence)** 这一概念的深刻诠释。
本章将带你经历一次从个体到集体的认知跃迁：从单棵树的分裂策略 (信息增益)，到多棵树的协同方式 (Bagging vs Boosting)，再到损失函数的梯度优化 (GBDT & XGBoost)。当你发现 AdaBoost 不是拍脑袋发明的，而是在最小化指数损失；当你理解 GBDT 拟合残差的本质是负梯度下降时，你将领悟到集成学习那令人战栗的统一之美。
目录# 引言 决策树(Decision Tree) 2.1 直觉:20个问题游戏 2.2 纯度的度量:熵与基尼 2.3 特征选择:最大化信息增益 2.4 决策树的构建算法 2.5 过拟合与剪枝 集成学习的总纲 3.1 为什么需要集成? 3.2 Bias-Variance Tradeoff 3.3 集成的两大流派:Bagging vs Boosting Bagging与随机森林 4.1 Bootstrap采样 4.2 Bagging的降方差机制 4.3 随机森林的双重随机性 4.4 Out-of-Bag估计 Boosting之魂:AdaBoost 5.1 AdaBoost算法流程 5.2 核心推导:为什么是指数损失? 5.3 样本权重更新公式的推导 5.4 AdaBoost的几何直觉 Boosting之神:GBDT与XGBoost 6.1 GBDT:梯度提升决策树 6.2 为什么拟合残差? 6.3 XGBoost:二阶泰勒展开 6.4 XGBoost的目标函数推导 6.5 XGBoost的工程优化 总结 附录:XGBoost核心公式 1. 引言# 集成学习 (Ensemble Learning) 的核心思想非常简单：如果你不能信任单个专家，那就组织一个委员会。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第09章 决策树与集成学习"><meta property="og:description" content="第09章 决策树与集成学习# “三个臭皮匠，顶个诸葛亮。” —— 中国谚语
“The whole is greater than the sum of its parts.” —— Aristotle
重要提示：集成学习是机器学习中最优雅的哲学之一。
当你意识到一群弱小的模型，通过恰当的组织方式，能够超越任何单一的强大模型时，你触碰到了群体智慧的数学本质。这不仅仅是工程技巧，更是对**“涌现” (emergence)** 这一概念的深刻诠释。
本章将带你经历一次从个体到集体的认知跃迁：从单棵树的分裂策略 (信息增益)，到多棵树的协同方式 (Bagging vs Boosting)，再到损失函数的梯度优化 (GBDT & XGBoost)。当你发现 AdaBoost 不是拍脑袋发明的，而是在最小化指数损失；当你理解 GBDT 拟合残差的本质是负梯度下降时，你将领悟到集成学习那令人战栗的统一之美。
目录# 引言 决策树(Decision Tree) 2.1 直觉:20个问题游戏 2.2 纯度的度量:熵与基尼 2.3 特征选择:最大化信息增益 2.4 决策树的构建算法 2.5 过拟合与剪枝 集成学习的总纲 3.1 为什么需要集成? 3.2 Bias-Variance Tradeoff 3.3 集成的两大流派:Bagging vs Boosting Bagging与随机森林 4.1 Bootstrap采样 4.2 Bagging的降方差机制 4.3 随机森林的双重随机性 4.4 Out-of-Bag估计 Boosting之魂:AdaBoost 5.1 AdaBoost算法流程 5.2 核心推导:为什么是指数损失? 5.3 样本权重更新公式的推导 5.4 AdaBoost的几何直觉 Boosting之神:GBDT与XGBoost 6.1 GBDT:梯度提升决策树 6.2 为什么拟合残差? 6.3 XGBoost:二阶泰勒展开 6.4 XGBoost的目标函数推导 6.5 XGBoost的工程优化 总结 附录:XGBoost核心公式 1. 引言# 集成学习 (Ensemble Learning) 的核心思想非常简单：如果你不能信任单个专家，那就组织一个委员会。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第09章 决策树与集成学习"><meta itemprop=description content="第09章 决策树与集成学习# “三个臭皮匠，顶个诸葛亮。” —— 中国谚语
“The whole is greater than the sum of its parts.” —— Aristotle
重要提示：集成学习是机器学习中最优雅的哲学之一。
当你意识到一群弱小的模型，通过恰当的组织方式，能够超越任何单一的强大模型时，你触碰到了群体智慧的数学本质。这不仅仅是工程技巧，更是对**“涌现” (emergence)** 这一概念的深刻诠释。
本章将带你经历一次从个体到集体的认知跃迁：从单棵树的分裂策略 (信息增益)，到多棵树的协同方式 (Bagging vs Boosting)，再到损失函数的梯度优化 (GBDT & XGBoost)。当你发现 AdaBoost 不是拍脑袋发明的，而是在最小化指数损失；当你理解 GBDT 拟合残差的本质是负梯度下降时，你将领悟到集成学习那令人战栗的统一之美。
目录# 引言 决策树(Decision Tree) 2.1 直觉:20个问题游戏 2.2 纯度的度量:熵与基尼 2.3 特征选择:最大化信息增益 2.4 决策树的构建算法 2.5 过拟合与剪枝 集成学习的总纲 3.1 为什么需要集成? 3.2 Bias-Variance Tradeoff 3.3 集成的两大流派:Bagging vs Boosting Bagging与随机森林 4.1 Bootstrap采样 4.2 Bagging的降方差机制 4.3 随机森林的双重随机性 4.4 Out-of-Bag估计 Boosting之魂:AdaBoost 5.1 AdaBoost算法流程 5.2 核心推导:为什么是指数损失? 5.3 样本权重更新公式的推导 5.4 AdaBoost的几何直觉 Boosting之神:GBDT与XGBoost 6.1 GBDT:梯度提升决策树 6.2 为什么拟合残差? 6.3 XGBoost:二阶泰勒展开 6.4 XGBoost的目标函数推导 6.5 XGBoost的工程优化 总结 附录:XGBoost核心公式 1. 引言# 集成学习 (Ensemble Learning) 的核心思想非常简单：如果你不能信任单个专家，那就组织一个委员会。"><meta itemprop=wordCount content="1870"><title>第09章 决策树与集成学习 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ class=active>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第09章 决策树与集成学习</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-决策树decision-tree>2. 决策树(Decision Tree)</a><ul><li><a href=#21-直觉20个问题游戏>2.1 直觉：20个问题游戏</a></li><li><a href=#22-纯度的度量熵与基尼>2.2 纯度的度量：熵与基尼</a><ul><li><a href=#1-熵entropy>(1) 熵(Entropy)</a></li><li><a href=#2-基尼不纯度gini-impurity>(2) 基尼不纯度(Gini Impurity)</a></li><li><a href=#3-熵-vs-gini有何区别>(3) 熵 vs Gini：有何区别？</a></li></ul></li><li><a href=#23-特征选择最大化信息增益>2.3 特征选择：最大化信息增益</a><ul><li><a href=#1-信息增益-information-gain>(1) 信息增益 (Information Gain)</a></li><li><a href=#2-信息增益比-gain-ratio>(2) 信息增益比 (Gain Ratio)</a></li><li><a href=#3-gini增益-gini-gain>(3) Gini增益 (Gini Gain)</a></li></ul></li><li><a href=#24-决策树的构建算法>2.4 决策树的构建算法</a></li><li><a href=#25-过拟合与剪枝>2.5 过拟合与剪枝</a><ul><li><a href=#1-预剪枝pre-pruning>(1) 预剪枝(Pre-pruning)</a></li><li><a href=#2-后剪枝post-pruning>(2) 后剪枝(Post-pruning)</a></li></ul></li></ul></li><li><a href=#3-集成学习的总纲>3. 集成学习的总纲</a><ul><li><a href=#31-为什么需要集成>3.1 为什么需要集成？</a></li><li><a href=#32-bias-variance-tradeoff>3.2 Bias-Variance Tradeoff</a></li><li><a href=#33-集成的两大流派bagging-vs-boosting>3.3 集成的两大流派：Bagging vs Boosting</a><ul><li><a href=#1-bagging-bootstrap-aggregating>(1) Bagging (Bootstrap Aggregating)</a></li><li><a href=#2-boosting>(2) Boosting</a></li></ul></li></ul></li><li><a href=#4-bagging与随机森林>4. Bagging与随机森林</a><ul><li><a href=#41-bootstrap采样>4.1 Bootstrap采样</a></li><li><a href=#42-bagging的降方差机制>4.2 Bagging的降方差机制</a></li><li><a href=#43-随机森林的双重随机性>4.3 随机森林的双重随机性</a></li><li><a href=#44-out-of-bag估计>4.4 Out-of-Bag估计</a></li></ul></li><li><a href=#5-boosting之魂adaboost>5. Boosting之魂:AdaBoost</a><ul><li><a href=#51-adaboost算法流程>5.1 AdaBoost算法流程</a></li><li><a href=#52-核心推导为什么是指数损失>5.2 核心推导：为什么是指数损失？</a><ul><li><a href=#1-指数损失>(1) 指数损失</a></li><li><a href=#2-前向分步算法>(2) 前向分步算法</a></li></ul></li><li><a href=#53-样本权重更新公式的推导>5.3 样本权重更新公式的推导</a><ul><li><a href=#1-优化-f_m>(1) 优化 $f_m$</a></li><li><a href=#2-优化-alpha_m>(2) 优化 $\alpha_m$</a></li><li><a href=#3-权重更新>(3) 权重更新</a></li></ul></li><li><a href=#54-adaboost的几何直觉>5.4 AdaBoost的几何直觉</a></li></ul></li><li><a href=#6-boosting之神gbdt与xgboost>6. Boosting之神:GBDT与XGBoost</a><ul><li><a href=#61-gbdt梯度提升决策树>6.1 GBDT:梯度提升决策树</a><ul><li><a href=#1-前向分步算法通用框架>(1) 前向分步算法（通用框架）</a></li><li><a href=#2-梯度下降的类比>(2) 梯度下降的类比</a></li></ul></li><li><a href=#62-为什么拟合残差>6.2 为什么拟合残差？</a></li><li><a href=#63-xgboost二阶泰勒展开>6.3 XGBoost：二阶泰勒展开</a></li><li><a href=#64-xgboost的目标函数推导>6.4 XGBoost的目标函数推导</a><ul><li><a href=#1-目标函数>(1) 目标函数</a></li><li><a href=#2-二阶泰勒展开>(2) 二阶泰勒展开</a></li><li><a href=#3-叶子节点的最优值>(3) 叶子节点的最优值</a></li><li><a href=#4-分裂增益>(4) 分裂增益</a></li></ul></li><li><a href=#65-xgboost的工程优化>6.5 XGBoost的工程优化</a><ul><li><a href=#1-分裂算法>(1) 分裂算法</a></li><li><a href=#2-系统优化>(2) 系统优化</a></li><li><a href=#3-正则化技巧>(3) 正则化技巧</a></li></ul></li></ul></li><li><a href=#7-总结>7. 总结</a><ul><li><a href=#1-决策树基石>(1) 决策树：基石</a></li><li><a href=#2-bagging降方差>(2) Bagging：降方差</a></li><li><a href=#3-boosting降偏差>(3) Boosting：降偏差</a><ul><li><a href=#adaboost>AdaBoost：</a></li><li><a href=#gbdt>GBDT：</a></li><li><a href=#xgboost>XGBoost：</a></li></ul></li><li><a href=#4-哲学思考>(4) 哲学思考</a></li></ul></li><li><a href=#8-附录xgboost核心公式>8. 附录:XGBoost核心公式</a><ul><li><a href=#1-目标函数-1>(1) 目标函数</a></li><li><a href=#2-二阶泰勒近似>(2) 二阶泰勒近似</a></li><li><a href=#3-叶子节点最优值>(3) 叶子节点最优值</a></li><li><a href=#4-最小损失>(4) 最小损失</a></li><li><a href=#5-分裂增益>(5) 分裂增益</a></li><li><a href=#6-常见损失函数的梯度>(6) 常见损失函数的梯度</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第09章-决策树与集成学习>第09章 决策树与集成学习<a class=anchor href=#%e7%ac%ac09%e7%ab%a0-%e5%86%b3%e7%ad%96%e6%a0%91%e4%b8%8e%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0>#</a></h1><blockquote class=book-hint><p>&ldquo;三个臭皮匠，顶个诸葛亮。&rdquo; —— 中国谚语</p><p>&ldquo;The whole is greater than the sum of its parts.&rdquo; —— Aristotle</p><p><strong>重要提示</strong>：集成学习是机器学习中最优雅的哲学之一。</p><p>当你意识到<strong>一群弱小的模型，通过恰当的组织方式，能够超越任何单一的强大模型</strong>时，你触碰到了群体智慧的数学本质。这不仅仅是工程技巧，更是对**&ldquo;涌现&rdquo; (emergence)** 这一概念的深刻诠释。</p><p>本章将带你经历一次从个体到集体的认知跃迁：从<strong>单棵树的分裂策略</strong> (信息增益)，到<strong>多棵树的协同方式</strong> (Bagging vs Boosting)，再到<strong>损失函数的梯度优化</strong> (GBDT & XGBoost)。当你发现 AdaBoost 不是拍脑袋发明的，而是在最小化指数损失；当你理解 GBDT 拟合残差的本质是负梯度下降时，你将领悟到集成学习那令人战栗的统一之美。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><a href=#1-%e5%bc%95%e8%a8%80>引言</a></li><li><a href=#2-%e5%86%b3%e7%ad%96%e6%a0%91decision-tree>决策树(Decision Tree)</a><ul><li>2.1 <a href=#21-%e7%9b%b4%e8%a7%8920%e4%b8%aa%e9%97%ae%e9%a2%98%e6%b8%b8%e6%88%8f>直觉:20个问题游戏</a></li><li>2.2 <a href=#22-%e7%ba%af%e5%ba%a6%e7%9a%84%e5%ba%a6%e9%87%8f%e7%86%b5%e4%b8%8e%e5%9f%ba%e5%b0%bc>纯度的度量:熵与基尼</a></li><li>2.3 <a href=#23-%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9%e6%9c%80%e5%a4%a7%e5%8c%96%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a>特征选择:最大化信息增益</a></li><li>2.4 <a href=#24-%e5%86%b3%e7%ad%96%e6%a0%91%e7%9a%84%e6%9e%84%e5%bb%ba%e7%ae%97%e6%b3%95>决策树的构建算法</a></li><li>2.5 <a href=#25-%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e5%89%aa%e6%9e%9d>过拟合与剪枝</a></li></ul></li><li><a href=#3-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%80%bb%e7%ba%b2>集成学习的总纲</a><ul><li>3.1 <a href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9b%86%e6%88%90>为什么需要集成?</a></li><li>3.2 <a href=#32-bias-variance-tradeoff>Bias-Variance Tradeoff</a></li><li>3.3 <a href=#33-%e9%9b%86%e6%88%90%e7%9a%84%e4%b8%a4%e5%a4%a7%e6%b5%81%e6%b4%bebagging-vs-boosting>集成的两大流派:Bagging vs Boosting</a></li></ul></li><li><a href=#4-bagging%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97>Bagging与随机森林</a><ul><li>4.1 <a href=#41-bootstrap%e9%87%87%e6%a0%b7>Bootstrap采样</a></li><li>4.2 <a href=#42-bagging%e7%9a%84%e9%99%8d%e6%96%b9%e5%b7%ae%e6%9c%ba%e5%88%b6>Bagging的降方差机制</a></li><li>4.3 <a href=#43-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%9a%84%e5%8f%8c%e9%87%8d%e9%9a%8f%e6%9c%ba%e6%80%a7>随机森林的双重随机性</a></li><li>4.4 <a href=#44-out-of-bag%e4%bc%b0%e8%ae%a1>Out-of-Bag估计</a></li></ul></li><li><a href=#5-boosting%e4%b9%8b%e9%ad%82adaboost>Boosting之魂:AdaBoost</a><ul><li>5.1 <a href=#51-adaboost%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>AdaBoost算法流程</a></li><li>5.2 <a href=#52-%e6%a0%b8%e5%bf%83%e6%8e%a8%e5%af%bc%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af%e6%8c%87%e6%95%b0%e6%8d%9f%e5%a4%b1>核心推导:为什么是指数损失?</a></li><li>5.3 <a href=#53-%e6%a0%b7%e6%9c%ac%e6%9d%83%e9%87%8d%e6%9b%b4%e6%96%b0%e5%85%ac%e5%bc%8f%e7%9a%84%e6%8e%a8%e5%af%bc>样本权重更新公式的推导</a></li><li>5.4 <a href=#54-adaboost%e7%9a%84%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>AdaBoost的几何直觉</a></li></ul></li><li><a href=#6-boosting%e4%b9%8b%e7%a5%9egbdt%e4%b8%8exgboost>Boosting之神:GBDT与XGBoost</a><ul><li>6.1 <a href=#61-gbdt%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e5%86%b3%e7%ad%96%e6%a0%91>GBDT:梯度提升决策树</a></li><li>6.2 <a href=#62-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%9f%e5%90%88%e6%ae%8b%e5%b7%ae>为什么拟合残差?</a></li><li>6.3 <a href=#63-xgboost%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80>XGBoost:二阶泰勒展开</a></li><li>6.4 <a href=#64-xgboost%e7%9a%84%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc>XGBoost的目标函数推导</a></li><li>6.5 <a href=#65-xgboost%e7%9a%84%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96>XGBoost的工程优化</a></li></ul></li><li><a href=#7-%e6%80%bb%e7%bb%93>总结</a></li><li><a href=#8-%e9%99%84%e5%bd%95xgboost%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f>附录:XGBoost核心公式</a></li></ol><hr><h2 id=1-引言>1. 引言<a class=anchor href=#1-%e5%bc%95%e8%a8%80>#</a></h2><p><strong>集成学习 (Ensemble Learning)</strong> 的核心思想非常简单：<strong>如果你不能信任单个专家，那就组织一个委员会</strong>。</p><p>这个想法可以追溯到18世纪法国数学家 <strong>Condorcet</strong> 的陪审团定理 (Jury Theorem):</p><blockquote class=book-hint><p>假设每个陪审员做出正确判断的概率是 $p > 0.5$，那么当陪审员人数趋于无穷时，<strong>多数投票的正确率趋于1</strong>。</p></blockquote><p>数学上，如果有 $n$ 个独立的陪审员，每人正确率为 $p$，则多数投票的正确率为：</p><p>$$
P(\text{正确}) = \sum_{k > n/2} \binom{n}{k} p^k (1-p)^{n-k}
$$</p><p>当 $n \to \infty$ 且 $p > 0.5$ 时，$P(\text{正确}) \to 1$。</p><p>这个简单的概率论结果揭示了集成学习的两个核心假设：</p><ol><li><strong>每个基学习器要"比瞎猜好一点"</strong>(准确率 $> 0.5$)</li><li><strong>基学习器之间要尽可能独立</strong></li></ol><p>但机器学习的集成不是简单的投票。我们有两个更深刻的问题：</p><ul><li><strong>如何让基学习器多样化？</strong>（Bagging vs Boosting）</li><li><strong>如何组合它们？</strong>（平均 vs 加权 vs 学习组合）</li></ul><p>本章将从最简单的基学习器——<strong>决策树</strong>——开始，逐步构建起集成学习的完整大厦。</p><hr><h2 id=2-决策树decision-tree>2. 决策树(Decision Tree)<a class=anchor href=#2-%e5%86%b3%e7%ad%96%e6%a0%91decision-tree>#</a></h2><h3 id=21-直觉20个问题游戏>2.1 直觉：20个问题游戏<a class=anchor href=#21-%e7%9b%b4%e8%a7%8920%e4%b8%aa%e9%97%ae%e9%a2%98%e6%b8%b8%e6%88%8f>#</a></h3><p>决策树的构建过程类似于一个古老的游戏：<strong>20个问题</strong>。</p><blockquote class=book-hint><p>游戏规则：我心里想一个物品，你可以问我20个是非问题，猜出这个物品。</p></blockquote><p>最优策略是什么？<strong>每次问题都要最大化信息增益</strong>，即让答案尽可能平均地分割可能的物品集合。</p><p>例如：</p><ul><li><strong>好问题</strong>：&ldquo;是动物吗？"（假设一半是动物，一半不是）</li><li><strong>坏问题</strong>：&ldquo;是猫吗？"（只有1/1000的概率）</li></ul><p>这个直觉就是决策树的核心：<strong>每次分裂，选择最能降低不确定性的特征</strong>。</p><h3 id=22-纯度的度量熵与基尼>2.2 纯度的度量：熵与基尼<a class=anchor href=#22-%e7%ba%af%e5%ba%a6%e7%9a%84%e5%ba%a6%e9%87%8f%e7%86%b5%e4%b8%8e%e5%9f%ba%e5%b0%bc>#</a></h3><p>如何度量"不确定性"或"不纯度&rdquo;？</p><h4 id=1-熵entropy>(1) 熵(Entropy)<a class=anchor href=#1-%e7%86%b5entropy>#</a></h4><p>来自信息论，由 Shannon 提出。对于随机变量 $Y$ 的分布 $p = (p_1, \ldots, p_C)$（$C$ 个类别），熵定义为：</p><p>$$
H(p) = -\sum_{c=1}^C p_c \log_2 p_c
$$</p><p><strong>物理意义</strong>：</p><ul><li>当 $Y$ 确定时（某个 $p_c = 1$），$H = 0$（无不确定性）</li><li>当 $Y$ 均匀分布时（所有 $p_c = 1/C$），$H = \log_2 C$（最大不确定性）</li><li>熵衡量的是<strong>平均编码长度</strong>：你需要多少bit来描述一个随机样本？</li></ul><p><strong>二分类的熵曲线</strong>：</p><p>设正类概率为 $p$，负类为 $1-p$，则：</p><p>$$
H(p) = -p \log_2 p - (1-p) \log_2 (1-p)
$$</p><p>这是一个"倒扣的碗&rdquo;：</p><ul><li>$p = 0$ 或 $p = 1$ 时，$H = 0$（纯净）</li><li>$p = 0.5$ 时，$H = 1$（最混乱）</li></ul><p><img src=assets/decision_tree_entropy.svg alt=熵减直觉></p><p>上图展示了：</p><ul><li><strong>左上</strong>：熵函数的"倒扣碗"形状</li><li><strong>右上</strong>：信息增益的直觉——分裂前后的熵减少量</li><li><strong>下方</strong>：一个具体例子——如何选择最佳分裂特征</li></ul><h4 id=2-基尼不纯度gini-impurity>(2) 基尼不纯度(Gini Impurity)<a class=anchor href=#2-%e5%9f%ba%e5%b0%bc%e4%b8%8d%e7%ba%af%e5%ba%a6gini-impurity>#</a></h4><p>另一个常用的度量是 <strong>Gini不纯度</strong>：</p><p>$$
\text{Gini}(p) = 1 - \sum_{c=1}^C p_c^2 = \sum_{c=1}^C p_c(1 - p_c)
$$</p><p><strong>物理意义</strong>：</p><ul><li>随机选两个样本，它们类别不同的概率</li><li>同样是"倒扣的碗"，但计算更简单（不需要对数）</li></ul><p><strong>二分类的Gini曲线</strong>：</p><p>$$
\text{Gini}(p) = 2p(1-p)
$$</p><p>与熵类似：</p><ul><li>$p = 0$ 或 $p = 1$ 时，$\text{Gini} = 0$</li><li>$p = 0.5$ 时，$\text{Gini} = 0.5$（最大）</li></ul><h4 id=3-熵-vs-gini有何区别>(3) 熵 vs Gini：有何区别？<a class=anchor href=#3-%e7%86%b5-vs-gini%e6%9c%89%e4%bd%95%e5%8c%ba%e5%88%ab>#</a></h4><p>它们形状相似，但有微妙差异：</p><ul><li><strong>熵</strong>对不纯度的惩罚更重（对数增长比线性快）</li><li><strong>Gini</strong>计算更快（无对数）</li><li>实践中性能差异很小，<strong>CART算法用Gini，ID3/C4.5用熵</strong></li></ul><h3 id=23-特征选择最大化信息增益>2.3 特征选择：最大化信息增益<a class=anchor href=#23-%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9%e6%9c%80%e5%a4%a7%e5%8c%96%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a>#</a></h3><h4 id=1-信息增益-information-gain>(1) 信息增益 (Information Gain)<a class=anchor href=#1-%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a-information-gain>#</a></h4><p>设数据集 $D$，当前熵为 $H(D)$。用特征 $A$ 将 $D$ 分成 $k$ 个子集 ${D_1, \ldots, D_k}$，则<strong>信息增益</strong>定义为：</p><p>$$
\text{IG}(D, A) = H(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} H(D_i)
$$</p><p><strong>直觉</strong>：</p><ul><li>$H(D)$：分裂前的不确定性</li><li>后一项：分裂后的<strong>加权平均不确定性</strong></li><li>$\text{IG}$：不确定性的<strong>减少量</strong></li></ul><p><strong>ID3算法</strong>：每次选择信息增益最大的特征进行分裂。</p><h4 id=2-信息增益比-gain-ratio>(2) 信息增益比 (Gain Ratio)<a class=anchor href=#2-%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a%e6%af%94-gain-ratio>#</a></h4><p>信息增益有个缺陷：<strong>偏好取值数量多的特征</strong>。</p><p>例如，用"身份证号"作为特征，每个样本一个取值，信息增益巨大（因为每个子集都纯净），但这是过拟合！</p><p><strong>C4.5算法</strong> 引入 <strong>信息增益比</strong> 来修正：</p><p>$$
\text{GainRatio}(D, A) = \frac{\text{IG}(D, A)}{H_A(D)}
$$</p><p>其中 $H_A(D)$ 是特征 $A$ 的<strong>固有值</strong>(Intrinsic Value):</p><p>$$
H_A(D) = -\sum_{i=1}^k \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
$$</p><p>它衡量特征 $A$ 自身的"分散程度"。取值越多，$H_A$ 越大，增益比越小，从而惩罚高基数特征。</p><h4 id=3-gini增益-gini-gain>(3) Gini增益 (Gini Gain)<a class=anchor href=#3-gini%e5%a2%9e%e7%9b%8a-gini-gain>#</a></h4><p><strong>CART算法</strong> (Classification and Regression Tree) 使用 Gini不纯度：</p><p>$$
\text{GiniGain}(D, A) = \text{Gini}(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} \text{Gini}(D_i)
$$</p><h3 id=24-决策树的构建算法>2.4 决策树的构建算法<a class=anchor href=#24-%e5%86%b3%e7%ad%96%e6%a0%91%e7%9a%84%e6%9e%84%e5%bb%ba%e7%ae%97%e6%b3%95>#</a></h3><p>决策树的构建是一个<strong>递归</strong>的过程：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>函数 BuildTree(D):
</span></span><span class=line><span class=cl>    如果 D 中所有样本属于同一类 C:
</span></span><span class=line><span class=cl>        返回叶节点，标记为 C
</span></span><span class=line><span class=cl>    如果 特征集为空 或 D 中样本在所有特征上取值相同:
</span></span><span class=line><span class=cl>        返回叶节点，标记为 D 中最多的类
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    选择最优特征 A*（最大化信息增益/Gini增益）
</span></span><span class=line><span class=cl>    为 A* 的每个取值 a 创建分支:
</span></span><span class=line><span class=cl>        D_a = D 中 A* = a 的样本
</span></span><span class=line><span class=cl>        递归调用 BuildTree(D_a)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    返回决策树</span></span></code></pre></div><p><strong>停止条件</strong>：</p><ol><li>节点内所有样本同类</li><li>特征集为空</li><li>样本数小于阈值（预剪枝）</li><li>树深度达到上限（预剪枝）</li></ol><h3 id=25-过拟合与剪枝>2.5 过拟合与剪枝<a class=anchor href=#25-%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e5%89%aa%e6%9e%9d>#</a></h3><p>决策树非常容易<strong>过拟合</strong>：如果不加限制，它会生长到每个叶子只有一个样本（训练误差为0，但泛化性能极差）。</p><h4 id=1-预剪枝pre-pruning>(1) 预剪枝(Pre-pruning)<a class=anchor href=#1-%e9%a2%84%e5%89%aa%e6%9e%9dpre-pruning>#</a></h4><p>在构建过程中提前停止：</p><ul><li>限制树的最大深度</li><li>限制叶节点的最小样本数</li><li>限制分裂的最小信息增益</li></ul><p><strong>优点</strong>：快速，避免过拟合
<strong>缺点</strong>：可能欠拟合（停得太早）</p><h4 id=2-后剪枝post-pruning>(2) 后剪枝(Post-pruning)<a class=anchor href=#2-%e5%90%8e%e5%89%aa%e6%9e%9dpost-pruning>#</a></h4><p>先生成完整的树，再自底向上地剪枝：</p><ul><li>对每个内部节点，考虑将其替换为叶节点</li><li>如果替换后验证集误差不增加，则剪枝</li><li><strong>C4.5的悲观剪枝</strong>：基于训练误差的统计检验</li></ul><p><strong>优点</strong>：通常比预剪枝效果好
<strong>缺点</strong>：需要验证集，计算代价大</p><hr><h2 id=3-集成学习的总纲>3. 集成学习的总纲<a class=anchor href=#3-%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%80%bb%e7%ba%b2>#</a></h2><h3 id=31-为什么需要集成>3.1 为什么需要集成？<a class=anchor href=#31-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e9%9b%86%e6%88%90>#</a></h3><p>单个决策树的问题：</p><ol><li><strong>高方差</strong>：对训练数据敏感，稍微改变样本，树的结构可能完全不同</li><li><strong>不稳定</strong>：容易过拟合</li><li><strong>局部最优</strong>：贪心算法，每次只选当前最优特征</li></ol><p>但决策树有个巨大优点：<strong>容易生成多样化的弱学习器</strong>！</p><ul><li>随机采样 → 不同的训练集 → 不同的树</li><li>随机特征 → 不同的分裂 → 不同的树</li></ul><p>这正是集成学习的基础。</p><h3 id=32-bias-variance-tradeoff>3.2 Bias-Variance Tradeoff<a class=anchor href=#32-bias-variance-tradeoff>#</a></h3><p>回顾第5章的泛化误差分解(假设噪声 $\sigma^2$ 不可约):</p><p>$$
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2[\hat{f}(x)] + \text{Var}[\hat{f}(x)] + \sigma^2
$$</p><p>其中：</p><ul><li><strong>偏差(Bias)</strong>：模型的<strong>平均预测</strong> 与 <strong>真实值</strong> 的差距，衡量欠拟合</li><li><strong>方差(Variance)</strong>：模型对<strong>训练集的敏感度</strong>，衡量过拟合</li></ul><p><strong>单个决策树</strong>：</p><ul><li>如果树很深：低偏差，高方差（过拟合）</li><li>如果树很浅：高偏差，低方差（欠拟合）</li></ul><p><strong>集成学习的两大策略</strong>：</p><ol><li><strong>Bagging</strong>（如随机森林）：<strong>降低方差</strong>，通过平均多个高方差模型</li><li><strong>Boosting</strong>（如AdaBoost、GBDT）：<strong>降低偏差</strong>，通过逐步拟合残差</li></ol><h3 id=33-集成的两大流派bagging-vs-boosting>3.3 集成的两大流派：Bagging vs Boosting<a class=anchor href=#33-%e9%9b%86%e6%88%90%e7%9a%84%e4%b8%a4%e5%a4%a7%e6%b5%81%e6%b4%bebagging-vs-boosting>#</a></h3><p><img src=assets/ensemble_bagging_boosting.svg alt="Bagging vs Boosting"></p><p>上图清晰地展示了两种范式的核心差异：</p><h4 id=1-bagging-bootstrap-aggregating>(1) Bagging (Bootstrap Aggregating)<a class=anchor href=#1-bagging-bootstrap-aggregating>#</a></h4><p><strong>策略</strong>：并行训练多个<strong>独立</strong>的强学习器，然后平均（回归）或投票（分类）</p><p>$$
\hat{f}(x) = \frac{1}{M} \sum_{m=1}^M f_m(x)
$$</p><p><strong>关键</strong>：</p><ul><li>基学习器之间<strong>独立</strong>（通过Bootstrap采样保证）</li><li>降低<strong>方差</strong>（平均的方差 = 单个方差 / $M$，如果独立）</li><li>基学习器要<strong>强</strong>（如深树）</li></ul><p><strong>代表</strong>：随机森林(Random Forest)</p><h4 id=2-boosting>(2) Boosting<a class=anchor href=#2-boosting>#</a></h4><p><strong>策略</strong>：串行训练多个<strong>弱</strong>学习器，每个学习器<strong>专注于前一个的错误</strong></p><p>$$
\hat{f}(x) = \sum_{m=1}^M \alpha_m f_m(x)
$$</p><p><strong>关键</strong>：</p><ul><li>基学习器之间<strong>相关</strong>（后者依赖前者）</li><li>降低<strong>偏差</strong>（逐步逼近真实函数）</li><li>基学习器要<strong>弱</strong>（如浅树，甚至决策桩）</li></ul><p><strong>代表</strong>：AdaBoost、GBDT、XGBoost</p><p><strong>直觉对比</strong>：</p><ul><li><strong>Bagging</strong>：&ldquo;三个臭皮匠，顶个诸葛亮&rdquo;——平均消除随机性</li><li><strong>Boosting</strong>：&ldquo;知错能改，善莫大焉&rdquo;——不断纠正错误</li></ul><hr><h2 id=4-bagging与随机森林>4. Bagging与随机森林<a class=anchor href=#4-bagging%e4%b8%8e%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97>#</a></h2><h3 id=41-bootstrap采样>4.1 Bootstrap采样<a class=anchor href=#41-bootstrap%e9%87%87%e6%a0%b7>#</a></h3><p><strong>Bootstrap</strong>（自助采样）是统计学中的一个经典技术：</p><p>给定数据集 $D = {(\mathbf{x}<em>i, y_i)}</em>{i=1}^N$，进行 $M$ 次采样：</p><ul><li>每次<strong>有放回地</strong>从 $D$ 中随机抽取 $N$ 个样本，得到 $D_m$</li><li>某个样本 $(\mathbf{x}_i, y_i)$ 没有被抽中的概率是：</li></ul><p>$$
\left(1 - \frac{1}{N}\right)^N \approx e^{-1} \approx 0.368
$$</p><p>因此，每个Bootstrap样本大约包含原始数据的 <strong>63.2%</strong>（不同的样本）。</p><h3 id=42-bagging的降方差机制>4.2 Bagging的降方差机制<a class=anchor href=#42-bagging%e7%9a%84%e9%99%8d%e6%96%b9%e5%b7%ae%e6%9c%ba%e5%88%b6>#</a></h3><p>假设每个基学习器的预测方差是 $\sigma^2$，它们之间的相关系数是 $\rho$。则 $M$ 个模型的平均预测的方差是：</p><p>$$
\text{Var}\left[\frac{1}{M}\sum_{m=1}^M f_m(x)\right] = \rho\sigma^2 + \frac{1-\rho}{M}\sigma^2
$$</p><p><strong>推导</strong>（简化，假设方差相同）：</p><p>$$
\begin{aligned}
\text{Var}\left[\frac{1}{M}\sum_{m=1}^M f_m\right] &= \frac{1}{M^2} \text{Var}\left[\sum_{m=1}^M f_m\right] \
&= \frac{1}{M^2} \left( M\sigma^2 + M(M-1)\rho\sigma^2 \right) \
&= \frac{\sigma^2}{M} + \frac{M-1}{M}\rho\sigma^2 \
&\approx \rho\sigma^2 + \frac{1-\rho}{M}\sigma^2
\end{aligned}
$$</p><p><strong>关键洞察</strong>：</p><ul><li>如果模型<strong>完全独立</strong>（$\rho = 0$），方差降低为 $\sigma^2 / M$（理想情况）</li><li>如果模型<strong>完全相关</strong>（$\rho = 1$），方差仍为 $\sigma^2$（无改进）</li><li>Bootstrap采样保证 $\rho &lt; 1$，但不够小</li></ul><p><strong>问题</strong>：用同一数据集的Bootstrap样本训练，模型之间仍有较高相关性。</p><p><strong>解决</strong>：随机森林的<strong>特征随机性</strong>。</p><h3 id=43-随机森林的双重随机性>4.3 随机森林的双重随机性<a class=anchor href=#43-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%9a%84%e5%8f%8c%e9%87%8d%e9%9a%8f%e6%9c%ba%e6%80%a7>#</a></h3><p><strong>随机森林(Random Forest)</strong> 在Bagging的基础上增加了<strong>特征随机性</strong>：</p><p><strong>算法</strong>：</p><ol><li><strong>样本随机</strong>：用Bootstrap采样得到 $M$ 个训练集 $D_m$</li><li><strong>特征随机</strong>：在每次分裂时，随机选择 $k$ 个特征（通常 $k = \sqrt{p}$，其中 $p$ 是总特征数），只从这 $k$ 个特征中选最优分裂</li><li>训练 $M$ 棵深树（不剪枝）</li><li>预测时：<ul><li><strong>分类</strong>：多数投票</li><li><strong>回归</strong>：平均</li></ul></li></ol><p><strong>为什么特征随机性有效？</strong></p><p>强特征会在大多数树中被选中，导致树之间相似（高 $\rho$）。特征随机性强制某些树"忽略"强特征，探索其他可能性，从而<strong>降低相关性</strong> $\rho$。</p><p><strong>超参数</strong>：</p><ul><li>$M$：树的数量（越多越好，但边际收益递减）</li><li>$k$：每次分裂考虑的特征数（默认 $\sqrt{p}$ 分类，$p/3$ 回归）</li><li>树的深度：通常不剪枝（单棵树方差大，但集成后平均掉）</li></ul><h3 id=44-out-of-bag估计>4.4 Out-of-Bag估计<a class=anchor href=#44-out-of-bag%e4%bc%b0%e8%ae%a1>#</a></h3><p>Bootstrap采样的一个副产品：<strong>袋外(Out-of-Bag, OOB)样本</strong>。</p><p>对于每棵树 $f_m$，约有37%的样本没有在 $D_m$ 中出现。这些样本可以用来：</p><ul><li><strong>验证</strong>：计算OOB误差，作为泛化误差的无偏估计</li><li><strong>特征重要性</strong>：打乱某个特征的值，看OOB误差增加多少</li></ul><p>OOB误差几乎等价于交叉验证，但<strong>无需额外计算</strong>！</p><hr><h2 id=5-boosting之魂adaboost>5. Boosting之魂:AdaBoost<a class=anchor href=#5-boosting%e4%b9%8b%e9%ad%82adaboost>#</a></h2><h3 id=51-adaboost算法流程>5.1 AdaBoost算法流程<a class=anchor href=#51-adaboost%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b>#</a></h3><p><strong>AdaBoost</strong>(Adaptive Boosting) 由 Freund & Schapire (1997) 提出，是Boosting的开山之作。</p><p><strong>算法</strong>（二分类，$y \in {-1, +1}$）：</p><ol><li><p><strong>初始化</strong>样本权重：$w_i^{(1)} = \frac{1}{N}$，$i = 1, \ldots, N$</p></li><li><p><strong>For</strong> $m = 1$ to $M$:</p><p>a. 用权重 $\mathbf{w}^{(m)}$ 训练基学习器 $f_m(x)$，得到预测</p><p>b. 计算<strong>加权错误率</strong>：
$$
\epsilon_m = \frac{\sum_{i=1}^N w_i^{(m)} \mathbb{I}[y_i \neq f_m(\mathbf{x}<em>i)]}{\sum</em>{i=1}^N w_i^{(m)}}
$$</p><p>c. 计算基学习器的<strong>权重</strong>：
$$
\alpha_m = \frac{1}{2} \ln \frac{1 - \epsilon_m}{\epsilon_m}
$$</p><p>d. <strong>更新样本权重</strong>：
$$
w_i^{(m+1)} = w_i^{(m)} \exp(-\alpha_m y_i f_m(\mathbf{x}_i))
$$
（然后归一化：$w_i^{(m+1)} \leftarrow w_i^{(m+1)} / \sum_j w_j^{(m+1)}$）</p></li><li><p><strong>输出</strong>最终分类器：
$$
H(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m f_m(x)\right)
$$</p></li></ol><p><strong>直觉</strong>：</p><ul><li>$\alpha_m$：基学习器的发言权（错误率越低，权重越大）</li><li>$w_i^{(m+1)}$：被误分类的样本权重增加（$y_i f_m(\mathbf{x}_i) &lt; 0$）</li><li>每轮关注"难样本"</li></ul><p>但这些公式是<strong>怎么来的</strong>？为什么是 $\ln \frac{1-\epsilon}{\epsilon}$？为什么是 $\exp(-\alpha y f)$？</p><h3 id=52-核心推导为什么是指数损失>5.2 核心推导：为什么是指数损失？<a class=anchor href=#52-%e6%a0%b8%e5%bf%83%e6%8e%a8%e5%af%bc%e4%b8%ba%e4%bb%80%e4%b9%88%e6%98%af%e6%8c%87%e6%95%b0%e6%8d%9f%e5%a4%b1>#</a></h3><p>AdaBoost 不是拍脑袋发明的！它是在最小化一个特定的损失函数：<strong>指数损失 (Exponential Loss)</strong>。</p><p><strong>定理</strong>（Friedman et al., 2000）：AdaBoost 等价于用<strong>前向分步算法</strong>(Forward Stagewise Additive Modeling) 最小化指数损失。</p><h4 id=1-指数损失>(1) 指数损失<a class=anchor href=#1-%e6%8c%87%e6%95%b0%e6%8d%9f%e5%a4%b1>#</a></h4><p>定义损失函数：</p><p>$$
L(y, f(x)) = \exp(-y f(x))
$$</p><p>对于整个数据集：</p><p>$$
L = \sum_{i=1}^N \exp\left(-y_i \sum_{m=1}^M \alpha_m f_m(\mathbf{x}_i)\right)
$$</p><p><strong>为什么选指数损失？</strong></p><ul><li>它是0-1损失的<strong>上界</strong>（可微的替代）</li><li>数学上容易优化（可加性）</li><li>对误分类点的惩罚呈指数增长（关注难样本）</li></ul><h4 id=2-前向分步算法>(2) 前向分步算法<a class=anchor href=#2-%e5%89%8d%e5%90%91%e5%88%86%e6%ad%a5%e7%ae%97%e6%b3%95>#</a></h4><p>直接优化 $L$ 很难（$M$ 个模型耦合）。<strong>前向分步算法</strong>采用<strong>贪心策略</strong>：</p><ul><li>假设前 $m-1$ 步已经得到 $f_{m-1}(x) = \sum_{k=1}^{m-1} \alpha_k f_k(x)$</li><li>第 $m$ 步只优化 $(f_m, \alpha_m)$，固定之前的</li></ul><p>目标：</p><p>$$
(\alpha_m, f_m) = \arg\min_{\alpha, f} \sum_{i=1}^N \exp\left(-y_i [f_{m-1}(\mathbf{x}_i) + \alpha f(\mathbf{x}_i)]\right)
$$</p><p>定义：</p><p>$$
\bar{w}<em>i^{(m)} = \exp(-y_i f</em>{m-1}(\mathbf{x}_i))
$$</p><p>（这是前 $m-1$ 步累积的"难度"）</p><p>则目标函数变为：</p><p>$$
\sum_{i=1}^N \bar{w}_i^{(m)} \exp(-y_i \alpha f(\mathbf{x}_i))
$$</p><h3 id=53-样本权重更新公式的推导>5.3 样本权重更新公式的推导<a class=anchor href=#53-%e6%a0%b7%e6%9c%ac%e6%9d%83%e9%87%8d%e6%9b%b4%e6%96%b0%e5%85%ac%e5%bc%8f%e7%9a%84%e6%8e%a8%e5%af%bc>#</a></h3><p>现在推导 $\alpha_m$ 和 $f_m$。</p><h4 id=1-优化-f_m>(1) 优化 $f_m$<a class=anchor href=#1-%e4%bc%98%e5%8c%96-f_m>#</a></h4><p>给定 $\alpha$，优化 $f$：</p><p>$$
f_m = \arg\min_f \sum_{i=1}^N \bar{w}_i^{(m)} \exp(-y_i \alpha f(\mathbf{x}_i))
$$</p><p>因为 $f(\mathbf{x}_i) \in {-1, +1}$，分两种情况：</p><ul><li>如果 $y_i = f(\mathbf{x}_i)$（正确）：$\exp(-\alpha)$</li><li>如果 $y_i \neq f(\mathbf{x}_i)$（错误）：$\exp(\alpha)$</li></ul><p>因此：</p><p>$$
\begin{aligned}
\sum_i \bar{w}_i^{(m)} \exp(-y_i \alpha f(\mathbf{x}<em>i)) &= e^{-\alpha} \sum</em>{y_i = f(\mathbf{x}_i)} \bar{w}<em>i^{(m)} + e^{\alpha} \sum</em>{y_i \neq f(\mathbf{x}<em>i)} \bar{w}<em>i^{(m)} \
&= e^{-\alpha} (W - W</em>{\text{err}}) + e^{\alpha} W</em>{\text{err}}
\end{aligned}
$$</p><p>其中 $W = \sum_i \bar{w}<em>i^{(m)}$，$W</em>{\text{err}} = \sum_{y_i \neq f(\mathbf{x}_i)} \bar{w}_i^{(m)}$。</p><p><strong>关键</strong>：这个式子与 $\alpha$ 无关！$f_m$ 只需最小化<strong>加权错误率</strong>：</p><p>$$
f_m = \arg\min_f \sum_{i=1}^N \bar{w}_i^{(m)} \mathbb{I}[y_i \neq f(\mathbf{x}_i)]
$$</p><p>这正是AdaBoost的步骤2a！</p><h4 id=2-优化-alpha_m>(2) 优化 $\alpha_m$<a class=anchor href=#2-%e4%bc%98%e5%8c%96-alpha_m>#</a></h4><p>给定 $f_m$，优化 $\alpha$：</p><p>$$
\alpha_m = \arg\min_\alpha \left[ e^{-\alpha} (W - W_{\text{err}}) + e^{\alpha} W_{\text{err}} \right]
$$</p><p>求导并令其为0：</p><p>$$
-e^{-\alpha} (W - W_{\text{err}}) + e^{\alpha} W_{\text{err}} = 0
$$</p><p>解得：</p><p>$$
e^{2\alpha} = \frac{W - W_{\text{err}}}{W_{\text{err}}} = \frac{1 - \epsilon}{\epsilon}
$$</p><p>其中 $\epsilon = W_{\text{err}} / W$ 是加权错误率。因此：</p><p>$$
\boxed{\alpha_m = \frac{1}{2} \ln \frac{1 - \epsilon_m}{\epsilon_m}}
$$</p><p><strong>完美对应</strong>！</p><h4 id=3-权重更新>(3) 权重更新<a class=anchor href=#3-%e6%9d%83%e9%87%8d%e6%9b%b4%e6%96%b0>#</a></h4><p>下一轮的权重是：</p><p>$$
\bar{w}_i^{(m+1)} = \bar{w}_i^{(m)} \exp(-y_i \alpha_m f_m(\mathbf{x}_i))
$$</p><p>代入 $\bar{w}<em>i^{(m)} = \exp(-y_i f</em>{m-1}(\mathbf{x}_i))$：</p><p>$$
\bar{w}<em>i^{(m+1)} = \exp\left(-y_i \sum</em>{k=1}^m \alpha_k f_k(\mathbf{x}_i)\right)
$$</p><p>递推关系：</p><p>$$
\boxed{w_i^{(m+1)} = w_i^{(m)} \exp(-\alpha_m y_i f_m(\mathbf{x}_i))}
$$</p><p>（归一化后得到概率分布）</p><p><strong>至此，AdaBoost的所有公式都从指数损失推导出来了！</strong></p><h3 id=54-adaboost的几何直觉>5.4 AdaBoost的几何直觉<a class=anchor href=#54-adaboost%e7%9a%84%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>#</a></h3><p><strong>为什么AdaBoost有效？</strong></p><p>从<strong>函数空间</strong>看：</p><ul><li>每个基学习器 $f_m$ 是一个"方向"</li><li>$\alpha_m$ 是沿该方向走多远</li><li>AdaBoost在函数空间中做<strong>梯度下降</strong>（指数损失的负梯度）</li></ul><p>从<strong>样本空间</strong>看：</p><ul><li>初始所有样本等权重（均匀分布）</li><li>每轮增加困难样本的权重</li><li>最终集成 = 一系列"专家"，每个专注于前一个的盲区</li></ul><p><strong>泛化能力</strong>：</p><ul><li>即使 $M$ 很大，AdaBoost也不容易过拟合（Schapire et al., 1998）</li><li>原因：增加 $M$ 会增加<strong>间隔(margin)</strong>，即 $y \sum_m \alpha_m f_m(x)$ 的绝对值</li><li>大间隔 → 强置信度 → 更好的泛化</li></ul><hr><h2 id=6-boosting之神gbdt与xgboost>6. Boosting之神:GBDT与XGBoost<a class=anchor href=#6-boosting%e4%b9%8b%e7%a5%9egbdt%e4%b8%8exgboost>#</a></h2><h3 id=61-gbdt梯度提升决策树>6.1 GBDT:梯度提升决策树<a class=anchor href=#61-gbdt%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e5%86%b3%e7%ad%96%e6%a0%91>#</a></h3><p><strong>GBDT</strong>(Gradient Boosting Decision Tree) 是AdaBoost的推广，由 Friedman (2001) 提出。</p><p><strong>核心思想</strong>：AdaBoost只能用指数损失。如果我想用<strong>任意损失函数</strong>（如平方损失、对数损失），怎么办？</p><p><strong>答案</strong>：用<strong>梯度</strong>代替残差！</p><h4 id=1-前向分步算法通用框架>(1) 前向分步算法（通用框架）<a class=anchor href=#1-%e5%89%8d%e5%90%91%e5%88%86%e6%ad%a5%e7%ae%97%e6%b3%95%e9%80%9a%e7%94%a8%e6%a1%86%e6%9e%b6>#</a></h4><p>目标：最小化损失</p><p>$$
L = \sum_{i=1}^N \ell(y_i, f(\mathbf{x}_i))
$$</p><p>其中 $f(x) = \sum_{m=1}^M f_m(x)$ 是基学习器的加权和。</p><p><strong>前向分步</strong>：第 $m$ 步，已有 $F_{m-1}(x)$，要找 $f_m$ 使得</p><p>$$
F_m(x) = F_{m-1}(x) + f_m(x)
$$</p><p>最小化：</p><p>$$
\sum_{i=1}^N \ell(y_i, F_{m-1}(\mathbf{x}_i) + f_m(\mathbf{x}_i))
$$</p><p><strong>困难</strong>：对于复杂的 $\ell$（如交叉熵），很难直接优化 $f_m$。</p><h4 id=2-梯度下降的类比>(2) 梯度下降的类比<a class=anchor href=#2-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%9a%84%e7%b1%bb%e6%af%94>#</a></h4><p>在参数空间，梯度下降是：</p><p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
$$</p><p>在<strong>函数空间</strong>，类比为：</p><p>$$
F_m(x) = F_{m-1}(x) - \eta \cdot g_m(x)
$$</p><p>其中 $g_m(x)$ 是损失函数对 $F_{m-1}(x)$ 的<strong>伪残差</strong>(pseudo-residual):</p><p>$$
g_m(\mathbf{x}_i) = \left[\frac{\partial \ell(y_i, F(\mathbf{x}<em>i))}{\partial F(\mathbf{x}<em>i)}\right]</em>{F = F</em>{m-1}}
$$</p><p><strong>GBDT的策略</strong>：</p><ul><li>计算伪残差 ${g_m(\mathbf{x}_i)}$</li><li>训练一棵树 $f_m$ 去<strong>拟合</strong> $g_m$（而不是 $y_i$）</li><li>更新 $F_m = F_{m-1} + \eta f_m$</li></ul><h3 id=62-为什么拟合残差>6.2 为什么拟合残差？<a class=anchor href=#62-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%9f%e5%90%88%e6%ae%8b%e5%b7%ae>#</a></h3><p>对于<strong>平方损失</strong> $\ell(y, F) = \frac{1}{2}(y - F)^2$，伪残差是：</p><p>$$
g_m(\mathbf{x}<em>i) = \frac{\partial}{\partial F} \frac{1}{2}(y_i - F)^2 \bigg|</em>{F = F_{m-1}(\mathbf{x}<em>i)} = -(y_i - F</em>{m-1}(\mathbf{x}_i)) = -r_i^{(m)}
$$</p><p>即<strong>负残差</strong>！</p><p>因此，<strong>拟合残差 = 沿负梯度方向</strong>。</p><p><strong>GBDT算法</strong>（回归）：</p><ol><li><p>初始化：$F_0(x) = \arg\min_c \sum_i \ell(y_i, c)$（通常是 $\bar{y}$）</p></li><li><p><strong>For</strong> $m = 1$ to $M$:</p><p>a. 计算伪残差：
$$
r_i^{(m)} = -\left[\frac{\partial \ell(y_i, F(\mathbf{x}<em>i))}{\partial F(\mathbf{x}<em>i)}\right]</em>{F = F</em>{m-1}}
$$</p><p>b. 训练回归树 $f_m$，拟合 ${(\mathbf{x}_i, r_i^{(m)})}$</p><p>c. <strong>更新</strong>：
$$
F_m(x) = F_{m-1}(x) + \eta f_m(x)
$$</p></li><li><p>输出 $F_M(x)$</p></li></ol><p><strong>直觉</strong>：</p><ul><li>每棵树修正前一个模型的错误</li><li>逐步逼近真实函数</li><li>类似于梯度下降，但在函数空间</li></ul><p><strong>为什么GBDT强大？</strong></p><ul><li>可用于<strong>任意可微损失</strong>（分类、回归、排序&mldr;）</li><li>树的结构能自动做<strong>特征组合</strong>（非线性交互）</li><li>天然支持<strong>缺失值处理</strong></li></ul><h3 id=63-xgboost二阶泰勒展开>6.3 XGBoost：二阶泰勒展开<a class=anchor href=#63-xgboost%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80>#</a></h3><p><strong>XGBoost</strong> (eXtreme Gradient Boosting) 由陈天奇 (2016) 提出，是GBDT的工程优化版本，也是Kaggle竞赛的"屠榜"利器。</p><p><strong>核心改进</strong>：</p><ol><li>用<strong>二阶泰勒展开</strong>近似损失函数（更精确）</li><li>在目标函数中加入<strong>正则项</strong>（防止过拟合）</li><li>高效的<strong>分裂算法</strong>（加权分位数、稀疏感知）</li><li>系统优化（并行化、缓存优化&mldr;）</li></ol><h3 id=64-xgboost的目标函数推导>6.4 XGBoost的目标函数推导<a class=anchor href=#64-xgboost%e7%9a%84%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc>#</a></h3><h4 id=1-目标函数>(1) 目标函数<a class=anchor href=#1-%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0>#</a></h4><p>第 $t$ 步的目标（加入正则化）：</p><p>$$
\mathcal{L}^{(t)} = \sum_{i=1}^N \ell(y_i, F_{t-1}(\mathbf{x}_i) + f_t(\mathbf{x}_i)) + \Omega(f_t)
$$</p><p>其中 $\Omega(f_t)$ 是树的复杂度惩罚：</p><p>$$
\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2
$$</p><ul><li>$T$：叶子节点数</li><li>$w_j$：第 $j$ 个叶子的输出值</li><li>$\gamma, \lambda$：正则化系数</li></ul><h4 id=2-二阶泰勒展开>(2) 二阶泰勒展开<a class=anchor href=#2-%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80>#</a></h4><p>对损失函数在 $F_{t-1}(\mathbf{x}_i)$ 处做泰勒展开：</p><p>$$
\ell(y_i, F_{t-1} + f_t) \approx \ell(y_i, F_{t-1}) + g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t^2(\mathbf{x}_i)
$$</p><p>其中：</p><p>$$
\begin{aligned}
g_i &= \frac{\partial \ell(y_i, F)}{\partial F}\bigg|<em>{F = F</em>{t-1}(\mathbf{x}<em>i)} \quad \text{(一阶梯度)} \
h_i &= \frac{\partial^2 \ell(y_i, F)}{\partial F^2}\bigg|</em>{F = F_{t-1}(\mathbf{x}_i)} \quad \text{(二阶梯度)}
\end{aligned}
$$</p><p>去掉常数项 $\ell(y_i, F_{t-1})$，目标函数简化为：</p><p>$$
\tilde{\mathcal{L}}^{(t)} = \sum_{i=1}^N \left[g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t^2(\mathbf{x}_i)\right] + \Omega(f_t)
$$</p><h4 id=3-叶子节点的最优值>(3) 叶子节点的最优值<a class=anchor href=#3-%e5%8f%b6%e5%ad%90%e8%8a%82%e7%82%b9%e7%9a%84%e6%9c%80%e4%bc%98%e5%80%bc>#</a></h4><p>设树 $f_t$ 将样本分到 $T$ 个叶子，定义：</p><ul><li>$I_j = {i \mid f_t(\mathbf{x}_i) \text{ 在叶子 } j}$：叶子 $j$ 的样本索引集</li><li>$w_j$：叶子 $j$ 的输出值</li></ul><p>则 $f_t(\mathbf{x}_i) = w_j$ 对于 $i \in I_j$。代入目标函数：</p><p>$$
\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^T \left[\left(\sum_{i \in I_j} g_i\right) w_j + \frac{1}{2}\left(\sum_{i \in I_j} h_i + \lambda\right) w_j^2\right] + \gamma T
$$</p><p>定义：</p><p>$$
G_j = \sum_{i \in I_j} g_i, \quad H_j = \sum_{i \in I_j} h_i
$$</p><p>则：</p><p>$$
\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^T \left[G_j w_j + \frac{1}{2}(H_j + \lambda) w_j^2\right] + \gamma T
$$</p><p>这是关于 $w_j$ 的二次函数！对 $w_j$ 求导并令其为0：</p><p>$$
\frac{\partial \tilde{\mathcal{L}}^{(t)}}{\partial w_j} = G_j + (H_j + \lambda) w_j = 0
$$</p><p>解得<strong>最优叶子值</strong>：</p><p>$$
\boxed{w_j^* = -\frac{G_j}{H_j + \lambda}}
$$</p><p>代入目标函数，得到<strong>最小损失</strong>：</p><p>$$
\boxed{\tilde{\mathcal{L}}^{(t)} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j + \lambda} + \gamma T}
$$</p><h4 id=4-分裂增益>(4) 分裂增益<a class=anchor href=#4-%e5%88%86%e8%a3%82%e5%a2%9e%e7%9b%8a>#</a></h4><p>如何选择最优分裂？</p><p>考虑将叶子 $I$ 分成左右两个子叶 $I_L$ 和 $I_R$。分裂前的损失：</p><p>$$
L_{\text{before}} = -\frac{1}{2} \frac{G^2}{H + \lambda} + \gamma
$$</p><p>分裂后的损失：</p><p>$$
L_{\text{after}} = -\frac{1}{2} \left(\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda}\right) + 2\gamma
$$</p><p><strong>分裂增益</strong>（损失减少量）：</p><p>$$
\boxed{\text{Gain} = \frac{1}{2} \left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{G^2}{H + \lambda}\right] - \gamma}
$$</p><ul><li>如果 $\text{Gain} > 0$，则分裂</li><li>否则，停止（$\gamma$ 控制分裂的代价）</li></ul><p><strong>关键洞察</strong>：</p><ul><li>XGBoost的分裂准则<strong>同时考虑了一阶和二阶梯度</strong></li><li>GBDT只用一阶梯度（残差）</li><li>二阶信息 → 更精确的牛顿步 → 更快收敛</li></ul><h3 id=65-xgboost的工程优化>6.5 XGBoost的工程优化<a class=anchor href=#65-xgboost%e7%9a%84%e5%b7%a5%e7%a8%8b%e4%bc%98%e5%8c%96>#</a></h3><p>XGBoost不仅在算法上优越，工程实现也极致：</p><h4 id=1-分裂算法>(1) 分裂算法<a class=anchor href=#1-%e5%88%86%e8%a3%82%e7%ae%97%e6%b3%95>#</a></h4><ul><li><strong>精确贪心</strong>：枚举所有特征的所有分裂点（小数据）</li><li><strong>近似算法</strong>：用加权分位数找候选分裂点（大数据）</li><li><strong>稀疏感知</strong>：缺失值分配到左/右子树，选最优方向</li></ul><h4 id=2-系统优化>(2) 系统优化<a class=anchor href=#2-%e7%b3%bb%e7%bb%9f%e4%bc%98%e5%8c%96>#</a></h4><ul><li><strong>列块(Column Block)</strong>：特征按列存储，预排序，便于并行查找分裂点</li><li><strong>缓存优化</strong>：梯度统计量缓存，减少计算</li><li><strong>核外计算(Out-of-core)</strong>：数据不装入内存，分块读取</li></ul><h4 id=3-正则化技巧>(3) 正则化技巧<a class=anchor href=#3-%e6%ad%a3%e5%88%99%e5%8c%96%e6%8a%80%e5%b7%a7>#</a></h4><ul><li><strong>列采样(Column Subsampling)</strong>：类似随机森林，每棵树只用部分特征</li><li><strong>行采样(Row Subsampling)</strong>：每棵树只用部分样本</li><li><strong>Shrinkage</strong>：学习率 $\eta &lt; 1$，放缓学习</li></ul><hr><h2 id=7-总结>7. 总结<a class=anchor href=#7-%e6%80%bb%e7%bb%93>#</a></h2><p>本章我们经历了集成学习的完整旅程：</p><h3 id=1-决策树基石>(1) 决策树：基石<a class=anchor href=#1-%e5%86%b3%e7%ad%96%e6%a0%91%e5%9f%ba%e7%9f%b3>#</a></h3><ul><li><strong>纯度度量</strong>：熵与Gini，本质都是"倒扣的碗"</li><li><strong>特征选择</strong>：最大化信息增益 = 最小化划分后的不确定性</li><li><strong>优缺点</strong>：易解释、处理非线性，但高方差、易过拟合</li></ul><h3 id=2-bagging降方差>(2) Bagging：降方差<a class=anchor href=#2-bagging%e9%99%8d%e6%96%b9%e5%b7%ae>#</a></h3><ul><li><strong>核心</strong>：Bootstrap + 平均 → 降低方差</li><li><strong>随机森林</strong>：样本随机 + 特征随机 → 降低相关性 → 进一步降方差</li><li><strong>OOB</strong>：免费的验证集</li></ul><h3 id=3-boosting降偏差>(3) Boosting：降偏差<a class=anchor href=#3-boosting%e9%99%8d%e5%81%8f%e5%b7%ae>#</a></h3><h4 id=adaboost>AdaBoost：<a class=anchor href=#adaboost>#</a></h4><ul><li><strong>本质</strong>：前向分步算法最小化指数损失</li><li><strong>公式</strong>：样本权重、模型权重都从指数损失推导出来</li><li><strong>优势</strong>：关注难样本，增大间隔，泛化性强</li></ul><h4 id=gbdt>GBDT：<a class=anchor href=#gbdt>#</a></h4><ul><li><strong>核心</strong>：梯度下降在函数空间</li><li><strong>拟合残差 = 沿负梯度</strong>（对于平方损失）</li><li><strong>灵活</strong>：可用于任意可微损失</li></ul><h4 id=xgboost>XGBoost：<a class=anchor href=#xgboost>#</a></h4><ul><li><strong>二阶泰勒展开</strong>：更精确的牛顿步</li><li><strong>正则化</strong>：树复杂度惩罚（叶子数 + 叶子值平方和）</li><li><strong>工程优化</strong>：分位数近似、稀疏感知、并行化</li></ul><h3 id=4-哲学思考>(4) 哲学思考<a class=anchor href=#4-%e5%93%b2%e5%ad%a6%e6%80%9d%e8%80%83>#</a></h3><p>集成学习的成功揭示了两个深刻的真理：</p><ol><li><strong>多样性 > 单一强大</strong>：一群弱而不同的模型 > 一个强而单一的模型</li><li><strong>合作 > 竞争</strong>：Bagging的平均、Boosting的接力，都是协同的艺术</li></ol><p>这也许是对复杂系统的一个隐喻：<strong>真正的智能不在于个体的完美，而在于群体的协同</strong>。</p><hr><h2 id=8-附录xgboost核心公式>8. 附录:XGBoost核心公式<a class=anchor href=#8-%e9%99%84%e5%bd%95xgboost%e6%a0%b8%e5%bf%83%e5%85%ac%e5%bc%8f>#</a></h2><h3 id=1-目标函数-1>(1) 目标函数<a class=anchor href=#1-%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0-1>#</a></h3><p>$$
\mathcal{L}^{(t)} = \sum_{i=1}^N \ell(y_i, \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)) + \Omega(f_t)
$$</p><p>其中正则项：</p><p>$$
\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2
$$</p><h3 id=2-二阶泰勒近似>(2) 二阶泰勒近似<a class=anchor href=#2-%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e8%bf%91%e4%bc%bc>#</a></h3><p>$$
\tilde{\mathcal{L}}^{(t)} \approx \sum_{i=1}^N \left[g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t^2(\mathbf{x}_i)\right] + \Omega(f_t)
$$</p><p>其中：</p><p>$$
g_i = \frac{\partial \ell(y_i, \hat{y}^{(t-1)})}{\partial \hat{y}^{(t-1)}}, \quad h_i = \frac{\partial^2 \ell(y_i, \hat{y}^{(t-1)})}{\partial (\hat{y}^{(t-1)})^2}
$$</p><h3 id=3-叶子节点最优值>(3) 叶子节点最优值<a class=anchor href=#3-%e5%8f%b6%e5%ad%90%e8%8a%82%e7%82%b9%e6%9c%80%e4%bc%98%e5%80%bc>#</a></h3><p>$$
w_j^* = -\frac{G_j}{H_j + \lambda}
$$</p><p>其中 $G_j = \sum_{i \in I_j} g_i$，$H_j = \sum_{i \in I_j} h_i$。</p><h3 id=4-最小损失>(4) 最小损失<a class=anchor href=#4-%e6%9c%80%e5%b0%8f%e6%8d%9f%e5%a4%b1>#</a></h3><p>$$
\tilde{\mathcal{L}}^{(t)} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j + \lambda} + \gamma T
$$</p><h3 id=5-分裂增益>(5) 分裂增益<a class=anchor href=#5-%e5%88%86%e8%a3%82%e5%a2%9e%e7%9b%8a>#</a></h3><p>$$
\text{Gain} = \frac{1}{2} \left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\right] - \gamma
$$</p><h3 id=6-常见损失函数的梯度>(6) 常见损失函数的梯度<a class=anchor href=#6-%e5%b8%b8%e8%a7%81%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e6%a2%af%e5%ba%a6>#</a></h3><table><thead><tr><th>任务</th><th>损失函数</th><th>$g_i$</th><th>$h_i$</th></tr></thead><tbody><tr><td>回归</td><td>$\frac{1}{2}(y_i - \hat{y}_i)^2$</td><td>$\hat{y}_i - y_i$</td><td>$1$</td></tr><tr><td>二分类(logistic)</td><td>$-[y_i \log p_i + (1-y_i)\log(1-p_i)]$</td><td>$p_i - y_i$</td><td>$p_i(1-p_i)$</td></tr><tr><td>多分类(softmax)</td><td>$-\sum_k y_{ik} \log p_{ik}$</td><td>$p_{ik} - y_{ik}$</td><td>$p_{ik}(1-p_{ik})$</td></tr></tbody></table><p>其中 $p_i = \sigma(\hat{y}_i) = \frac{1}{1 + e^{-\hat{y}_i}}$。</p><hr><p><strong>参考文献</strong>：</p><ul><li>Breiman, L. (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5-32.</li><li>Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. <em>Journal of Computer and System Sciences</em>, 55(1), 119-139.</li><li>Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. <em>Annals of Statistics</em>, 1189-1232.</li><li>Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. <em>KDD</em>, 785-794.</li></ul></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第08章 核方法</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/ class="flex align-center"><span>第10章 逻辑回归与最大熵模型</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-决策树decision-tree>2. 决策树(Decision Tree)</a><ul><li><a href=#21-直觉20个问题游戏>2.1 直觉：20个问题游戏</a></li><li><a href=#22-纯度的度量熵与基尼>2.2 纯度的度量：熵与基尼</a><ul><li><a href=#1-熵entropy>(1) 熵(Entropy)</a></li><li><a href=#2-基尼不纯度gini-impurity>(2) 基尼不纯度(Gini Impurity)</a></li><li><a href=#3-熵-vs-gini有何区别>(3) 熵 vs Gini：有何区别？</a></li></ul></li><li><a href=#23-特征选择最大化信息增益>2.3 特征选择：最大化信息增益</a><ul><li><a href=#1-信息增益-information-gain>(1) 信息增益 (Information Gain)</a></li><li><a href=#2-信息增益比-gain-ratio>(2) 信息增益比 (Gain Ratio)</a></li><li><a href=#3-gini增益-gini-gain>(3) Gini增益 (Gini Gain)</a></li></ul></li><li><a href=#24-决策树的构建算法>2.4 决策树的构建算法</a></li><li><a href=#25-过拟合与剪枝>2.5 过拟合与剪枝</a><ul><li><a href=#1-预剪枝pre-pruning>(1) 预剪枝(Pre-pruning)</a></li><li><a href=#2-后剪枝post-pruning>(2) 后剪枝(Post-pruning)</a></li></ul></li></ul></li><li><a href=#3-集成学习的总纲>3. 集成学习的总纲</a><ul><li><a href=#31-为什么需要集成>3.1 为什么需要集成？</a></li><li><a href=#32-bias-variance-tradeoff>3.2 Bias-Variance Tradeoff</a></li><li><a href=#33-集成的两大流派bagging-vs-boosting>3.3 集成的两大流派：Bagging vs Boosting</a><ul><li><a href=#1-bagging-bootstrap-aggregating>(1) Bagging (Bootstrap Aggregating)</a></li><li><a href=#2-boosting>(2) Boosting</a></li></ul></li></ul></li><li><a href=#4-bagging与随机森林>4. Bagging与随机森林</a><ul><li><a href=#41-bootstrap采样>4.1 Bootstrap采样</a></li><li><a href=#42-bagging的降方差机制>4.2 Bagging的降方差机制</a></li><li><a href=#43-随机森林的双重随机性>4.3 随机森林的双重随机性</a></li><li><a href=#44-out-of-bag估计>4.4 Out-of-Bag估计</a></li></ul></li><li><a href=#5-boosting之魂adaboost>5. Boosting之魂:AdaBoost</a><ul><li><a href=#51-adaboost算法流程>5.1 AdaBoost算法流程</a></li><li><a href=#52-核心推导为什么是指数损失>5.2 核心推导：为什么是指数损失？</a><ul><li><a href=#1-指数损失>(1) 指数损失</a></li><li><a href=#2-前向分步算法>(2) 前向分步算法</a></li></ul></li><li><a href=#53-样本权重更新公式的推导>5.3 样本权重更新公式的推导</a><ul><li><a href=#1-优化-f_m>(1) 优化 $f_m$</a></li><li><a href=#2-优化-alpha_m>(2) 优化 $\alpha_m$</a></li><li><a href=#3-权重更新>(3) 权重更新</a></li></ul></li><li><a href=#54-adaboost的几何直觉>5.4 AdaBoost的几何直觉</a></li></ul></li><li><a href=#6-boosting之神gbdt与xgboost>6. Boosting之神:GBDT与XGBoost</a><ul><li><a href=#61-gbdt梯度提升决策树>6.1 GBDT:梯度提升决策树</a><ul><li><a href=#1-前向分步算法通用框架>(1) 前向分步算法（通用框架）</a></li><li><a href=#2-梯度下降的类比>(2) 梯度下降的类比</a></li></ul></li><li><a href=#62-为什么拟合残差>6.2 为什么拟合残差？</a></li><li><a href=#63-xgboost二阶泰勒展开>6.3 XGBoost：二阶泰勒展开</a></li><li><a href=#64-xgboost的目标函数推导>6.4 XGBoost的目标函数推导</a><ul><li><a href=#1-目标函数>(1) 目标函数</a></li><li><a href=#2-二阶泰勒展开>(2) 二阶泰勒展开</a></li><li><a href=#3-叶子节点的最优值>(3) 叶子节点的最优值</a></li><li><a href=#4-分裂增益>(4) 分裂增益</a></li></ul></li><li><a href=#65-xgboost的工程优化>6.5 XGBoost的工程优化</a><ul><li><a href=#1-分裂算法>(1) 分裂算法</a></li><li><a href=#2-系统优化>(2) 系统优化</a></li><li><a href=#3-正则化技巧>(3) 正则化技巧</a></li></ul></li></ul></li><li><a href=#7-总结>7. 总结</a><ul><li><a href=#1-决策树基石>(1) 决策树：基石</a></li><li><a href=#2-bagging降方差>(2) Bagging：降方差</a></li><li><a href=#3-boosting降偏差>(3) Boosting：降偏差</a><ul><li><a href=#adaboost>AdaBoost：</a></li><li><a href=#gbdt>GBDT：</a></li><li><a href=#xgboost>XGBoost：</a></li></ul></li><li><a href=#4-哲学思考>(4) 哲学思考</a></li></ul></li><li><a href=#8-附录xgboost核心公式>8. 附录:XGBoost核心公式</a><ul><li><a href=#1-目标函数-1>(1) 目标函数</a></li><li><a href=#2-二阶泰勒近似>(2) 二阶泰勒近似</a></li><li><a href=#3-叶子节点最优值>(3) 叶子节点最优值</a></li><li><a href=#4-最小损失>(4) 最小损失</a></li><li><a href=#5-分裂增益>(5) 分裂增益</a></li><li><a href=#6-常见损失函数的梯度>(6) 常见损失函数的梯度</a></li></ul></li></ul></nav></div></aside></main></body></html>