<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第10章：逻辑回归与最大熵模型# “The entropy of the universe tends to a maximum.” —— Rudolf Clausius
重要提示：本章将揭示一个惊人的数学事实：逻辑回归 (Logistic Regression) 只是 最大熵模型 (Maximum Entropy Model) 的一个特例。
当我们承认由于信息不足而必须保留"最大的不确定性"时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。
我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。
目录# 引言：分类问题的两种视角 逻辑回归 (Logistic Regression) 2.1 Sigmoid 函数的由来 2.2 极大似然估计 (MLE) 2.3 信息论视角：最小化交叉熵 最大熵模型 (Maximum Entropy Model) 3.1 最大熵原理：无知是的智慧 3.2 最大熵模型的定义 3.3 Lagrange 对偶推导 殊途同归：逻辑回归与最大熵的等价性 4.1 从最大熵推导逻辑回归 4.2 多分类推广：Softmax 回归 模型学习算法 5.1 改进的迭代尺度法 (IIS) 5.2 拟牛顿法 (BFGS/L-BFGS) 总结 推荐阅读 1. 引言：分类问题的两种视角# 在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带"回归"，但它是一个纯粹的分类模型。
理解逻辑回归有两种路径：
统计学视角：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。 信息论视角：在满足数据约束的前提下，选择熵最大的分布。 本章将证明，这两种视角最终指向了同一个数学形式。
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第10章 逻辑回归与最大熵模型"><meta property="og:description" content='第10章：逻辑回归与最大熵模型# “The entropy of the universe tends to a maximum.” —— Rudolf Clausius
重要提示：本章将揭示一个惊人的数学事实：逻辑回归 (Logistic Regression) 只是 最大熵模型 (Maximum Entropy Model) 的一个特例。
当我们承认由于信息不足而必须保留"最大的不确定性"时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。
我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。
目录# 引言：分类问题的两种视角 逻辑回归 (Logistic Regression) 2.1 Sigmoid 函数的由来 2.2 极大似然估计 (MLE) 2.3 信息论视角：最小化交叉熵 最大熵模型 (Maximum Entropy Model) 3.1 最大熵原理：无知是的智慧 3.2 最大熵模型的定义 3.3 Lagrange 对偶推导 殊途同归：逻辑回归与最大熵的等价性 4.1 从最大熵推导逻辑回归 4.2 多分类推广：Softmax 回归 模型学习算法 5.1 改进的迭代尺度法 (IIS) 5.2 拟牛顿法 (BFGS/L-BFGS) 总结 推荐阅读 1. 引言：分类问题的两种视角# 在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带"回归"，但它是一个纯粹的分类模型。
理解逻辑回归有两种路径：
统计学视角：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。 信息论视角：在满足数据约束的前提下，选择熵最大的分布。 本章将证明，这两种视角最终指向了同一个数学形式。'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第10章 逻辑回归与最大熵模型"><meta itemprop=description content='第10章：逻辑回归与最大熵模型# “The entropy of the universe tends to a maximum.” —— Rudolf Clausius
重要提示：本章将揭示一个惊人的数学事实：逻辑回归 (Logistic Regression) 只是 最大熵模型 (Maximum Entropy Model) 的一个特例。
当我们承认由于信息不足而必须保留"最大的不确定性"时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。
我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。
目录# 引言：分类问题的两种视角 逻辑回归 (Logistic Regression) 2.1 Sigmoid 函数的由来 2.2 极大似然估计 (MLE) 2.3 信息论视角：最小化交叉熵 最大熵模型 (Maximum Entropy Model) 3.1 最大熵原理：无知是的智慧 3.2 最大熵模型的定义 3.3 Lagrange 对偶推导 殊途同归：逻辑回归与最大熵的等价性 4.1 从最大熵推导逻辑回归 4.2 多分类推广：Softmax 回归 模型学习算法 5.1 改进的迭代尺度法 (IIS) 5.2 拟牛顿法 (BFGS/L-BFGS) 总结 推荐阅读 1. 引言：分类问题的两种视角# 在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带"回归"，但它是一个纯粹的分类模型。
理解逻辑回归有两种路径：
统计学视角：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。 信息论视角：在满足数据约束的前提下，选择熵最大的分布。 本章将证明，这两种视角最终指向了同一个数学形式。'><meta itemprop=wordCount content="784"><title>第10章 逻辑回归与最大熵模型 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/ class=active>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第10章 逻辑回归与最大熵模型</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言分类问题的两种视角>1. 引言：分类问题的两种视角</a></li><li><a href=#2-逻辑回归-logistic-regression>2. 逻辑回归 (Logistic Regression)</a><ul><li><a href=#21-sigmoid-函数的由来>2.1 Sigmoid 函数的由来</a></li><li><a href=#22-极大似然估计-mle>2.2 极大似然估计 (MLE)</a></li><li><a href=#23-信息论视角最小化交叉熵>2.3 信息论视角：最小化交叉熵</a></li></ul></li><li><a href=#3-最大熵模型-maximum-entropy-model>3. 最大熵模型 (Maximum Entropy Model)</a><ul><li><a href=#31-最大熵原理无知是的智慧>3.1 最大熵原理：无知是的智慧</a></li><li><a href=#32-最大熵模型的定义>3.2 最大熵模型的定义</a></li><li><a href=#33-lagrange-对偶推导>3.3 Lagrange 对偶推导</a></li></ul></li><li><a href=#4-殊途同归逻辑回归与最大熵的等价性>4. 殊途同归：逻辑回归与最大熵的等价性</a><ul><li><a href=#41-从最大熵推导逻辑回归>4.1 从最大熵推导逻辑回归</a></li><li><a href=#42-多分类推广softmax-回归>4.2 多分类推广：Softmax 回归</a></li></ul></li><li><a href=#5-模型学习算法>5. 模型学习算法</a><ul><li><a href=#51-改进的迭代尺度法-iis>5.1 改进的迭代尺度法 (IIS)</a></li><li><a href=#52-拟牛顿法-bfgs--l-bfgs>5.2 拟牛顿法 (BFGS / L-BFGS)</a></li></ul></li><li><a href=#6-总结>6. 总结</a></li><li><a href=#7-推荐阅读>7. 推荐阅读</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第10章逻辑回归与最大熵模型>第10章：逻辑回归与最大熵模型<a class=anchor href=#%e7%ac%ac10%e7%ab%a0%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b>#</a></h1><blockquote class=book-hint><p>&ldquo;The entropy of the universe tends to a maximum.&rdquo; —— Rudolf Clausius</p><p><strong>重要提示</strong>：本章将揭示一个惊人的数学事实：<strong>逻辑回归 (Logistic Regression)</strong> 只是 <strong>最大熵模型 (Maximum Entropy Model)</strong> 的一个特例。</p><p>当我们承认由于信息不足而必须保留"最大的不确定性"时，我们自然而然地推导出了 Sigmoid 函数和 Softmax 回归。这不是巧合，这是信息论对概率模型的最优约束。</p><p>我们将从最基础的二分类逻辑回归出发，一路探寻到最大熵原理的宏大视角，最终在对偶理论的顶峰看到两者的会师。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><a href=#1-%e5%bc%95%e8%a8%80%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e7%9a%84%e4%b8%a4%e7%a7%8d%e8%a7%86%e8%a7%92>引言：分类问题的两种视角</a></li><li><a href=#2-%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92-logistic-regression>逻辑回归 (Logistic Regression)</a><ul><li>2.1 <a href=#21-sigmoid-%e5%87%bd%e6%95%b0%e7%9a%84%e7%94%b1%e6%9d%a5>Sigmoid 函数的由来</a></li><li>2.2 <a href=#22-%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1-mle>极大似然估计 (MLE)</a></li><li>2.3 <a href=#23-%e4%bf%a1%e6%81%af%e8%ae%ba%e8%a7%86%e8%a7%92%e6%9c%80%e5%b0%8f%e5%8c%96%e4%ba%a4%e5%8f%89%e7%86%b5>信息论视角：最小化交叉熵</a></li></ul></li><li><a href=#3-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b-maximum-entropy-model>最大熵模型 (Maximum Entropy Model)</a><ul><li>3.1 <a href=#31-%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86%e6%97%a0%e7%9f%a5%e6%98%af%e7%9a%84%e6%99%ba%e6%85%a7>最大熵原理：无知是的智慧</a></li><li>3.2 <a href=#32-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9a%e4%b9%89>最大熵模型的定义</a></li><li>3.3 <a href=#33-lagrange-%e5%af%b9%e5%81%b6%e6%8e%a8%e5%af%bc>Lagrange 对偶推导</a></li></ul></li><li><a href=#4-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7>殊途同归：逻辑回归与最大熵的等价性</a><ul><li>4.1 <a href=#41-%e4%bb%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e6%8e%a8%e5%af%bc%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92>从最大熵推导逻辑回归</a></li><li>4.2 <a href=#42-%e5%a4%9a%e5%88%86%e7%b1%bb%e6%8e%a8%e5%b9%bfsoftmax-%e5%9b%9e%e5%bd%92>多分类推广：Softmax 回归</a></li></ul></li><li><a href=#5-%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95>模型学习算法</a><ul><li>5.1 <a href=#51-%e6%94%b9%e8%bf%9b%e7%9a%84%e8%bf%ad%e4%bb%a3%e5%b0%ba%e5%ba%a6%e6%b3%95-iis>改进的迭代尺度法 (IIS)</a></li><li>5.2 <a href=#52-%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95-bfgsl-bfgs>拟牛顿法 (BFGS/L-BFGS)</a></li></ul></li><li><a href=#6-%e6%80%bb%e7%bb%93>总结</a></li><li><a href=#7-%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb>推荐阅读</a></li></ol><hr><h2 id=1-引言分类问题的两种视角>1. 引言：分类问题的两种视角<a class=anchor href=#1-%e5%bc%95%e8%a8%80%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98%e7%9a%84%e4%b8%a4%e7%a7%8d%e8%a7%86%e8%a7%92>#</a></h2><p>在前面的章节中，我们学习了感知机（几何视角）和 SVM（几何+优化视角）。逻辑回归虽然名字里带"回归"，但它是一个纯粹的<strong>分类模型</strong>。</p><p>理解逻辑回归有两种路径：</p><ol><li><strong>统计学视角</strong>：假设数据服从伯努利分布，利用广义线性模型 (GLM) 建模对数几率 (Log-Odds)。</li><li><strong>信息论视角</strong>：在满足数据约束的前提下，选择熵最大的分布。</li></ol><p>本章将证明，这两种视角最终指向了同一个数学形式。</p><hr><h2 id=2-逻辑回归-logistic-regression>2. 逻辑回归 (Logistic Regression)<a class=anchor href=#2-%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92-logistic-regression>#</a></h2><h3 id=21-sigmoid-函数的由来>2.1 Sigmoid 函数的由来<a class=anchor href=#21-sigmoid-%e5%87%bd%e6%95%b0%e7%9a%84%e7%94%b1%e6%9d%a5>#</a></h3><p>对于二分类问题 $y \in {0, 1}$，我们希望找到一个模型 $P(Y=1|x)$。既然是概率，取值必须在 $[0, 1]$ 之间。</p><p>线性模型 $w^T x + b$ 的取值范围是 $(-\infty, +\infty)$。如何将它映射到 $[0, 1]$？</p><p><strong>几率 (Odds)</strong> 是一个很好的中间量：
$$
\text{Odds} = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{p}{1-p} \in [0, +\infty)
$$</p><p><strong>对数几率 (Log-Odds / Logit)</strong>：
$$
\ln \frac{p}{1-p} \in (-\infty, +\infty)
$$</p><p>我们假设<strong>对数几率是 $x$ 的线性函数</strong>：
$$
\ln \frac{P(Y=1|x)}{1 - P(Y=1|x)} = w \cdot x + b
$$</p><p>解出 $P(Y=1|x)$：
$$
P(Y=1|x) = \frac{e^{w \cdot x + b}}{1 + e^{w \cdot x + b}} = \frac{1}{1 + e^{-(w \cdot x + b)}}
$$</p><p>这就是著名的 <strong>Sigmoid 函数</strong> $\sigma(z) = \frac{1}{1+e^{-z}}$。</p><p><img src=assets/sigmoid_function.svg alt="Sigmoid Function"></p><blockquote class=book-hint><p><strong>物理意义</strong>：Sigmoid 函数将任意实数"挤压"到 $(0, 1)$ 区间，且在 $z=0$ 处变化最快（区分度最高），在两端趋于平缓（通过概率表达不确定性）。</p></blockquote><h3 id=22-极大似然估计-mle>2.2 极大似然估计 (MLE)<a class=anchor href=#22-%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1-mle>#</a></h3><p>给定训练数据 $D = {(x_i, y_i)}_{i=1}^N$，我们要估计参数 $\theta = (w, b)$。</p><p>假设样本独立同分布，似然函数为：
$$
L(\theta) = \prod_{i=1}^N P(y_i | x_i; \theta)
$$</p><p>其中：
$$
P(y_i | x_i) = \begin{cases} p_i & \text{if } y_i = 1 \ 1-p_i & \text{if } y_i = 0 \end{cases} \quad \Rightarrow \quad P(y_i | x_i) = p_i^{y_i} (1-p_i)^{1-y_i}
$$</p><p><strong>对数似然函数</strong>：
$$
\begin{aligned}
\ell(\theta) &= \sum_{i=1}^N \left[ y_i \ln p_i + (1-y_i) \ln (1-p_i) \right] \
&= \sum_{i=1}^N \left[ y_i \ln \frac{p_i}{1-p_i} + \ln (1-p_i) \right]
\end{aligned}
$$</p><p>代入 $p_i = \sigma(w \cdot x_i + b)$，注意到 $\ln \frac{p_i}{1-p_i} = w \cdot x_i + b$：
$$
\ell(w, b) = \sum_{i=1}^N \left[ y_i (w \cdot x_i + b) - \ln (1 + e^{w \cdot x_i + b}) \right]
$$</p><p>为了求解最优参数，我们通常<strong>最小化负对数似然 (NLL)</strong>：
$$
J(w, b) = -\ell(w, b) = \sum_{i=1}^N \left[ -y_i (w \cdot x_i + b) + \ln (1 + e^{w \cdot x_i + b}) \right]
$$</p><p>这是一个<strong>凸函数</strong>（Hessian 矩阵正定），可以通过梯度下降法或牛顿法找到全局最优解。</p><h3 id=23-信息论视角最小化交叉熵>2.3 信息论视角：最小化交叉熵<a class=anchor href=#23-%e4%bf%a1%e6%81%af%e8%ae%ba%e8%a7%86%e8%a7%92%e6%9c%80%e5%b0%8f%e5%8c%96%e4%ba%a4%e5%8f%89%e7%86%b5>#</a></h3><p>仔细观察上面的 $J(w, b)$，它正是<strong>交叉熵 (Cross Entropy)</strong>！</p><p>$$
H(P_{\text{data}}, P_{\text{model}}) = -\frac{1}{N} \sum_{i=1}^N \sum_{y \in {0,1}} P_{\text{data}}(y|x_i) \ln P_{\text{model}}(y|x_i)
$$</p><p>由于 $P_{\text{data}}(y|x_i)$ 是经验分布（one-hot：$y_i=1$ 时概率为1，否则为0），这简化为：
$$
-\frac{1}{N} \sum_{i=1}^N \ln P_{\text{model}}(y_i|x_i)
$$</p><blockquote class=book-hint><p><strong>结论</strong>：<strong>最大化似然 = 最小化交叉熵</strong>。这不仅适用于逻辑回归，也适用于所有概率分类模型（包括深度神经网络）。</p></blockquote><hr><h2 id=3-最大熵模型-maximum-entropy-model>3. 最大熵模型 (Maximum Entropy Model)<a class=anchor href=#3-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b-maximum-entropy-model>#</a></h2><p>如果说逻辑回归是从"几率是线性的"这个假设出发，那么最大熵模型则是从更底层的哲学原理出发。</p><h3 id=31-最大熵原理无知是的智慧>3.1 最大熵原理：无知是的智慧<a class=anchor href=#31-%e6%9c%80%e5%a4%a7%e7%86%b5%e5%8e%9f%e7%90%86%e6%97%a0%e7%9f%a5%e6%98%af%e7%9a%84%e6%99%ba%e6%85%a7>#</a></h3><p><strong>最大熵原理 (Principle of Maximum Entropy)</strong>：</p><blockquote class=book-hint><p>在满足所有已知约束条件的情况下，我们应该选择<strong>熵最大</strong>的概率分布。</p></blockquote><p><strong>为什么？</strong></p><ul><li>熵代表不确定性。</li><li>最大化熵 = <strong>不作任何没有根据的假设</strong>。</li><li>如果我们只知道骰子的均值是3.5，我们应该假设它是均匀的（1-6概率相等），而不是假设它只出3和4。</li></ul><h3 id=32-最大熵模型的定义>3.2 最大熵模型的定义<a class=anchor href=#32-%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9a%e4%b9%89>#</a></h3><p>对于分类问题，我们想要得到条件概率分布 $P(Y|X)$。</p><p><strong>已知信息</strong>由训练数据集 $D = {(x_i, y_i)}_{i=1}^N$ 提供。我们可以计算特征函数 $f_k(x, y)$ 在经验分布上的期望：</p><p><strong>经验期望</strong>：
$$
\tilde{E}<em>P(f_k) = \sum</em>{x, y} \tilde{P}(x, y) f_k(x, y) = \frac{1}{N} \sum_{i=1}^N f_k(x_i, y_i)
$$</p><p>我们要求模型预测的期望与经验期望一致：</p><p><strong>模型期望</strong>：
$$
E_P(f_k) = \sum_{x, y} \tilde{P}(x) P(y|x) f_k(x, y) = \frac{1}{N} \sum_{i=1}^N \sum_{y} P(y|x_i) f_k(x_i, y)
$$</p><p><strong>约束条件</strong>：
$$
E_P(f_k) = \tilde{E}_P(f_k), \quad k = 1, \dots, K
$$</p><p>以及概率归一化条件：
$$
\sum_y P(y|x) = 1
$$</p><p><strong>目标</strong>：最大化条件熵
$$
H(P) = -\sum_{x, y} \tilde{P}(x) P(y|x) \ln P(y|x)
$$</p><h3 id=33-lagrange-对偶推导>3.3 Lagrange 对偶推导<a class=anchor href=#33-lagrange-%e5%af%b9%e5%81%b6%e6%8e%a8%e5%af%bc>#</a></h3><p>这是一个典型的约束优化问题。引入 Lagrange 乘子 $w_k$（对应特征约束）和 $\lambda$（对应归一化约束）：</p><p>$$
\min_P \max_{w} L(P, w)
$$</p><p>Lagrange 函数：
$$
L(P, w) = -H(P) + \sum_{k=1}^K w_k (E_P(f_k) - \tilde{E}_P(f_k)) + \sum_x \lambda(x) (\sum_y P(y|x) - 1)
$$</p><p>对 $P(y|x)$ 求偏导并令为0：
$$
\frac{\partial L}{\partial P(y|x)} = \tilde{P}(x) (1 + \ln P(y|x)) - \tilde{P}(x) \sum_k w_k f_k(x, y) - \lambda(x) = 0
$$</p><p>解得：
$$
P(y|x) = \exp \left( \sum_{k=1}^K w_k f_k(x, y) + \frac{\lambda(x)}{\tilde{P}(x)} - 1 \right)
$$</p><p>由于 $\sum_y P(y|x) = 1$，我们可以消去 $\lambda(x)$，得到最终形式：</p><p>$$
\boxed{
P_w(y|x) = \frac{1}{Z_w(x)} \exp \left( \sum_{k=1}^K w_k f_k(x, y) \right)
}
$$</p><p>其中 $Z_w(x)$ 是归一化因子（配分函数）：
$$
Z_w(x) = \sum_y \exp \left( \sum_{k=1}^K w_k f_k(x, y) \right)
$$</p><blockquote class=book-hint><p><strong>这就是最大熵模型！</strong> 它的形式是指数族分布 (Exponential Family)。</p></blockquote><hr><h2 id=4-殊途同归逻辑回归与最大熵的等价性>4. 殊途同归：逻辑回归与最大熵的等价性<a class=anchor href=#4-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e4%b8%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7>#</a></h2><h3 id=41-从最大熵推导逻辑回归>4.1 从最大熵推导逻辑回归<a class=anchor href=#41-%e4%bb%8e%e6%9c%80%e5%a4%a7%e7%86%b5%e6%8e%a8%e5%af%bc%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92>#</a></h3><p>看看二分类逻辑回归。假设特征函数就是输入特征本身：
$$
f_k(x, y) = \begin{cases} x^{(k)} & \text{if } y = 1 \ 0 & \text{if } y = 0 \end{cases}
$$</p><p>对于 $y=1$：
$$
P(y=1|x) = \frac{\exp(\sum w_k x^{(k)})}{\exp(0) + \exp(\sum w_k x^{(k)})} = \frac{e^{w \cdot x}}{1 + e^{w \cdot x}} = \frac{1}{1 + e^{-w \cdot x}}
$$</p><p><strong>完全一致！</strong></p><p>逻辑回归正是<strong>特征函数为 $x$ 及其分量</strong>时的最大熵模型。</p><h3 id=42-多分类推广softmax-回归>4.2 多分类推广：Softmax 回归<a class=anchor href=#42-%e5%a4%9a%e5%88%86%e7%b1%bb%e6%8e%a8%e5%b9%bfsoftmax-%e5%9b%9e%e5%bd%92>#</a></h3><p>如果在最大熵模型中，$y$ 可以取 $K$ 个类别，$f(x, y)$ 是 Indicator function，那么我们就得到了 <strong>Softmax 回归</strong> (Multinomial Logistic Regression)：</p><p>$$
P(y=k|x) = \frac{\exp(w_k \cdot x)}{\sum_{j=1}^K \exp(w_j \cdot x)}
$$</p><blockquote class=book-hint><p><strong>深刻洞见</strong>：为什么深度学习最后一层常用 Softmax？因为它是在给定前一层输出作为特征的情况下，<strong>熵最大</strong>的分类分布。它是最"诚实"的概率输出。</p></blockquote><hr><h2 id=5-模型学习算法>5. 模型学习算法<a class=anchor href=#5-%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95>#</a></h2><p>最大熵模型的目标函数（对偶问题）等价于逻辑回归的对数似然函数。求解 $w$ 通常使用数值优化方法。</p><h3 id=51-改进的迭代尺度法-iis>5.1 改进的迭代尺度法 (IIS)<a class=anchor href=#51-%e6%94%b9%e8%bf%9b%e7%9a%84%e8%bf%ad%e4%bb%a3%e5%b0%ba%e5%ba%a6%e6%b3%95-iis>#</a></h3><p><strong>Improved Iterative Scaling (IIS)</strong> 是一种专门用于最大熵模型的早期算法。</p><ul><li>思想：寻找一个增量 $\delta$，使得对数似然下界提升。</li><li>缺点：收敛慢，现在很少使用。</li></ul><h3 id=52-拟牛顿法-bfgs--l-bfgs>5.2 拟牛顿法 (BFGS / L-BFGS)<a class=anchor href=#52-%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%b3%95-bfgs--l-bfgs>#</a></h3><p>现代机器学习库（如 sklearn, liblinear）主要使用<strong>拟牛顿法</strong>。</p><p><strong>牛顿法</strong>需要计算 Hessian 矩阵 $H$（二阶导）：
$$
w_{t+1} = w_t - H^{-1} \nabla J(w_t)
$$
但 $H^{-1}$ 计算太贵 ($O(D^3)$)。</p><p><strong>BFGS</strong>通过迭代近似 $H^{-1}$。
<strong>L-BFGS (Limited-memory BFGS)</strong> 进一步优化，只存储最近 $m$ 次的更新，将空间复杂度从 $O(D^2)$ 降到 $O(D)$。这是目前求解大规模逻辑回归/最大熵模型的<strong>标准算法</strong>。</p><hr><h2 id=6-总结>6. 总结<a class=anchor href=#6-%e6%80%bb%e7%bb%93>#</a></h2><p>本章虽然篇幅不长，但理论密度极高。</p><ol><li><p><strong>逻辑回归</strong>：</p><ul><li>形式上是 Sigmoid / Softmax</li><li>本质上是线性模型的对数几率推广</li><li>学习目标是最小化 Cross Entropy</li></ul></li><li><p><strong>最大熵模型</strong>：</p><ul><li>哲学上是"无知即不假设"</li><li>数学上是带有特征约束的最大熵分布</li><li>形式上导出了指数族分布</li></ul></li><li><p><strong>大统一</strong>：</p><ul><li><strong>逻辑回归 = 最大熵模型</strong>（在特定特征函数下）</li><li>它们都是<strong>对数线性模型 (Log-linear Model)</strong></li><li>它们的损失函数都是<strong>凸函数</strong>，有全局最优解</li></ul></li></ol><p><img src=assets/maxent_structure.svg alt="MaxEnt vs LR"></p><blockquote class=book-hint><p><strong>&ldquo;Entia non sunt multiplicanda praeter necessitatem.&rdquo; (Entities should not be multiplied beyond necessity.)</strong> —— Occam&rsquo;s Razor</p><p>最大熵原理是奥卡姆剃刀在统计学中的数学表达：除此之外，不再做任何多余的假设。</p></blockquote><hr><h2 id=7-推荐阅读>7. 推荐阅读<a class=anchor href=#7-%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb>#</a></h2><ul><li><strong>《统计学习方法》 - 第6章</strong> (李航)：详细推导了IIS算法</li><li><strong>Jaynes, E. T. (1957). &ldquo;Information Theory and Statistical Mechanics&rdquo;</strong>：最大熵原理的奠基之作</li><li><strong>Berger et al. (1996). &ldquo;A Maximum Entropy Approach to Natural Language Processing&rdquo;</strong>：将最大熵引入NLP的经典论文</li></ul></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第09章 决策树与集成学习</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/ class="flex align-center"><span>第11章 广义线性模型(GLM)</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言分类问题的两种视角>1. 引言：分类问题的两种视角</a></li><li><a href=#2-逻辑回归-logistic-regression>2. 逻辑回归 (Logistic Regression)</a><ul><li><a href=#21-sigmoid-函数的由来>2.1 Sigmoid 函数的由来</a></li><li><a href=#22-极大似然估计-mle>2.2 极大似然估计 (MLE)</a></li><li><a href=#23-信息论视角最小化交叉熵>2.3 信息论视角：最小化交叉熵</a></li></ul></li><li><a href=#3-最大熵模型-maximum-entropy-model>3. 最大熵模型 (Maximum Entropy Model)</a><ul><li><a href=#31-最大熵原理无知是的智慧>3.1 最大熵原理：无知是的智慧</a></li><li><a href=#32-最大熵模型的定义>3.2 最大熵模型的定义</a></li><li><a href=#33-lagrange-对偶推导>3.3 Lagrange 对偶推导</a></li></ul></li><li><a href=#4-殊途同归逻辑回归与最大熵的等价性>4. 殊途同归：逻辑回归与最大熵的等价性</a><ul><li><a href=#41-从最大熵推导逻辑回归>4.1 从最大熵推导逻辑回归</a></li><li><a href=#42-多分类推广softmax-回归>4.2 多分类推广：Softmax 回归</a></li></ul></li><li><a href=#5-模型学习算法>5. 模型学习算法</a><ul><li><a href=#51-改进的迭代尺度法-iis>5.1 改进的迭代尺度法 (IIS)</a></li><li><a href=#52-拟牛顿法-bfgs--l-bfgs>5.2 拟牛顿法 (BFGS / L-BFGS)</a></li></ul></li><li><a href=#6-总结>6. 总结</a></li><li><a href=#7-推荐阅读>7. 推荐阅读</a></li></ul></nav></div></aside></main></body></html>