<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第 08 章 核方法 (Kernel Methods)# “The curse of dimensionality is the blessing of kernel methods.”
很多时候，我们在低维空间撞得头破血流（比如 XOR 问题），却不知道只要退后一步，升到一个更高的维度，一切都会因为稀疏而变得线性可分。
核方法是机器学习中**“升维打击”**的数学实现。它的魔力在于：我们可以在无限维的空间中挥舞利剑，却只需要支付有限维的计算代价。本章将揭示这个"免费午餐"背后的数学秘密——核技巧 (Kernel Trick)。
与前序章节的联系：
第 5 章（线性回归） 为我们建立了岭回归的原始形式：$w^* = (X^T X + \lambda I)^{-1} X^T y$ 本章将展示如何通过核技巧将其推广到无穷维特征空间：$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$ 两者通过对偶性完美呼应：线性岭回归在特征空间优化，核岭回归在样本空间优化 目录# 直觉：维度打击 代价：维度的诅咒 救赎：核技巧 (Kernel Trick) Mercer 定理：什么样的函数能当核？ RBF 核：通往无穷维 应用：核化一切 6.1 表示定理 6.2 核化的一般步骤 6.3 案例：核岭回归 6.4 其他可核化算法 总结与展望 1. 直觉：维度打击# 1.1 XOR 问题：二维空间的绝望# 考虑经典的 XOR (异或) 问题：
$x_1$ $x_2$ 类别 0 0 0 0 1 1 1 0 1 1 1 0 问题：在二维平面上，不存在一条直线能够将两个类别分开。
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第08章 核方法"><meta property="og:description" content='第 08 章 核方法 (Kernel Methods)# “The curse of dimensionality is the blessing of kernel methods.”
很多时候，我们在低维空间撞得头破血流（比如 XOR 问题），却不知道只要退后一步，升到一个更高的维度，一切都会因为稀疏而变得线性可分。
核方法是机器学习中**“升维打击”**的数学实现。它的魔力在于：我们可以在无限维的空间中挥舞利剑，却只需要支付有限维的计算代价。本章将揭示这个"免费午餐"背后的数学秘密——核技巧 (Kernel Trick)。
与前序章节的联系：
第 5 章（线性回归） 为我们建立了岭回归的原始形式：$w^* = (X^T X + \lambda I)^{-1} X^T y$ 本章将展示如何通过核技巧将其推广到无穷维特征空间：$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$ 两者通过对偶性完美呼应：线性岭回归在特征空间优化，核岭回归在样本空间优化 目录# 直觉：维度打击 代价：维度的诅咒 救赎：核技巧 (Kernel Trick) Mercer 定理：什么样的函数能当核？ RBF 核：通往无穷维 应用：核化一切 6.1 表示定理 6.2 核化的一般步骤 6.3 案例：核岭回归 6.4 其他可核化算法 总结与展望 1. 直觉：维度打击# 1.1 XOR 问题：二维空间的绝望# 考虑经典的 XOR (异或) 问题：
$x_1$ $x_2$ 类别 0 0 0 0 1 1 1 0 1 1 1 0 问题：在二维平面上，不存在一条直线能够将两个类别分开。'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第08章 核方法"><meta itemprop=description content='第 08 章 核方法 (Kernel Methods)# “The curse of dimensionality is the blessing of kernel methods.”
很多时候，我们在低维空间撞得头破血流（比如 XOR 问题），却不知道只要退后一步，升到一个更高的维度，一切都会因为稀疏而变得线性可分。
核方法是机器学习中**“升维打击”**的数学实现。它的魔力在于：我们可以在无限维的空间中挥舞利剑，却只需要支付有限维的计算代价。本章将揭示这个"免费午餐"背后的数学秘密——核技巧 (Kernel Trick)。
与前序章节的联系：
第 5 章（线性回归） 为我们建立了岭回归的原始形式：$w^* = (X^T X + \lambda I)^{-1} X^T y$ 本章将展示如何通过核技巧将其推广到无穷维特征空间：$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$ 两者通过对偶性完美呼应：线性岭回归在特征空间优化，核岭回归在样本空间优化 目录# 直觉：维度打击 代价：维度的诅咒 救赎：核技巧 (Kernel Trick) Mercer 定理：什么样的函数能当核？ RBF 核：通往无穷维 应用：核化一切 6.1 表示定理 6.2 核化的一般步骤 6.3 案例：核岭回归 6.4 其他可核化算法 总结与展望 1. 直觉：维度打击# 1.1 XOR 问题：二维空间的绝望# 考虑经典的 XOR (异或) 问题：
$x_1$ $x_2$ 类别 0 0 0 0 1 1 1 0 1 1 1 0 问题：在二维平面上，不存在一条直线能够将两个类别分开。'><meta itemprop=wordCount content="3058"><title>第08章 核方法 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/ class=active>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第08章 核方法</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-直觉维度打击>1. 直觉：维度打击</a><ul><li><a href=#11-xor-问题二维空间的绝望>1.1 XOR 问题：二维空间的绝望</a></li><li><a href=#12-升维的魔法三维空间的救赎>1.2 升维的魔法：三维空间的救赎</a></li><li><a href=#13-推导为什么升维有效>1.3 推导：为什么升维有效？</a></li></ul></li><li><a href=#2-代价维度的诅咒>2. 代价：维度的诅咒</a><ul><li><a href=#21-多项式特征的维度爆炸>2.1 多项式特征的维度爆炸</a></li><li><a href=#22-复杂度爆炸实例>2.2 复杂度爆炸实例</a></li><li><a href=#23-无穷维的梦魇>2.3 无穷维的梦魇</a></li></ul></li><li><a href=#3-救赎核技巧-kernel-trick>3. 救赎：核技巧 (Kernel Trick)</a><ul><li><a href=#31-核心观察我们不需要-phix>3.1 核心观察：我们不需要 $\phi(x)$</a></li><li><a href=#32-核函数的定义>3.2 核函数的定义</a></li><li><a href=#33-核心推导多项式核>3.3 核心推导：多项式核</a></li><li><a href=#34-一般多项式核>3.4 一般多项式核</a></li><li><a href=#35-核技巧的威力总结>3.5 核技巧的威力总结</a></li></ul></li><li><a href=#4-mercer-定理什么样的函数能当核>4. Mercer 定理：什么样的函数能当核？</a><ul><li><a href=#41-问题的提出>4.1 问题的提出</a></li><li><a href=#42-核矩阵-gram-matrix>4.2 核矩阵 (Gram Matrix)</a></li><li><a href=#43-mercer-定理标准形式>4.3 Mercer 定理（标准形式）</a></li><li><a href=#44-定理的证明充分性>4.4 定理的证明（充分性）</a></li><li><a href=#45-定理的证明必要性>4.5 定理的证明（必要性）</a></li><li><a href=#46-核函数的组合性质>4.6 核函数的组合性质</a></li></ul></li><li><a href=#5-rbf-核通往无穷维>5. RBF 核：通往无穷维</a><ul><li><a href=#51-径向基函数核的定义>5.1 径向基函数核的定义</a></li><li><a href=#52-rbf-核对应无穷维特征空间>5.2 RBF 核对应无穷维特征空间</a></li><li><a href=#53-多维推广>5.3 多维推广</a></li><li><a href=#54-rbf-核的威力与风险>5.4 RBF 核的威力与风险</a></li><li><a href=#55-常见核函数总结>5.5 常见核函数总结</a></li></ul></li><li><a href=#6-应用核化一切>6. 应用：核化一切</a><ul><li><a href=#61-表示定理-representer-theorem>6.1 表示定理 (Representer Theorem)</a></li><li><a href=#62-核化的一般步骤>6.2 核化的一般步骤</a></li><li><a href=#63-案例核岭回归-kernel-ridge-regression>6.3 案例：核岭回归 (Kernel Ridge Regression)</a><ul><li><a href=#631-原始岭回归>6.3.1 原始岭回归</a></li><li><a href=#632-核化推导>6.3.2 核化推导</a></li><li><a href=#633-预测>6.3.3 预测</a></li><li><a href=#634-算法总结>6.3.4 算法总结</a></li><li><a href=#635-线性岭回归-vs-核岭回归完整对比>6.3.5 线性岭回归 vs 核岭回归：完整对比</a></li><li><a href=#636-python-实现示例>6.3.6 Python 实现示例</a></li></ul></li><li><a href=#64-其他可核化算法>6.4 其他可核化算法</a></li></ul></li><li><a href=#7-总结与展望>7. 总结与展望</a><ul><li><a href=#71-核方法的本质>7.1 核方法的本质</a></li><li><a href=#72-三大支柱定理>7.2 三大支柱定理</a></li><li><a href=#73-核方法的优势>7.3 核方法的优势</a></li><li><a href=#74-核方法的局限>7.4 核方法的局限</a></li><li><a href=#75-实践建议>7.5 实践建议</a></li><li><a href=#76-现代发展>7.6 现代发展</a></li><li><a href=#77-与其他方法的关系>7.7 与其他方法的关系</a></li><li><a href=#78-最后的思考>7.8 最后的思考</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第-08-章-核方法-kernel-methods>第 08 章 核方法 (Kernel Methods)<a class=anchor href=#%e7%ac%ac-08-%e7%ab%a0-%e6%a0%b8%e6%96%b9%e6%b3%95-kernel-methods>#</a></h1><blockquote class=book-hint><p>&ldquo;The curse of dimensionality is the blessing of kernel methods.&rdquo;</p><p>很多时候，我们在低维空间撞得头破血流（比如 XOR 问题），却不知道只要退后一步，升到一个更高的维度，一切都会因为稀疏而变得线性可分。</p><p>核方法是机器学习中**&ldquo;升维打击&rdquo;**的数学实现。它的魔力在于：我们可以在无限维的空间中挥舞利剑，却只需要支付有限维的计算代价。本章将揭示这个"免费午餐"背后的数学秘密——核技巧 (Kernel Trick)。</p></blockquote><p><strong>与前序章节的联系</strong>：</p><ul><li><strong>第 5 章（线性回归）</strong> 为我们建立了岭回归的原始形式：$w^* = (X^T X + \lambda I)^{-1} X^T y$</li><li><strong>本章</strong>将展示如何通过核技巧将其推广到无穷维特征空间：$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$</li><li>两者通过<strong>对偶性</strong>完美呼应：线性岭回归在特征空间优化，核岭回归在样本空间优化</li></ul><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><a href=#1-%e7%9b%b4%e8%a7%89%e7%bb%b4%e5%ba%a6%e6%89%93%e5%87%bb>直觉：维度打击</a></li><li><a href=#2-%e4%bb%a3%e4%bb%b7%e7%bb%b4%e5%ba%a6%e7%9a%84%e8%af%85%e5%92%92>代价：维度的诅咒</a></li><li><a href=#3-%e6%95%91%e8%b5%8e%e6%a0%b8%e6%8a%80%e5%b7%a7-kernel-trick>救赎：核技巧 (Kernel Trick)</a></li><li><a href=#4-mercer-%e5%ae%9a%e7%90%86%e4%bb%80%e4%b9%88%e6%a0%b7%e7%9a%84%e5%87%bd%e6%95%b0%e8%83%bd%e5%bd%93%e6%a0%b8>Mercer 定理：什么样的函数能当核？</a></li><li><a href=#5-rbf-%e6%a0%b8%e9%80%9a%e5%be%80%e6%97%a0%e7%a9%b7%e7%bb%b4>RBF 核：通往无穷维</a></li><li><a href=#6-%e5%ba%94%e7%94%a8%e6%a0%b8%e5%8c%96%e4%b8%80%e5%88%87>应用：核化一切</a><ul><li>6.1 <a href=#61-%e8%a1%a8%e7%a4%ba%e5%ae%9a%e7%90%86-representer-theorem>表示定理</a></li><li>6.2 <a href=#62-%e6%a0%b8%e5%8c%96%e7%9a%84%e4%b8%80%e8%88%ac%e6%ad%a5%e9%aa%a4>核化的一般步骤</a></li><li>6.3 <a href=#63-%e6%a1%88%e4%be%8b%e6%a0%b8%e5%b2%ad%e5%9b%9e%e5%bd%92-kernel-ridge-regression>案例：核岭回归</a></li><li>6.4 <a href=#64-%e5%85%b6%e4%bb%96%e5%8f%af%e6%a0%b8%e5%8c%96%e7%ae%97%e6%b3%95>其他可核化算法</a></li></ul></li><li><a href=#7-%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b>总结与展望</a></li></ol><hr><h2 id=1-直觉维度打击>1. 直觉：维度打击<a class=anchor href=#1-%e7%9b%b4%e8%a7%89%e7%bb%b4%e5%ba%a6%e6%89%93%e5%87%bb>#</a></h2><h3 id=11-xor-问题二维空间的绝望>1.1 XOR 问题：二维空间的绝望<a class=anchor href=#11-xor-%e9%97%ae%e9%a2%98%e4%ba%8c%e7%bb%b4%e7%a9%ba%e9%97%b4%e7%9a%84%e7%bb%9d%e6%9c%9b>#</a></h3><p>考虑经典的 XOR (异或) 问题：</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th><th>类别</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table><p><strong>问题</strong>：在二维平面上，<strong>不存在一条直线</strong>能够将两个类别分开。</p><p>无论你怎么尝试，任何线性分类器 $w_1 x_1 + w_2 x_2 + b = 0$ 都会犯错。这是线性模型的根本局限。</p><p><img src=assets/xor_kernel_mapping.svg alt="XOR 问题的维度映射"></p><h3 id=12-升维的魔法三维空间的救赎>1.2 升维的魔法：三维空间的救赎<a class=anchor href=#12-%e5%8d%87%e7%bb%b4%e7%9a%84%e9%ad%94%e6%b3%95%e4%b8%89%e7%bb%b4%e7%a9%ba%e9%97%b4%e7%9a%84%e6%95%91%e8%b5%8e>#</a></h3><p>定义映射 $\phi: \mathbb{R}^2 \to \mathbb{R}^3$：</p><p>$$
\phi(x) = \phi\begin{pmatrix} x_1 \ x_2 \end{pmatrix} = \begin{pmatrix} x_1 \ x_2 \ x_1 x_2 \end{pmatrix}
$$</p><p>将 XOR 数据映射到三维空间：</p><p>$$
\begin{aligned}
\phi\begin{pmatrix} 0 \ 0 \end{pmatrix} &= \begin{pmatrix} 0 \ 0 \ 0 \end{pmatrix} \quad &\text{(类别 0)} \[0.5em]
\phi\begin{pmatrix} 0 \ 1 \end{pmatrix} &= \begin{pmatrix} 0 \ 1 \ 0 \end{pmatrix} \quad &\text{(类别 1)} \[0.5em]
\phi\begin{pmatrix} 1 \ 0 \end{pmatrix} &= \begin{pmatrix} 1 \ 0 \ 0 \end{pmatrix} \quad &\text{(类别 1)} \[0.5em]
\phi\begin{pmatrix} 1 \ 1 \end{pmatrix} &= \begin{pmatrix} 1 \ 1 \ 1 \end{pmatrix} \quad &\text{(类别 0)}
\end{aligned}
$$</p><p><strong>奇迹发生</strong>：在三维空间中，一个简单的平面 $z_3 = 0.5$ 就能完美分割两个类别！</p><h3 id=13-推导为什么升维有效>1.3 推导：为什么升维有效？<a class=anchor href=#13-%e6%8e%a8%e5%af%bc%e4%b8%ba%e4%bb%80%e4%b9%88%e5%8d%87%e7%bb%b4%e6%9c%89%e6%95%88>#</a></h3><p><strong>Cover 定理</strong> (Cover&rsquo;s Theorem)：</p><blockquote class=book-hint><p>将复杂模式投射到高维空间，模式更可能线性可分。</p></blockquote><p><strong>直觉</strong>：</p><ul><li>在 $d$ 维空间中，$n$ 个点被随机二分的方式有 $2^n$ 种</li><li>用超平面线性可分的方式数量随着维度 $d$ 增加而增加</li><li>当 $d \geq n-1$ 时，几乎所有的二分都是线性可分的</li></ul><p><strong>数学表达</strong>：</p><p>在 $d$ 维空间中，$n$ 个一般位置的点，线性可分的二分数量为：</p><p>$$
C(n, d) = 2 \sum_{k=0}^{d} \binom{n-1}{k}
$$</p><p>当 $d = n-1$ 时，$C(n, d) = 2^n$，即所有二分都可分！</p><p><strong>XOR 的验证</strong>：</p><p>对于 XOR，$n = 4$，原始维度 $d = 2$：</p><ul><li>$C(4, 2) = 2(\binom{3}{0} + \binom{3}{1} + \binom{3}{2}) = 2(1 + 3 + 3) = 14 &lt; 2^4 = 16$</li></ul><p>升维到 $d = 3$：</p><ul><li>$C(4, 3) = 2(\binom{3}{0} + \binom{3}{1} + \binom{3}{2} + \binom{3}{3}) = 2(1 + 3 + 3 + 1) = 16 = 2^4$</li></ul><p>所有二分都可分，包括 XOR！</p><hr><h2 id=2-代价维度的诅咒>2. 代价：维度的诅咒<a class=anchor href=#2-%e4%bb%a3%e4%bb%b7%e7%bb%b4%e5%ba%a6%e7%9a%84%e8%af%85%e5%92%92>#</a></h2><h3 id=21-多项式特征的维度爆炸>2.1 多项式特征的维度爆炸<a class=anchor href=#21-%e5%a4%9a%e9%a1%b9%e5%bc%8f%e7%89%b9%e5%be%81%e7%9a%84%e7%bb%b4%e5%ba%a6%e7%88%86%e7%82%b8>#</a></h3><p>假设我们要将 $n$ 维输入映射到 $d$ 阶多项式特征空间。</p><p><strong>例子</strong>：2 维输入 $x = (x_1, x_2)$，2 阶多项式映射：</p><p>$$
\phi(x) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)
$$</p><p>维度从 $2 \to 6$。</p><p><strong>一般情况</strong>：$n$ 维输入，$d$ 阶多项式，特征空间维度为：</p><p>$$
\text{dim}(\phi(x)) = \binom{n+d}{d} = \frac{(n+d)!}{n! , d!}
$$</p><p><strong>推导</strong>：</p><p>$d$ 阶多项式的单项式数量，等价于将 $d$ 个相同的球放入 $n$ 个不同的盒子（允许空盒），即多重集合问题：</p><p>$$
\binom{n+d}{d} = \binom{n+d}{n}
$$</p><h3 id=22-复杂度爆炸实例>2.2 复杂度爆炸实例<a class=anchor href=#22-%e5%a4%8d%e6%9d%82%e5%ba%a6%e7%88%86%e7%82%b8%e5%ae%9e%e4%be%8b>#</a></h3><table><thead><tr><th>$n$ (维度)</th><th>$d$ (阶数)</th><th>特征维度</th><th>增长倍数</th></tr></thead><tbody><tr><td>10</td><td>2</td><td>66</td><td>6.6×</td></tr><tr><td>10</td><td>3</td><td>286</td><td>28.6×</td></tr><tr><td>10</td><td>5</td><td>3,003</td><td>300×</td></tr><tr><td>100</td><td>2</td><td>5,151</td><td>51.5×</td></tr><tr><td>100</td><td>3</td><td>176,851</td><td>1,769×</td></tr><tr><td>100</td><td>5</td><td>79,208,745</td><td>792,087×</td></tr></tbody></table><p><strong>问题</strong>：</p><ol><li><strong>存储开销</strong>：需要 $O(\binom{n+d}{d})$ 空间存储 $\phi(x)$</li><li><strong>计算开销</strong>：计算 $\phi(x)$ 需要 $O(\binom{n+d}{d})$ 时间</li><li><strong>内积开销</strong>：计算 $\phi(x)^T \phi(z)$ 需要 $O(\binom{n+d}{d})$ 次乘法</li></ol><p>当 $n = 100$, $d = 3$ 时，每个内积需要 <strong>176,851 次乘法</strong>！</p><h3 id=23-无穷维的梦魇>2.3 无穷维的梦魇<a class=anchor href=#23-%e6%97%a0%e7%a9%b7%e7%bb%b4%e7%9a%84%e6%a2%a6%e9%ad%87>#</a></h3><p>如果我们想要包含<strong>所有阶数</strong>的多项式特征（类似泰勒展开）：</p><p>$$
\phi(x) = (1, x_1, x_2, \ldots, x_1^2, x_1 x_2, \ldots, x_1^3, \ldots)
$$</p><p>特征空间变成<strong>无穷维</strong>！</p><p>这似乎是不可能计算的。但核方法会告诉我们：这不仅可能，而且高效。</p><hr><h2 id=3-救赎核技巧-kernel-trick>3. 救赎：核技巧 (Kernel Trick)<a class=anchor href=#3-%e6%95%91%e8%b5%8e%e6%a0%b8%e6%8a%80%e5%b7%a7-kernel-trick>#</a></h2><h3 id=31-核心观察我们不需要-phix>3.1 核心观察：我们不需要 $\phi(x)$<a class=anchor href=#31-%e6%a0%b8%e5%bf%83%e8%a7%82%e5%af%9f%e6%88%91%e4%bb%ac%e4%b8%8d%e9%9c%80%e8%a6%81-phix>#</a></h3><p>回顾线性分类器的决策函数（如 SVM）：</p><p>$$
f(x) = w^T \phi(x) + b
$$</p><p><strong>关键发现</strong>：在很多算法中（SVM、岭回归等），最优解 $w$ 可以表示为：</p><p>$$
w = \sum_{i=1}^m \alpha_i \phi(x_i)
$$</p><p>代入决策函数：</p><p>$$
f(x) = \sum_{i=1}^m \alpha_i \phi(x_i)^T \phi(x) + b = \sum_{i=1}^m \alpha_i \underbrace{\langle \phi(x_i), \phi(x) \rangle}_{K(x_i, x)} + b
$$</p><p><strong>结论</strong>：我们只需要计算<strong>内积</strong> $\phi(x_i)^T \phi(x)$，而不需要知道 $\phi(x)$ 本身！</p><h3 id=32-核函数的定义>3.2 核函数的定义<a class=anchor href=#32-%e6%a0%b8%e5%87%bd%e6%95%b0%e7%9a%84%e5%ae%9a%e4%b9%89>#</a></h3><p><strong>定义</strong>：核函数 $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ 满足：</p><p>$$
K(x, z) = \langle \phi(x), \phi(z) \rangle
$$</p><p>其中 $\phi: \mathcal{X} \to \mathcal{H}$ 是从输入空间到某个希尔伯特空间的映射。</p><p><strong>核技巧的承诺</strong>：</p><blockquote class=book-hint><p>如果 $K(x, z)$ 可以在原始空间（低维）中<strong>高效计算</strong>，我们就可以：</p><ol><li>避免显式计算 $\phi(x)$（避免维度爆炸）</li><li>避免在高维空间中存储和计算（节省空间和时间）</li><li>享受高维空间的表达能力（解决非线性问题）</li></ol></blockquote><h3 id=33-核心推导多项式核>3.3 核心推导：多项式核<a class=anchor href=#33-%e6%a0%b8%e5%bf%83%e6%8e%a8%e5%af%bc%e5%a4%9a%e9%a1%b9%e5%bc%8f%e6%a0%b8>#</a></h3><p><strong>目标</strong>：证明 $(x^T z)^2$ 等价于某个二阶多项式映射的内积。</p><p>设 $x = (x_1, x_2)^T$，$z = (z_1, z_2)^T$。</p><p><strong>步骤 1</strong>：展开内积的平方</p><p>$$
(x^T z)^2 = (x_1 z_1 + x_2 z_2)^2
$$</p><p><strong>步骤 2</strong>：展开平方</p><p>$$
\begin{aligned}
(x^T z)^2 &= (x_1 z_1)^2 + 2(x_1 z_1)(x_2 z_2) + (x_2 z_2)^2 \
&= x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2
\end{aligned}
$$</p><p><strong>步骤 3</strong>：配成内积形式</p><p>注意到可以写成：</p><p>$$
(x^T z)^2 = x_1^2 z_1^2 + (\sqrt{2} x_1 x_2)(\sqrt{2} z_1 z_2) + x_2^2 z_2^2
$$</p><p><strong>步骤 4</strong>：识别特征映射</p><p>定义：</p><p>$$
\phi(x) = \begin{pmatrix} x_1^2 \ \sqrt{2} x_1 x_2 \ x_2^2 \end{pmatrix}, \quad
\phi(z) = \begin{pmatrix} z_1^2 \ \sqrt{2} z_1 z_2 \ z_2^2 \end{pmatrix}
$$</p><p>则：</p><p>$$
\phi(x)^T \phi(z) = x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2 = (x^T z)^2
$$</p><p><strong>验证完毕</strong>：</p><p>$$
\boxed{K(x, z) = (x^T z)^2 = \langle \phi(x), \phi(z) \rangle}
$$</p><p><strong>计算复杂度对比</strong>：</p><table><thead><tr><th>方法</th><th>计算 $\phi(x)$</th><th>计算内积</th><th>总计</th></tr></thead><tbody><tr><td>显式映射</td><td>$O(n^2)$</td><td>$O(n^2)$</td><td>$O(n^2)$</td></tr><tr><td>核函数</td><td>-</td><td>$O(n)$</td><td>$O(n)$</td></tr></tbody></table><p>对于 $n = 100$，这是 <strong>100 倍</strong>的加速！</p><h3 id=34-一般多项式核>3.4 一般多项式核<a class=anchor href=#34-%e4%b8%80%e8%88%ac%e5%a4%9a%e9%a1%b9%e5%bc%8f%e6%a0%b8>#</a></h3><p><strong>$d$ 阶多项式核</strong>：</p><p>$$
K(x, z) = (x^T z + c)^d
$$</p><p>其中 $c \geq 0$ 是常数。</p><p><strong>特征空间维度</strong>：$\binom{n+d}{d}$</p><p><strong>计算复杂度</strong>：$O(n)$（计算 $x^T z$ 然后取 $d$ 次方）</p><p><strong>加速比</strong>：$\frac{\binom{n+d}{d}}{n} \approx \frac{n^d}{d! \cdot n} = \frac{n^{d-1}}{d!}$</p><p><strong>例子</strong>：$n = 100$, $d = 3$</p><ul><li>特征维度：176,851</li><li>核计算：100 次乘法 + 1 次立方</li><li>加速比：约 1,769 倍</li></ul><h3 id=35-核技巧的威力总结>3.5 核技巧的威力总结<a class=anchor href=#35-%e6%a0%b8%e6%8a%80%e5%b7%a7%e7%9a%84%e5%a8%81%e5%8a%9b%e6%80%bb%e7%bb%93>#</a></h3><p><strong>核技巧的本质</strong>：</p><blockquote class=book-hint><p>用 $O(n)$ 的计算代价，获得 $O(n^d)$ 甚至 $O(\infty)$ 维空间的表达能力。</p></blockquote><p>这是机器学习中少有的"免费午餐"——不是真的免费，而是数学的巧妙重组。</p><p><img src=assets/kernel_functions.svg alt=常见核函数></p><hr><h2 id=4-mercer-定理什么样的函数能当核>4. Mercer 定理：什么样的函数能当核？<a class=anchor href=#4-mercer-%e5%ae%9a%e7%90%86%e4%bb%80%e4%b9%88%e6%a0%b7%e7%9a%84%e5%87%bd%e6%95%b0%e8%83%bd%e5%bd%93%e6%a0%b8>#</a></h2><h3 id=41-问题的提出>4.1 问题的提出<a class=anchor href=#41-%e9%97%ae%e9%a2%98%e7%9a%84%e6%8f%90%e5%87%ba>#</a></h3><p>不是任意函数都能作为核函数。</p><p><strong>反例 1</strong>：$K(x, z) = -x^T z$</p><p>如果这是核函数，应该有 $K(x, x) = |\phi(x)|^2 \geq 0$。</p><p>但 $K(x, x) = -x^T x = -|x|^2 &lt; 0$（当 $x \neq 0$）。</p><p>矛盾！所以它不是核函数。</p><p><strong>反例 2</strong>：$K(x, z) = (x^T z)^3$（在某些情况下）</p><p>虽然看起来像多项式核，但对于某些数据，核矩阵可能不是半正定的。</p><p><strong>问题</strong>：什么样的函数 $K(x, z)$ 能保证对应某个 $\phi$？</p><h3 id=42-核矩阵-gram-matrix>4.2 核矩阵 (Gram Matrix)<a class=anchor href=#42-%e6%a0%b8%e7%9f%a9%e9%98%b5-gram-matrix>#</a></h3><p>给定数据集 ${x_1, \ldots, x_m}$ 和核函数 $K$，定义<strong>核矩阵</strong>：</p><p>$$
\mathbf{K} = \begin{pmatrix}
K(x_1, x_1) & K(x_1, x_2) & \cdots & K(x_1, x_m) \
K(x_2, x_1) & K(x_2, x_2) & \cdots & K(x_2, x_m) \
\vdots & \vdots & \ddots & \vdots \
K(x_m, x_1) & K(x_m, x_2) & \cdots & K(x_m, x_m)
\end{pmatrix}
$$</p><p><strong>性质</strong>：</p><ol><li>对称性：$\mathbf{K}<em>{ij} = K(x_i, x_j) = K(x_j, x_i) = \mathbf{K}</em>{ji}$</li><li>如果 $K$ 是核，则 $\mathbf{K}$ 应该满足什么性质？</li></ol><h3 id=43-mercer-定理标准形式>4.3 Mercer 定理（标准形式）<a class=anchor href=#43-mercer-%e5%ae%9a%e7%90%86%e6%a0%87%e5%87%86%e5%bd%a2%e5%bc%8f>#</a></h3><p><strong>Mercer 定理</strong> (Mercer&rsquo;s Theorem)：</p><blockquote class=book-hint><p>一个对称函数 $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ 是核函数，当且仅当对于任意有限数据集 ${x_1, \ldots, x_m}$，核矩阵 $\mathbf{K}$ 是<strong>半正定</strong> (positive semi-definite) 的。</p></blockquote><p><strong>半正定的定义</strong>：</p><p>矩阵 $\mathbf{K}$ 是半正定的，如果对于任意向量 $\alpha \in \mathbb{R}^m$：</p><p>$$
\alpha^T \mathbf{K} \alpha = \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j K(x_i, x_j) \geq 0
$$</p><p><strong>直觉解释：为什么半正定矩阵能分解成内积？</strong></p><p>这是线性代数中<strong>特征分解</strong>的优美应用：</p><ol><li><strong>半正定 ⇒ 非负特征值</strong>：半正定矩阵 $\mathbf{K}$ 的所有特征值 $\lambda_i \geq 0$</li><li><strong>特征分解 = 坐标系旋转</strong>：分解 $\mathbf{K} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T$ 等价于在新坐标系下重新表示数据</li><li><strong>构造特征映射</strong>：将每个点 $x_i$ 映射到 $\phi(x_i) = \sqrt{\mathbf{\Lambda}} \mathbf{U}_{i,:}^T$，本质是用特征值"拉伸"各维度</li><li><strong>内积恢复核</strong>：新坐标系下的内积 $\langle \phi(x_i), \phi(x_j) \rangle$ 恰好等于原核矩阵元素 $K(x_i, x_j)$</li></ol><p><strong>本质</strong>：半正定矩阵 $\mathbf{K}$ 可以看作是某个隐藏特征空间中的"距离/相似度矩阵"，特征分解帮我们<strong>逆向工程</strong>出这个隐藏空间的坐标 $\phi(x)$。</p><p><strong>直观例子</strong>：</p><p>考虑 3 个点的核矩阵：</p><p>$$
\mathbf{K} = \begin{pmatrix}
1.0 & 0.8 & 0.3 \
0.8 & 1.0 & 0.5 \
0.3 & 0.5 & 1.0
\end{pmatrix}
$$</p><p>特征分解后得到：$\lambda_1 = 1.95, \lambda_2 = 0.73, \lambda_3 = 0.32$（均非负 ✓）</p><p>构造特征映射：</p><p>$$
\phi(x_1) = \begin{pmatrix} \sqrt{1.95} \cdot u_{11} \ \sqrt{0.73} \cdot u_{12} \ \sqrt{0.32} \cdot u_{13} \end{pmatrix}
$$</p><p>则 $K(x_1, x_2) = \phi(x_1)^T \phi(x_2) = 0.8$ 自动满足！</p><p>这就是 Mercer 定理的构造性证明：<strong>从核矩阵反推特征空间</strong>。</p><h3 id=44-定理的证明充分性>4.4 定理的证明（充分性）<a class=anchor href=#44-%e5%ae%9a%e7%90%86%e7%9a%84%e8%af%81%e6%98%8e%e5%85%85%e5%88%86%e6%80%a7>#</a></h3><p><strong>命题</strong>：如果 $K(x, z) = \langle \phi(x), \phi(z) \rangle$，则 $\mathbf{K}$ 半正定。</p><p><strong>证明</strong>：</p><p>$$
\begin{aligned}
\alpha^T \mathbf{K} \alpha &= \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j K(x_i, x_j) \
&= \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j \langle \phi(x_i), \phi(x_j) \rangle \
&= \left\langle \sum_{i=1}^m \alpha_i \phi(x_i), \sum_{j=1}^m \alpha_j \phi(x_j) \right\rangle \
&= \left| \sum_{i=1}^m \alpha_i \phi(x_i) \right|^2 \geq 0
\end{aligned}
$$</p><p>最后一步使用了范数的非负性。</p><h3 id=45-定理的证明必要性>4.5 定理的证明（必要性）<a class=anchor href=#45-%e5%ae%9a%e7%90%86%e7%9a%84%e8%af%81%e6%98%8e%e5%bf%85%e8%a6%81%e6%80%a7>#</a></h3><p><strong>命题</strong>：如果 $\mathbf{K}$ 半正定，则存在 $\phi$ 使得 $K(x, z) = \langle \phi(x), \phi(z) \rangle$。</p><p><strong>证明</strong>（构造性）：</p><p><strong>步骤 1</strong>：对 $\mathbf{K}$ 进行特征值分解</p><p>由于 $\mathbf{K}$ 对称半正定：</p><p>$$
\mathbf{K} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T
$$</p><p>其中：</p><ul><li>$\mathbf{\Lambda} = \text{diag}(\lambda_1, \ldots, \lambda_m)$，$\lambda_i \geq 0$</li><li>$\mathbf{U} = [u_1, \ldots, u_m]$ 是正交矩阵</li></ul><p><strong>步骤 2</strong>：构造特征映射</p><p>定义 $\phi: \mathcal{X} \to \mathbb{R}^m$，对于 $x_i$：</p><p>$$
\phi(x_i) = \sqrt{\mathbf{\Lambda}} \mathbf{U}<em>{i,:}^T = \sum</em>{k=1}^m \sqrt{\lambda_k} u_{ik} , e_k
$$</p><p>其中 $e_k$ 是标准基向量，$\mathbf{U}_{i,:}$ 是 $\mathbf{U}$ 的第 $i$ 行。</p><p>更直观地写成列向量形式：</p><p>$$
\phi(x_i) = \begin{pmatrix}
\sqrt{\lambda_1} u_{i1} \
\sqrt{\lambda_2} u_{i2} \
\vdots \
\sqrt{\lambda_m} u_{im}
\end{pmatrix}
$$</p><p><strong>几何直觉</strong>：</p><ul><li>$\mathbf{U}_{i,:}$（第 $i$ 行）是 $x_i$ 在特征向量基下的<strong>旋转坐标</strong></li><li>$\sqrt{\mathbf{\Lambda}}$ 对每个坐标轴进行<strong>拉伸</strong>（拉伸量 = $\sqrt{\lambda_k}$）</li><li>结果：$x_i$ 被映射到一个 $m$ 维空间，使得点之间的内积恰好等于原始核值</li></ul><p>这正是**主成分分析（PCA）**的几何：特征值 = 方差，特征向量 = 主方向。</p><p><strong>步骤 3</strong>：验证内积</p><p>$$
\begin{aligned}
\langle \phi(x_i), \phi(x_j) \rangle &= \sum_{k=1}^m (\sqrt{\lambda_k} u_{ik})(\sqrt{\lambda_k} u_{jk}) \
&= \sum_{k=1}^m \lambda_k u_{ik} u_{jk} \
&= (\mathbf{U} \mathbf{\Lambda} \mathbf{U}^T)<em>{ij} \
&= \mathbf{K}</em>{ij} = K(x_i, x_j)
\end{aligned}
$$</p><p><strong>证毕</strong>。</p><p><strong>可视化建议</strong>：</p><p>Mercer 定理的核心可以用以下流程图表示：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>核矩阵 K (m×m)
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>特征分解 K = UΛU^T
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>构造特征映射 φ(xᵢ) = √Λ Uᵢ,:^T
</span></span><span class=line><span class=cl>    ↓
</span></span><span class=line><span class=cl>验证内积 ⟨φ(xᵢ), φ(xⱼ)⟩ = Kᵢⱼ ✓</span></span></code></pre></div><p>这个过程展示了如何从"观测到的相似度矩阵 $\mathbf{K}$"<strong>逆向工程</strong>出"隐藏的特征空间坐标 $\phi(x)$"。</p><h3 id=46-核函数的组合性质>4.6 核函数的组合性质<a class=anchor href=#46-%e6%a0%b8%e5%87%bd%e6%95%b0%e7%9a%84%e7%bb%84%e5%90%88%e6%80%a7%e8%b4%a8>#</a></h3><p><strong>定理</strong>：如果 $K_1, K_2$ 是核函数，则以下也是核函数：</p><ol><li><strong>线性组合</strong>：$K(x,z) = \alpha K_1(x,z) + \beta K_2(x,z)$，其中 $\alpha, \beta \geq 0$</li><li><strong>乘积</strong>：$K(x,z) = K_1(x,z) \cdot K_2(x,z)$</li><li><strong>函数复合</strong>：$K(x,z) = f(x) K_1(x,z) f(z)$，其中 $f$ 是任意函数</li><li><strong>指数</strong>：$K(x,z) = \exp(K_1(x,z))$（如果 $K_1$ 有界）</li><li><strong>多项式</strong>：$K(x,z) = p(K_1(x,z))$，其中 $p$ 是非负系数多项式</li></ol><p><strong>证明（乘积）</strong>：</p><p>设 $K_1(x,z) = \langle \phi_1(x), \phi_1(z) \rangle$，$K_2(x,z) = \langle \phi_2(x), \phi_2(z) \rangle$。</p><p>设 $\phi_1(x) \in \mathbb{R}^{d_1}$，$\phi_2(x) \in \mathbb{R}^{d_2}$。</p><p>定义 $\phi(x) = \phi_1(x) \otimes \phi_2(x)$ (张量积)，即：</p><p>$$
\phi(x) = \begin{pmatrix}
\phi_1^{(1)}(x) \phi_2^{(1)}(x) \
\phi_1^{(1)}(x) \phi_2^{(2)}(x) \
\vdots \
\phi_1^{(i)}(x) \phi_2^{(j)}(x) \
\vdots \
\phi_1^{(d_1)}(x) \phi_2^{(d_2)}(x)
\end{pmatrix} \in \mathbb{R}^{d_1 \times d_2}
$$</p><p>则：</p><p>$$
\begin{aligned}
\langle \phi(x), \phi(z) \rangle &= \sum_{i=1}^{d_1} \sum_{j=1}^{d_2} \phi_1^{(i)}(x) \phi_2^{(j)}(x) \phi_1^{(i)}(z) \phi_2^{(j)}(z) \
&= \left(\sum_{i=1}^{d_1} \phi_1^{(i)}(x) \phi_1^{(i)}(z)\right) \left(\sum_{j=1}^{d_2} \phi_2^{(j)}(x) \phi_2^{(j)}(z)\right) \
&= K_1(x,z) \cdot K_2(x,z)
\end{aligned}
$$</p><p><strong>证毕</strong>。</p><hr><h2 id=5-rbf-核通往无穷维>5. RBF 核：通往无穷维<a class=anchor href=#5-rbf-%e6%a0%b8%e9%80%9a%e5%be%80%e6%97%a0%e7%a9%b7%e7%bb%b4>#</a></h2><h3 id=51-径向基函数核的定义>5.1 径向基函数核的定义<a class=anchor href=#51-%e5%be%84%e5%90%91%e5%9f%ba%e5%87%bd%e6%95%b0%e6%a0%b8%e7%9a%84%e5%ae%9a%e4%b9%89>#</a></h3><p><strong>RBF 核</strong> (Radial Basis Function Kernel) 或<strong>高斯核</strong> (Gaussian Kernel)：</p><p>$$
K(x, z) = \exp\left(-\gamma |x - z|^2\right)
$$</p><p>其中 $\gamma > 0$ 是带宽参数。</p><p>常见形式（$\gamma = \frac{1}{2\sigma^2}$）：</p><p>$$
K(x, z) = \exp\left(-\frac{|x - z|^2}{2\sigma^2}\right)
$$</p><p><strong>性质</strong>：</p><ol><li><strong>对称性</strong>：$K(x, z) = K(z, x)$</li><li><strong>有界性</strong>：$0 &lt; K(x, z) \leq 1$</li><li><strong>归一化</strong>：$K(x, x) = 1$</li><li><strong>径向性</strong>：只依赖于 $|x - z|$，与方向无关</li></ol><h3 id=52-rbf-核对应无穷维特征空间>5.2 RBF 核对应无穷维特征空间<a class=anchor href=#52-rbf-%e6%a0%b8%e5%af%b9%e5%ba%94%e6%97%a0%e7%a9%b7%e7%bb%b4%e7%89%b9%e5%be%81%e7%a9%ba%e9%97%b4>#</a></h3><p><strong>核心问题</strong>：$K(x, z) = \exp(-\gamma |x-z|^2)$ 对应的 $\phi(x)$ 是什么？</p><p><strong>定理</strong>：RBF 核对应于<strong>无穷维希尔伯特空间</strong>的内积。</p><p><strong>证明</strong>（一维情况）：</p><p>设 $x, z \in \mathbb{R}$，$\gamma = \frac{1}{2}$：</p><p>$$
K(x, z) = \exp\left(-\frac{1}{2}(x-z)^2\right)
$$</p><p><strong>步骤 1</strong>：展开平方项</p><p>$$
K(x, z) = \exp\left(-\frac{1}{2}(x^2 - 2xz + z^2)\right)
$$</p><p>分离变量：</p><p>$$
K(x, z) = \exp\left(-\frac{x^2}{2}\right) \exp(xz) \exp\left(-\frac{z^2}{2}\right)
$$</p><p><strong>步骤 2</strong>：泰勒展开 $\exp(xz)$</p><p>$$
\exp(xz) = \sum_{k=0}^{\infty} \frac{(xz)^k}{k!} = \sum_{k=0}^{\infty} \frac{x^k z^k}{k!}
$$</p><p><strong>步骤 3</strong>：代入</p><p>$$
\begin{aligned}
K(x, z) &= \exp\left(-\frac{x^2}{2}\right) \exp\left(-\frac{z^2}{2}\right) \sum_{k=0}^{\infty} \frac{x^k z^k}{k!} \
&= \sum_{k=0}^{\infty} \frac{1}{k!} \left[\exp\left(-\frac{x^2}{2}\right) x^k\right] \left[\exp\left(-\frac{z^2}{2}\right) z^k\right]
\end{aligned}
$$</p><p><strong>步骤 4</strong>：识别特征映射结构</p><p>观察步骤 3 的结果，我们可以直接定义特征映射：</p><p>$$
\psi_k(x) = \frac{1}{\sqrt{k!}} \exp\left(-\frac{x^2}{2}\right) x^k
$$</p><p>则核函数变为：</p><p>$$
\begin{aligned}
K(x, z) &= \sum_{k=0}^{\infty} \frac{1}{k!} \left[\exp\left(-\frac{x^2}{2}\right) x^k\right] \left[\exp\left(-\frac{z^2}{2}\right) z^k\right] \
&= \sum_{k=0}^{\infty} \left[\frac{1}{\sqrt{k!}} \exp\left(-\frac{x^2}{2}\right) x^k\right] \left[\frac{1}{\sqrt{k!}} \exp\left(-\frac{z^2}{2}\right) z^k\right] \
&= \sum_{k=0}^{\infty} \psi_k(x) \psi_k(z) = \langle \psi(x), \psi(z) \rangle
\end{aligned}
$$</p><p><strong>关键：归一化系数 $\frac{1}{\sqrt{k!}}$ 的作用</strong></p><p>为什么需要 $\sqrt{k!}$ 而不是 $k!$？</p><ul><li>泰勒展开给出 $\frac{x^k z^k}{k!}$，如果直接分配，每边得到 $\frac{1}{\sqrt{k!}}$</li><li>这保证了级数的<strong>平方收敛性</strong>（每个分量的 $L^2$ 范数有限）</li><li>物理意义：$\sqrt{k!}$ 正是厄米多项式（Hermite polynomials）归一化的来源</li></ul><p>$$
\psi(x) = \begin{pmatrix}
\psi_0(x) \
\psi_1(x) \
\psi_2(x) \
\vdots
\end{pmatrix} = \begin{pmatrix}
\exp(-x^2/2) \
\frac{\exp(-x^2/2) x}{\sqrt{1!}} \
\frac{\exp(-x^2/2) x^2}{\sqrt{2!}} \
\frac{\exp(-x^2/2) x^3}{\sqrt{3!}} \
\vdots
\end{pmatrix}
$$</p><p>其中：</p><p>$$</p><p><strong>这是一个无穷维向量！</strong></p><p><strong>步骤 5</strong>：验证收敛性与范数</p><p>级数的平方收敛性（$L^2$ 范数有限）：</p><p>$$
\begin{aligned}
|\psi(x)|^2 &= \sum_{k=0}^{\infty} \psi_k(x)^2 \
&= \sum_{k=0}^{\infty} \left[\frac{\exp(-x^2/2) \cdot x^k}{\sqrt{k!}}\right]^2 \
&= \exp(-x^2) \sum_{k=0}^{\infty} \frac{x^{2k}}{k!} \
&= \exp(-x^2) \cdot \exp(x^2) \quad \text{(泰勒展开)} \
&= 1
\end{aligned}
$$</p><p><strong>完美！</strong> 特征映射 $\psi(x)$ 的范数恒为 1，这意味着：</p><ul><li>无穷维级数收敛（良定义）</li><li>所有点都被映射到单位超球面上</li><li>RBF 核的归一化性质 $K(x,x) = 1$ 在特征空间中对应 $|\psi(x)|^2 = 1$</li></ul><p><strong>推导完毕</strong>：我们成功地将 RBF 核表示为无穷维希尔伯特空间中的内积。</p><h3 id=53-多维推广>5.3 多维推广<a class=anchor href=#53-%e5%a4%9a%e7%bb%b4%e6%8e%a8%e5%b9%bf>#</a></h3><p>对于 $x, z \in \mathbb{R}^n$：</p><p>$$
K(x, z) = \exp\left(-\gamma |x-z|^2\right) = \exp\left(-\gamma \sum_{i=1}^n (x_i - z_i)^2\right)
$$</p><p>利用指数的乘法性：</p><p>$$
K(x, z) = \prod_{i=1}^n \exp\left(-\gamma (x_i - z_i)^2\right) = \prod_{i=1}^n K_i(x_i, z_i)
$$</p><p>每个 $K_i$ 是一维 RBF 核，对应无穷维特征。</p><p>根据核的乘积性质（见 4.6 节），总的特征空间是这 $n$ 个无穷维空间的<strong>张量积</strong>：</p><p>$$
\phi(x) = \phi_1(x_1) \otimes \phi_2(x_2) \otimes \cdots \otimes \phi_n(x_n)
$$</p><p><strong>维度爆炸</strong>：$\infty \otimes \infty \otimes \cdots \otimes \infty = \infty$（但仍可通过核技巧高效计算）</p><p><strong>总结：RBF 核的三层魔法</strong></p><ol><li><strong>泰勒展开</strong>：将指数函数展开为无穷级数 $\exp(xz) = \sum_{k=0}^\infty \frac{(xz)^k}{k!}$</li><li><strong>分离变量</strong>：将交叉项 $x^k z^k$ 分配给两个特征向量 $\psi_k(x)$ 和 $\psi_k(z)$</li><li><strong>归一化</strong>：用 $\sqrt{k!}$ 保证级数收敛（单位范数）</li></ol><p>最终效果：用 $O(n)$ 的计算（计算 $|x-z|^2$ 和一次指数运算）实现无穷维空间的内积。</p><h3 id=54-rbf-核的威力与风险>5.4 RBF 核的威力与风险<a class=anchor href=#54-rbf-%e6%a0%b8%e7%9a%84%e5%a8%81%e5%8a%9b%e4%b8%8e%e9%a3%8e%e9%99%a9>#</a></h3><p><strong>为什么 RBF 核这么强大？</strong></p><ol><li><strong>无穷维表达能力</strong>：可以拟合任意复杂的决策边界</li><li><strong>计算高效</strong>：$K(x, z)$ 只需 $O(n)$ 时间（计算 $|x-z|^2$）</li><li><strong>参数简单</strong>：只有一个参数 $\gamma$（或 $\sigma$）</li><li><strong>万能逼近</strong>：在适当条件下可以逼近任意连续函数</li></ol><p><strong>参数 $\gamma$ 的影响</strong>：</p><ul><li><p><strong>$\gamma$ 很大</strong>（$\sigma$ 很小）：</p><ul><li>核函数"尖锐"，只有非常接近的点才有非零值</li><li>决策边界非常复杂，每个训练点都是一个"孤岛"</li><li><strong>过拟合</strong>风险高</li></ul></li><li><p><strong>$\gamma$ 很小</strong>（$\sigma$ 很大）：</p><ul><li>核函数"平坦"，所有点的相似度都接近 1</li><li>决策边界趋于线性</li><li><strong>欠拟合</strong>风险高</li></ul></li></ul><p><strong>实践建议</strong>：</p><ul><li>初始值：$\gamma = \frac{1}{n \cdot \text{Var}(X)}$</li><li>范围：$\gamma \in [10^{-4}, 10^{1}]$（对数尺度搜索）</li><li>使用交叉验证选择</li></ul><h3 id=55-常见核函数总结>5.5 常见核函数总结<a class=anchor href=#55-%e5%b8%b8%e8%a7%81%e6%a0%b8%e5%87%bd%e6%95%b0%e6%80%bb%e7%bb%93>#</a></h3><table><thead><tr><th>核函数</th><th>公式</th><th>特征空间维度</th><th>适用场景</th></tr></thead><tbody><tr><td>线性核</td><td>$K(x,z) = x^T z$</td><td>$n$</td><td>线性可分问题</td></tr><tr><td>多项式核</td><td>$K(x,z) = (x^T z + c)^d$</td><td>$\binom{n+d}{d}$</td><td>多项式模式</td></tr><tr><td>RBF 核</td><td>$K(x,z) = \exp(-\gamma |x-z|^2)$</td><td>$\infty$</td><td>通用，默认首选</td></tr><tr><td>Sigmoid 核</td><td>$K(x,z) = \tanh(\alpha x^T z + c)$</td><td>-</td><td>模拟神经网络</td></tr><tr><td>拉普拉斯核</td><td>$K(x,z) = \exp(-\gamma |x-z|_1)$</td><td>$\infty$</td><td>对异常值鲁棒</td></tr></tbody></table><hr><h2 id=6-应用核化一切>6. 应用：核化一切<a class=anchor href=#6-%e5%ba%94%e7%94%a8%e6%a0%b8%e5%8c%96%e4%b8%80%e5%88%87>#</a></h2><h3 id=61-表示定理-representer-theorem>6.1 表示定理 (Representer Theorem)<a class=anchor href=#61-%e8%a1%a8%e7%a4%ba%e5%ae%9a%e7%90%86-representer-theorem>#</a></h3><p><strong>核心洞察</strong>：许多机器学习算法的最优解都可以表示为训练样本的线性组合。</p><p><strong>表示定理</strong>：</p><p>考虑正则化经验风险最小化问题：</p><p>$$
\min_{w \in \mathcal{H}} \left[\sum_{i=1}^m L(y_i, \langle w, \phi(x_i) \rangle) + \lambda \Omega(|w|)\right]
$$</p><p>其中：</p><ul><li>$L$ 是损失函数</li><li>$\Omega$ 是单调递增的正则化项</li><li>$\lambda > 0$ 是正则化参数</li><li>$\mathcal{H}$ 是希尔伯特空间</li></ul><p><strong>定理</strong>：最优解 $w^*$ 必然在训练样本张成的子空间中，即：</p><p>$$
w^* = \sum_{i=1}^m \alpha_i \phi(x_i)
$$</p><p><strong>证明</strong>：</p><p><strong>步骤 1</strong>：将 $w$ 分解</p><p>将 $w$ 分解为两部分：</p><p>$$
w = w_\parallel + w_\perp
$$</p><p>其中：</p><ul><li>$w_\parallel \in \text{span}{\phi(x_1), \ldots, \phi(x_m)}$（数据子空间）</li><li>$w_\perp \perp \text{span}{\phi(x_1), \ldots, \phi(x_m)}$（正交补空间）</li></ul><p><strong>步骤 2</strong>：分析损失项</p><p>对于任意 $i$：</p><p>$$
\langle w, \phi(x_i) \rangle = \langle w_\parallel + w_\perp, \phi(x_i) \rangle = \langle w_\parallel, \phi(x_i) \rangle + \underbrace{\langle w_\perp, \phi(x_i) \rangle}_{=0}
$$</p><p>因为 $w_\perp$ 垂直于 $\phi(x_i)$。</p><p>所以：</p><p>$$
\sum_{i=1}^m L(y_i, \langle w, \phi(x_i) \rangle) = \sum_{i=1}^m L(y_i, \langle w_\parallel, \phi(x_i) \rangle)
$$</p><p><strong>步骤 3</strong>：分析正则化项</p><p>$$
|w|^2 = |w_\parallel|^2 + |w_\perp|^2 \geq |w_\parallel|^2
$$</p><p>因此：</p><p>$$
\Omega(|w|) \geq \Omega(|w_\parallel|)
$$</p><p>（因为 $\Omega$ 单调递增）</p><p><strong>步骤 4</strong>：结论</p><p>目标函数：</p><p>$$
\sum_{i=1}^m L(y_i, \langle w, \phi(x_i) \rangle) + \lambda \Omega(|w|)
= \sum_{i=1}^m L(y_i, \langle w_\parallel, \phi(x_i) \rangle) + \lambda \Omega(|w|)
\geq \sum_{i=1}^m L(y_i, \langle w_\parallel, \phi(x_i) \rangle) + \lambda \Omega(|w_\parallel|)
$$</p><p>取 $w_\perp = 0$（即 $w = w_\parallel$）时，目标函数更小。</p><p>因此最优解必然在 $\text{span}{\phi(x_1), \ldots, \phi(x_m)}$ 中：</p><p>$$
w^* = \sum_{i=1}^m \alpha_i \phi(x_i)
$$</p><p><strong>证毕</strong>。</p><h3 id=62-核化的一般步骤>6.2 核化的一般步骤<a class=anchor href=#62-%e6%a0%b8%e5%8c%96%e7%9a%84%e4%b8%80%e8%88%ac%e6%ad%a5%e9%aa%a4>#</a></h3><p>任何算法的核化遵循以下步骤：</p><ol><li><strong>应用表示定理</strong>：证明 $w^* = \sum_{i=1}^m \alpha_i \phi(x_i)$</li><li><strong>代入目标函数</strong>：将 $w$ 用 $\alpha$ 表示</li><li><strong>引入核矩阵</strong>：将所有 $\langle \phi(x_i), \phi(x_j) \rangle$ 替换为 $K(x_i, x_j)$</li><li><strong>求解对偶问题</strong>：优化关于 $\alpha$ 的问题</li><li><strong>预测</strong>：$f(x) = \sum_{i=1}^m \alpha_i K(x_i, x)$</li></ol><h3 id=63-案例核岭回归-kernel-ridge-regression>6.3 案例：核岭回归 (Kernel Ridge Regression)<a class=anchor href=#63-%e6%a1%88%e4%be%8b%e6%a0%b8%e5%b2%ad%e5%9b%9e%e5%bd%92-kernel-ridge-regression>#</a></h3><h4 id=631-原始岭回归>6.3.1 原始岭回归<a class=anchor href=#631-%e5%8e%9f%e5%a7%8b%e5%b2%ad%e5%9b%9e%e5%bd%92>#</a></h4><p>给定数据 ${(x_i, y_i)}_{i=1}^m$，岭回归求解：</p><p>$$
\min_{w} \sum_{i=1}^m (y_i - w^T x_i)^2 + \lambda |w|^2
$$</p><p>解析解（第 5 章）：</p><p>$$
w^* = (X^T X + \lambda I)^{-1} X^T y
$$</p><p>其中 $X = [x_1, \ldots, x_m]^T \in \mathbb{R}^{m \times n}$。</p><p><strong>问题</strong>：</p><ul><li>需要计算 $n \times n$ 矩阵的逆（$n$ 是特征维度）</li><li>如果 $n$ 很大（或无穷大），无法计算</li></ul><h4 id=632-核化推导>6.3.2 核化推导<a class=anchor href=#632-%e6%a0%b8%e5%8c%96%e6%8e%a8%e5%af%bc>#</a></h4><p>假设数据映射到特征空间 $\phi(x)$，优化问题变为：</p><p>$$
\min_{w} \sum_{i=1}^m (y_i - \langle w, \phi(x_i) \rangle)^2 + \lambda |w|^2
$$</p><p>记 $\Phi = [\phi(x_1), \ldots, \phi(x_m)]^T \in \mathbb{R}^{m \times D}$（$D$ 可能是 $\infty$）。</p><p><strong>步骤 1</strong>：应用表示定理</p><p>$$
w = \Phi^T \alpha = \sum_{i=1}^m \alpha_i \phi(x_i)
$$</p><p><strong>步骤 2</strong>：代入目标函数</p><p>$$
\begin{aligned}
\langle w, \phi(x_i) \rangle &= \langle \sum_{j=1}^m \alpha_j \phi(x_j), \phi(x_i) \rangle
= \sum_{j=1}^m \alpha_j \langle \phi(x_j), \phi(x_i) \rangle \
&= \sum_{j=1}^m \alpha_j K(x_j, x_i) = (\mathbf{K} \alpha)_i
\end{aligned}
$$</p><p>其中 $\mathbf{K}_{ij} = K(x_i, x_j)$。</p><p>正则化项：</p><p>$$
|w|^2 = \langle \Phi^T \alpha, \Phi^T \alpha \rangle = \alpha^T \Phi \Phi^T \alpha = \alpha^T \mathbf{K} \alpha
$$</p><p><strong>步骤 3</strong>：核化的目标函数</p><p>$$
\min_{\alpha} |y - \mathbf{K} \alpha|^2 + \lambda \alpha^T \mathbf{K} \alpha
$$</p><p><strong>步骤 4</strong>：求导</p><p>$$
\frac{\partial}{\partial \alpha} \left[(y - \mathbf{K} \alpha)^T (y - \mathbf{K} \alpha) + \lambda \alpha^T \mathbf{K} \alpha\right] = 0
$$</p><p>展开：</p><p>$$
-2 \mathbf{K}^T (y - \mathbf{K} \alpha) + 2 \lambda \mathbf{K} \alpha = 0
$$</p><p>因为 $\mathbf{K}$ 对称（$\mathbf{K}^T = \mathbf{K}$）：</p><p>$$
\mathbf{K} (y - \mathbf{K} \alpha) + \lambda \mathbf{K} \alpha = 0
$$</p><p>$$
\mathbf{K} y = \mathbf{K}^2 \alpha + \lambda \mathbf{K} \alpha = \mathbf{K}(\mathbf{K} + \lambda I) \alpha
$$</p><p>假设 $\mathbf{K}$ 可逆（或使用伪逆）：</p><p>$$
y = (\mathbf{K} + \lambda I) \alpha
$$</p><p>解得：</p><p>$$
\boxed{\alpha^* = (\mathbf{K} + \lambda I)^{-1} y}
$$</p><h4 id=633-预测>6.3.3 预测<a class=anchor href=#633-%e9%a2%84%e6%b5%8b>#</a></h4><p>对于新样本 $x_{\text{new}}$：</p><p>$$
\begin{aligned}
\hat{y}<em>{\text{new}} &= \langle w^*, \phi(x</em>{\text{new}}) \rangle \
&= \sum_{i=1}^m \alpha_i^* \langle \phi(x_i), \phi(x_{\text{new}}) \rangle \
&= \sum_{i=1}^m \alpha_i^* K(x_i, x_{\text{new}})
\end{aligned}
$$</p><p>定义 $k_{\text{new}} = [K(x_1, x_{\text{new}}), \ldots, K(x_m, x_{\text{new}})]^T$：</p><p>$$
\boxed{\hat{y}<em>{\text{new}} = k</em>{\text{new}}^T \alpha^* = k_{\text{new}}^T (\mathbf{K} + \lambda I)^{-1} y}
$$</p><h4 id=634-算法总结>6.3.4 算法总结<a class=anchor href=#634-%e7%ae%97%e6%b3%95%e6%80%bb%e7%bb%93>#</a></h4><p><strong>核岭回归算法</strong>：</p><p><strong>输入</strong>：</p><ul><li>训练集 ${(x_i, y_i)}_{i=1}^m$</li><li>核函数 $K(\cdot, \cdot)$</li><li>正则化参数 $\lambda$</li></ul><p><strong>训练</strong>：</p><ol><li>计算核矩阵 $\mathbf{K} \in \mathbb{R}^{m \times m}$：$\mathbf{K}_{ij} = K(x_i, x_j)$</li><li>求解：$\alpha = (\mathbf{K} + \lambda I)^{-1} y$</li></ol><p><strong>预测</strong> $x_{\text{new}}$：</p><ol><li>计算核向量 $k_{\text{new}}$：$k_{\text{new}, i} = K(x_i, x_{\text{new}})$</li><li>预测：$\hat{y}<em>{\text{new}} = k</em>{\text{new}}^T \alpha$</li></ol><p><strong>复杂度分析</strong>：</p><table><thead><tr><th>操作</th><th>原始形式</th><th>对偶形式（核化）</th></tr></thead><tbody><tr><td>训练</td><td>$O(n^2 m + n^3)$</td><td>$O(m^2 n + m^3)$</td></tr><tr><td>预测</td><td>$O(n)$</td><td>$O(mn)$</td></tr><tr><td>特征维度限制</td><td>必须有限</td><td>可以无穷</td></tr></tbody></table><p><strong>何时使用核化</strong>：</p><ul><li>当 $m \ll n$（样本少、特征多）</li><li>当 $n = \infty$（RBF 核等无穷维特征）</li><li>当数据非线性可分</li></ul><h4 id=635-线性岭回归-vs-核岭回归完整对比>6.3.5 线性岭回归 vs 核岭回归：完整对比<a class=anchor href=#635-%e7%ba%bf%e6%80%a7%e5%b2%ad%e5%9b%9e%e5%bd%92-vs-%e6%a0%b8%e5%b2%ad%e5%9b%9e%e5%bd%92%e5%ae%8c%e6%95%b4%e5%af%b9%e6%af%94>#</a></h4><p>以下表格展示了第 5 章的线性岭回归与核岭回归的<strong>对偶呼应</strong>：</p><table><thead><tr><th>维度</th><th>线性岭回归（第 5 章）</th><th>核岭回归（本章）</th></tr></thead><tbody><tr><td><strong>优化问题</strong></td><td>$\min_w |y - Xw|^2 + \lambda |w|^2$</td><td>$\min_w |y - \Phi w|^2 + \lambda |w|^2$</td></tr><tr><td><strong>特征空间</strong></td><td>原始空间 $\mathbb{R}^n$</td><td>隐式高维空间 $\mathcal{H}$ (可能 $\infty$ 维)</td></tr><tr><td><strong>参数形式</strong></td><td>直接求 $w \in \mathbb{R}^n$</td><td>表示为 $w = \sum_{i=1}^m \alpha_i \phi(x_i)$</td></tr><tr><td><strong>解析解</strong></td><td>$w^* = (X^T X + \lambda I)^{-1} X^T y$</td><td>$\alpha^* = (\mathbf{K} + \lambda I)^{-1} y$</td></tr><tr><td><strong>矩阵维度</strong></td><td>$(X^T X)$: $n \times n$</td><td>$\mathbf{K}$: $m \times m$</td></tr><tr><td><strong>求逆复杂度</strong></td><td>$O(n^3)$</td><td>$O(m^3)$</td></tr><tr><td><strong>预测公式</strong></td><td>$\hat{y} = w^T x$</td><td>$\hat{y} = \sum_{i=1}^m \alpha_i K(x_i, x)$</td></tr><tr><td><strong>预测复杂度</strong></td><td>$O(n)$</td><td>$O(mn)$</td></tr><tr><td><strong>适用场景</strong></td><td>$n \ll m$（特征少、样本多）</td><td>$m \ll n$ 或 $n = \infty$</td></tr><tr><td><strong>模型能力</strong></td><td>线性决策边界</td><td>非线性决策边界</td></tr><tr><td><strong>可解释性</strong></td><td>高（可查看 $w$ 的每个系数）</td><td>低（$w$ 在隐式空间）</td></tr><tr><td><strong>数学本质</strong></td><td>在原始空间正则化</td><td>在特征空间正则化 + 核技巧</td></tr></tbody></table><p><strong>核心洞察</strong>：</p><ol><li><p><strong>互补性</strong>：$n$ 和 $m$ 的角色互换</p><ul><li>线性：复杂度取决于<strong>特征数</strong> $n$</li><li>核化：复杂度取决于<strong>样本数</strong> $m$</li></ul></li><li><p><strong>统一性</strong>：当使用线性核 $K(x, z) = x^T z$ 时，核岭回归退化为线性岭回归（见下方推导）</p></li><li><p><strong>表达力权衡</strong>：</p><ul><li>线性岭回归：快速、可解释，但表达力受限</li><li>核岭回归：强大、灵活，但计算成本更高</li></ul></li></ol><p><strong>特殊情况：线性核的统一</strong></p><p>当 $K(x, z) = x^T z$（线性核）时：</p><p>$$
\begin{aligned}
\mathbf{K} &= XX^T \in \mathbb{R}^{m \times m} \
\alpha^* &= (XX^T + \lambda I)^{-1} y \
w^* &= X^T \alpha^* = X^T (XX^T + \lambda I)^{-1} y
\end{aligned}
$$</p><p>根据 Woodbury 矩阵恒等式：</p><p>$$
X^T (XX^T + \lambda I)^{-1} = (X^T X + \lambda I)^{-1} X^T
$$</p><p>因此：</p><p>$$
w^* = (X^T X + \lambda I)^{-1} X^T y
$$</p><p>这正是线性岭回归的解！<strong>两者在线性核下完全等价</strong>。</p><h4 id=636-python-实现示例>6.3.6 Python 实现示例<a class=anchor href=#636-python-%e5%ae%9e%e7%8e%b0%e7%a4%ba%e4%be%8b>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>KernelRidgeRegression</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;rbf&#39;</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>lambda_</span><span class=o>=</span><span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span> <span class=o>=</span> <span class=n>kernel</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lambda_</span> <span class=o>=</span> <span class=n>lambda_</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_kernel_function</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X1</span><span class=p>,</span> <span class=n>X2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;计算核矩阵&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span> <span class=o>==</span> <span class=s1>&#39;rbf&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># RBF 核: exp(-gamma * ||x - z||^2)</span>
</span></span><span class=line><span class=cl>            <span class=c1># ||x - z||^2 = ||x||^2 + ||z||^2 - 2 x^T z</span>
</span></span><span class=line><span class=cl>            <span class=n>X1_norm</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>X1</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>X2_norm</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>X2</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=p>(</span><span class=n>X1_norm</span> <span class=o>+</span> <span class=n>X2_norm</span> <span class=o>-</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>X1</span> <span class=o>@</span> <span class=n>X2</span><span class=o>.</span><span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span> <span class=o>==</span> <span class=s1>&#39;linear&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=n>X1</span> <span class=o>@</span> <span class=n>X2</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span> <span class=o>==</span> <span class=s1>&#39;poly&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 多项式核: (x^T z + 1)^d</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=p>(</span><span class=n>X1</span> <span class=o>@</span> <span class=n>X2</span><span class=o>.</span><span class=n>T</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>K</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;训练&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>X_train</span> <span class=o>=</span> <span class=n>X</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y_train</span> <span class=o>=</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算核矩阵</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_kernel_function</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 求解 alpha = (K + lambda I)^{-1} y</span>
</span></span><span class=line><span class=cl>        <span class=n>m</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>solve</span><span class=p>(</span><span class=n>K</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>lambda_</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>m</span><span class=p>),</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;预测&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算核向量 k(x)</span>
</span></span><span class=line><span class=cl>        <span class=n>K_new</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_kernel_function</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>X_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 预测 y = k(x)^T alpha</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>K_new</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用示例</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>X_train</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>])</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>KernelRidgeRegression</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;rbf&#39;</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>lambda_</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X_test</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span></span></span></code></pre></div><h3 id=64-其他可核化算法>6.4 其他可核化算法<a class=anchor href=#64-%e5%85%b6%e4%bb%96%e5%8f%af%e6%a0%b8%e5%8c%96%e7%ae%97%e6%b3%95>#</a></h3><table><thead><tr><th>算法</th><th>核化版本</th><th>应用</th></tr></thead><tbody><tr><td>线性回归</td><td>核回归</td><td>非线性回归</td></tr><tr><td>岭回归</td><td>核岭回归</td><td>高维非线性回归</td></tr><tr><td>逻辑回归</td><td>核逻辑回归</td><td>非线性分类</td></tr><tr><td>PCA</td><td>核 PCA (KPCA)</td><td>非线性降维</td></tr><tr><td>LDA</td><td>核 LDA</td><td>非线性判别分析</td></tr><tr><td>感知机</td><td>核感知机</td><td>在线学习</td></tr><tr><td>K-means</td><td>核 K-means</td><td>非线性聚类</td></tr></tbody></table><p><strong>核心思想一致</strong>：表示定理 + 核技巧 + 对偶优化。</p><hr><h2 id=7-总结与展望>7. 总结与展望<a class=anchor href=#7-%e6%80%bb%e7%bb%93%e4%b8%8e%e5%b1%95%e6%9c%9b>#</a></h2><h3 id=71-核方法的本质>7.1 核方法的本质<a class=anchor href=#71-%e6%a0%b8%e6%96%b9%e6%b3%95%e7%9a%84%e6%9c%ac%e8%b4%a8>#</a></h3><p>核方法是一种**&ldquo;计算的艺术&rdquo;**：</p><ol><li><strong>升维的智慧</strong>：将非线性问题转化为高维空间的线性问题</li><li><strong>计算的技巧</strong>：通过核技巧避免显式计算高维特征</li><li><strong>数学的优雅</strong>：Mercer 定理保证了核函数的合法性</li></ol><p><strong>核心等式</strong>：</p><p>$$
\boxed{K(x, z) = \langle \phi(x), \phi(z) \rangle}
$$</p><p>左边是 $O(n)$ 的计算，右边可能是 $O(\infty)$ 的内积。</p><h3 id=72-三大支柱定理>7.2 三大支柱定理<a class=anchor href=#72-%e4%b8%89%e5%a4%a7%e6%94%af%e6%9f%b1%e5%ae%9a%e7%90%86>#</a></h3><p>核方法的数学基础由三大定理构成，它们共同回答了"为什么核技巧可行"：</p><ol><li><p><strong>Cover 定理</strong>：高维空间更可能线性可分</p><ul><li>回答：<strong>为什么要升维？</strong></li><li>本质：维度的诅咒反转为祝福</li></ul></li><li><p><strong>Mercer 定理</strong>：核函数 $\Leftrightarrow$ 半正定核矩阵 $\Leftrightarrow$ 存在特征映射 $\phi$</p><ul><li>回答：<strong>什么样的函数可以当核？</strong></li><li>本质：特征分解逆向构造隐藏空间</li></ul></li><li><p><strong>表示定理</strong>：最优解在数据子空间中，即 $w^* = \sum_i \alpha_i \phi(x_i)$</p><ul><li>回答：<strong>为什么只需要核矩阵就够了？</strong></li><li>本质：正则化保证解的有限性</li></ul></li></ol><p><strong>三者协同</strong>：Cover 定理驱动升维，Mercer 定理保证合法性，表示定理实现核技巧。</p><h3 id=73-核方法的优势>7.3 核方法的优势<a class=anchor href=#73-%e6%a0%b8%e6%96%b9%e6%b3%95%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h3><ol><li><strong>处理非线性</strong>：无需手工设计特征</li><li><strong>计算高效</strong>：避免维度爆炸</li><li><strong>理论坚实</strong>：有完整的数学基础</li><li><strong>应用广泛</strong>：可核化几乎所有线性算法</li></ol><h3 id=74-核方法的局限>7.4 核方法的局限<a class=anchor href=#74-%e6%a0%b8%e6%96%b9%e6%b3%95%e7%9a%84%e5%b1%80%e9%99%90>#</a></h3><ol><li><strong>核矩阵存储</strong>：$O(m^2)$ 空间（大数据困难）</li><li><strong>核矩阵求逆</strong>：$O(m^3)$ 时间复杂度</li><li><strong>核函数选择</strong>：需要领域知识或交叉验证</li><li><strong>可解释性差</strong>：无穷维特征难以可视化</li></ol><h3 id=75-实践建议>7.5 实践建议<a class=anchor href=#75-%e5%ae%9e%e8%b7%b5%e5%bb%ba%e8%ae%ae>#</a></h3><p><strong>核函数选择指南</strong>：</p><table><thead><tr><th>情况</th><th>推荐核</th><th>原因</th></tr></thead><tbody><tr><td>数据线性可分</td><td>线性核</td><td>最简单，不易过拟合</td></tr><tr><td>特征有明确意义</td><td>多项式核</td><td>可解释性好</td></tr><tr><td>不清楚数据结构</td><td>RBF 核</td><td>通用，几乎总是好的起点</td></tr><tr><td>文本、序列数据</td><td>字符串核</td><td>专门设计</td></tr></tbody></table><p><strong>参数调优</strong>：</p><ul><li>RBF 核：$\gamma \in [10^{-4}, 10^{1}]$（对数尺度）</li><li>正则化：$\lambda \in [10^{-5}, 10^{2}]$</li><li>使用交叉验证</li></ul><h3 id=76-现代发展>7.6 现代发展<a class=anchor href=#76-%e7%8e%b0%e4%bb%a3%e5%8f%91%e5%b1%95>#</a></h3><p><strong>大规模核方法</strong>：</p><ul><li><strong>随机特征</strong> (Random Features)：用随机投影近似核函数</li><li><strong>Nyström 方法</strong>：用子采样近似核矩阵</li><li><strong>FastFood</strong>：快速近似 RBF 核</li></ul><p><strong>深度学习时代的核方法</strong>：</p><ul><li><strong>神经切线核</strong> (Neural Tangent Kernel)：无限宽神经网络等价于核方法</li><li><strong>深度核学习</strong>：用神经网络学习核函数</li><li><strong>核与注意力机制</strong>：Transformer 可以看作核方法的变体</li></ul><h3 id=77-与其他方法的关系>7.7 与其他方法的关系<a class=anchor href=#77-%e4%b8%8e%e5%85%b6%e4%bb%96%e6%96%b9%e6%b3%95%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h3><p><strong>核方法 vs 神经网络</strong>：</p><table><thead><tr><th>方面</th><th>核方法</th><th>神经网络</th></tr></thead><tbody><tr><td>非线性建模</td><td>隐式（核技巧）</td><td>显式（层级组合）</td></tr><tr><td>训练</td><td>凸优化</td><td>非凸优化</td></tr><tr><td>可扩展性</td><td>受限于 $O(m^2)$</td><td>可处理大数据</td></tr><tr><td>可解释性</td><td>差</td><td>更差（但有进步）</td></tr><tr><td>特征工程</td><td>核函数选择</td><td>自动学习</td></tr></tbody></table><h3 id=78-最后的思考>7.8 最后的思考<a class=anchor href=#78-%e6%9c%80%e5%90%8e%e7%9a%84%e6%80%9d%e8%80%83>#</a></h3><p><strong>核方法的哲学</strong>：</p><blockquote class=book-hint><p>&ldquo;当问题在低维空间难以解决时，升维；
当计算在高维空间难以承受时，核化。&rdquo;</p></blockquote><p>这不仅仅是一个技巧，而是一种<strong>思维方式</strong>：</p><ul><li>通过改变问题的表示空间来简化问题</li><li>通过数学变换来降低计算复杂度</li><li>在表达能力和计算效率之间找到平衡</li></ul><p>核方法提醒我们：机器学习的核心不是暴力计算，而是<strong>巧妙的数学洞察</strong>。</p><hr><p><strong>思考题</strong>：</p><ol><li><p>为什么 $K(x, z) = -|x - z|^2$ 不是合法核函数？</p></li><li><p>证明：如果 $K_1, K_2$ 是核，则 $K(x,z) = K_1(x,z) + K_2(x,z)$ 也是核。</p></li><li><p>RBF 核的 $\gamma \to 0$ 和 $\gamma \to \infty$ 分别对应什么情况？</p></li><li><p>设计一个"字符串核"：给定两个字符串，如何度量它们的相似度？</p></li><li><p>为什么说神经网络可以看作"可学习的核方法"？</p></li></ol><hr><p><strong>延伸阅读</strong>：</p><ul><li>Schölkopf & Smola (2002), <em>Learning with Kernels</em></li><li>Shawe-Taylor & Cristianini (2004), <em>Kernel Methods for Pattern Analysis</em></li><li>Rahimi & Recht (2007), <em>Random Features for Large-Scale Kernel Machines</em></li><li>Jacot et al. (2018), <em>Neural Tangent Kernel</em></li></ul></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第07章 支持向量机(SVM)</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ class="flex align-center"><span>第09章 决策树与集成学习</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-直觉维度打击>1. 直觉：维度打击</a><ul><li><a href=#11-xor-问题二维空间的绝望>1.1 XOR 问题：二维空间的绝望</a></li><li><a href=#12-升维的魔法三维空间的救赎>1.2 升维的魔法：三维空间的救赎</a></li><li><a href=#13-推导为什么升维有效>1.3 推导：为什么升维有效？</a></li></ul></li><li><a href=#2-代价维度的诅咒>2. 代价：维度的诅咒</a><ul><li><a href=#21-多项式特征的维度爆炸>2.1 多项式特征的维度爆炸</a></li><li><a href=#22-复杂度爆炸实例>2.2 复杂度爆炸实例</a></li><li><a href=#23-无穷维的梦魇>2.3 无穷维的梦魇</a></li></ul></li><li><a href=#3-救赎核技巧-kernel-trick>3. 救赎：核技巧 (Kernel Trick)</a><ul><li><a href=#31-核心观察我们不需要-phix>3.1 核心观察：我们不需要 $\phi(x)$</a></li><li><a href=#32-核函数的定义>3.2 核函数的定义</a></li><li><a href=#33-核心推导多项式核>3.3 核心推导：多项式核</a></li><li><a href=#34-一般多项式核>3.4 一般多项式核</a></li><li><a href=#35-核技巧的威力总结>3.5 核技巧的威力总结</a></li></ul></li><li><a href=#4-mercer-定理什么样的函数能当核>4. Mercer 定理：什么样的函数能当核？</a><ul><li><a href=#41-问题的提出>4.1 问题的提出</a></li><li><a href=#42-核矩阵-gram-matrix>4.2 核矩阵 (Gram Matrix)</a></li><li><a href=#43-mercer-定理标准形式>4.3 Mercer 定理（标准形式）</a></li><li><a href=#44-定理的证明充分性>4.4 定理的证明（充分性）</a></li><li><a href=#45-定理的证明必要性>4.5 定理的证明（必要性）</a></li><li><a href=#46-核函数的组合性质>4.6 核函数的组合性质</a></li></ul></li><li><a href=#5-rbf-核通往无穷维>5. RBF 核：通往无穷维</a><ul><li><a href=#51-径向基函数核的定义>5.1 径向基函数核的定义</a></li><li><a href=#52-rbf-核对应无穷维特征空间>5.2 RBF 核对应无穷维特征空间</a></li><li><a href=#53-多维推广>5.3 多维推广</a></li><li><a href=#54-rbf-核的威力与风险>5.4 RBF 核的威力与风险</a></li><li><a href=#55-常见核函数总结>5.5 常见核函数总结</a></li></ul></li><li><a href=#6-应用核化一切>6. 应用：核化一切</a><ul><li><a href=#61-表示定理-representer-theorem>6.1 表示定理 (Representer Theorem)</a></li><li><a href=#62-核化的一般步骤>6.2 核化的一般步骤</a></li><li><a href=#63-案例核岭回归-kernel-ridge-regression>6.3 案例：核岭回归 (Kernel Ridge Regression)</a><ul><li><a href=#631-原始岭回归>6.3.1 原始岭回归</a></li><li><a href=#632-核化推导>6.3.2 核化推导</a></li><li><a href=#633-预测>6.3.3 预测</a></li><li><a href=#634-算法总结>6.3.4 算法总结</a></li><li><a href=#635-线性岭回归-vs-核岭回归完整对比>6.3.5 线性岭回归 vs 核岭回归：完整对比</a></li><li><a href=#636-python-实现示例>6.3.6 Python 实现示例</a></li></ul></li><li><a href=#64-其他可核化算法>6.4 其他可核化算法</a></li></ul></li><li><a href=#7-总结与展望>7. 总结与展望</a><ul><li><a href=#71-核方法的本质>7.1 核方法的本质</a></li><li><a href=#72-三大支柱定理>7.2 三大支柱定理</a></li><li><a href=#73-核方法的优势>7.3 核方法的优势</a></li><li><a href=#74-核方法的局限>7.4 核方法的局限</a></li><li><a href=#75-实践建议>7.5 实践建议</a></li><li><a href=#76-现代发展>7.6 现代发展</a></li><li><a href=#77-与其他方法的关系>7.7 与其他方法的关系</a></li><li><a href=#78-最后的思考>7.8 最后的思考</a></li></ul></li></ul></nav></div></aside></main></body></html>