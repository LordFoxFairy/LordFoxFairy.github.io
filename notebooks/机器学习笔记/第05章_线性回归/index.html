<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第05章 线性回归# “Simplicity is the ultimate sophistication.” —— Leonardo da Vinci
重要提示：线性回归是机器学习的 “Hello World”，但请不要轻视它。
它是寻找真理的第一步。当我们试图用一条直线去拟合混沌的世界时，我们在坚持一种古老的信仰：世界在本质上是简单的。
本章将带你经历一次认知的跃迁：从几何的投影(最小二乘)，到概率的似然(高斯噪声)，再到信念的约束(贝叶斯正则化)。当你发现这三种截然不同的视角最终指向同一个公式时，你将领悟到数学那令人战栗的统一之美。这不仅仅是推导公式，这是在触摸统计学习的灵魂。
目录# 引言 最小二乘法(Least Squares Estimation, LSE) 2.1 目标函数 2.2 几何视角:投影 2.3 正规方程的物理意义 2.4 解析解 概率视角:最大似然估计(MLE) 3.1 概率模型 3.2 似然函数 3.3 MLE ⟺ LSE 正则化(Regularization) 4.1 问题的提出 4.2 Ridge 回归(L2 正则化) 4.3 Lasso 回归(L1 正则化) 贝叶斯视角:最大后验估计(MAP) 5.1 先验分布 5.2 Ridge = 高斯先验 5.3 Lasso = 拉普拉斯先验 5.4 为什么拉普拉斯先验导致稀疏性? 总结 1. 引言# 回归问题的目标是预测连续值。给定训练数据 ${(\mathbf{x}i, y_i)}{i=1}^N$，其中 $\mathbf{x}_i \in \mathbb{R}^p$ 是特征向量，$y_i \in \mathbb{R}$ 是标签，我们希望学习一个函数 $f: \mathbb{R}^p \to \mathbb{R}$，使得对新的输入 $\mathbf{x}$，能够准确预测 $y = f(\mathbf{x})$。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第05章 线性回归"><meta property="og:description" content="第05章 线性回归# “Simplicity is the ultimate sophistication.” —— Leonardo da Vinci
重要提示：线性回归是机器学习的 “Hello World”，但请不要轻视它。
它是寻找真理的第一步。当我们试图用一条直线去拟合混沌的世界时，我们在坚持一种古老的信仰：世界在本质上是简单的。
本章将带你经历一次认知的跃迁：从几何的投影(最小二乘)，到概率的似然(高斯噪声)，再到信念的约束(贝叶斯正则化)。当你发现这三种截然不同的视角最终指向同一个公式时，你将领悟到数学那令人战栗的统一之美。这不仅仅是推导公式，这是在触摸统计学习的灵魂。
目录# 引言 最小二乘法(Least Squares Estimation, LSE) 2.1 目标函数 2.2 几何视角:投影 2.3 正规方程的物理意义 2.4 解析解 概率视角:最大似然估计(MLE) 3.1 概率模型 3.2 似然函数 3.3 MLE ⟺ LSE 正则化(Regularization) 4.1 问题的提出 4.2 Ridge 回归(L2 正则化) 4.3 Lasso 回归(L1 正则化) 贝叶斯视角:最大后验估计(MAP) 5.1 先验分布 5.2 Ridge = 高斯先验 5.3 Lasso = 拉普拉斯先验 5.4 为什么拉普拉斯先验导致稀疏性? 总结 1. 引言# 回归问题的目标是预测连续值。给定训练数据 ${(\mathbf{x}i, y_i)}{i=1}^N$，其中 $\mathbf{x}_i \in \mathbb{R}^p$ 是特征向量，$y_i \in \mathbb{R}$ 是标签，我们希望学习一个函数 $f: \mathbb{R}^p \to \mathbb{R}$，使得对新的输入 $\mathbf{x}$，能够准确预测 $y = f(\mathbf{x})$。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第05章 线性回归"><meta itemprop=description content="第05章 线性回归# “Simplicity is the ultimate sophistication.” —— Leonardo da Vinci
重要提示：线性回归是机器学习的 “Hello World”，但请不要轻视它。
它是寻找真理的第一步。当我们试图用一条直线去拟合混沌的世界时，我们在坚持一种古老的信仰：世界在本质上是简单的。
本章将带你经历一次认知的跃迁：从几何的投影(最小二乘)，到概率的似然(高斯噪声)，再到信念的约束(贝叶斯正则化)。当你发现这三种截然不同的视角最终指向同一个公式时，你将领悟到数学那令人战栗的统一之美。这不仅仅是推导公式，这是在触摸统计学习的灵魂。
目录# 引言 最小二乘法(Least Squares Estimation, LSE) 2.1 目标函数 2.2 几何视角:投影 2.3 正规方程的物理意义 2.4 解析解 概率视角:最大似然估计(MLE) 3.1 概率模型 3.2 似然函数 3.3 MLE ⟺ LSE 正则化(Regularization) 4.1 问题的提出 4.2 Ridge 回归(L2 正则化) 4.3 Lasso 回归(L1 正则化) 贝叶斯视角:最大后验估计(MAP) 5.1 先验分布 5.2 Ridge = 高斯先验 5.3 Lasso = 拉普拉斯先验 5.4 为什么拉普拉斯先验导致稀疏性? 总结 1. 引言# 回归问题的目标是预测连续值。给定训练数据 ${(\mathbf{x}i, y_i)}{i=1}^N$，其中 $\mathbf{x}_i \in \mathbb{R}^p$ 是特征向量，$y_i \in \mathbb{R}$ 是标签，我们希望学习一个函数 $f: \mathbb{R}^p \to \mathbb{R}$，使得对新的输入 $\mathbf{x}$，能够准确预测 $y = f(\mathbf{x})$。"><meta itemprop=wordCount content="958"><title>第05章 线性回归 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/ class=active>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第05章 线性回归</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-最小二乘法least-squares-estimation-lse>2. 最小二乘法(Least Squares Estimation, LSE)</a><ul><li><a href=#21-目标函数>2.1 目标函数</a></li><li><a href=#22-几何视角投影>2.2 几何视角:投影</a></li><li><a href=#23-正规方程的物理意义>2.3 正规方程的物理意义</a></li><li><a href=#24-解析解>2.4 解析解</a></li></ul></li><li><a href=#3-概率视角最大似然估计mle>3. 概率视角:最大似然估计(MLE)</a><ul><li><a href=#31-概率模型>3.1 概率模型</a></li><li><a href=#32-似然函数>3.2 似然函数</a></li><li><a href=#33-mle--lse>3.3 MLE ⟺ LSE</a></li></ul></li><li><a href=#4-正则化regularization>4. 正则化(Regularization)</a><ul><li><a href=#41-问题的提出>4.1 问题的提出</a></li><li><a href=#42-ridge-回归l2-正则化>4.2 Ridge 回归(L2 正则化)</a><ul><li><a href=#解析解>解析解</a></li><li><a href=#几何直觉>几何直觉</a></li></ul></li><li><a href=#43-lasso-回归l1-正则化>4.3 Lasso 回归(L1 正则化)</a><ul><li><a href=#稀疏性sparsity>稀疏性(Sparsity)</a></li><li><a href=#几何解释为什么-l1-产生稀疏性>几何解释:为什么 L1 产生稀疏性？</a></li></ul></li></ul></li><li><a href=#5-贝叶斯视角最大后验估计map>5. 贝叶斯视角:最大后验估计(MAP)</a><ul><li><a href=#51-先验分布>5.1 先验分布</a></li><li><a href=#52-ridge--高斯先验>5.2 Ridge = 高斯先验</a></li><li><a href=#53-lasso--拉普拉斯先验>5.3 Lasso = 拉普拉斯先验</a></li><li><a href=#54-为什么拉普拉斯先验导致稀疏性>5.4 为什么拉普拉斯先验导致稀疏性？</a></li></ul></li><li><a href=#6-总结>6. 总结</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第05章-线性回归>第05章 线性回归<a class=anchor href=#%e7%ac%ac05%e7%ab%a0-%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92>#</a></h1><blockquote class=book-hint><p>&ldquo;Simplicity is the ultimate sophistication.&rdquo; —— Leonardo da Vinci</p><p><strong>重要提示</strong>：线性回归是机器学习的 &ldquo;Hello World&rdquo;，但请不要轻视它。</p><p>它是寻找真理的第一步。当我们试图用一条直线去拟合混沌的世界时，我们在坚持一种古老的信仰：<strong>世界在本质上是简单的</strong>。</p><p>本章将带你经历一次认知的跃迁：从<strong>几何的投影</strong>(最小二乘)，到<strong>概率的似然</strong>(高斯噪声)，再到<strong>信念的约束</strong>(贝叶斯正则化)。当你发现这三种截然不同的视角最终指向同一个公式时，你将领悟到数学那令人战栗的统一之美。这不仅仅是推导公式，这是在触摸统计学习的灵魂。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><a href=#1-%e5%bc%95%e8%a8%80>引言</a></li><li><a href=#2-%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95least-squares-estimation-lse>最小二乘法(Least Squares Estimation, LSE)</a><ul><li>2.1 <a href=#21-%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0>目标函数</a></li><li>2.2 <a href=#22-%e5%87%a0%e4%bd%95%e8%a7%86%e8%a7%92%e6%8a%95%e5%bd%b1>几何视角:投影</a></li><li>2.3 <a href=#23-%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89>正规方程的物理意义</a></li><li>2.4 <a href=#24-%e8%a7%a3%e6%9e%90%e8%a7%a3>解析解</a></li></ul></li><li><a href=#3-%e6%a6%82%e7%8e%87%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1mle>概率视角:最大似然估计(MLE)</a><ul><li>3.1 <a href=#31-%e6%a6%82%e7%8e%87%e6%a8%a1%e5%9e%8b>概率模型</a></li><li>3.2 <a href=#32-%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0>似然函数</a></li><li>3.3 <a href=#33-mle--lse>MLE ⟺ LSE</a></li></ul></li><li><a href=#4-%e6%ad%a3%e5%88%99%e5%8c%96regularization>正则化(Regularization)</a><ul><li>4.1 <a href=#41-%e9%97%ae%e9%a2%98%e7%9a%84%e6%8f%90%e5%87%ba>问题的提出</a></li><li>4.2 <a href=#42-ridge-%e5%9b%9e%e5%bd%92l2-%e6%ad%a3%e5%88%99%e5%8c%96>Ridge 回归(L2 正则化)</a></li><li>4.3 <a href=#43-lasso-%e5%9b%9e%e5%bd%92l1-%e6%ad%a3%e5%88%99%e5%8c%96>Lasso 回归(L1 正则化)</a></li></ul></li><li><a href=#5-%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e5%90%8e%e9%aa%8c%e4%bc%b0%e8%ae%a1map>贝叶斯视角:最大后验估计(MAP)</a><ul><li>5.1 <a href=#51-%e5%85%88%e9%aa%8c%e5%88%86%e5%b8%83>先验分布</a></li><li>5.2 <a href=#52-ridge--%e9%ab%98%e6%96%af%e5%85%88%e9%aa%8c>Ridge = 高斯先验</a></li><li>5.3 <a href=#53-lasso--%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c>Lasso = 拉普拉斯先验</a></li><li>5.4 <a href=#54-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c%e5%af%bc%e8%87%b4%e7%a8%80%e7%96%8f%e6%80%a7>为什么拉普拉斯先验导致稀疏性?</a></li></ul></li><li><a href=#6-%e6%80%bb%e7%bb%93>总结</a></li></ol><hr><h2 id=1-引言>1. 引言<a class=anchor href=#1-%e5%bc%95%e8%a8%80>#</a></h2><p><strong>回归问题</strong>的目标是预测<strong>连续值</strong>。给定训练数据 ${(\mathbf{x}<em>i, y_i)}</em>{i=1}^N$，其中 $\mathbf{x}_i \in \mathbb{R}^p$ 是特征向量，$y_i \in \mathbb{R}$ 是标签，我们希望学习一个函数 $f: \mathbb{R}^p \to \mathbb{R}$，使得对新的输入 $\mathbf{x}$，能够准确预测 $y = f(\mathbf{x})$。</p><p><strong>与第4章的连接</strong>：在第4章我们学习了指数族分布与广义线性模型(GLM)的统一框架。线性回归是GLM最基础的特例——它对应于<strong>高斯分布</strong>的指数族形式。本章将从三个视角(几何、概率、贝叶斯)深入理解线性回归，你将看到第4章的抽象理论如何在最简单的模型中得到具体体现。</p><p><strong>线性回归</strong>假设这个函数是线性的：</p><p>$$
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b
$$</p><p>为了简化记号，我们通常在 $\mathbf{x}$ 中加入常数项 1（即 $\mathbf{x} \leftarrow [1, \mathbf{x}]^T$），将偏置 $b$ 吸收到权重 $\mathbf{w}$ 中，于是模型简化为：</p><p>$$
f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}
$$</p><p>用矩阵形式，设计矩阵 $X \in \mathbb{R}^{N \times p}$ 的第 $i$ 行是 $\mathbf{x}_i^T$，标签向量 $\mathbf{y} = [y_1, \ldots, y_N]^T$，则预测值为：</p><p>$$
\hat{\mathbf{y}} = X\mathbf{w}
$$</p><p>我们的任务是：<strong>如何确定最优的 $\mathbf{w}$？</strong></p><hr><h2 id=2-最小二乘法least-squares-estimation-lse>2. 最小二乘法(Least Squares Estimation, LSE)<a class=anchor href=#2-%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95least-squares-estimation-lse>#</a></h2><h3 id=21-目标函数>2.1 目标函数<a class=anchor href=#21-%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0>#</a></h3><p>最直观的想法是让预测值 $\hat{\mathbf{y}}$ 与真实值 $\mathbf{y}$ 尽可能接近，即最小化<strong>残差平方和</strong>：</p><p>$$
L(\mathbf{w}) = |\mathbf{y} - X\mathbf{w}|^2 = \sum_{i=1}^N (y_i - \mathbf{w}^T \mathbf{x}_i)^2
$$</p><p>这就是<strong>最小二乘法(Ordinary Least Squares, OLS)</strong>。</p><h3 id=22-几何视角投影>2.2 几何视角:投影<a class=anchor href=#22-%e5%87%a0%e4%bd%95%e8%a7%86%e8%a7%92%e6%8a%95%e5%bd%b1>#</a></h3><p>这里有个深刻的几何直觉，<strong>甚至不需要求导</strong>！</p><p>把 $X$ 看成一个 $N \times p$ 的矩阵，它的列向量张成 $\mathbb{R}^N$ 中的一个 $p$ 维子空间，称为 $X$ 的<strong>列空间</strong> $\text{Col}(X)$。</p><ul><li>$X\mathbf{w}$ 是列空间中的任意向量(线性组合)。</li><li>$\mathbf{y}$ 是 $\mathbb{R}^N$ 中的一个向量，通常不在列空间中。</li></ul><p>最小化 $|\mathbf{y} - X\mathbf{w}|^2$ 等价于：<strong>在列空间中找到离 $\mathbf{y}$ 最近的点</strong>。</p><p>根据投影定理，这个最近点就是 $\mathbf{y}$ 在列空间上的<strong>正交投影</strong> $\hat{\mathbf{y}} = X\mathbf{w}^*$。投影的几何特征是：</p><p>$$
\boxed{\mathbf{y} - X\mathbf{w}^* \perp \text{Col}(X)}
$$</p><p>即<strong>残差向量 $\mathbf{y} - X\mathbf{w}^*$ 与 $X$ 的每一列都正交</strong>：</p><p>$$
X^T (\mathbf{y} - X\mathbf{w}^*) = \mathbf{0}
$$</p><p>展开得到：</p><p>$$
\boxed{X^T X \mathbf{w}^* = X^T \mathbf{y}}
$$</p><p>这就是<strong>正规方程(Normal Equation)</strong>。</p><h3 id=23-正规方程的物理意义>2.3 正规方程的物理意义<a class=anchor href=#23-%e6%ad%a3%e8%a7%84%e6%96%b9%e7%a8%8b%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89>#</a></h3><p>正规方程 $X^T X \mathbf{w} = X^T \mathbf{y}$ 的几何含义非常优雅：</p><ul><li><strong>左边</strong> $X^T X \mathbf{w}$：残差 $X\mathbf{w} - \mathbf{y}$ 与 $X$ 的每一列的内积。</li><li><strong>右边</strong> $X^T \mathbf{y}$：标签 $\mathbf{y}$ 与 $X$ 的每一列的内积。</li></ul><p>方程的意义是：<strong>残差必须与设计矩阵 $X$ 的列空间正交</strong>，即 $X^T(X\mathbf{w} - \mathbf{y}) = 0$。</p><p>这不是求导的结果，而是<strong>投影的几何本质</strong>！</p><h3 id=24-解析解>2.4 解析解<a class=anchor href=#24-%e8%a7%a3%e6%9e%90%e8%a7%a3>#</a></h3><p>若 $X^T X$ 可逆(即 $X$ 列满秩)，则：</p><p>$$
\boxed{\mathbf{w}^* = (X^T X)^{-1} X^T \mathbf{y}}
$$</p><p>$(X^T X)^{-1} X^T$ 称为 $X$ 的<strong>伪逆(Moore-Penrose pseudoinverse)</strong>，记作 $X^+$。</p><hr><h2 id=3-概率视角最大似然估计mle>3. 概率视角:最大似然估计(MLE)<a class=anchor href=#3-%e6%a6%82%e7%8e%87%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1mle>#</a></h2><h3 id=31-概率模型>3.1 概率模型<a class=anchor href=#31-%e6%a6%82%e7%8e%87%e6%a8%a1%e5%9e%8b>#</a></h3><p>从另一个角度看，我们可以假设数据是由以下过程生成的：</p><p>$$
y_i = \mathbf{w}^T \mathbf{x}_i + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$</p><p>即<strong>真实标签 = 线性模型 + 高斯噪声</strong>。</p><p>因此，给定 $\mathbf{x}_i$ 和 $\mathbf{w}$，$y_i$ 的条件分布为：</p><p>$$
p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \mathcal{N}(y_i \mid \mathbf{w}^T \mathbf{x}_i, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T \mathbf{x}_i)^2}{2\sigma^2}\right)
$$</p><h3 id=32-似然函数>3.2 似然函数<a class=anchor href=#32-%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0>#</a></h3><p>假设数据点之间独立同分布(i.i.d.)，则似然函数为：</p><p>$$
p(\mathbf{y} \mid X, \mathbf{w}) = \prod_{i=1}^N p(y_i \mid \mathbf{x}<em>i, \mathbf{w}) = \prod</em>{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^T \mathbf{x}_i)^2}{2\sigma^2}\right)
$$</p><p>取对数(对数似然)：</p><p>$$
\log p(\mathbf{y} \mid X, \mathbf{w}) = -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - \mathbf{w}^T \mathbf{x}_i)^2
$$</p><h3 id=33-mle--lse>3.3 MLE ⟺ LSE<a class=anchor href=#33-mle--lse>#</a></h3><p><strong>最大化对数似然</strong>等价于<strong>最小化</strong>：</p><p>$$
\sum_{i=1}^N (y_i - \mathbf{w}^T \mathbf{x}_i)^2 = |\mathbf{y} - X\mathbf{w}|^2
$$</p><p>这正是最小二乘法的目标函数！</p><p><strong>结论</strong>：在高斯噪声假设下，<strong>最大似然估计(MLE)等价于最小二乘法(LSE)</strong>。</p><p>这揭示了一个深刻的联系：看似无关的几何投影和概率推断，<strong>殊途同归</strong>。</p><p><strong>与GLM的联系</strong>：线性回归是GLM的特例(第4章 §4.5)。在指数族框架下：</p><ul><li>高斯分布：$\eta = \mathbf{w}^T \mathbf{x}$，$A(\eta) = \frac{\sigma^2 \eta^2}{2}$，$\mu = \nabla_\eta A(\eta) = \eta = \mathbf{w}^T \mathbf{x}$</li><li>GLM统一梯度公式：$\nabla_\mathbf{w} \mathcal{L} = -\sum_{i=1}^N (y_i - \mu_i) \mathbf{x}_i$</li></ul><p>对于线性回归，$\mu_i = \mathbf{w}^T \mathbf{x}_i$，因此梯度恰好是 $-\sum (y_i - \mathbf{w}^T \mathbf{x}_i) \mathbf{x}_i$——这正是我们从几何和概率两个角度推导出的结果！GLM框架揭示了这个公式背后的深层结构。</p><hr><h2 id=4-正则化regularization>4. 正则化(Regularization)<a class=anchor href=#4-%e6%ad%a3%e5%88%99%e5%8c%96regularization>#</a></h2><h3 id=41-问题的提出>4.1 问题的提出<a class=anchor href=#41-%e9%97%ae%e9%a2%98%e7%9a%84%e6%8f%90%e5%87%ba>#</a></h3><p>在实际应用中，最小二乘法存在两个主要问题：</p><ol><li><strong>过拟合(Overfitting)</strong>：当特征数 $p$ 很大或样本数 $N$ 很小时，模型容易拟合噪声，泛化能力差。</li><li><strong>病态矩阵(Ill-conditioned Matrix)</strong>：当 $X$ 的列之间高度相关(多重共线性)时，$X^T X$ 接近奇异，求逆数值不稳定，甚至不可逆。</li></ol><p><strong>解决方案</strong>：在目标函数中加入<strong>正则化项</strong>，约束 $\mathbf{w}$ 的大小。</p><h3 id=42-ridge-回归l2-正则化>4.2 Ridge 回归(L2 正则化)<a class=anchor href=#42-ridge-%e5%9b%9e%e5%bd%92l2-%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h3><p>Ridge 回归在损失函数中加入 $\mathbf{w}$ 的 <strong>L2 范数</strong>：</p><p>$$
L_{\text{Ridge}}(\mathbf{w}) = |\mathbf{y} - X\mathbf{w}|^2 + \lambda |\mathbf{w}|^2
$$</p><p>其中 $\lambda > 0$ 是正则化参数，控制惩罚强度。</p><h4 id=解析解>解析解<a class=anchor href=#%e8%a7%a3%e6%9e%90%e8%a7%a3>#</a></h4><p>对 $\mathbf{w}$ 求导并令其为零：</p><p>$$
\frac{\partial L_{\text{Ridge}}}{\partial \mathbf{w}} = -2X^T(\mathbf{y} - X\mathbf{w}) + 2\lambda \mathbf{w} = 0
$$</p><p>整理得：</p><p>$$
(X^T X + \lambda I) \mathbf{w} = X^T \mathbf{y}
$$</p><p>解为：</p><p>$$
\boxed{\mathbf{w}_{\text{Ridge}} = (X^T X + \lambda I)^{-1} X^T \mathbf{y}}
$$</p><h4 id=几何直觉>几何直觉<a class=anchor href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>#</a></h4><ul><li><strong>给对角线加 $\lambda I$</strong>：即使 $X^T X$ 奇异，$X^T X + \lambda I$ 也是满秩的，保证可逆性。</li><li><strong>约束 $|\mathbf{w}|^2$</strong>：限制权重的长度，防止某些权重过大导致模型对噪声敏感。</li></ul><p>Ridge 回归等价于约束优化问题：</p><p>$$
\min_{\mathbf{w}} |\mathbf{y} - X\mathbf{w}|^2 \quad \text{s.t.} \quad |\mathbf{w}|^2 \leq t
$$</p><p>几何上，$|\mathbf{w}|^2 \leq t$ 是一个<strong>超球</strong>，优化过程是在球内寻找最优点。</p><h3 id=43-lasso-回归l1-正则化>4.3 Lasso 回归(L1 正则化)<a class=anchor href=#43-lasso-%e5%9b%9e%e5%bd%92l1-%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h3><p>Lasso 回归使用 <strong>L1 范数</strong>：</p><p>$$
L_{\text{Lasso}}(\mathbf{w}) = |\mathbf{y} - X\mathbf{w}|^2 + \lambda |\mathbf{w}|_1
$$</p><p>其中 $|\mathbf{w}|<em>1 = \sum</em>{j=1}^p |w_j|$。</p><h4 id=稀疏性sparsity>稀疏性(Sparsity)<a class=anchor href=#%e7%a8%80%e7%96%8f%e6%80%a7sparsity>#</a></h4><p>Lasso 的一个重要特性是<strong>产生稀疏解</strong>，即许多 $w_j$ 会被压缩到<strong>恰好为 0</strong>。这使得 Lasso 天然具备<strong>特征选择</strong>的能力。</p><h4 id=几何解释为什么-l1-产生稀疏性>几何解释:为什么 L1 产生稀疏性？<a class=anchor href=#%e5%87%a0%e4%bd%95%e8%a7%a3%e9%87%8a%e4%b8%ba%e4%bb%80%e4%b9%88-l1-%e4%ba%a7%e7%94%9f%e7%a8%80%e7%96%8f%e6%80%a7>#</a></h4><p>等价的约束优化形式：</p><p>$$
\min_{\mathbf{w}} |\mathbf{y} - X\mathbf{w}|^2 \quad \text{s.t.} \quad |\mathbf{w}|_1 \leq t
$$</p><p>在二维情况下($\mathbf{w} = [w_1, w_2]^T$)，我们可以用图形直观理解：</p><p><img src=assets/l1_l2_constraint.svg alt="L1 vs L2 Constraint"></p><p><strong>关键观察</strong>：</p><ul><li><strong>L2(圆形)</strong>：等高线与圆的切点通常不在坐标轴上，解是"收缩"但不为零。</li><li><strong>L1(菱形)</strong>：菱形的"尖角"在坐标轴上(如 $(t, 0)$ 或 $(0, t)$)，等高线与菱形的<strong>第一次接触往往发生在顶点</strong>，导致某个 $w_j = 0$。</li></ul><p>这就是 <strong>L1 正则化产生稀疏性的几何原因</strong>：菱形的尖锐顶点使得优化倾向于让某些权重恰好为 0。</p><hr><h2 id=5-贝叶斯视角最大后验估计map>5. 贝叶斯视角:最大后验估计(MAP)<a class=anchor href=#5-%e8%b4%9d%e5%8f%b6%e6%96%af%e8%a7%86%e8%a7%92%e6%9c%80%e5%a4%a7%e5%90%8e%e9%aa%8c%e4%bc%b0%e8%ae%a1map>#</a></h2><h3 id=51-先验分布>5.1 先验分布<a class=anchor href=#51-%e5%85%88%e9%aa%8c%e5%88%86%e5%b8%83>#</a></h3><p>在贝叶斯框架中，我们对参数 $\mathbf{w}$ 引入<strong>先验分布</strong> $p(\mathbf{w})$，然后结合似然 $p(\mathbf{y} \mid X, \mathbf{w})$ 计算<strong>后验分布</strong>：</p><p>$$
p(\mathbf{w} \mid X, \mathbf{y}) \propto p(\mathbf{y} \mid X, \mathbf{w}) \cdot p(\mathbf{w})
$$</p><p><strong>最大后验估计(MAP)</strong> 就是找使后验概率最大的 $\mathbf{w}$：</p><p>$$
\mathbf{w}<em>{\text{MAP}} = \arg\max</em>{\mathbf{w}} \log p(\mathbf{w} \mid X, \mathbf{y}) = \arg\max_{\mathbf{w}} \left[ \log p(\mathbf{y} \mid X, \mathbf{w}) + \log p(\mathbf{w}) \right]
$$</p><h3 id=52-ridge--高斯先验>5.2 Ridge = 高斯先验<a class=anchor href=#52-ridge--%e9%ab%98%e6%96%af%e5%85%88%e9%aa%8c>#</a></h3><p>假设 $\mathbf{w}$ 的先验是<strong>零均值高斯分布</strong>：</p><p>$$
p(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \tau^2 I) \propto \exp\left(-\frac{|\mathbf{w}|^2}{2\tau^2}\right)
$$</p><p>则对数先验为：</p><p>$$
\log p(\mathbf{w}) = -\frac{|\mathbf{w}|^2}{2\tau^2} + \text{const}
$$</p><p>结合高斯似然：</p><p>$$
\log p(\mathbf{y} \mid X, \mathbf{w}) = -\frac{|\mathbf{y} - X\mathbf{w}|^2}{2\sigma^2} + \text{const}
$$</p><p>MAP 目标函数为：</p><p>$$
\mathbf{w}<em>{\text{MAP}} = \arg\min</em>{\mathbf{w}} \left[ |\mathbf{y} - X\mathbf{w}|^2 + \frac{\sigma^2}{\tau^2} |\mathbf{w}|^2 \right]
$$</p><p>令 $\lambda = \frac{\sigma^2}{\tau^2}$，这正是 <strong>Ridge 回归</strong>！</p><p><strong>结论</strong>：<strong>Ridge 回归 = 高斯先验下的 MAP 估计</strong>。</p><h3 id=53-lasso--拉普拉斯先验>5.3 Lasso = 拉普拉斯先验<a class=anchor href=#53-lasso--%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c>#</a></h3><p>假设 $\mathbf{w}$ 的先验是<strong>拉普拉斯分布</strong>：</p><p>$$
p(\mathbf{w}) = \prod_{j=1}^p \frac{1}{2b} \exp\left(-\frac{|w_j|}{b}\right) \propto \exp\left(-\frac{|\mathbf{w}|_1}{b}\right)
$$</p><p>则对数先验为：</p><p>$$
\log p(\mathbf{w}) = -\frac{|\mathbf{w}|_1}{b} + \text{const}
$$</p><p>MAP 目标函数为：</p><p>$$
\mathbf{w}<em>{\text{MAP}} = \arg\min</em>{\mathbf{w}} \left[ |\mathbf{y} - X\mathbf{w}|^2 + \frac{\sigma^2}{b} |\mathbf{w}|_1 \right]
$$</p><p>这正是 <strong>Lasso 回归</strong>！</p><p><strong>结论</strong>：<strong>Lasso 回归 = 拉普拉斯先验下的 MAP 估计</strong>。</p><h3 id=54-为什么拉普拉斯先验导致稀疏性>5.4 为什么拉普拉斯先验导致稀疏性？<a class=anchor href=#54-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c%e5%af%bc%e8%87%b4%e7%a8%80%e7%96%8f%e6%80%a7>#</a></h3><p>拉普拉斯分布的概率密度函数在 $w_j = 0$ 处有一个<strong>尖峰</strong>：</p><p><img src=assets/laplace_gaussian_prior.svg alt="Laplace vs Gaussian Prior"></p><p>拉普拉斯先验在原点不可导，这种"尖锐性"在优化时会<strong>促使参数恰好为 0</strong>，而不仅仅是接近 0。</p><p>这与 L1 范数的菱形约束区域的几何解释一致：<strong>拉普拉斯先验的尖峰 = L1 约束的尖角</strong>，两者都导致<strong>稀疏解</strong>。</p><hr><h2 id=6-总结>6. 总结<a class=anchor href=#6-%e6%80%bb%e7%bb%93>#</a></h2><p>我们从三个视角深入理解了线性回归：</p><table><thead><tr><th>视角</th><th>方法</th><th>核心思想</th></tr></thead><tbody><tr><td><strong>几何</strong></td><td>最小二乘法(LSE)</td><td>$\mathbf{y}$ 在 $X$ 列空间的正交投影，残差与列空间正交</td></tr><tr><td><strong>频率</strong></td><td>最大似然估计(MLE)</td><td>高斯噪声假设下，MLE ⟺ LSE</td></tr><tr><td><strong>贝叶斯</strong></td><td>最大后验估计(MAP)</td><td>高斯先验 $\rightarrow$ Ridge，拉普拉斯先验 $\rightarrow$ Lasso</td></tr></tbody></table><p><strong>正则化的双重解释</strong>：</p><table><thead><tr><th>方法</th><th>优化视角</th><th>概率视角</th><th>几何约束</th><th>稀疏性</th></tr></thead><tbody><tr><td><strong>Ridge</strong></td><td>$\min |\mathbf{y} - X\mathbf{w}|^2 + \lambda |\mathbf{w}|^2$</td><td>高斯先验 $\mathcal{N}(0, \tau^2 I)$</td><td>圆形 $|\mathbf{w}|^2 \leq t$</td><td>无</td></tr><tr><td><strong>Lasso</strong></td><td>$\min |\mathbf{y} - X\mathbf{w}|^2 + \lambda |\mathbf{w}|_1$</td><td>拉普拉斯先验 $\text{Laplace}(0, b)$</td><td>菱形 $|\mathbf{w}|_1 \leq t$</td><td>有</td></tr></tbody></table><p><strong>关键洞察</strong>：</p><ol><li><strong>正规方程的几何意义</strong>：$X^T(X\mathbf{w} - \mathbf{y}) = 0$ 表示残差与列空间正交，这是投影的本质，无需求导。</li><li><strong>MLE = LSE</strong>：在高斯噪声假设下，概率推断与几何投影殊途同归。</li><li><strong>L1 的稀疏性</strong>：菱形约束区域的尖角(优化视角)与拉普拉斯先验的尖峰(概率视角)共同导致权重恰好为 0。</li></ol><p>线性回归不仅是机器学习的基石，更是理解优化、几何、概率之间深刻联系的绝佳范例。</p><hr><h2 id=参考文献>参考文献<a class=anchor href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae>#</a></h2><ol><li><strong>Bishop, C. M.</strong> (2006). <em>Pattern Recognition and Machine Learning</em>. Springer, Chapter 3.</li><li><strong>Murphy, K. P.</strong> (2022). <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press, Chapter 11.</li><li><strong>Hastie, T., Tibshirani, R., & Friedman, J.</strong> (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer, Chapter 3.</li><li><strong>Tibshirani, R.</strong> (1996). Regression Shrinkage and Selection via the Lasso. <em>Journal of the Royal Statistical Society: Series B</em>, 58(1), 267-288.</li><li><strong>Hoerl, A. E., & Kennard, R. W.</strong> (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. <em>Technometrics</em>, 12(1), 55-67.</li></ol></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第04章 概率分布 指数族与共轭先验</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/ class="flex align-center"><span>第06章 感知机</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-最小二乘法least-squares-estimation-lse>2. 最小二乘法(Least Squares Estimation, LSE)</a><ul><li><a href=#21-目标函数>2.1 目标函数</a></li><li><a href=#22-几何视角投影>2.2 几何视角:投影</a></li><li><a href=#23-正规方程的物理意义>2.3 正规方程的物理意义</a></li><li><a href=#24-解析解>2.4 解析解</a></li></ul></li><li><a href=#3-概率视角最大似然估计mle>3. 概率视角:最大似然估计(MLE)</a><ul><li><a href=#31-概率模型>3.1 概率模型</a></li><li><a href=#32-似然函数>3.2 似然函数</a></li><li><a href=#33-mle--lse>3.3 MLE ⟺ LSE</a></li></ul></li><li><a href=#4-正则化regularization>4. 正则化(Regularization)</a><ul><li><a href=#41-问题的提出>4.1 问题的提出</a></li><li><a href=#42-ridge-回归l2-正则化>4.2 Ridge 回归(L2 正则化)</a><ul><li><a href=#解析解>解析解</a></li><li><a href=#几何直觉>几何直觉</a></li></ul></li><li><a href=#43-lasso-回归l1-正则化>4.3 Lasso 回归(L1 正则化)</a><ul><li><a href=#稀疏性sparsity>稀疏性(Sparsity)</a></li><li><a href=#几何解释为什么-l1-产生稀疏性>几何解释:为什么 L1 产生稀疏性？</a></li></ul></li></ul></li><li><a href=#5-贝叶斯视角最大后验估计map>5. 贝叶斯视角:最大后验估计(MAP)</a><ul><li><a href=#51-先验分布>5.1 先验分布</a></li><li><a href=#52-ridge--高斯先验>5.2 Ridge = 高斯先验</a></li><li><a href=#53-lasso--拉普拉斯先验>5.3 Lasso = 拉普拉斯先验</a></li><li><a href=#54-为什么拉普拉斯先验导致稀疏性>5.4 为什么拉普拉斯先验导致稀疏性？</a></li></ul></li><li><a href=#6-总结>6. 总结</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></aside></main></body></html>