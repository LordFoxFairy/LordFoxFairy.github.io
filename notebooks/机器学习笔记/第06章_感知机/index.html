<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第06章 感知机 (Perceptron)# “The perceptron has probably given rise to more hope, and more disappointment, than any other idea in AI.” —— Marvin Minsky
重要提示:这不仅仅是一章关于分类器的笔记,这是人类试图用数学模拟大脑的第一次史诗般的尝试。
感知机是现代深度学习的线粒体。虽然它结构简单,但它蕴含了神经网络最核心的灵魂——通过误差修正自我。
本章我们将见证两个极端:一个是数学上的奇迹——Novikoff 定理证明了只要真理(线性可分)存在,感知机就一定能找到它;另一个是历史的悲剧——Minsky 如何用一个简单的 XOR 问题,将 AI 推入了长达二十年的寒冬。这是一个关于希望、幻灭与重生的故事。
目录# 定义 几何直观 超平面的性质 损失函数推导 3.1 从 0-1 Loss 开始 3.2 过渡到距离损失 3.3 感知机损失函数 随机梯度下降 4.1 梯度计算 4.2 更新规则 4.3 几何直觉 感知机算法 Novikoff 收敛定理 6.1 定理陈述 6.2 定理的意义 6.3 证明思路 (可选) 感知机 vs SVM XOR 问题与 AI 的寒冬 8.1 XOR 的反例 8.2 历史的教训 总结 附录: 对偶形式 (Dual Form) 1. 定义# 感知机是二分类的线性判别模型:
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第06章 感知机"><meta property="og:description" content="第06章 感知机 (Perceptron)# “The perceptron has probably given rise to more hope, and more disappointment, than any other idea in AI.” —— Marvin Minsky
重要提示:这不仅仅是一章关于分类器的笔记,这是人类试图用数学模拟大脑的第一次史诗般的尝试。
感知机是现代深度学习的线粒体。虽然它结构简单,但它蕴含了神经网络最核心的灵魂——通过误差修正自我。
本章我们将见证两个极端:一个是数学上的奇迹——Novikoff 定理证明了只要真理(线性可分)存在,感知机就一定能找到它;另一个是历史的悲剧——Minsky 如何用一个简单的 XOR 问题,将 AI 推入了长达二十年的寒冬。这是一个关于希望、幻灭与重生的故事。
目录# 定义 几何直观 超平面的性质 损失函数推导 3.1 从 0-1 Loss 开始 3.2 过渡到距离损失 3.3 感知机损失函数 随机梯度下降 4.1 梯度计算 4.2 更新规则 4.3 几何直觉 感知机算法 Novikoff 收敛定理 6.1 定理陈述 6.2 定理的意义 6.3 证明思路 (可选) 感知机 vs SVM XOR 问题与 AI 的寒冬 8.1 XOR 的反例 8.2 历史的教训 总结 附录: 对偶形式 (Dual Form) 1. 定义# 感知机是二分类的线性判别模型:"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第06章 感知机"><meta itemprop=description content="第06章 感知机 (Perceptron)# “The perceptron has probably given rise to more hope, and more disappointment, than any other idea in AI.” —— Marvin Minsky
重要提示:这不仅仅是一章关于分类器的笔记,这是人类试图用数学模拟大脑的第一次史诗般的尝试。
感知机是现代深度学习的线粒体。虽然它结构简单,但它蕴含了神经网络最核心的灵魂——通过误差修正自我。
本章我们将见证两个极端:一个是数学上的奇迹——Novikoff 定理证明了只要真理(线性可分)存在,感知机就一定能找到它;另一个是历史的悲剧——Minsky 如何用一个简单的 XOR 问题,将 AI 推入了长达二十年的寒冬。这是一个关于希望、幻灭与重生的故事。
目录# 定义 几何直观 超平面的性质 损失函数推导 3.1 从 0-1 Loss 开始 3.2 过渡到距离损失 3.3 感知机损失函数 随机梯度下降 4.1 梯度计算 4.2 更新规则 4.3 几何直觉 感知机算法 Novikoff 收敛定理 6.1 定理陈述 6.2 定理的意义 6.3 证明思路 (可选) 感知机 vs SVM XOR 问题与 AI 的寒冬 8.1 XOR 的反例 8.2 历史的教训 总结 附录: 对偶形式 (Dual Form) 1. 定义# 感知机是二分类的线性判别模型:"><meta itemprop=wordCount content="807"><title>第06章 感知机 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/ class=active>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第06章 感知机</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-定义>1. 定义</a></li><li><a href=#2-几何直观>2. 几何直观</a><ul><li><a href=#超平面的性质>超平面的性质</a></li></ul></li><li><a href=#3-损失函数推导>3. 损失函数推导</a><ul><li><a href=#31-从-0-1-loss-开始>3.1 从 0-1 Loss 开始</a></li><li><a href=#32-过渡到距离损失>3.2 过渡到距离损失</a></li><li><a href=#33-感知机损失函数>3.3 感知机损失函数</a></li></ul></li><li><a href=#4-随机梯度下降>4. 随机梯度下降</a><ul><li><a href=#41-梯度计算>4.1 梯度计算</a></li><li><a href=#42-更新规则>4.2 更新规则</a></li><li><a href=#43-几何直觉>4.3 几何直觉</a></li></ul></li><li><a href=#5-感知机算法>5. 感知机算法</a></li><li><a href=#6-novikoff-收敛定理>6. Novikoff 收敛定理</a><ul><li><a href=#61-定理陈述>6.1 定理陈述</a></li><li><a href=#62-定理的意义>6.2 定理的意义</a></li><li><a href=#63-证明思路-可选>6.3 证明思路 (可选)</a></li></ul></li><li><a href=#7-感知机-vs-svm>7. 感知机 vs SVM</a></li><li><a href=#8-xor-问题与-ai-的寒冬>8. XOR 问题与 AI 的寒冬</a><ul><li><a href=#81-xor-的反例>8.1 XOR 的反例</a></li><li><a href=#82-历史的教训>8.2 历史的教训</a></li></ul></li><li><a href=#9-总结>9. 总结</a></li><li><a href=#附录-对偶形式-dual-form>附录: 对偶形式 (Dual Form)</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第06章-感知机-perceptron>第06章 感知机 (Perceptron)<a class=anchor href=#%e7%ac%ac06%e7%ab%a0-%e6%84%9f%e7%9f%a5%e6%9c%ba-perceptron>#</a></h1><blockquote class=book-hint><p>&ldquo;The perceptron has probably given rise to more hope, and more disappointment, than any other idea in AI.&rdquo; —— Marvin Minsky</p></blockquote><p><strong>重要提示</strong>:这不仅仅是一章关于分类器的笔记,这是人类试图用数学模拟大脑的第一次<strong>史诗般的尝试</strong>。</p><p>感知机是现代深度学习的<strong>线粒体</strong>。虽然它结构简单,但它蕴含了神经网络最核心的灵魂——<strong>通过误差修正自我</strong>。</p><p>本章我们将见证两个极端:一个是数学上的<strong>奇迹</strong>——Novikoff 定理证明了只要真理(线性可分)存在,感知机就一定能找到它;另一个是历史的<strong>悲剧</strong>——Minsky 如何用一个简单的 XOR 问题,将 AI 推入了长达二十年的寒冬。这是一个关于希望、幻灭与重生的故事。</p><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ol><li><a href=#1-%e5%ae%9a%e4%b9%89>定义</a></li><li><a href=#2-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82>几何直观</a><ul><li>超平面的性质</li></ul></li><li><a href=#3-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc>损失函数推导</a><ul><li>3.1 <a href=#31-%e4%bb%8e-0-1-loss-%e5%bc%80%e5%a7%8b>从 0-1 Loss 开始</a></li><li>3.2 <a href=#32-%e8%bf%87%e6%b8%a1%e5%88%b0%e8%b7%9d%e7%a6%bb%e6%8d%9f%e5%a4%b1>过渡到距离损失</a></li><li>3.3 <a href=#33-%e6%84%9f%e7%9f%a5%e6%9c%ba%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>感知机损失函数</a></li></ul></li><li><a href=#4-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d>随机梯度下降</a><ul><li>4.1 <a href=#41-%e6%a2%af%e5%ba%a6%e8%ae%a1%e7%ae%97>梯度计算</a></li><li>4.2 <a href=#42-%e6%9b%b4%e6%96%b0%e8%a7%84%e5%88%99>更新规则</a></li><li>4.3 <a href=#43-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>几何直觉</a></li></ul></li><li><a href=#5-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%ae%97%e6%b3%95>感知机算法</a></li><li><a href=#6-novikoff-%e6%94%b6%e6%95%9b%e5%ae%9a%e7%90%86>Novikoff 收敛定理</a><ul><li>6.1 <a href=#61-%e5%ae%9a%e7%90%86%e9%99%88%e8%bf%b0>定理陈述</a></li><li>6.2 <a href=#62-%e5%ae%9a%e7%90%86%e7%9a%84%e6%84%8f%e4%b9%89>定理的意义</a></li><li>6.3 <a href=#63-%e8%af%81%e6%98%8e%e6%80%9d%e8%b7%af-%e5%8f%af%e9%80%89>证明思路 (可选)</a></li></ul></li><li><a href=#7-%e6%84%9f%e7%9f%a5%e6%9c%ba-vs-svm>感知机 vs SVM</a></li><li><a href=#8-xor-%e9%97%ae%e9%a2%98%e4%b8%8e-ai-%e7%9a%84%e5%af%92%e5%86%ac>XOR 问题与 AI 的寒冬</a><ul><li>8.1 <a href=#81-xor-%e7%9a%84%e5%8f%8d%e4%be%8b>XOR 的反例</a></li><li>8.2 <a href=#82-%e5%8e%86%e5%8f%b2%e7%9a%84%e6%95%99%e8%ae%ad>历史的教训</a></li></ul></li><li><a href=#9-%e6%80%bb%e7%bb%93>总结</a></li><li><a href=#%e9%99%84%e5%bd%95-%e5%af%b9%e5%81%b6%e5%bd%a2%e5%bc%8f-dual-form>附录: 对偶形式 (Dual Form)</a></li></ol><hr><h2 id=1-定义>1. 定义<a class=anchor href=#1-%e5%ae%9a%e4%b9%89>#</a></h2><p>感知机是二分类的线性判别模型:</p><p>$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)
$$</p><p>其中:</p><ul><li>$\mathbf{w} \in \mathbb{R}^d$ 是权重向量</li><li>$b \in \mathbb{R}$ 是偏置</li><li>$\text{sign}(z) = \begin{cases} +1, & z \geq 0 \ -1, & z &lt; 0 \end{cases}$</li></ul><p><strong>本质</strong>: 用超平面 $\mathbf{w}^T \mathbf{x} + b = 0$ 将空间一分为二。</p><hr><h2 id=2-几何直观>2. 几何直观<a class=anchor href=#2-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%82>#</a></h2><h3 id=超平面的性质>超平面的性质<a class=anchor href=#%e8%b6%85%e5%b9%b3%e9%9d%a2%e7%9a%84%e6%80%a7%e8%b4%a8>#</a></h3><p><img src=assets/perceptron_geometry.svg alt=超平面几何示意图></p><p><strong>图示说明</strong>：</p><p>这幅图展示了感知机的几何本质——用一个超平面 $\mathbf{w}^T \mathbf{x} + b = 0$ (黑色实线) 将空间一分为二:</p><ul><li><strong>淡蓝色区域</strong>: 满足 $\mathbf{w}^T \mathbf{x} + b > 0$ 的正类区域，绿色点散布其中</li><li><strong>淡红色区域</strong>: 满足 $\mathbf{w}^T \mathbf{x} + b &lt; 0$ 的负类区域，红色点散布其中</li></ul><p><strong>法向量 $\mathbf{w}$ 是关键</strong>：</p><ul><li>$\mathbf{w}$ 垂直于超平面，像一个"指南针"，永远指向正类区域</li><li><strong>它指向哪里，哪里就是正类</strong> (淡蓝色区域)</li><li>它背对的方向就是负类 (淡红色区域)</li></ul><p>关键几何事实:</p><ul><li><strong>法向量</strong>: $\mathbf{w}$ 不仅指示方向，还决定了超平面的法线朝向</li><li><strong>决策</strong>: $\mathbf{w}^T \mathbf{x} + b$ 的符号告诉我们点在哪一侧——正值在蓝色区域 (+1)，负值在红色区域 (-1)</li><li><strong>分类</strong>: 本质上是在计算点相对于超平面的"方位"</li></ul><p><strong>点到超平面的距离</strong>:</p><p>$$
d = \frac{|\mathbf{w}^T \mathbf{x} + b|}{|\mathbf{w}|}
$$</p><hr><h2 id=3-损失函数推导>3. 损失函数推导<a class=anchor href=#3-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e6%8e%a8%e5%af%bc>#</a></h2><h3 id=31-从-0-1-loss-开始>3.1 从 0-1 Loss 开始<a class=anchor href=#31-%e4%bb%8e-0-1-loss-%e5%bc%80%e5%a7%8b>#</a></h3><p>最自然的想法: 最小化误分类点数</p><p>$$
L_\text{0-1}(\mathbf{w}, b) = \sum_{i=1}^N \mathbb{I}{y_i (\mathbf{w}^T \mathbf{x}_i + b) \leq 0}
$$</p><p><strong>问题</strong>: 不连续、不可微、难优化。</p><h3 id=32-过渡到距离损失>3.2 过渡到距离损失<a class=anchor href=#32-%e8%bf%87%e6%b8%a1%e5%88%b0%e8%b7%9d%e7%a6%bb%e6%8d%9f%e5%a4%b1>#</a></h3><p><strong>观察</strong>: 误分类点 $(\mathbf{x}_i, y_i)$ 满足</p><p>$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) &lt; 0
$$</p><p>这意味着:</p><ul><li>如果 $y_i = +1$, 则 $\mathbf{w}^T \mathbf{x}_i + b &lt; 0$ (点在错误一侧)</li><li>如果 $y_i = -1$, 则 $\mathbf{w}^T \mathbf{x}_i + b > 0$ (点在错误一侧)</li></ul><p><strong>几何意义</strong>: 点 $\mathbf{x}_i$ 到超平面的距离是</p><p>$$
d_i = \frac{|\mathbf{w}^T \mathbf{x}_i + b|}{|\mathbf{w}|}
$$</p><p>对于<strong>误分类点</strong>, $y_i (\mathbf{w}^T \mathbf{x}_i + b) &lt; 0$, 所以</p><p>$$
-y_i (\mathbf{w}^T \mathbf{x}_i + b) = |\mathbf{w}^T \mathbf{x}_i + b| > 0
$$</p><p>这个量<strong>总是正的</strong>, 且恰好正比于点到超平面的距离!</p><h3 id=33-感知机损失函数>3.3 感知机损失函数<a class=anchor href=#33-%e6%84%9f%e7%9f%a5%e6%9c%ba%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>#</a></h3><p>$$
L(\mathbf{w}, b) = -\sum_{\mathbf{x}_i \in M} y_i (\mathbf{w}^T \mathbf{x}_i + b)
$$</p><p>其中 $M$ 是误分类点集合。</p><p><strong>理解</strong>:</p><ul><li>忽略 $|\mathbf{w}|$ (不影响优化方向, 可吸收进学习率)</li><li>只对误分类点计算损失</li><li>这是 <strong>Hinge Loss 的特例</strong> (SVM 会见到完整版)</li></ul><hr><h2 id=4-随机梯度下降>4. 随机梯度下降<a class=anchor href=#4-%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d>#</a></h2><h3 id=41-梯度计算>4.1 梯度计算<a class=anchor href=#41-%e6%a2%af%e5%ba%a6%e8%ae%a1%e7%ae%97>#</a></h3><p>对单个误分类点 $(\mathbf{x}_i, y_i)$:</p><p>$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{w}} &= -y_i \mathbf{x}_i \
\frac{\partial L}{\partial b} &= -y_i
\end{aligned}
$$</p><h3 id=42-更新规则>4.2 更新规则<a class=anchor href=#42-%e6%9b%b4%e6%96%b0%e8%a7%84%e5%88%99>#</a></h3><p>$$
\begin{aligned}
\mathbf{w} &\leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i \
b &\leftarrow b + \eta y_i
\end{aligned}
$$</p><p>其中 $\eta > 0$ 是学习率。</p><h3 id=43-几何直觉>4.3 几何直觉<a class=anchor href=#43-%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>#</a></h3><p><strong>关键理解</strong>: 被误分类的样本在"纠正"超平面的方向。</p><p>假设当前点 $(\mathbf{x}_i, y_i)$ 被误分类:</p><ul><li><p>如果 $y_i = +1$, 意味着 $\mathbf{w}^T \mathbf{x}_i + b &lt; 0$ (点在负半空间)</p><ul><li>更新: $\mathbf{w} \leftarrow \mathbf{w} + \eta \mathbf{x}_i$ (让 $\mathbf{w}$ 向 $\mathbf{x}_i$ 靠近)</li><li>效果: 使得 $\mathbf{w}^T \mathbf{x}_i$ 增大, 更可能分对</li></ul></li><li><p>如果 $y_i = -1$, 意味着 $\mathbf{w}^T \mathbf{x}_i + b > 0$ (点在正半空间)</p><ul><li>更新: $\mathbf{w} \leftarrow \mathbf{w} - \eta \mathbf{x}_i$ (让 $\mathbf{w}$ 远离 $\mathbf{x}_i$)</li><li>效果: 使得 $\mathbf{w}^T \mathbf{x}_i$ 减小, 更可能分对</li></ul></li></ul><p><strong>形象比喻</strong>: 每个误分类点都在"拉扯"超平面, 试图让它转向正确的位置。</p><p><img src=assets/perceptron_update.svg alt=感知机更新过程></p><p><strong>图示说明</strong>：</p><p>这幅图讲述了一个三幕剧——感知机如何从错误中学习。</p><p><strong>第一幕: The Mistake (犯错现场)</strong></p><p>红色的点 $x$ 原本是正类 ($y=+1$)，却被灰色的超平面挡在了下方。问题出在哪里？注意看 $w_{\text{old}}$ 与 $x$ 的夹角——是钝角！它们几乎反向。这就是错误的根源：法向量指错了方向。</p><p><strong>第二幕: The Correction (强制纠正)</strong></p><p>关键动作来了：黄色的更新向量 $\eta x$ 像一只强有力的手，抓住 $w_{\text{old}}$，往 $x$ 的方向狠狠推了一把。这不是温柔的调整，而是物理上的强行拉扯。这就是更新公式 $w_{\text{new}} = w_{\text{old}} + \eta x$ 的物理图像——向量相加，力的合成。</p><p><strong>第三幕: The Result (完美收场)</strong></p><p>绿色的新法向量 $w_{\text{new}}$ 诞生了！它已经转向 $x$ 的方向。随之而来的是绿色的新超平面，像一道旋转的门，把 $x$ 从下方"推"到了上方。现在，$x$ 终于位于超平面的正确一侧——分类成功！</p><p><strong>核心直觉</strong>: 每个被误分类的点都在用自己的位置向量"纠正"法向量的方向，就像一群舵手在调整船帆，直到所有人都满意为止。</p><hr><h2 id=5-感知机算法>5. 感知机算法<a class=anchor href=#5-%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%ae%97%e6%b3%95>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: 训练集 D = {(x_1, y_1), ..., (x_N, y_N)}, 学习率 η
</span></span><span class=line><span class=cl>输出: w, b
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1. 初始化 w = 0, b = 0
</span></span><span class=line><span class=cl>2. repeat:
</span></span><span class=line><span class=cl>     随机选择一个误分类点 (x_i, y_i), 即满足
</span></span><span class=line><span class=cl>         y_i (w^T x_i + b) ≤ 0
</span></span><span class=line><span class=cl>     更新:
</span></span><span class=line><span class=cl>         w ← w + η y_i x_i
</span></span><span class=line><span class=cl>         b ← b + η y_i
</span></span><span class=line><span class=cl>   until 没有误分类点</span></span></code></pre></div><hr><h2 id=6-novikoff-收敛定理>6. Novikoff 收敛定理<a class=anchor href=#6-novikoff-%e6%94%b6%e6%95%9b%e5%ae%9a%e7%90%86>#</a></h2><h3 id=61-定理陈述>6.1 定理陈述<a class=anchor href=#61-%e5%ae%9a%e7%90%86%e9%99%88%e8%bf%b0>#</a></h3><p><strong>假设</strong>: 数据线性可分, 即存在 $\mathbf{w}^<em>, b^</em>$ 使得</p><p>$$
\frac{y_i (\mathbf{w}^{<em>T} \mathbf{x}_i + b^</em>)}{|\mathbf{w}^*|} \geq \gamma > 0, \quad \forall i
$$</p><p>其中 $\gamma$ 称为<strong>间隔</strong> (margin)。</p><p>再设 $R = \max_i |\mathbf{x}_i|$ (数据的最大范数)。</p><p><strong>结论</strong>: 感知机算法的误分类次数 $k$ 满足</p><p>$$
k \leq \left(\frac{R}{\gamma}\right)^2
$$</p><h3 id=62-定理的意义>6.2 定理的意义<a class=anchor href=#62-%e5%ae%9a%e7%90%86%e7%9a%84%e6%84%8f%e4%b9%89>#</a></h3><p><strong>核心信息</strong>:</p><ol><li><strong>必停性</strong>: 只要数据线性可分, 算法一定收敛</li><li><strong>收敛速度</strong>: 依赖于间隔 $\gamma$ 和数据规模 $R$<ul><li>间隔越大 ($\gamma$ 大), 收敛越快</li><li>数据越集中 ($R$ 小), 收敛越快</li></ul></li><li><strong>无需证明细节</strong>: 关键是理解"间隔"的作用</li></ol><p><strong>直观比喻</strong>：</p><p>想象你在黑暗中摸索一条"能分开两堆石头的线"：</p><ul><li><p><strong>间隔 $\gamma$</strong>：两堆石头之间的"安全距离"</p><ul><li>距离大：随便画一条线就能分开，很容易找到（收敛快）</li><li>距离小：需要精确定位，反复调整（收敛慢）</li></ul></li><li><p><strong>数据规模 $R$</strong>：石头堆的"分散程度"</p><ul><li>石头集中：活动范围小，容易控制（收敛快）</li><li>石头分散：需要考虑更大范围（收敛慢）</li></ul></li></ul><p><strong>数学直觉</strong>：</p><p>上界 $k \leq \left(\frac{R}{\gamma}\right)^2$ 告诉我们：</p><ul><li>最坏情况下的更新次数是 $O(R^2/\gamma^2)$</li><li>这是一个<strong>有限数</strong>！只要 $\gamma > 0$（线性可分），算法必然停止</li><li>SVM 的灵感就来源于此：既然间隔 $\gamma$ 这么重要，为什么不直接<strong>最大化间隔</strong>？</li></ul><h3 id=63-证明思路-可选>6.3 证明思路 (可选)<a class=anchor href=#63-%e8%af%81%e6%98%8e%e6%80%9d%e8%b7%af-%e5%8f%af%e9%80%89>#</a></h3><p>核心技巧: 同时跟踪两个量</p><ul><li>$\mathbf{w}^T \mathbf{w}^*$ 每次更新至少增加 $\gamma$</li><li>$|\mathbf{w}|^2$ 每次更新至多增加 $R^2$</li></ul><p>通过 Cauchy-Schwarz 不等式推出上界。(白板推导时可展开)</p><hr><h2 id=7-感知机-vs-svm>7. 感知机 vs SVM<a class=anchor href=#7-%e6%84%9f%e7%9f%a5%e6%9c%ba-vs-svm>#</a></h2><table><thead><tr><th>维度</th><th>感知机</th><th>SVM</th></tr></thead><tbody><tr><td><strong>目标</strong></td><td>找到任意一个分离超平面</td><td>找到<strong>间隔最大</strong>的分离超平面</td></tr><tr><td><strong>解的唯一性</strong></td><td>不唯一 (依赖初始化和顺序)</td><td>唯一 (凸优化)</td></tr><tr><td><strong>对噪声的鲁棒性</strong></td><td>弱 (离群点会影响解)</td><td>强 (最大间隔 + 软间隔)</td></tr><tr><td><strong>损失函数</strong></td><td>$-y(\mathbf{w}^T \mathbf{x} + b)$ (仅误分类点)</td><td>Hinge Loss (所有点)</td></tr><tr><td><strong>算法</strong></td><td>在线学习 (SGD)</td><td>批量优化 (QP)</td></tr></tbody></table><p><strong>比喻</strong>:</p><ul><li>感知机: &ldquo;及格就行&rdquo; (60分万岁)</li><li>SVM: &ldquo;追求完美&rdquo; (满分+安全距离)</li></ul><hr><h2 id=8-xor-问题与-ai-的寒冬>8. XOR 问题与 AI 的寒冬<a class=anchor href=#8-xor-%e9%97%ae%e9%a2%98%e4%b8%8e-ai-%e7%9a%84%e5%af%92%e5%86%ac>#</a></h2><h3 id=81-xor-的反例>8.1 XOR 的反例<a class=anchor href=#81-xor-%e7%9a%84%e5%8f%8d%e4%be%8b>#</a></h3><p>异或问题:</p><table><thead><tr><th>$x_1$</th><th>$x_2$</th><th>$y$</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>-1</td></tr><tr><td>0</td><td>1</td><td>+1</td></tr><tr><td>1</td><td>0</td><td>+1</td></tr><tr><td>1</td><td>1</td><td>-1</td></tr></tbody></table><p><img src=assets/xor_problem.svg alt=XOR问题示意图></p><p><strong>问题</strong>: 无法用一条直线分开对角的两类点!</p><h3 id=82-历史的教训>8.2 历史的教训<a class=anchor href=#82-%e5%8e%86%e5%8f%b2%e7%9a%84%e6%95%99%e8%ae%ad>#</a></h3><ul><li><strong>1958</strong>: Rosenblatt 提出感知机, AI 界充满希望与期待</li><li><strong>1969</strong>: Minsky & Papert 在著作《Perceptrons》中严格证明:<ul><li>单层感知机无法解决 XOR 等非线性问题</li><li>多层感知机虽理论可行,但当时缺乏有效训练算法 (反向传播尚未出现)</li></ul></li><li><strong>结果</strong>: 这一打击导致 AI 进入第一次寒冬 (1970s-1980s),研究资金骤减</li><li><strong>转折</strong>: 1986年,Rumelhart等人重新发现反向传播算法,证明多层网络可训练</li><li><strong>教训</strong>: 线性模型的局限性催生了非线性方法 (核方法、深度神经网络)</li><li><strong>启示</strong>: Minsky 的批评虽然一度终结了感知机研究,但也为后续的非线性革命埋下了伏笔</li></ul><hr><h2 id=9-总结>9. 总结<a class=anchor href=#9-%e6%80%bb%e7%bb%93>#</a></h2><p><strong>感知机的价值</strong>:</p><ol><li><strong>历史地位</strong>: 最早的学习算法之一, 神经网络的前身</li><li><strong>理论意义</strong>: Novikoff 定理保证收敛性</li><li><strong>实用性</strong>: 在线学习、简单高效</li><li><strong>局限性</strong>: 只能解决线性可分问题</li></ol><p><strong>现代视角</strong>:</p><ul><li>感知机 → 单层神经网络 (激活函数为 sign)</li><li>多层感知机 (MLP) → 深度学习</li><li>SVM 吸收了感知机思想, 加入"最大间隔"原则</li></ul><p><strong>关键思想</strong>:</p><blockquote class=book-hint><p>优化不是最小化误分类数 (不可微),
而是最小化误分类点到超平面的距离和 (可微)。
这种"替代损失"的思想贯穿整个机器学习。</p></blockquote><hr><h2 id=附录-对偶形式-dual-form>附录: 对偶形式 (Dual Form)<a class=anchor href=#%e9%99%84%e5%bd%95-%e5%af%b9%e5%81%b6%e5%bd%a2%e5%bc%8f-dual-form>#</a></h2><p>将更新规则改写:</p><p>$$
\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}<em>i, \quad b = \sum</em>{i=1}^N \alpha_i y_i
$$</p><p>其中 $\alpha_i$ 是样本 $i$ 被误分类的累计次数。</p><p><strong>好处</strong>: 只需计算样本间的内积 $\mathbf{x}_i^T \mathbf{x}_j$ → 核技巧的前奏!</p><p>(这部分可在讲 SVM 时深入展开)</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第05章 线性回归</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/ class="flex align-center"><span>第07章 支持向量机(SVM)</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#1-定义>1. 定义</a></li><li><a href=#2-几何直观>2. 几何直观</a><ul><li><a href=#超平面的性质>超平面的性质</a></li></ul></li><li><a href=#3-损失函数推导>3. 损失函数推导</a><ul><li><a href=#31-从-0-1-loss-开始>3.1 从 0-1 Loss 开始</a></li><li><a href=#32-过渡到距离损失>3.2 过渡到距离损失</a></li><li><a href=#33-感知机损失函数>3.3 感知机损失函数</a></li></ul></li><li><a href=#4-随机梯度下降>4. 随机梯度下降</a><ul><li><a href=#41-梯度计算>4.1 梯度计算</a></li><li><a href=#42-更新规则>4.2 更新规则</a></li><li><a href=#43-几何直觉>4.3 几何直觉</a></li></ul></li><li><a href=#5-感知机算法>5. 感知机算法</a></li><li><a href=#6-novikoff-收敛定理>6. Novikoff 收敛定理</a><ul><li><a href=#61-定理陈述>6.1 定理陈述</a></li><li><a href=#62-定理的意义>6.2 定理的意义</a></li><li><a href=#63-证明思路-可选>6.3 证明思路 (可选)</a></li></ul></li><li><a href=#7-感知机-vs-svm>7. 感知机 vs SVM</a></li><li><a href=#8-xor-问题与-ai-的寒冬>8. XOR 问题与 AI 的寒冬</a><ul><li><a href=#81-xor-的反例>8.1 XOR 的反例</a></li><li><a href=#82-历史的教训>8.2 历史的教训</a></li></ul></li><li><a href=#9-总结>9. 总结</a></li><li><a href=#附录-对偶形式-dual-form>附录: 对偶形式 (Dual Form)</a></li></ul></nav></div></aside></main></body></html>