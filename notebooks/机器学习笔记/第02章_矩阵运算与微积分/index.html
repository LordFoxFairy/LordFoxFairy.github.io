<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第02章：矩阵运算与微积分# 前言
线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于你真正需要的那部分：
向量空间：理解数据的结构和维度 投影：理解线性回归的几何本质 矩阵微积分：理解梯度下降和反向传播 我们的目标是几何直觉 + 计算技巧。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。
目录# 2.1 基础数据结构 标量、向量、矩阵与张量 核心运算 转置、逆矩阵与伪逆 迹与行列式 2.2 向量空间：理解数据的结构 线性组合与张成空间 线性相关与线性无关 基与维度 秩：矩阵的本质维度 秩-零化度定理 四个基本子空间 2.3 度量与正交 范数：测量大小 内积与角度 正交与正交矩阵 2.4 投影：线性回归的几何本质 为什么我们需要投影 从最小二乘推导投影矩阵 投影矩阵的性质 几何直觉 2.5 矩阵微积分：反向传播的数学基础 为什么需要矩阵求导 布局约定：分母布局 标量对向量求导 标量对矩阵求导 向量对向量求导：雅可比矩阵 链式法则 重要公式推导 实战：线性回归的梯度 2.1 基础数据结构# 标量、向量、矩阵与张量# 数学对象的定义由其维度决定：
标量 (Scalar)：$x \in \mathbb{R}$。单个数值，如温度、距离。
向量 (Vector)：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。本书默认向量为列向量。
例如，$\mathbf{x} = \begin{bmatrix} 2 \ 3 \end{bmatrix}$ 表示2D平面上的一个点。
矩阵 (Matrix)：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的线性变换。
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第02章 矩阵运算与微积分"><meta property="og:description" content="第02章：矩阵运算与微积分# 前言
线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于你真正需要的那部分：
向量空间：理解数据的结构和维度 投影：理解线性回归的几何本质 矩阵微积分：理解梯度下降和反向传播 我们的目标是几何直觉 + 计算技巧。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。
目录# 2.1 基础数据结构 标量、向量、矩阵与张量 核心运算 转置、逆矩阵与伪逆 迹与行列式 2.2 向量空间：理解数据的结构 线性组合与张成空间 线性相关与线性无关 基与维度 秩：矩阵的本质维度 秩-零化度定理 四个基本子空间 2.3 度量与正交 范数：测量大小 内积与角度 正交与正交矩阵 2.4 投影：线性回归的几何本质 为什么我们需要投影 从最小二乘推导投影矩阵 投影矩阵的性质 几何直觉 2.5 矩阵微积分：反向传播的数学基础 为什么需要矩阵求导 布局约定：分母布局 标量对向量求导 标量对矩阵求导 向量对向量求导：雅可比矩阵 链式法则 重要公式推导 实战：线性回归的梯度 2.1 基础数据结构# 标量、向量、矩阵与张量# 数学对象的定义由其维度决定：
标量 (Scalar)：$x \in \mathbb{R}$。单个数值，如温度、距离。
向量 (Vector)：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。本书默认向量为列向量。
例如，$\mathbf{x} = \begin{bmatrix} 2 \ 3 \end{bmatrix}$ 表示2D平面上的一个点。
矩阵 (Matrix)：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的线性变换。"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第02章 矩阵运算与微积分"><meta itemprop=description content="第02章：矩阵运算与微积分# 前言
线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于你真正需要的那部分：
向量空间：理解数据的结构和维度 投影：理解线性回归的几何本质 矩阵微积分：理解梯度下降和反向传播 我们的目标是几何直觉 + 计算技巧。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。
目录# 2.1 基础数据结构 标量、向量、矩阵与张量 核心运算 转置、逆矩阵与伪逆 迹与行列式 2.2 向量空间：理解数据的结构 线性组合与张成空间 线性相关与线性无关 基与维度 秩：矩阵的本质维度 秩-零化度定理 四个基本子空间 2.3 度量与正交 范数：测量大小 内积与角度 正交与正交矩阵 2.4 投影：线性回归的几何本质 为什么我们需要投影 从最小二乘推导投影矩阵 投影矩阵的性质 几何直觉 2.5 矩阵微积分：反向传播的数学基础 为什么需要矩阵求导 布局约定：分母布局 标量对向量求导 标量对矩阵求导 向量对向量求导：雅可比矩阵 链式法则 重要公式推导 实战：线性回归的梯度 2.1 基础数据结构# 标量、向量、矩阵与张量# 数学对象的定义由其维度决定：
标量 (Scalar)：$x \in \mathbb{R}$。单个数值，如温度、距离。
向量 (Vector)：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。本书默认向量为列向量。
例如，$\mathbf{x} = \begin{bmatrix} 2 \ 3 \end{bmatrix}$ 表示2D平面上的一个点。
矩阵 (Matrix)：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的线性变换。"><meta itemprop=wordCount content="2179"><title>第02章 矩阵运算与微积分 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/ class=active>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第02章 矩阵运算与微积分</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#21-基础数据结构>2.1 基础数据结构</a><ul><li><a href=#标量向量矩阵与张量>标量、向量、矩阵与张量</a></li><li><a href=#核心运算>核心运算</a><ul><li><a href=#1-矩阵乘法>1. 矩阵乘法</a></li><li><a href=#2-内积点积>2. 内积（点积）</a></li><li><a href=#3-外积>3. 外积</a></li><li><a href=#4-逐元素运算hadamard积>4. 逐元素运算（Hadamard积）</a></li></ul></li><li><a href=#转置逆矩阵与伪逆>转置、逆矩阵与伪逆</a><ul><li><a href=#转置>转置</a></li><li><a href=#逆矩阵>逆矩阵</a></li><li><a href=#伪逆moore-penrose逆>伪逆（Moore-Penrose逆）</a></li></ul></li><li><a href=#迹与行列式>迹与行列式</a><ul><li><a href=#迹>迹</a></li><li><a href=#行列式>行列式</a></li></ul></li></ul></li><li><a href=#22-向量空间理解数据的结构>2.2 向量空间：理解数据的结构</a><ul><li><a href=#线性组合与张成空间>线性组合与张成空间</a></li><li><a href=#线性相关与线性无关>线性相关与线性无关</a></li><li><a href=#基与维度>基与维度</a></li><li><a href=#秩矩阵的本质维度>秩：矩阵的本质维度</a></li><li><a href=#秩-零化度定理>秩-零化度定理</a></li><li><a href=#四个基本子空间>四个基本子空间</a></li></ul></li><li><a href=#23-度量与正交>2.3 度量与正交</a><ul><li><a href=#范数测量大小>范数：测量大小</a><ul><li><a href=#lp-范数>$L^p$ 范数</a></li><li><a href=#矩阵范数>矩阵范数</a></li></ul></li><li><a href=#内积与角度>内积与角度</a></li><li><a href=#正交与正交矩阵>正交与正交矩阵</a></li></ul></li><li><a href=#24-投影线性回归的几何本质>2.4 投影：线性回归的几何本质</a><ul><li><a href=#为什么我们需要投影>为什么我们需要投影</a></li><li><a href=#从最小二乘推导投影矩阵>从最小二乘推导投影矩阵</a></li><li><a href=#投影矩阵的性质>投影矩阵的性质</a></li><li><a href=#几何直觉>几何直觉</a></li></ul></li><li><a href=#25-矩阵微积分反向传播的数学基础>2.5 矩阵微积分：反向传播的数学基础</a><ul><li><a href=#为什么需要矩阵求导>为什么需要矩阵求导</a></li><li><a href=#布局约定分母布局>布局约定：分母布局</a></li><li><a href=#标量对向量求导>标量对向量求导</a></li><li><a href=#标量对矩阵求导>标量对矩阵求导</a></li><li><a href=#向量对向量求导雅可比矩阵>向量对向量求导：雅可比矩阵</a></li><li><a href=#链式法则>链式法则</a></li><li><a href=#重要公式推导>重要公式推导</a><ul><li><a href=#推导1nabla_mathbfx-mathbfxt-mathbfa-mathbfx>推导1：$\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x})$</a></li><li><a href=#推导2fracpartialpartial-mathbfx-texttrmathbfa-mathbfx-mathbfb>推导2：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})$</a></li><li><a href=#推导3fracpartialpartial-mathbfx-texttrmathbfxt-mathbfa-mathbfx>推导3：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$</a></li></ul></li><li><a href=#实战线性回归的梯度>实战：线性回归的梯度</a></li></ul></li><li><a href=#常用矩阵求导公式速查表>常用矩阵求导公式速查表</a></li><li><a href=#小结>小结</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第02章矩阵运算与微积分>第02章：矩阵运算与微积分<a class=anchor href=#%e7%ac%ac02%e7%ab%a0%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97%e4%b8%8e%e5%be%ae%e7%a7%af%e5%88%86>#</a></h1><blockquote class=book-hint><p><strong>前言</strong></p><p>线性代数和微积分是机器学习的数学工具箱。本章不是线性代数的完整教程，而是聚焦于<strong>你真正需要的那部分</strong>：</p><ul><li><strong>向量空间</strong>：理解数据的结构和维度</li><li><strong>投影</strong>：理解线性回归的几何本质</li><li><strong>矩阵微积分</strong>：理解梯度下降和反向传播</li></ul><p>我们的目标是<strong>几何直觉</strong> + <strong>计算技巧</strong>。矩阵分解（特征值、SVD等）虽然重要，但将在第3章详细展开。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#21-%e5%9f%ba%e7%a1%80%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84>2.1 基础数据结构</a><ul><li><a href=#%e6%a0%87%e9%87%8f%e5%90%91%e9%87%8f%e7%9f%a9%e9%98%b5%e4%b8%8e%e5%bc%a0%e9%87%8f>标量、向量、矩阵与张量</a></li><li><a href=#%e6%a0%b8%e5%bf%83%e8%bf%90%e7%ae%97>核心运算</a></li><li><a href=#%e8%bd%ac%e7%bd%ae%e9%80%86%e7%9f%a9%e9%98%b5%e4%b8%8e%e4%bc%aa%e9%80%86>转置、逆矩阵与伪逆</a></li><li><a href=#%e8%bf%b9%e4%b8%8e%e8%a1%8c%e5%88%97%e5%bc%8f>迹与行列式</a></li></ul></li><li><a href=#22-%e5%90%91%e9%87%8f%e7%a9%ba%e9%97%b4%e7%90%86%e8%a7%a3%e6%95%b0%e6%8d%ae%e7%9a%84%e7%bb%93%e6%9e%84>2.2 向量空间：理解数据的结构</a><ul><li><a href=#%e7%ba%bf%e6%80%a7%e7%bb%84%e5%90%88%e4%b8%8e%e5%bc%a0%e6%88%90%e7%a9%ba%e9%97%b4>线性组合与张成空间</a></li><li><a href=#%e7%ba%bf%e6%80%a7%e7%9b%b8%e5%85%b3%e4%b8%8e%e7%ba%bf%e6%80%a7%e6%97%a0%e5%85%b3>线性相关与线性无关</a></li><li><a href=#%e5%9f%ba%e4%b8%8e%e7%bb%b4%e5%ba%a6>基与维度</a></li><li><a href=#%e7%a7%a9%e7%9f%a9%e9%98%b5%e7%9a%84%e6%9c%ac%e8%b4%a8%e7%bb%b4%e5%ba%a6>秩：矩阵的本质维度</a></li><li><a href=#%e7%a7%a9-%e9%9b%b6%e5%8c%96%e5%ba%a6%e5%ae%9a%e7%90%86>秩-零化度定理</a></li><li><a href=#%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4>四个基本子空间</a></li></ul></li><li><a href=#23-%e5%ba%a6%e9%87%8f%e4%b8%8e%e6%ad%a3%e4%ba%a4>2.3 度量与正交</a><ul><li><a href=#%e8%8c%83%e6%95%b0%e6%b5%8b%e9%87%8f%e5%a4%a7%e5%b0%8f>范数：测量大小</a></li><li><a href=#%e5%86%85%e7%a7%af%e4%b8%8e%e8%a7%92%e5%ba%a6>内积与角度</a></li><li><a href=#%e6%ad%a3%e4%ba%a4%e4%b8%8e%e6%ad%a3%e4%ba%a4%e7%9f%a9%e9%98%b5>正交与正交矩阵</a></li></ul></li><li><a href=#24-%e6%8a%95%e5%bd%b1%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e5%87%a0%e4%bd%95%e6%9c%ac%e8%b4%a8>2.4 投影：线性回归的几何本质</a><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e6%8a%95%e5%bd%b1>为什么我们需要投影</a></li><li><a href=#%e4%bb%8e%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%8e%a8%e5%af%bc%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5>从最小二乘推导投影矩阵</a></li><li><a href=#%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5%e7%9a%84%e6%80%a7%e8%b4%a8>投影矩阵的性质</a></li><li><a href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>几何直觉</a></li></ul></li><li><a href=#25-%e7%9f%a9%e9%98%b5%e5%be%ae%e7%a7%af%e5%88%86%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80>2.5 矩阵微积分：反向传播的数学基础</a><ul><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc>为什么需要矩阵求导</a></li><li><a href=#%e5%b8%83%e5%b1%80%e7%ba%a6%e5%ae%9a%e5%88%86%e6%af%8d%e5%b8%83%e5%b1%80>布局约定：分母布局</a></li><li><a href=#%e6%a0%87%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc>标量对向量求导</a></li><li><a href=#%e6%a0%87%e9%87%8f%e5%af%b9%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc>标量对矩阵求导</a></li><li><a href=#%e5%90%91%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5>向量对向量求导：雅可比矩阵</a></li><li><a href=#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99>链式法则</a></li><li><a href=#%e9%87%8d%e8%a6%81%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc>重要公式推导</a></li><li><a href=#%e5%ae%9e%e6%88%98%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e6%a2%af%e5%ba%a6>实战：线性回归的梯度</a></li></ul></li></ul><hr><h2 id=21-基础数据结构>2.1 基础数据结构<a class=anchor href=#21-%e5%9f%ba%e7%a1%80%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84>#</a></h2><h3 id=标量向量矩阵与张量>标量、向量、矩阵与张量<a class=anchor href=#%e6%a0%87%e9%87%8f%e5%90%91%e9%87%8f%e7%9f%a9%e9%98%b5%e4%b8%8e%e5%bc%a0%e9%87%8f>#</a></h3><p>数学对象的定义由其维度决定：</p><ul><li><p><strong>标量 (Scalar)</strong>：$x \in \mathbb{R}$。单个数值，如温度、距离。</p></li><li><p><strong>向量 (Vector)</strong>：$\mathbf{x} \in \mathbb{R}^n$。$n$ 个数的有序排列，代表空间中的一个点或方向。<strong>本书默认向量为列向量</strong>。</p><p>例如，$\mathbf{x} = \begin{bmatrix} 2 \ 3 \end{bmatrix}$ 表示2D平面上的一个点。</p></li><li><p><strong>矩阵 (Matrix)</strong>：$\mathbf{A} \in \mathbb{R}^{m \times n}$。$m$ 行 $n$ 列的二维数组，代表从 $n$ 维空间到 $m$ 维空间的<strong>线性变换</strong>。</p><p>$\mathbf{A}_{ij}$ 表示矩阵第 $i$ 行第 $j$ 列的元素。</p></li><li><p><strong>张量 (Tensor)</strong>：$\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_k}$。多维数组。</p><p>例如，图像是三阶张量 $\mathbb{R}^{H \times W \times C}$（高度、宽度、通道数）。</p></li></ul><p><strong>符号约定</strong>：</p><ul><li>标量：$x, y, \lambda$</li><li>向量：$\mathbf{x}, \mathbf{y}, \mathbf{v}$</li><li>矩阵：$\mathbf{A}, \mathbf{X}, \mathbf{W}$</li><li>张量：$\mathcal{X}, \mathcal{Y}$</li></ul><hr><h3 id=核心运算>核心运算<a class=anchor href=#%e6%a0%b8%e5%bf%83%e8%bf%90%e7%ae%97>#</a></h3><h4 id=1-矩阵乘法>1. 矩阵乘法<a class=anchor href=#1-%e7%9f%a9%e9%98%b5%e4%b9%98%e6%b3%95>#</a></h4><p>对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{B} \in \mathbb{R}^{n \times p}$，其乘积 $\mathbf{C} = \mathbf{A}\mathbf{B} \in \mathbb{R}^{m \times p}$：</p><p>$$
\mathbf{C}<em>{ij} = \sum</em>{k=1}^n \mathbf{A}<em>{ik} \mathbf{B}</em>{kj}
$$</p><p><strong>几何意义</strong>：矩阵乘法 = 线性变换的复合。$\mathbf{AB}\mathbf{x}$ 表示先对 $\mathbf{x}$ 应用 $\mathbf{B}$ 变换，再应用 $\mathbf{A}$ 变换。</p><p><strong>性质</strong>：</p><ul><li>满足结合律：$(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC})$</li><li><strong>不满足交换律</strong>：$\mathbf{AB} \neq \mathbf{BA}$（这很重要！）</li></ul><h4 id=2-内积点积>2. 内积（点积）<a class=anchor href=#2-%e5%86%85%e7%a7%af%e7%82%b9%e7%a7%af>#</a></h4><p>对于向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$：</p><p>$$
\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T \mathbf{y} = \sum_{i=1}^n x_i y_i
$$</p><p><strong>几何意义</strong>：衡量两个向量的相关性。如果夹角小，内积大；如果垂直，内积为0。</p><h4 id=3-外积>3. 外积<a class=anchor href=#3-%e5%a4%96%e7%a7%af>#</a></h4><p>对于 $\mathbf{x} \in \mathbb{R}^m$ 和 $\mathbf{y} \in \mathbb{R}^n$，外积 $\mathbf{xy}^T \in \mathbb{R}^{m \times n}$：</p><p>$$
(\mathbf{xy}^T)_{ij} = x_i y_j
$$</p><p><strong>几何意义</strong>：生成一个秩为1的矩阵。这在SVD和推荐系统中很有用。</p><h4 id=4-逐元素运算hadamard积>4. 逐元素运算（Hadamard积）<a class=anchor href=#4-%e9%80%90%e5%85%83%e7%b4%a0%e8%bf%90%e7%ae%97hadamard%e7%a7%af>#</a></h4><p>对于相同形状的矩阵 $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$：</p><p>$$
(\mathbf{A} \odot \mathbf{B})<em>{ij} = \mathbf{A}</em>{ij} \mathbf{B}_{ij}
$$</p><p><strong>应用</strong>：神经网络中的激活函数、dropout等都是逐元素操作。</p><hr><h3 id=转置逆矩阵与伪逆>转置、逆矩阵与伪逆<a class=anchor href=#%e8%bd%ac%e7%bd%ae%e9%80%86%e7%9f%a9%e9%98%b5%e4%b8%8e%e4%bc%aa%e9%80%86>#</a></h3><h4 id=转置>转置<a class=anchor href=#%e8%bd%ac%e7%bd%ae>#</a></h4><p>矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 的转置 $\mathbf{A}^T \in \mathbb{R}^{n \times m}$：</p><p>$$
(\mathbf{A}^T)<em>{ij} = \mathbf{A}</em>{ji}
$$</p><p><strong>性质</strong>：</p><ul><li>$(\mathbf{A}^T)^T = \mathbf{A}$</li><li>$(\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T$（注意顺序反转）</li></ul><h4 id=逆矩阵>逆矩阵<a class=anchor href=#%e9%80%86%e7%9f%a9%e9%98%b5>#</a></h4><p>对于方阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$，如果存在 $\mathbf{A}^{-1}$ 使得：</p><p>$$
\mathbf{A} \mathbf{A}^{-1} = \mathbf{A}^{-1} \mathbf{A} = \mathbf{I}
$$</p><p>则称 $\mathbf{A}$ 可逆。</p><p><strong>性质</strong>：</p><ul><li>$(\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$（注意顺序反转）</li><li>$(\mathbf{A}^T)^{-1} = (\mathbf{A}^{-1})^T$</li></ul><p><strong>实践警告</strong>：在数值计算中，永远不要显式计算逆矩阵！使用 <code>np.linalg.solve(A, b)</code> 而不是 <code>np.dot(np.linalg.inv(A), b)</code>。前者更快、更稳定。</p><h4 id=伪逆moore-penrose逆>伪逆（Moore-Penrose逆）<a class=anchor href=#%e4%bc%aa%e9%80%86moore-penrose%e9%80%86>#</a></h4><p>对于非方阵或奇异矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$，其伪逆 $\mathbf{A}^+ \in \mathbb{R}^{n \times m}$ 满足：</p><p>$$
\mathbf{A}^+ = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \quad \text{(当 } \mathbf{A} \text{ 列满秩)}
$$</p><p><strong>应用</strong>：求解超定或欠定的线性系统，线性回归中处理奇异设计矩阵。</p><hr><h3 id=迹与行列式>迹与行列式<a class=anchor href=#%e8%bf%b9%e4%b8%8e%e8%a1%8c%e5%88%97%e5%bc%8f>#</a></h3><h4 id=迹>迹<a class=anchor href=#%e8%bf%b9>#</a></h4><p>矩阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$ 的迹是对角线元素之和：</p><p>$$
\text{tr}(\mathbf{A}) = \sum_{i=1}^n \mathbf{A}_{ii}
$$</p><p><strong>性质</strong>（这些在求导时非常有用）：</p><ol><li>$\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})$</li><li>$\text{tr}(c\mathbf{A}) = c \cdot \text{tr}(\mathbf{A})$</li><li><strong>循环性</strong>：$\text{tr}(\mathbf{ABC}) = \text{tr}(\mathbf{BCA}) = \text{tr}(\mathbf{CAB})$</li><li>$\text{tr}(\mathbf{A}^T) = \text{tr}(\mathbf{A})$</li><li>$\text{tr}(\mathbf{AB}) = \text{tr}(\mathbf{BA})$（即使 $\mathbf{AB} \neq \mathbf{BA}$）</li></ol><p><strong>技巧</strong>：利用迹的循环性，$\mathbf{x}^T \mathbf{A} \mathbf{x} = \text{tr}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = \text{tr}(\mathbf{x} \mathbf{x}^T \mathbf{A}) = \text{tr}(\mathbf{A} \mathbf{x} \mathbf{x}^T)$。这在矩阵求导时很有用。</p><h4 id=行列式>行列式<a class=anchor href=#%e8%a1%8c%e5%88%97%e5%bc%8f>#</a></h4><p>行列式 $\det(\mathbf{A})$ 或 $|\mathbf{A}|$ 是方阵的一个标量值，几何上代表<strong>体积缩放因子</strong>。</p><p><strong>性质</strong>：</p><ul><li>$\det(\mathbf{AB}) = \det(\mathbf{A}) \det(\mathbf{B})$</li><li>$\det(\mathbf{A}^T) = \det(\mathbf{A})$</li><li>$\det(\mathbf{A}^{-1}) = 1/\det(\mathbf{A})$</li><li>$\mathbf{A}$ 可逆 $\Leftrightarrow$ $\det(\mathbf{A}) \neq 0$</li></ul><p><strong>应用</strong>：判断矩阵是否可逆，计算体积变换。</p><hr><h2 id=22-向量空间理解数据的结构>2.2 向量空间：理解数据的结构<a class=anchor href=#22-%e5%90%91%e9%87%8f%e7%a9%ba%e9%97%b4%e7%90%86%e8%a7%a3%e6%95%b0%e6%8d%ae%e7%9a%84%e7%bb%93%e6%9e%84>#</a></h2><h3 id=线性组合与张成空间>线性组合与张成空间<a class=anchor href=#%e7%ba%bf%e6%80%a7%e7%bb%84%e5%90%88%e4%b8%8e%e5%bc%a0%e6%88%90%e7%a9%ba%e9%97%b4>#</a></h3><p>给定向量 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \in \mathbb{R}^m$，它们的<strong>线性组合</strong>是：</p><p>$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n, \quad c_i \in \mathbb{R}
$$</p><p>这些向量的<strong>张成空间</strong> $\text{span}(\mathbf{v}_1, \ldots, \mathbf{v}_n)$ 是所有可能的线性组合的集合。</p><p><strong>直觉</strong>：</p><ul><li>一个非零向量 $\mathbf{v}$ 张成一条直线</li><li>两个不共线的向量张成一个平面</li><li>三个不共面的向量张成整个3D空间</li></ul><p><strong>机器学习中的例子</strong>：在线性回归中，$\mathbf{X}\mathbf{w}$ 是设计矩阵 $\mathbf{X}$ 的列向量的线性组合，结果必然在 $\mathbf{X}$ 的列空间中。如果真实标签 $\mathbf{y}$ 不在这个空间里，我们只能找到最近的点（投影）。</p><hr><h3 id=线性相关与线性无关>线性相关与线性无关<a class=anchor href=#%e7%ba%bf%e6%80%a7%e7%9b%b8%e5%85%b3%e4%b8%8e%e7%ba%bf%e6%80%a7%e6%97%a0%e5%85%b3>#</a></h3><p>向量组 ${\mathbf{v}_1, \ldots, \mathbf{v}_n}$ <strong>线性相关</strong>，如果存在不全为零的系数 $c_1, \ldots, c_n$ 使得：</p><p>$$
c_1 \mathbf{v}_1 + \cdots + c_n \mathbf{v}_n = \mathbf{0}
$$</p><p>否则称为<strong>线性无关</strong>。</p><p><strong>直觉</strong>：线性相关 = 有冗余信息。线性无关 = 每个向量都提供新的方向。</p><p><strong>判断方法</strong>：</p><ul><li>将向量排成矩阵 $\mathbf{A} = [\mathbf{v}_1 ; \mathbf{v}_2 ; \cdots ; \mathbf{v}_n]$</li><li>如果 $\text{rank}(\mathbf{A}) = n$，则线性无关</li><li>如果 $\text{rank}(\mathbf{A}) &lt; n$，则线性相关</li></ul><hr><h3 id=基与维度>基与维度<a class=anchor href=#%e5%9f%ba%e4%b8%8e%e7%bb%b4%e5%ba%a6>#</a></h3><p>向量空间 $V$ 的一组<strong>基</strong>是满足以下条件的向量集：</p><ol><li>它们线性无关</li><li>它们张成整个空间 $V$</li></ol><p><strong>维度</strong> $\dim(V)$ = 基中向量的个数。</p><p><strong>标准基</strong>：$\mathbb{R}^n$ 的标准基是 ${\mathbf{e}_1, \ldots, \mathbf{e}_n}$，其中 $\mathbf{e}_i$ 是第 $i$ 个分量为1、其余为0的向量。</p><p><strong>例子</strong>：</p><ul><li>$\mathbb{R}^2$ 的标准基：$\left{\begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix}\right}$</li><li>另一组基：$\left{\begin{bmatrix} 1 \ 1 \end{bmatrix}, \begin{bmatrix} 1 \ -1 \end{bmatrix}\right}$</li></ul><p><strong>重要性质</strong>：同一空间的所有基都有相同的向量个数（这就是为什么维度是良定义的）。</p><hr><h3 id=秩矩阵的本质维度>秩：矩阵的本质维度<a class=anchor href=#%e7%a7%a9%e7%9f%a9%e9%98%b5%e7%9a%84%e6%9c%ac%e8%b4%a8%e7%bb%b4%e5%ba%a6>#</a></h3><p>矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 的<strong>秩</strong> $\text{rank}(\mathbf{A})$ 定义为：</p><p>$$
\text{rank}(\mathbf{A}) = \dim(\text{列空间}) = \dim(\text{行空间})
$$</p><p><strong>等价定义</strong>：</p><ul><li>列空间的维度 = 线性无关的列向量的最大个数</li><li>行空间的维度 = 线性无关的行向量的最大个数</li><li>最大的非零奇异值的个数（通过SVD）</li></ul><p><strong>性质</strong>：</p><ul><li>$\text{rank}(\mathbf{A}) \leq \min(m, n)$</li><li>$\text{rank}(\mathbf{AB}) \leq \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))$</li><li>$\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A}^T)$</li><li>$\text{rank}(\mathbf{A}^T \mathbf{A}) = \text{rank}(\mathbf{A})$</li></ul><p><strong>直觉</strong>：秩表示矩阵变换后空间的"真实维度"。如果 $\mathbf{A} \in \mathbb{R}^{100 \times 50}$ 的秩只有10，说明虽然有50个特征，但本质上只有10个独立的方向。</p><p><strong>机器学习中的意义</strong>：</p><ul><li><strong>低秩假设</strong>：许多真实数据矩阵（如用户-物品评分矩阵）的秩远小于其维度，这是矩阵分解方法的基础</li><li><strong>正则化</strong>：当设计矩阵不满秩时，$\mathbf{X}^T\mathbf{X}$ 不可逆，需要正则化（Ridge回归）</li></ul><hr><h3 id=秩-零化度定理>秩-零化度定理<a class=anchor href=#%e7%a7%a9-%e9%9b%b6%e5%8c%96%e5%ba%a6%e5%ae%9a%e7%90%86>#</a></h3><p>对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$：</p><p>$$
\text{rank}(\mathbf{A}) + \dim(\text{零空间}) = n
$$</p><p>其中零空间（核空间）$\text{null}(\mathbf{A}) = {\mathbf{x} : \mathbf{A}\mathbf{x} = \mathbf{0}}$。</p><p><strong>直觉</strong>：</p><ul><li>$n$ 个输入维度</li><li>$\text{rank}(\mathbf{A})$ 个维度被保留（输出空间）</li><li>$n - \text{rank}(\mathbf{A})$ 个维度被压缩到零（丢失信息）</li></ul><p><strong>例子</strong>：如果 $\mathbf{A} \in \mathbb{R}^{3 \times 5}$ 的秩为2，则：</p><ul><li>列空间是 $\mathbb{R}^3$ 中的2D子空间（一个平面）</li><li>零空间是 $\mathbb{R}^5$ 中的3D子空间</li><li>$2 + 3 = 5$ ✓</li></ul><hr><h3 id=四个基本子空间>四个基本子空间<a class=anchor href=#%e5%9b%9b%e4%b8%aa%e5%9f%ba%e6%9c%ac%e5%ad%90%e7%a9%ba%e9%97%b4>#</a></h3><p>对于 $\mathbf{A} \in \mathbb{R}^{m \times n}$，有四个重要的子空间：</p><table><thead><tr><th>子空间</th><th>定义</th><th>维度</th><th>所在空间</th></tr></thead><tbody><tr><td>列空间 $C(\mathbf{A})$</td><td>$\mathbf{A}$ 的列向量张成的空间</td><td>$r$</td><td>$\mathbb{R}^m$</td></tr><tr><td>零空间 $N(\mathbf{A})$</td><td>${\mathbf{x} : \mathbf{A}\mathbf{x} = \mathbf{0}}$</td><td>$n - r$</td><td>$\mathbb{R}^n$</td></tr><tr><td>行空间 $C(\mathbf{A}^T)$</td><td>$\mathbf{A}^T$ 的列空间</td><td>$r$</td><td>$\mathbb{R}^n$</td></tr><tr><td>左零空间 $N(\mathbf{A}^T)$</td><td>${\mathbf{y} : \mathbf{A}^T\mathbf{y} = \mathbf{0}}$</td><td>$m - r$</td><td>$\mathbb{R}^m$</td></tr></tbody></table><p>其中 $r = \text{rank}(\mathbf{A})$。</p><p><strong>正交关系</strong>：</p><ul><li>行空间 $\perp$ 零空间（在 $\mathbb{R}^n$ 中）</li><li>列空间 $\perp$ 左零空间（在 $\mathbb{R}^m$ 中）</li></ul><p><strong>图示</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入空间 ℝⁿ              输出空间 ℝᵐ
</span></span><span class=line><span class=cl>┌─────────────┐         ┌─────────────┐
</span></span><span class=line><span class=cl>│  行空间 (r) │ ──A──&gt; │  列空间 (r) │
</span></span><span class=line><span class=cl>│      ⊕      │         │      ⊕      │
</span></span><span class=line><span class=cl>│ 零空间(n-r) │ ──0──&gt; │左零空间(m-r)│
</span></span><span class=line><span class=cl>└─────────────┘         └─────────────┘</span></span></code></pre></div><p><strong>机器学习中的应用</strong>：</p><ul><li><strong>列空间</strong>：$\mathbf{Ax} = \mathbf{b}$ 有解 $\Leftrightarrow$ $\mathbf{b} \in C(\mathbf{A})$</li><li><strong>零空间</strong>：神经网络中，如果两个不同的输入 $\mathbf{x}_1, \mathbf{x}_2$ 产生相同的输出，则 $\mathbf{x}_1 - \mathbf{x}_2 \in N(\mathbf{A})$</li><li><strong>SVD</strong>：将矩阵分解为这四个子空间的正交基的组合（第3章）</li></ul><hr><h2 id=23-度量与正交>2.3 度量与正交<a class=anchor href=#23-%e5%ba%a6%e9%87%8f%e4%b8%8e%e6%ad%a3%e4%ba%a4>#</a></h2><h3 id=范数测量大小>范数：测量大小<a class=anchor href=#%e8%8c%83%e6%95%b0%e6%b5%8b%e9%87%8f%e5%a4%a7%e5%b0%8f>#</a></h3><p><strong>范数</strong>是向量"长度"的推广，满足：</p><ol><li>非负性：$|\mathbf{x}| \geq 0$，且 $|\mathbf{x}| = 0 \Leftrightarrow \mathbf{x} = \mathbf{0}$</li><li>齐次性：$|c\mathbf{x}| = |c| |\mathbf{x}|$</li><li>三角不等式：$|\mathbf{x} + \mathbf{y}| \leq |\mathbf{x}| + |\mathbf{y}|$</li></ol><h4 id=lp-范数>$L^p$ 范数<a class=anchor href=#lp-%e8%8c%83%e6%95%b0>#</a></h4><p>$$
|\mathbf{x}|<em>p = \left(\sum</em>{i=1}^n |x_i|^p\right)^{1/p}
$$</p><p><strong>常用范数</strong>：</p><ol><li><p><strong>$L^1$ 范数</strong>（曼哈顿距离）：
$$
|\mathbf{x}|<em>1 = \sum</em>{i=1}^n |x_i|
$$
<strong>应用</strong>：Lasso回归，促进稀疏解（许多系数为0）</p></li><li><p><strong>$L^2$ 范数</strong>（欧几里得距离）：
$$
|\mathbf{x}|<em>2 = \sqrt{\sum</em>{i=1}^n x_i^2} = \sqrt{\mathbf{x}^T \mathbf{x}}
$$
<strong>应用</strong>：最常用，Ridge回归，神经网络权重衰减</p></li><li><p><strong>$L^\infty$ 范数</strong>（最大范数）：
$$
|\mathbf{x}|_\infty = \max_i |x_i|
$$
<strong>应用</strong>：对抗攻击中的扰动约束</p></li></ol><p><strong>为什么 $L^1$ 产生稀疏解？</strong></p><p>考虑约束优化 $\min |\mathbf{x}|_p$ subject to $\mathbf{a}^T \mathbf{x} = c$。</p><ul><li>$L^1$ 球是菱形，有尖角 → 最优解倾向于在坐标轴上（某些分量为0）</li><li>$L^2$ 球是圆形，光滑 → 最优解一般不在坐标轴上</li></ul><h4 id=矩阵范数>矩阵范数<a class=anchor href=#%e7%9f%a9%e9%98%b5%e8%8c%83%e6%95%b0>#</a></h4><p><strong>Frobenius范数</strong>（最常用）：
$$
|\mathbf{A}|<em>F = \sqrt{\sum</em>{i,j} \mathbf{A}_{ij}^2} = \sqrt{\text{tr}(\mathbf{A}^T \mathbf{A})}
$$</p><p><strong>谱范数</strong>（最大奇异值）：
$$
|\mathbf{A}|<em>2 = \sigma</em>{\max}(\mathbf{A})
$$</p><hr><h3 id=内积与角度>内积与角度<a class=anchor href=#%e5%86%85%e7%a7%af%e4%b8%8e%e8%a7%92%e5%ba%a6>#</a></h3><p>内积 $\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T \mathbf{y}$ 与角度的关系：</p><p>$$
\mathbf{x}^T \mathbf{y} = |\mathbf{x}|_2 |\mathbf{y}|_2 \cos \theta
$$</p><p>其中 $\theta$ 是 $\mathbf{x}$ 和 $\mathbf{y}$ 的夹角。</p><p><strong>余弦相似度</strong>：
$$
\cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{|\mathbf{x}|_2 |\mathbf{y}|_2}
$$</p><ul><li>$\cos \theta = 1$：同向</li><li>$\cos \theta = 0$：正交</li><li>$\cos \theta = -1$：反向</li></ul><p><strong>应用</strong>：文本相似度（TF-IDF向量）、推荐系统（用户/物品嵌入）。</p><hr><h3 id=正交与正交矩阵>正交与正交矩阵<a class=anchor href=#%e6%ad%a3%e4%ba%a4%e4%b8%8e%e6%ad%a3%e4%ba%a4%e7%9f%a9%e9%98%b5>#</a></h3><p>向量 $\mathbf{x}$ 和 $\mathbf{y}$ <strong>正交</strong>，记作 $\mathbf{x} \perp \mathbf{y}$，如果：</p><p>$$
\mathbf{x}^T \mathbf{y} = 0
$$</p><p><strong>正交矩阵</strong> $\mathbf{Q} \in \mathbb{R}^{n \times n}$ 满足：</p><p>$$
\mathbf{Q}^T \mathbf{Q} = \mathbf{Q} \mathbf{Q}^T = \mathbf{I}
$$</p><p>即 $\mathbf{Q}^{-1} = \mathbf{Q}^T$。</p><p><strong>性质</strong>：</p><ul><li>正交矩阵的列（行）向量构成标准正交基</li><li>正交变换<strong>保持长度和角度</strong>：$|\mathbf{Qx}|_2 = |\mathbf{x}|_2$</li><li>$\det(\mathbf{Q}) = \pm 1$</li></ul><p><strong>为什么正交矩阵很好？</strong></p><ol><li>数值稳定：条件数为1，不会放大误差</li><li>计算高效：求逆只需转置</li><li>几何清晰：旋转和反射，不改变形状</li></ol><p><strong>应用</strong>：PCA、SVD、QR分解、Gram-Schmidt正交化。</p><hr><h2 id=24-投影线性回归的几何本质>2.4 投影：线性回归的几何本质<a class=anchor href=#24-%e6%8a%95%e5%bd%b1%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e5%87%a0%e4%bd%95%e6%9c%ac%e8%b4%a8>#</a></h2><h3 id=为什么我们需要投影>为什么我们需要投影<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e9%9c%80%e8%a6%81%e6%8a%95%e5%bd%b1>#</a></h3><p><strong>问题</strong>：给定矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和向量 $\mathbf{b} \in \mathbb{R}^m$，求解 $\mathbf{Ax} = \mathbf{b}$。</p><p><strong>现实</strong>：通常 $\mathbf{b}$ 不在 $\mathbf{A}$ 的列空间 $C(\mathbf{A})$ 中，方程无解！</p><p><strong>例子（线性回归）</strong>：</p><ul><li>$\mathbf{A}$：设计矩阵（样本 × 特征）</li><li>$\mathbf{x}$：权重</li><li>$\mathbf{b}$：真实标签</li><li>数据有噪声 → $\mathbf{b} \notin C(\mathbf{A})$</li></ul><p><strong>解决方案</strong>：找到 $C(\mathbf{A})$ 中最接近 $\mathbf{b}$ 的点 $\mathbf{p}$，即 $\mathbf{b}$ 在 $C(\mathbf{A})$ 上的<strong>正交投影</strong>。</p><hr><h3 id=从最小二乘推导投影矩阵>从最小二乘推导投影矩阵<a class=anchor href=#%e4%bb%8e%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%8e%a8%e5%af%bc%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5>#</a></h3><p><strong>目标</strong>：最小化残差的平方和</p><p>$$
\min_{\mathbf{x}} |\mathbf{b} - \mathbf{Ax}|_2^2
$$</p><p><strong>推导</strong>：</p><p>设投影为 $\mathbf{p} = \mathbf{A}\hat{\mathbf{x}}$，残差为 $\mathbf{e} = \mathbf{b} - \mathbf{p} = \mathbf{b} - \mathbf{A}\hat{\mathbf{x}}$。</p><p><strong>关键几何直觉</strong>：最优解时，残差 $\mathbf{e}$ 必须垂直于列空间 $C(\mathbf{A})$，即：</p><p>$$
\mathbf{e} \perp C(\mathbf{A}) \quad \Rightarrow \quad \mathbf{A}^T \mathbf{e} = \mathbf{0}
$$</p><p>因为 $\mathbf{A}$ 的每一列都在 $C(\mathbf{A})$ 中。</p><p>代入 $\mathbf{e} = \mathbf{b} - \mathbf{A}\hat{\mathbf{x}}$：</p><p>$$
\mathbf{A}^T (\mathbf{b} - \mathbf{A}\hat{\mathbf{x}}) = \mathbf{0}
$$</p><p>展开得到<strong>正规方程</strong>：</p><p>$$
\boxed{\mathbf{A}^T \mathbf{A} \hat{\mathbf{x}} = \mathbf{A}^T \mathbf{b}}
$$</p><p>假设 $\mathbf{A}$ 列满秩（即 $\mathbf{A}^T \mathbf{A}$ 可逆），则：</p><p>$$
\hat{\mathbf{x}} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
$$</p><p>投影 $\mathbf{p} = \mathbf{A}\hat{\mathbf{x}}$：</p><p>$$
\mathbf{p} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
$$</p><p>定义<strong>投影矩阵</strong>：</p><p>$$
\boxed{\mathbf{P} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T}
$$</p><p>则 $\mathbf{p} = \mathbf{P}\mathbf{b}$。</p><hr><h3 id=投影矩阵的性质>投影矩阵的性质<a class=anchor href=#%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5%e7%9a%84%e6%80%a7%e8%b4%a8>#</a></h3><ol><li><p><strong>对称性</strong>：$\mathbf{P}^T = \mathbf{P}$</p><p>证明：$\mathbf{P}^T = [\mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T]^T = \mathbf{A}[(\mathbf{A}^T \mathbf{A})^{-1}]^T \mathbf{A}^T = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T = \mathbf{P}$</p></li><li><p><strong>幂等性</strong>：$\mathbf{P}^2 = \mathbf{P}$</p><p>证明：
$$
\begin{align}
\mathbf{P}^2 &= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \cdot \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \
&= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} (\mathbf{A}^T \mathbf{A}) (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \
&= \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T = \mathbf{P}
\end{align}
$$</p><p><strong>直觉</strong>：投影两次 = 投影一次（已经在子空间里了）</p></li><li><p><strong>秩</strong>：$\text{rank}(\mathbf{P}) = \text{rank}(\mathbf{A})$</p></li><li><p><strong>残差投影矩阵</strong>：$\mathbf{I} - \mathbf{P}$ 将向量投影到 $C(\mathbf{A})$ 的正交补空间</p><p>验证：$(\mathbf{I} - \mathbf{P})^2 = \mathbf{I} - 2\mathbf{P} + \mathbf{P}^2 = \mathbf{I} - \mathbf{P}$ ✓</p></li></ol><hr><h3 id=几何直觉>几何直觉<a class=anchor href=#%e5%87%a0%e4%bd%95%e7%9b%b4%e8%a7%89>#</a></h3><p><strong>2D例子</strong>：投影到直线</p><p>设 $\mathbf{A} = \mathbf{a}$ 是单位向量，则：</p><p>$$
\mathbf{P} = \mathbf{a}(\mathbf{a}^T \mathbf{a})^{-1} \mathbf{a}^T = \mathbf{a}\mathbf{a}^T
$$</p><p>对于任意 $\mathbf{b}$：</p><p>$$
\mathbf{p} = (\mathbf{a}\mathbf{a}^T) \mathbf{b} = \mathbf{a}(\mathbf{a}^T \mathbf{b}) = (\mathbf{a}^T \mathbf{b}) \mathbf{a}
$$</p><p>这就是向量 $\mathbf{b}$ 在 $\mathbf{a}$ 方向上的分量！</p><p><strong>图示</strong>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>      b
</span></span><span class=line><span class=cl>      │╲
</span></span><span class=line><span class=cl>      │ ╲ e = b - p
</span></span><span class=line><span class=cl>      │  ╲
</span></span><span class=line><span class=cl>      │   ╲
</span></span><span class=line><span class=cl>    p │────┴─→ a
</span></span><span class=line><span class=cl>      │
</span></span><span class=line><span class=cl>      └────────</span></span></code></pre></div><p><strong>线性回归的几何图像</strong>：</p><ul><li>$\mathbf{y} \in \mathbb{R}^m$：真实标签（$m$ 个样本）</li><li>$C(\mathbf{X})$：特征空间张成的子空间（维度 $&lt; m$）</li><li>$\hat{\mathbf{y}} = \mathbf{X}\hat{\mathbf{w}}$：预测值（投影）</li><li>$\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$：残差（垂直于 $C(\mathbf{X})$）</li></ul><p>最小二乘法 = 找到 $C(\mathbf{X})$ 中最接近 $\mathbf{y}$ 的点。</p><hr><h2 id=25-矩阵微积分反向传播的数学基础>2.5 矩阵微积分：反向传播的数学基础<a class=anchor href=#25-%e7%9f%a9%e9%98%b5%e5%be%ae%e7%a7%af%e5%88%86%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80>#</a></h2><h3 id=为什么需要矩阵求导>为什么需要矩阵求导<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc>#</a></h3><p><strong>梯度下降</strong>的核心是计算损失函数关于参数的导数：</p><p>$$
\mathbf{w}_{t+1} = \mathbf{w}<em>t - \eta \nabla</em>{\mathbf{w}} L(\mathbf{w}_t)
$$</p><p>在深度学习中：</p><ul><li>参数是矩阵或向量（权重、偏置）</li><li>需要计算 $\frac{\partial L}{\partial \mathbf{W}}$、$\frac{\partial L}{\partial \mathbf{b}}$ 等</li></ul><p><strong>传统微积分不够用</strong>：</p><ul><li>标量对标量求导：$\frac{dy}{dx}$</li><li><strong>我们需要</strong>：向量对向量、矩阵对矩阵的求导</li></ul><hr><h3 id=布局约定分母布局>布局约定：分母布局<a class=anchor href=#%e5%b8%83%e5%b1%80%e7%ba%a6%e5%ae%9a%e5%88%86%e6%af%8d%e5%b8%83%e5%b1%80>#</a></h3><p>矩阵求导有两种布局约定，我们采用<strong>分母布局</strong>（Denominator Layout），这是深度学习中的主流。</p><table><thead><tr><th>分子维度</th><th>分母维度</th><th>结果形状</th><th>例子</th></tr></thead><tbody><tr><td>标量</td><td>向量 $\mathbf{x} \in \mathbb{R}^n$</td><td>$n \times 1$ 列向量</td><td>$\nabla_{\mathbf{x}} f$</td></tr><tr><td>标量</td><td>矩阵 $\mathbf{X} \in \mathbb{R}^{m \times n}$</td><td>$m \times n$ 矩阵</td><td>$\frac{\partial f}{\partial \mathbf{X}}$</td></tr><tr><td>向量 $\mathbf{f} \in \mathbb{R}^m$</td><td>向量 $\mathbf{x} \in \mathbb{R}^n$</td><td>$n \times m$ 矩阵</td><td>雅可比矩阵 $\mathbf{J}$</td></tr></tbody></table><p><strong>记忆技巧</strong>：结果的形状是"分母在前"。</p><hr><h3 id=标量对向量求导>标量对向量求导<a class=anchor href=#%e6%a0%87%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc>#</a></h3><p>设 $f: \mathbb{R}^n \to \mathbb{R}$，则梯度：</p><p>$$
\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \ \frac{\partial f}{\partial x_2} \ \vdots \ \frac{\partial f}{\partial x_n} \end{bmatrix} \in \mathbb{R}^n
$$</p><p><strong>常用公式</strong>：</p><ol><li><p><strong>线性</strong>：$f(\mathbf{x}) = \mathbf{a}^T \mathbf{x}$
$$
\nabla_{\mathbf{x}} (\mathbf{a}^T \mathbf{x}) = \mathbf{a}
$$</p></li><li><p><strong>二次型</strong>：$f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}$
$$
\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}
$$</p><p>特别地，若 $\mathbf{A}$ 对称：
$$
\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}
$$</p></li><li><p><strong>范数平方</strong>：$f(\mathbf{x}) = |\mathbf{x}|<em>2^2 = \mathbf{x}^T \mathbf{x}$
$$
\nabla</em>{\mathbf{x}} (\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}
$$</p></li></ol><hr><h3 id=标量对矩阵求导>标量对矩阵求导<a class=anchor href=#%e6%a0%87%e9%87%8f%e5%af%b9%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc>#</a></h3><p>设 $f: \mathbb{R}^{m \times n} \to \mathbb{R}$，则：</p><p>$$
\frac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \frac{\partial f}{\partial \mathbf{X}<em>{11}} & \cdots & \frac{\partial f}{\partial \mathbf{X}</em>{1n}} \ \vdots & \ddots & \vdots \ \frac{\partial f}{\partial \mathbf{X}<em>{m1}} & \cdots & \frac{\partial f}{\partial \mathbf{X}</em>{mn}} \end{bmatrix} \in \mathbb{R}^{m \times n}
$$</p><p><strong>常用公式</strong>：</p><ol><li><p><strong>线性</strong>：$f(\mathbf{X}) = \text{tr}(\mathbf{A}^T \mathbf{X})$
$$
\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A}^T \mathbf{X}) = \mathbf{A}
$$</p></li><li><p><strong>二次型</strong>：$f(\mathbf{X}) = \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$
$$
\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{X}
$$</p></li><li><p><strong>Frobenius范数平方</strong>：$f(\mathbf{X}) = |\mathbf{X}|_F^2 = \text{tr}(\mathbf{X}^T \mathbf{X})$
$$
\frac{\partial}{\partial \mathbf{X}} |\mathbf{X}|_F^2 = 2\mathbf{X}
$$</p></li></ol><hr><h3 id=向量对向量求导雅可比矩阵>向量对向量求导：雅可比矩阵<a class=anchor href=#%e5%90%91%e9%87%8f%e5%af%b9%e5%90%91%e9%87%8f%e6%b1%82%e5%af%bc%e9%9b%85%e5%8f%af%e6%af%94%e7%9f%a9%e9%98%b5>#</a></h3><p>设 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，雅可比矩阵：</p><p>$$
\mathbf{J} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \ \vdots & \ddots & \vdots \ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} \in \mathbb{R}^{m \times n}
$$</p><p><strong>注意</strong>：分母布局下，第 $i$ 行是 $f_i$ 对 $\mathbf{x}$ 的梯度的转置。</p><p><strong>例子</strong>：线性变换 $\mathbf{f}(\mathbf{x}) = \mathbf{A}\mathbf{x}$
$$
\frac{\partial (\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}^T
$$</p><p>等等，为什么是 $\mathbf{A}^T$ 而不是 $\mathbf{A}$？因为分母布局！后面会详细解释。</p><hr><h3 id=链式法则>链式法则<a class=anchor href=#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99>#</a></h3><p><strong>标量链</strong>：设 $y = f(u)$，$u = g(x)$，则：
$$
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
$$</p><p><strong>向量链</strong>：设 $y = f(\mathbf{u})$，$\mathbf{u} = g(\mathbf{x})$，则：
$$
\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial y}{\partial \mathbf{u}} \frac{\partial \mathbf{u}}{\partial \mathbf{x}}
$$</p><p>这里 $\frac{\partial y}{\partial \mathbf{u}} \in \mathbb{R}^{1 \times m}$（行向量），$\frac{\partial \mathbf{u}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$（雅可比）。</p><p><strong>反向传播本质</strong>：链式法则的递归应用。</p><hr><h3 id=重要公式推导>重要公式推导<a class=anchor href=#%e9%87%8d%e8%a6%81%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc>#</a></h3><h4 id=推导1nabla_mathbfx-mathbfxt-mathbfa-mathbfx>推导1：$\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x})$<a class=anchor href=#%e6%8e%a8%e5%af%bc1nabla_mathbfx-mathbfxt-mathbfa-mathbfx>#</a></h4><p><strong>方法1：直接展开</strong></p><p>$$
f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x} = \sum_{i=1}^n \sum_{j=1}^n x_i \mathbf{A}_{ij} x_j
$$</p><p>对 $x_k$ 求偏导：
$$
\frac{\partial f}{\partial x_k} = \sum_{j=1}^n \mathbf{A}<em>{kj} x_j + \sum</em>{i=1}^n x_i \mathbf{A}_{ik}
$$</p><p>第一项来自 $i=k$ 的项，第二项来自 $j=k$ 的项。</p><p>用矩阵形式：
$$
\nabla_{\mathbf{x}} f = \mathbf{A}\mathbf{x} + \mathbf{A}^T \mathbf{x} = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}
$$</p><p><strong>方法2：微分法</strong>（更优雅）</p><p>计算微分 $df$：
$$
\begin{align}
df &= d(\mathbf{x}^T \mathbf{A} \mathbf{x}) \
&= (d\mathbf{x}^T) \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{A} (d\mathbf{x}) \
&= (d\mathbf{x})^T \mathbf{A} \mathbf{x} + \mathbf{x}^T \mathbf{A}^T (d\mathbf{x})^T \quad \text{(利用标量转置)} \
&= (d\mathbf{x})^T (\mathbf{A}\mathbf{x} + \mathbf{A}^T \mathbf{x}) \
&= (d\mathbf{x})^T (\mathbf{A} + \mathbf{A}^T) \mathbf{x}
\end{align}
$$</p><p>由 $df = \nabla_{\mathbf{x}} f \cdot d\mathbf{x} = (\nabla_{\mathbf{x}} f)^T d\mathbf{x}$，得：
$$
\boxed{\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}}
$$</p><h4 id=推导2fracpartialpartial-mathbfx-texttrmathbfa-mathbfx-mathbfb>推导2：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})$<a class=anchor href=#%e6%8e%a8%e5%af%bc2fracpartialpartial-mathbfx-texttrmathbfa-mathbfx-mathbfb>#</a></h4><p><strong>技巧</strong>：利用迹的循环性和线性</p><p>$$
f(\mathbf{X}) = \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})
$$</p><p>计算微分：
$$
\begin{align}
df &= d[\text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})] \
&= \text{tr}(\mathbf{A} , d\mathbf{X} , \mathbf{B}) \quad \text{(迹的线性性)} \
&= \text{tr}(\mathbf{B} \mathbf{A} , d\mathbf{X}) \quad \text{(迹的循环性)}
\end{align}
$$</p><p>已知 $\text{tr}(\mathbf{C}^T d\mathbf{X}) = \langle \mathbf{C}, d\mathbf{X} \rangle$，因此：
$$
df = \text{tr}[(\mathbf{B}\mathbf{A})^T d\mathbf{X}] = \text{tr}[(\mathbf{A}^T \mathbf{B}^T) d\mathbf{X}]
$$</p><p>所以：
$$
\boxed{\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B}) = \mathbf{A}^T \mathbf{B}^T}
$$</p><p><strong>特殊情况</strong>：</p><ul><li>$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}) = \mathbf{I}$</li><li>$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A}\mathbf{X}) = \mathbf{A}^T$</li><li>$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}\mathbf{A}) = \mathbf{A}^T$</li></ul><h4 id=推导3fracpartialpartial-mathbfx-texttrmathbfxt-mathbfa-mathbfx>推导3：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$<a class=anchor href=#%e6%8e%a8%e5%af%bc3fracpartialpartial-mathbfx-texttrmathbfxt-mathbfa-mathbfx>#</a></h4><p>$$
\begin{align}
df &= d[\text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})] \
&= \text{tr}(d\mathbf{X}^T \mathbf{A} \mathbf{X}) + \text{tr}(\mathbf{X}^T \mathbf{A} , d\mathbf{X}) \
&= \text{tr}(\mathbf{X}^T \mathbf{A}^T d\mathbf{X}) + \text{tr}(\mathbf{X}^T \mathbf{A} , d\mathbf{X}) \quad \text{(利用 } \text{tr}(\mathbf{A}^T) = \text{tr}(\mathbf{A})\text{)} \
&= \text{tr}[(\mathbf{A}^T \mathbf{X})^T d\mathbf{X}] + \text{tr}[(\mathbf{A}\mathbf{X})^T d\mathbf{X}] \
&= \text{tr}[(\mathbf{A}^T \mathbf{X} + \mathbf{A}\mathbf{X})^T d\mathbf{X}]
\end{align}
$$</p><p>因此：
$$
\boxed{\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{X}}
$$</p><hr><h3 id=实战线性回归的梯度>实战：线性回归的梯度<a class=anchor href=#%e5%ae%9e%e6%88%98%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e7%9a%84%e6%a2%af%e5%ba%a6>#</a></h3><p><strong>问题</strong>：最小化平方损失</p><p>$$
L(\mathbf{w}) = \frac{1}{2} |\mathbf{y} - \mathbf{X}\mathbf{w}|_2^2
$$</p><p>其中 $\mathbf{X} \in \mathbb{R}^{m \times n}$，$\mathbf{y} \in \mathbb{R}^m$，$\mathbf{w} \in \mathbb{R}^n$。</p><p><strong>目标</strong>：计算 $\nabla_{\mathbf{w}} L$。</p><p><strong>推导</strong>：</p><p>$$
\begin{align}
L(\mathbf{w}) &= \frac{1}{2} (\mathbf{y} - \mathbf{X}\mathbf{w})^T (\mathbf{y} - \mathbf{X}\mathbf{w}) \
&= \frac{1}{2} (\mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X}\mathbf{w} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w})
\end{align}
$$</p><p>注意 $\mathbf{y}^T \mathbf{X}\mathbf{w}$ 是标量，所以等于其转置：
$$
\mathbf{y}^T \mathbf{X}\mathbf{w} = \mathbf{w}^T \mathbf{X}^T \mathbf{y}
$$</p><p>因此：
$$
L(\mathbf{w}) = \frac{1}{2} \mathbf{y}^T \mathbf{y} - \mathbf{w}^T \mathbf{X}^T \mathbf{y} + \frac{1}{2} \mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}
$$</p><p>逐项求导：</p><ol><li><p>$\nabla_{\mathbf{w}} (\mathbf{y}^T \mathbf{y}) = \mathbf{0}$（不含 $\mathbf{w}$）</p></li><li><p>$\nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{y}) = \mathbf{X}^T \mathbf{y}$（线性项）</p></li><li><p>$\nabla_{\mathbf{w}} (\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}) = 2\mathbf{X}^T \mathbf{X} \mathbf{w}$（二次型，$\mathbf{X}^T \mathbf{X}$ 对称）</p></li></ol><p>综合：
$$
\boxed{\nabla_{\mathbf{w}} L = -\mathbf{X}^T \mathbf{y} + \mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T (\mathbf{X}\mathbf{w} - \mathbf{y})}
$$</p><p>令梯度为零：
$$
\mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T \mathbf{y}
$$</p><p>这正是<strong>正规方程</strong>！解得：
$$
\hat{\mathbf{w}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$</p><p><strong>连接投影</strong>：注意到 $\hat{\mathbf{y}} = \mathbf{X}\hat{\mathbf{w}} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{P}\mathbf{y}$，正是投影矩阵！</p><hr><h2 id=常用矩阵求导公式速查表>常用矩阵求导公式速查表<a class=anchor href=#%e5%b8%b8%e7%94%a8%e7%9f%a9%e9%98%b5%e6%b1%82%e5%af%bc%e5%85%ac%e5%bc%8f%e9%80%9f%e6%9f%a5%e8%a1%a8>#</a></h2><table><thead><tr><th>函数 $f(\mathbf{x})$</th><th>梯度 $\nabla_{\mathbf{x}} f$</th><th>备注</th></tr></thead><tbody><tr><td>$\mathbf{a}^T \mathbf{x}$</td><td>$\mathbf{a}$</td><td>线性</td></tr><tr><td>$\mathbf{x}^T \mathbf{a}$</td><td>$\mathbf{a}$</td><td>同上</td></tr><tr><td>$\mathbf{x}^T \mathbf{x}$</td><td>$2\mathbf{x}$</td><td>范数平方</td></tr><tr><td>$\mathbf{x}^T \mathbf{A} \mathbf{x}$</td><td>$(\mathbf{A} + \mathbf{A}^T)\mathbf{x}$</td><td>二次型</td></tr><tr><td>$\mathbf{x}^T \mathbf{A} \mathbf{x}$ ($\mathbf{A}$ 对称)</td><td>$2\mathbf{A}\mathbf{x}$</td><td>对称二次型</td></tr><tr><td>$|\mathbf{Ax} - \mathbf{b}|_2^2$</td><td>$2\mathbf{A}^T(\mathbf{Ax} - \mathbf{b})$</td><td>最小二乘</td></tr><tr><td>$\log \det(\mathbf{X})$</td><td>$\mathbf{X}^{-T}$</td><td>对数行列式</td></tr></tbody></table><table><thead><tr><th>函数 $f(\mathbf{X})$</th><th>梯度 $\frac{\partial f}{\partial \mathbf{X}}$</th><th>备注</th></tr></thead><tbody><tr><td>$\text{tr}(\mathbf{X})$</td><td>$\mathbf{I}$</td><td>迹</td></tr><tr><td>$\text{tr}(\mathbf{AX})$</td><td>$\mathbf{A}^T$</td><td>线性</td></tr><tr><td>$\text{tr}(\mathbf{AXB})$</td><td>$\mathbf{A}^T \mathbf{B}^T$</td><td>三矩阵积</td></tr><tr><td>$\text{tr}(\mathbf{X}^T \mathbf{A})$</td><td>$\mathbf{A}$</td><td>转置迹</td></tr><tr><td>$\text{tr}(\mathbf{X}^T \mathbf{AX})$</td><td>$(\mathbf{A} + \mathbf{A}^T)\mathbf{X}$</td><td>二次型</td></tr><tr><td>$|\mathbf{X}|_F^2$</td><td>$2\mathbf{X}$</td><td>Frobenius范数</td></tr></tbody></table><hr><h2 id=小结>小结<a class=anchor href=#%e5%b0%8f%e7%bb%93>#</a></h2><p>本章构建了机器学习所需的线性代数和微积分工具箱：</p><p><strong>2.1 基础数据结构</strong></p><ul><li>矩阵、向量是数据和变换的表示</li><li>迹和行列式的循环性质在求导中很有用</li></ul><p><strong>2.2 向量空间</strong></p><ul><li><strong>秩</strong>：矩阵的本质维度，决定了信息保留量</li><li><strong>秩-零化度定理</strong>：$\text{rank}(\mathbf{A}) + \dim(\text{null}(\mathbf{A})) = n$</li><li><strong>四个基本子空间</strong>：列空间、零空间、行空间、左零空间</li></ul><p><strong>2.3 度量与正交</strong></p><ul><li>$L^1$ 范数产生稀疏解，$L^2$ 范数产生光滑解</li><li>正交矩阵保持长度和角度，数值稳定</li></ul><p><strong>2.4 投影</strong></p><ul><li><strong>核心思想</strong>：数据通常不在我们想要的子空间里，投影找到最接近的点</li><li><strong>投影矩阵</strong>：$\mathbf{P} = \mathbf{A}(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T$</li><li><strong>线性回归 = 几何投影</strong>：$\hat{\mathbf{y}} = \mathbf{P}\mathbf{y}$</li></ul><p><strong>2.5 矩阵微积分</strong></p><ul><li><strong>分母布局</strong>：结果形状是"分母在前"</li><li><strong>核心技巧</strong>：微分法 + 迹的循环性</li><li><strong>反向传播 = 链式法则</strong></li></ul><p><strong>下一章预告</strong>：第3章将深入矩阵分解（特征值、SVD、Cholesky），这是PCA、推荐系统、线性判别分析的基础。</p><hr><p><strong>练习题</strong></p><ol><li><p>证明投影矩阵的幂等性：$\mathbf{P}^2 = \mathbf{P}$。</p></li><li><p>设 $\mathbf{A} \in \mathbb{R}^{3 \times 5}$ 的秩为2，求零空间的维度。</p></li><li><p>计算 $\nabla_{\mathbf{w}} \frac{1}{2}\mathbf{w}^T \mathbf{w} + \lambda |\mathbf{w}|_1$（提示：$L^1$ 项不可导，考虑次梯度）。</p></li><li><p>为什么在实践中用 <code>np.linalg.solve(A, b)</code> 而不是 <code>np.dot(np.linalg.inv(A), b)</code>？</p></li><li><p>推导逻辑回归损失函数 $L(\mathbf{w}) = -\sum_{i=1}^m [y_i \log \sigma(\mathbf{w}^T \mathbf{x}_i) + (1-y_i)\log(1-\sigma(\mathbf{w}^T \mathbf{x}_i))]$ 的梯度，其中 $\sigma(z) = 1/(1+e^{-z})$。</p></li></ol></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第01章 机器学习概览</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/ class="flex align-center"><span>第03章 SVD与矩阵分解</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#21-基础数据结构>2.1 基础数据结构</a><ul><li><a href=#标量向量矩阵与张量>标量、向量、矩阵与张量</a></li><li><a href=#核心运算>核心运算</a><ul><li><a href=#1-矩阵乘法>1. 矩阵乘法</a></li><li><a href=#2-内积点积>2. 内积（点积）</a></li><li><a href=#3-外积>3. 外积</a></li><li><a href=#4-逐元素运算hadamard积>4. 逐元素运算（Hadamard积）</a></li></ul></li><li><a href=#转置逆矩阵与伪逆>转置、逆矩阵与伪逆</a><ul><li><a href=#转置>转置</a></li><li><a href=#逆矩阵>逆矩阵</a></li><li><a href=#伪逆moore-penrose逆>伪逆（Moore-Penrose逆）</a></li></ul></li><li><a href=#迹与行列式>迹与行列式</a><ul><li><a href=#迹>迹</a></li><li><a href=#行列式>行列式</a></li></ul></li></ul></li><li><a href=#22-向量空间理解数据的结构>2.2 向量空间：理解数据的结构</a><ul><li><a href=#线性组合与张成空间>线性组合与张成空间</a></li><li><a href=#线性相关与线性无关>线性相关与线性无关</a></li><li><a href=#基与维度>基与维度</a></li><li><a href=#秩矩阵的本质维度>秩：矩阵的本质维度</a></li><li><a href=#秩-零化度定理>秩-零化度定理</a></li><li><a href=#四个基本子空间>四个基本子空间</a></li></ul></li><li><a href=#23-度量与正交>2.3 度量与正交</a><ul><li><a href=#范数测量大小>范数：测量大小</a><ul><li><a href=#lp-范数>$L^p$ 范数</a></li><li><a href=#矩阵范数>矩阵范数</a></li></ul></li><li><a href=#内积与角度>内积与角度</a></li><li><a href=#正交与正交矩阵>正交与正交矩阵</a></li></ul></li><li><a href=#24-投影线性回归的几何本质>2.4 投影：线性回归的几何本质</a><ul><li><a href=#为什么我们需要投影>为什么我们需要投影</a></li><li><a href=#从最小二乘推导投影矩阵>从最小二乘推导投影矩阵</a></li><li><a href=#投影矩阵的性质>投影矩阵的性质</a></li><li><a href=#几何直觉>几何直觉</a></li></ul></li><li><a href=#25-矩阵微积分反向传播的数学基础>2.5 矩阵微积分：反向传播的数学基础</a><ul><li><a href=#为什么需要矩阵求导>为什么需要矩阵求导</a></li><li><a href=#布局约定分母布局>布局约定：分母布局</a></li><li><a href=#标量对向量求导>标量对向量求导</a></li><li><a href=#标量对矩阵求导>标量对矩阵求导</a></li><li><a href=#向量对向量求导雅可比矩阵>向量对向量求导：雅可比矩阵</a></li><li><a href=#链式法则>链式法则</a></li><li><a href=#重要公式推导>重要公式推导</a><ul><li><a href=#推导1nabla_mathbfx-mathbfxt-mathbfa-mathbfx>推导1：$\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x})$</a></li><li><a href=#推导2fracpartialpartial-mathbfx-texttrmathbfa-mathbfx-mathbfb>推导2：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})$</a></li><li><a href=#推导3fracpartialpartial-mathbfx-texttrmathbfxt-mathbfa-mathbfx>推导3：$\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}^T \mathbf{A} \mathbf{X})$</a></li></ul></li><li><a href=#实战线性回归的梯度>实战：线性回归的梯度</a></li></ul></li><li><a href=#常用矩阵求导公式速查表>常用矩阵求导公式速查表</a></li><li><a href=#小结>小结</a></li></ul></nav></div></aside></main></body></html>