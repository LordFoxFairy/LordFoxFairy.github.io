<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第07章：支持向量机 (SVM)# “Nothing is more practical than a good theory.” —— Vladimir Vapnik
重要提示：如果说感知机是神经网络的始祖，那么支持向量机 (SVM) 就是统计学习理论的皇冠。
在深度学习爆发之前的二十年里，SVM 统治着机器学习的世界。它的强大不仅仅在于分类效果，更在于其背后坚如磐石的数学理论——VC 维理论和结构风险最小化。
本章我们将见证一个算法如何将几何直觉（最大间隔）转化为一个凸优化问题，并通过对偶性（Duality）巧妙地通过"支持向量"来稀疏化模型。这不仅是一个算法，这是数学美学的典范。
目录# 一、引言：感知机的遗憾 二、几何间隔 (Margin)：最宽的分界线 2.1 函数间隔 vs 几何间隔 2.2 最大化间隔的数学表达 2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？ 三、对偶问题 (Duality)：优雅的转换 3.1 为何要对偶？ 3.2 Lagrange 函数构建 3.3 对偶问题推导 3.4 KKT 条件与支持向量 3.5 对偶问题的物理意义 四、软间隔 (Soft Margin)：拥抱不完美 4.1 现实世界并不完美 4.2 引入松弛变量 $\xi$ 4.3 软间隔的对偶问题 4.4 Hinge Loss：打通优化视角 4.5 SVM = Hinge Loss + L2 正则化 五、SMO 算法：高效求解对偶问题 六、本章小结 七、推荐阅读 一、引言：感知机的遗憾# 在第6章中，我们学习了感知机算法。Novikoff 定理保证了只要数据线性可分，感知机就一定能找到一个分离超平面。但这个定理也暴露了一个致命的问题：
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第07章 支持向量机(SVM)"><meta property="og:description" content='第07章：支持向量机 (SVM)# “Nothing is more practical than a good theory.” —— Vladimir Vapnik
重要提示：如果说感知机是神经网络的始祖，那么支持向量机 (SVM) 就是统计学习理论的皇冠。
在深度学习爆发之前的二十年里，SVM 统治着机器学习的世界。它的强大不仅仅在于分类效果，更在于其背后坚如磐石的数学理论——VC 维理论和结构风险最小化。
本章我们将见证一个算法如何将几何直觉（最大间隔）转化为一个凸优化问题，并通过对偶性（Duality）巧妙地通过"支持向量"来稀疏化模型。这不仅是一个算法，这是数学美学的典范。
目录# 一、引言：感知机的遗憾 二、几何间隔 (Margin)：最宽的分界线 2.1 函数间隔 vs 几何间隔 2.2 最大化间隔的数学表达 2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？ 三、对偶问题 (Duality)：优雅的转换 3.1 为何要对偶？ 3.2 Lagrange 函数构建 3.3 对偶问题推导 3.4 KKT 条件与支持向量 3.5 对偶问题的物理意义 四、软间隔 (Soft Margin)：拥抱不完美 4.1 现实世界并不完美 4.2 引入松弛变量 $\xi$ 4.3 软间隔的对偶问题 4.4 Hinge Loss：打通优化视角 4.5 SVM = Hinge Loss + L2 正则化 五、SMO 算法：高效求解对偶问题 六、本章小结 七、推荐阅读 一、引言：感知机的遗憾# 在第6章中，我们学习了感知机算法。Novikoff 定理保证了只要数据线性可分，感知机就一定能找到一个分离超平面。但这个定理也暴露了一个致命的问题：'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第07章 支持向量机(SVM)"><meta itemprop=description content='第07章：支持向量机 (SVM)# “Nothing is more practical than a good theory.” —— Vladimir Vapnik
重要提示：如果说感知机是神经网络的始祖，那么支持向量机 (SVM) 就是统计学习理论的皇冠。
在深度学习爆发之前的二十年里，SVM 统治着机器学习的世界。它的强大不仅仅在于分类效果，更在于其背后坚如磐石的数学理论——VC 维理论和结构风险最小化。
本章我们将见证一个算法如何将几何直觉（最大间隔）转化为一个凸优化问题，并通过对偶性（Duality）巧妙地通过"支持向量"来稀疏化模型。这不仅是一个算法，这是数学美学的典范。
目录# 一、引言：感知机的遗憾 二、几何间隔 (Margin)：最宽的分界线 2.1 函数间隔 vs 几何间隔 2.2 最大化间隔的数学表达 2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？ 三、对偶问题 (Duality)：优雅的转换 3.1 为何要对偶？ 3.2 Lagrange 函数构建 3.3 对偶问题推导 3.4 KKT 条件与支持向量 3.5 对偶问题的物理意义 四、软间隔 (Soft Margin)：拥抱不完美 4.1 现实世界并不完美 4.2 引入松弛变量 $\xi$ 4.3 软间隔的对偶问题 4.4 Hinge Loss：打通优化视角 4.5 SVM = Hinge Loss + L2 正则化 五、SMO 算法：高效求解对偶问题 六、本章小结 七、推荐阅读 一、引言：感知机的遗憾# 在第6章中，我们学习了感知机算法。Novikoff 定理保证了只要数据线性可分，感知机就一定能找到一个分离超平面。但这个定理也暴露了一个致命的问题：'><meta itemprop=wordCount content="2595"><title>第07章 支持向量机(SVM) | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/ class=active>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第07章 支持向量机(SVM)</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一引言感知机的遗憾>一、引言：感知机的遗憾</a></li><li><a href=#二几何间隔-margin最宽的分界线>二、几何间隔 (Margin)：最宽的分界线</a><ul><li><a href=#21-函数间隔-vs-几何间隔>2.1 函数间隔 vs 几何间隔</a></li><li><a href=#22-最大化间隔的数学表达>2.2 最大化间隔的数学表达</a></li><li><a href=#23-为什么最大化间隔等价于最小化-frac12mathbfw2>2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？</a></li></ul></li><li><a href=#三对偶问题-duality优雅的转换>三、对偶问题 (Duality)：优雅的转换</a><ul><li><a href=#31-为何要对偶>3.1 为何要对偶？</a></li><li><a href=#32-lagrange-函数构建>3.2 Lagrange 函数构建</a></li><li><a href=#33-对偶问题推导>3.3 对偶问题推导</a></li><li><a href=#34-kkt-条件与支持向量>3.4 KKT 条件与支持向量</a></li><li><a href=#35-对偶问题的物理意义>3.5 对偶问题的物理意义</a></li></ul></li><li><a href=#heading>$$
b^* = \frac{1}{|SV|} \sum_{s \in SV} \left[ y_s - \sum_{i=1}^N \alpha_i^* y_i (\mathbf{x}_i^T \mathbf{x}_s) \right]
$$</a></li><li><a href=#四软间隔-soft-margin拥抱不完美>四、软间隔 (Soft Margin)：拥抱不完美</a><ul><li><a href=#41-现实世界并不完美>4.1 现实世界并不完美</a></li><li><a href=#42-引入松弛变量-xi>4.2 引入松弛变量 $\xi$</a></li><li><a href=#43-软间隔的对偶问题>4.3 软间隔的对偶问题</a></li><li><a href=#44-hinge-loss打通优化视角>4.4 Hinge Loss：打通优化视角</a></li><li><a href=#45-svm--hinge-loss--l2-正则化>4.5 SVM = Hinge Loss + L2 正则化</a></li></ul></li><li><a href=#五smo-算法高效求解对偶问题>五、SMO 算法：高效求解对偶问题</a><ul><li><a href=#51-核心思想在约束直线上跳舞>5.1 核心思想：在约束直线上跳舞</a></li><li><a href=#52-为什么每次选两个>5.2 为什么每次选两个？</a></li><li><a href=#53-算法框架简化版>5.3 算法框架（简化版）</a></li><li><a href=#54-smo-的优势>5.4 SMO 的优势</a></li><li><a href=#55-直观类比>5.5 直观类比</a></li><li><a href=#56-smo-vs-梯度下降>5.6 SMO vs 梯度下降</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a></li><li><a href=#七推荐阅读>七、推荐阅读</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第07章支持向量机-svm>第07章：支持向量机 (SVM)<a class=anchor href=#%e7%ac%ac07%e7%ab%a0%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba-svm>#</a></h1><blockquote class=book-hint><p>&ldquo;Nothing is more practical than a good theory.&rdquo; —— Vladimir Vapnik</p><p><strong>重要提示</strong>：如果说感知机是神经网络的始祖，那么支持向量机 (SVM) 就是统计学习理论的<strong>皇冠</strong>。</p><p>在深度学习爆发之前的二十年里，SVM 统治着机器学习的世界。它的强大不仅仅在于分类效果，更在于其背后坚如磐石的数学理论——<strong>VC 维理论</strong>和<strong>结构风险最小化</strong>。</p><p>本章我们将见证一个算法如何将几何直觉（最大间隔）转化为一个凸优化问题，并通过对偶性（Duality）巧妙地通过"支持向量"来稀疏化模型。这不仅是一个算法，这是数学美学的典范。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e5%bc%95%e8%a8%80%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e9%81%97%e6%86%be>一、引言：感知机的遗憾</a></li><li><a href=#%e4%ba%8c%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94-margin%e6%9c%80%e5%ae%bd%e7%9a%84%e5%88%86%e7%95%8c%e7%ba%bf>二、几何间隔 (Margin)：最宽的分界线</a><ul><li><a href=#21-%e5%87%bd%e6%95%b0%e9%97%b4%e9%9a%94-vs-%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94>2.1 函数间隔 vs 几何间隔</a></li><li><a href=#22-%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%9a%84%e6%95%b0%e5%ad%a6%e8%a1%a8%e8%be%be>2.2 最大化间隔的数学表达</a></li><li><a href=#23-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%ad%89%e4%bb%b7%e4%ba%8e%e6%9c%80%e5%b0%8f%e5%8c%96-frac12%5cmathbf%7bw%7d2>2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？</a></li></ul></li><li><a href=#%e4%b8%89%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98-duality%e4%bc%98%e9%9b%85%e7%9a%84%e8%bd%ac%e6%8d%a2>三、对偶问题 (Duality)：优雅的转换</a><ul><li><a href=#31-%e4%b8%ba%e4%bd%95%e8%a6%81%e5%af%b9%e5%81%b6>3.1 为何要对偶？</a></li><li><a href=#32-lagrange-%e5%87%bd%e6%95%b0%e6%9e%84%e5%bb%ba>3.2 Lagrange 函数构建</a></li><li><a href=#33-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e6%8e%a8%e5%af%bc>3.3 对偶问题推导</a></li><li><a href=#34-kkt-%e6%9d%a1%e4%bb%b6%e4%b8%8e%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f>3.4 KKT 条件与支持向量</a></li><li><a href=#35-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89>3.5 对偶问题的物理意义</a></li></ul></li><li><a href=#%e5%9b%9b%e8%bd%af%e9%97%b4%e9%9a%94-soft-margin%e6%8b%a5%e6%8a%b1%e4%b8%8d%e5%ae%8c%e7%be%8e>四、软间隔 (Soft Margin)：拥抱不完美</a><ul><li><a href=#41-%e7%8e%b0%e5%ae%9e%e4%b8%96%e7%95%8c%e5%b9%b6%e4%b8%8d%e5%ae%8c%e7%be%8e>4.1 现实世界并不完美</a></li><li><a href=#42-%e5%bc%95%e5%85%a5%e6%9d%be%e5%bc%9b%e5%8f%98%e9%87%8f-xi>4.2 引入松弛变量 $\xi$</a></li><li><a href=#43-%e8%bd%af%e9%97%b4%e9%9a%94%e7%9a%84%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98>4.3 软间隔的对偶问题</a></li><li><a href=#44-hinge-loss%e6%89%93%e9%80%9a%e4%bc%98%e5%8c%96%e8%a7%86%e8%a7%92>4.4 Hinge Loss：打通优化视角</a></li><li><a href=#45-svm--hinge-loss--l2-%e6%ad%a3%e5%88%99%e5%8c%96>4.5 SVM = Hinge Loss + L2 正则化</a></li></ul></li><li><a href=#%e4%ba%94smo-%e7%ae%97%e6%b3%95%e9%ab%98%e6%95%88%e6%b1%82%e8%a7%a3%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98>五、SMO 算法：高效求解对偶问题</a></li><li><a href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>六、本章小结</a></li><li><a href=#%e4%b8%83%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb>七、推荐阅读</a></li></ul><hr><h2 id=一引言感知机的遗憾>一、引言：感知机的遗憾<a class=anchor href=#%e4%b8%80%e5%bc%95%e8%a8%80%e6%84%9f%e7%9f%a5%e6%9c%ba%e7%9a%84%e9%81%97%e6%86%be>#</a></h2><p>在第6章中，我们学习了感知机算法。Novikoff 定理保证了只要数据线性可分，感知机就一定能找到一个分离超平面。但这个定理也暴露了一个致命的问题：</p><p><strong>感知机的解不唯一！</strong></p><p>对于同一个数据集，根据样本访问顺序和初始化的不同，感知机可能收敛到完全不同的超平面。这些超平面虽然都能正确分类训练数据，但它们的泛化能力可能天差地别。</p><p><img src=assets/perceptron_vs_svm.svg alt="SVM vs Perceptron Solutions"></p><p><strong>核心洞见</strong>：我们不仅要找一个"能分开"的超平面，更要找"分得最宽"的那个！</p><p>这就是 <strong>最大间隔 (Maximum Margin)</strong> 的思想——SVM 的灵魂。</p><hr><h2 id=二几何间隔-margin最宽的分界线>二、几何间隔 (Margin)：最宽的分界线<a class=anchor href=#%e4%ba%8c%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94-margin%e6%9c%80%e5%ae%bd%e7%9a%84%e5%88%86%e7%95%8c%e7%ba%bf>#</a></h2><h3 id=21-函数间隔-vs-几何间隔>2.1 函数间隔 vs 几何间隔<a class=anchor href=#21-%e5%87%bd%e6%95%b0%e9%97%b4%e9%9a%94-vs-%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94>#</a></h3><p>给定训练样本 $(\mathbf{x}_i, y_i)$ 和超平面 $(\mathbf{w}, b)$，我们定义：</p><p><strong>函数间隔 (Functional Margin)</strong>：</p><p>$$
\hat{\gamma}_i = y_i (\mathbf{w}^T \mathbf{x}_i + b)
$$</p><p><strong>物理意义</strong>：</p><ul><li>如果 $y_i = +1$ 且 $\mathbf{w}^T \mathbf{x}_i + b > 0$，则 $\hat{\gamma}_i > 0$（分类正确）</li><li>如果 $y_i = -1$ 且 $\mathbf{w}^T \mathbf{x}_i + b &lt; 0$，则 $\hat{\gamma}_i > 0$（分类正确）</li><li>$\hat{\gamma}_i$ 越大，分类越"自信"</li></ul><p>但函数间隔有个致命问题：<strong>它不具有尺度不变性</strong>。如果我们把 $(\mathbf{w}, b)$ 同时放大 2 倍，超平面没有变化，但函数间隔翻倍了！</p><p><strong>几何间隔 (Geometric Margin)</strong>：</p><p>$$
\gamma_i = \frac{y_i (\mathbf{w}^T \mathbf{x}_i + b)}{|\mathbf{w}|} = \frac{\hat{\gamma}_i}{|\mathbf{w}|}
$$</p><p>这正是样本点 $\mathbf{x}_i$ 到超平面的<strong>带符号距离</strong> (Signed Distance)。</p><blockquote class=book-hint><p><strong>关键</strong>：几何间隔具有尺度不变性——$(\mathbf{w}, b)$ 放大 $k$ 倍，$\gamma_i$ 不变。</p></blockquote><p>对于整个数据集，定义<strong>间隔 (Margin)</strong> 为所有样本中的最小几何间隔：</p><p>$$
\gamma = \min_{i=1,\dots,N} \gamma_i = \min_{i=1,\dots,N} \frac{y_i (\mathbf{w}^T \mathbf{x}_i + b)}{|\mathbf{w}|}
$$</p><hr><h3 id=22-最大化间隔的数学表达>2.2 最大化间隔的数学表达<a class=anchor href=#22-%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%9a%84%e6%95%b0%e5%ad%a6%e8%a1%a8%e8%be%be>#</a></h3><p>SVM 的核心思想是：<strong>找到使间隔最大的超平面</strong>。</p><p>$$
\begin{aligned}
\max_{\mathbf{w}, b} \quad & \gamma \
\text{s.t.} \quad & \frac{y_i (\mathbf{w}^T \mathbf{x}_i + b)}{|\mathbf{w}|} \geq \gamma, \quad i = 1, \dots, N
\end{aligned}
$$</p><p>等价于：</p><p>$$
\begin{aligned}
\max_{\mathbf{w}, b} \quad & \frac{\gamma}{|\mathbf{w}|} \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq \hat{\gamma}, \quad i = 1, \dots, N
\end{aligned}
$$</p><p>这里 $\hat{\gamma} = \gamma |\mathbf{w}|$ 是函数间隔。</p><hr><h3 id=23-为什么最大化间隔等价于最小化-frac12mathbfw2>2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？<a class=anchor href=#23-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%80%e5%a4%a7%e5%8c%96%e9%97%b4%e9%9a%94%e7%ad%89%e4%bb%b7%e4%ba%8e%e6%9c%80%e5%b0%8f%e5%8c%96-frac12mathbfw2>#</a></h3><blockquote class=book-hint><p><strong>核心问题</strong>：如何把上面的优化问题转化为标准形式？</p></blockquote><p><strong>Step 1：利用尺度不变性</strong></p><p>由于 $(\mathbf{w}, b)$ 放大 $k$ 倍不改变超平面，我们可以<strong>固定函数间隔 $\hat{\gamma} = 1$</strong>。这是一个技巧性的约定，但不失一般性。</p><blockquote class=book-hint><p><strong>关键洞见</strong>：此时几何间隔变为
$$
\gamma = \frac{\hat{\gamma}}{|\mathbf{w}|} = \frac{1}{|\mathbf{w}|}
$$
因此，<strong>最大化间隔 $\gamma$ 就等价于最大化 $\frac{1}{|\mathbf{w}|}$，也就是最小化 $|\mathbf{w}|$</strong>！</p></blockquote><p>此时问题变为：</p><p>$$
\begin{aligned}
\max_{\mathbf{w}, b} \quad & \frac{1}{|\mathbf{w}|} \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, \dots, N
\end{aligned}
$$</p><p><strong>Step 2：转化为最小化问题</strong></p><p>最大化 $\frac{1}{|\mathbf{w}|}$ 等价于最小化 $|\mathbf{w}|$，进一步等价于最小化 $|\mathbf{w}|^2$（平方不改变单调性，但使问题可微）：</p><p>$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} |\mathbf{w}|^2 \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, \dots, N
\end{aligned}
$$</p><blockquote class=book-hint><p><strong>为什么加系数 $\frac{1}{2}$？</strong></p><p>纯粹是为了求导方便：$\frac{d}{dw} \frac{1}{2}|\mathbf{w}|^2 = \mathbf{w}$，不需要系数 2。</p></blockquote><hr><p><strong>推导小结</strong>：</p><p>$$
\boxed{
\begin{aligned}
\text{最大化间隔} \quad & \Leftrightarrow \quad \max \frac{1}{|\mathbf{w}|} \
& \Leftrightarrow \quad \min |\mathbf{w}| \
& \Leftrightarrow \quad \min \frac{1}{2} |\mathbf{w}|^2
\end{aligned}
}
$$</p><p>这个优化问题称为 <strong>硬间隔 SVM (Hard-Margin SVM)</strong> 的原始问题 (Primal Problem)。</p><p><img src=assets/svm_margin_geometry.svg alt="SVM Margin Geometry"></p><p><strong>几何直觉</strong>：</p><ul><li>约束条件 $y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1$ 确保所有点都被正确分类，且至少有距离 $\frac{1}{|\mathbf{w}|}$</li><li>最小化 $|\mathbf{w}|^2$ 就是最大化间隔 $\frac{1}{|\mathbf{w}|}$</li><li>图中绿色圆圈标记的点是<strong>支持向量</strong>，它们恰好落在间隔边界上</li></ul><hr><h2 id=三对偶问题-duality优雅的转换>三、对偶问题 (Duality)：优雅的转换<a class=anchor href=#%e4%b8%89%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98-duality%e4%bc%98%e9%9b%85%e7%9a%84%e8%bd%ac%e6%8d%a2>#</a></h2><h3 id=31-为何要对偶>3.1 为何要对偶？<a class=anchor href=#31-%e4%b8%ba%e4%bd%95%e8%a6%81%e5%af%b9%e5%81%b6>#</a></h3><p>原始问题已经是一个凸二次规划 (Convex QP)，可以直接求解。那为什么还要转换到对偶问题呢？</p><p><strong>两大理由</strong>：</p><ol><li><p><strong>约束优化更容易解</strong>：</p><ul><li>原始问题有 $N$ 个不等式约束（每个样本一个），优化变量是 $d+1$ 维的 $(\mathbf{w}, b)$</li><li>对偶问题虽然有 $N$ 个变量（每个样本一个 Lagrange 乘子 $\alpha_i$），但约束更简单（箱约束 $\alpha_i \geq 0$）</li><li>现有的 QP Solver（如 SMO 算法）对对偶问题更高效</li></ul></li><li><p><strong>自然引入核函数</strong>：</p><ul><li>对偶问题的解只依赖于样本间的<strong>内积</strong> $\mathbf{x}_i^T \mathbf{x}_j$</li><li>这为核技巧 (Kernel Trick) 打开了大门——我们可以把 $\mathbf{x}_i^T \mathbf{x}_j$ 替换为 $K(\mathbf{x}_i, \mathbf{x}_j)$，从而在高维甚至无限维空间中工作，而无需显式计算特征映射</li></ul></li></ol><hr><h3 id=32-lagrange-函数构建>3.2 Lagrange 函数构建<a class=anchor href=#32-lagrange-%e5%87%bd%e6%95%b0%e6%9e%84%e5%bb%ba>#</a></h3><p>对于带不等式约束的优化问题：</p><p>$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} |\mathbf{w}|^2 \
\text{s.t.} \quad & 1 - y_i (\mathbf{w}^T \mathbf{x}_i + b) \leq 0, \quad i = 1, \dots, N
\end{aligned}
$$</p><p>引入 Lagrange 乘子 $\alpha_i \geq 0$，构造 Lagrange 函数：</p><p>$$
L(\mathbf{w}, b, \alpha) = \frac{1}{2} |\mathbf{w}|^2 - \sum_{i=1}^N \alpha_i \left[ y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1 \right]
$$</p><p><strong>物理意义</strong>：</p><ul><li>$\frac{1}{2}|\mathbf{w}|^2$ 是目标函数（要最小化）</li><li>$-\alpha_i [y_i(\mathbf{w}^T \mathbf{x}_i + b) - 1]$ 是约束的惩罚项<ul><li>当约束满足时 $y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1$，这一项 $\leq 0$</li><li>当约束违反时，$\alpha_i$ 增大，惩罚增大</li></ul></li></ul><hr><h3 id=33-对偶问题推导>3.3 对偶问题推导<a class=anchor href=#33-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e6%8e%a8%e5%af%bc>#</a></h3><p>根据 Lagrange 对偶理论，原始问题等价于：</p><p>$$
\min_{\mathbf{w}, b} \max_{\alpha_i \geq 0} L(\mathbf{w}, b, \alpha)
$$</p><p>对偶问题是：</p><p>$$
\max_{\alpha_i \geq 0} \min_{\mathbf{w}, b} L(\mathbf{w}, b, \alpha)
$$</p><blockquote class=book-hint><p><strong>强对偶性</strong>：对于凸优化问题且满足 Slater 条件（存在严格可行解），强对偶性成立，即两者最优值相等。SVM 满足这些条件。</p></blockquote><hr><p><strong>Step 1：对 $\mathbf{w}$ 和 $b$ 求偏导</strong></p><p>$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{w}} &= \mathbf{w} - \sum_{i=1}^N \alpha_i y_i \mathbf{x}<em>i = 0 \
\frac{\partial L}{\partial b} &= -\sum</em>{i=1}^N \alpha_i y_i = 0
\end{aligned}
$$</p><p>得到：</p><p>$$
\boxed{
\begin{aligned}
\mathbf{w} &= \sum_{i=1}^N \alpha_i y_i \mathbf{x}<em>i \
\sum</em>{i=1}^N \alpha_i y_i &= 0
\end{aligned}
}
$$</p><blockquote class=book-hint><p><strong>核心洞见</strong>：最优权重 $\mathbf{w}$ 是训练样本的<strong>线性组合</strong>！系数 $\alpha_i$ 衡量样本 $i$ 的"重要性"。</p></blockquote><hr><p><strong>Step 2：代入 Lagrange 函数</strong></p><p>将 $\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$ 代入 $L(\mathbf{w}, b, \alpha)$：</p><p>$$
\begin{aligned}
L &= \frac{1}{2} \left| \sum_{i=1}^N \alpha_i y_i \mathbf{x}<em>i \right|^2 - \sum</em>{i=1}^N \alpha_i y_i \left( \sum_{j=1}^N \alpha_j y_j \mathbf{x}_j^T \mathbf{x}<em>i + b \right) + \sum</em>{i=1}^N \alpha_i
\end{aligned}
$$</p><p>展开第一项：</p><p>$$
\left| \sum_{i=1}^N \alpha_i y_i \mathbf{x}<em>i \right|^2 = \sum</em>{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$</p><p>展开第二项：</p><p>$$
\sum_{i=1}^N \alpha_i y_i \sum_{j=1}^N \alpha_j y_j \mathbf{x}<em>j^T \mathbf{x}<em>i = \sum</em>{i=1}^N \sum</em>{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$</p><p>第二项中的 $b$ 项：</p><p>$$
\sum_{i=1}^N \alpha_i y_i b = b \sum_{i=1}^N \alpha_i y_i = 0 \quad \text{(由约束 $\sum_i \alpha_i y_i = 0$)}
$$</p><p>因此：</p><p>$$
L = \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}<em>j) - \sum</em>{i,j} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j) + \sum_i \alpha_i
$$</p><p>$$
= \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$</p><hr><p><strong>对偶问题 (Dual Problem)</strong>：</p><p>$$
\boxed{
\begin{aligned}
\max_{\alpha} \quad & \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}<em>j) \
\text{s.t.} \quad & \sum</em>{i=1}^N \alpha_i y_i = 0 \
& \alpha_i \geq 0, \quad i = 1, \dots, N
\end{aligned}
}
$$</p><p>或者写成最小化形式：</p><p>$$
\boxed{
\begin{aligned}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}<em>i^T \mathbf{x}<em>j) - \sum</em>{i=1}^N \alpha_i \
\text{s.t.} \quad & \sum</em>{i=1}^N \alpha_i y_i = 0 \
& \alpha_i \geq 0, \quad i = 1, \dots, N
\end{aligned}
}
$$</p><hr><h3 id=34-kkt-条件与支持向量>3.4 KKT 条件与支持向量<a class=anchor href=#34-kkt-%e6%9d%a1%e4%bb%b6%e4%b8%8e%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f>#</a></h3><p>对于最优解 $(\mathbf{w}^<em>, b^</em>, \alpha^*)$，必须满足 <strong>KKT (Karush-Kuhn-Tucker) 条件</strong>：</p><ol><li><p><strong>梯度条件</strong>：
$$
\begin{aligned}
\nabla_\mathbf{w} L &= 0 \quad \Rightarrow \quad \mathbf{w}^* = \sum_{i=1}^N \alpha_i^* y_i \mathbf{x}<em>i \
\frac{\partial L}{\partial b} &= 0 \quad \Rightarrow \quad \sum</em>{i=1}^N \alpha_i^* y_i = 0
\end{aligned}
$$</p></li><li><p><strong>原始可行性</strong>：
$$
y_i (\mathbf{w}^{<em>T} \mathbf{x}_i + b^</em>) \geq 1, \quad \forall i
$$</p></li><li><p><strong>对偶可行性</strong>：
$$
\alpha_i^* \geq 0, \quad \forall i
$$</p></li><li><p><strong>互补松弛性 (Complementary Slackness)</strong>：
$$
\boxed{\alpha_i^* \left[ y_i (\mathbf{w}^{<em>T} \mathbf{x}_i + b^</em>) - 1 \right] = 0, \quad \forall i}
$$</p></li></ol><p><strong>互补松弛性的物理意义</strong>：</p><p><img src=assets/svm_kkt_geometric.svg alt=KKT条件的几何意义></p><p>这个条件将样本分为两类：</p><ul><li><p><em><em>$\alpha_i^</em> = 0$</em>*：样本 $i$ 的约束不起作用，$y_i (\mathbf{w}^{<em>T} \mathbf{x}_i + b^</em>) > 1$（点在间隔边界外侧）</p><ul><li>这些点对模型没有贡献（$\mathbf{w}^* = \sum \alpha_i y_i \mathbf{x}_i$ 中 $\alpha_i = 0$）</li><li>可以删除这些样本，模型不变</li></ul></li><li><p><em><em>$\alpha_i^</em> > 0$</em>*：必须 $y_i (\mathbf{w}^{<em>T} \mathbf{x}_i + b^</em>) = 1$（点恰好在间隔边界上）</p><ul><li>这些点称为 <strong>支持向量 (Support Vectors)</strong></li><li>它们"支撑"着分离超平面，决定了间隔的大小</li><li>移动支持向量会改变超平面，移动其他点则不会</li></ul></li></ul><hr><p><strong>支持向量的稀疏性</strong>：</p><p>在实践中，通常只有很少一部分样本是支持向量（$\alpha_i > 0$）。这带来两大好处：</p><ol><li><strong>模型稀疏</strong>：$\mathbf{w} = \sum_{i \in SV} \alpha_i y_i \mathbf{x}_i$，只需要存储支持向量</li><li><strong>预测高效</strong>：$f(x) = \sum_{i \in SV} \alpha_i y_i (\mathbf{x}_i^T \mathbf{x}) + b$，只需计算与支持向量的内积</li></ol><blockquote class=book-hint><p><strong>比喻</strong>：在一群人中选代表投票，只有站在边界上的人（支持向量）有投票权，远离边界的人可以回家睡觉了。</p></blockquote><hr><h3 id=35-对偶问题的物理意义>3.5 对偶问题的物理意义<a class=anchor href=#35-%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98%e7%9a%84%e7%89%a9%e7%90%86%e6%84%8f%e4%b9%89>#</a></h3><p>让我们重新审视对偶问题的目标函数：</p><p>$$
\max_{\alpha} \quad \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$
<strong>第一项 $\sum_i \alpha_i$</strong>：</p><ul><li>鼓励 $\alpha_i$ 变大</li><li>物理意义：每个样本都"想要"贡献更多权重</li></ul><p><strong>第二项 $-\frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)$</strong>：</p><ul><li>惩罚同类样本之间的权重组合（$y_i = y_j$ 时，$y_i y_j = +1$，内积越大惩罚越大）</li><li>鼓励异类样本之间的权重组合（$y_i \neq y_j$ 时，$y_i y_j = -1$）</li><li>物理意义：模型倾向于选择"有代表性"的样本（不重复、分散）</li></ul><p><strong>约束 $\sum_i \alpha_i y_i = 0$</strong>：</p><ul><li>正负样本的权重必须平衡</li><li>防止模型偏向某一类</li></ul><hr><p><strong>计算 $b^*$</strong>：</p><p>对于任意支持向量 $\mathbf{x}_s$（满足 $\alpha_s > 0$），有：</p><p>$$
y_s (\mathbf{w}^{<em>T} \mathbf{x}_s + b^</em>) = 1
$$</p><p>因此：</p><p>$$
b^* = y_s - \mathbf{w}^{<em>T} \mathbf{x}<em>s = y_s - \sum</em>{i=1}^N \alpha_i^</em> y_i (\mathbf{x}_i^T \mathbf{x}_s)
$$</p><p>实践中通常对所有支持向量取平均以提高数值稳定性：</p><h2 id=heading>$$
b^* = \frac{1}{|SV|} \sum_{s \in SV} \left[ y_s - \sum_{i=1}^N \alpha_i^* y_i (\mathbf{x}_i^T \mathbf{x}_s) \right]
$$<a class=anchor href=#heading>#</a></h2><h2 id=四软间隔-soft-margin拥抱不完美>四、软间隔 (Soft Margin)：拥抱不完美<a class=anchor href=#%e5%9b%9b%e8%bd%af%e9%97%b4%e9%9a%94-soft-margin%e6%8b%a5%e6%8a%b1%e4%b8%8d%e5%ae%8c%e7%be%8e>#</a></h2><h3 id=41-现实世界并不完美>4.1 现实世界并不完美<a class=anchor href=#41-%e7%8e%b0%e5%ae%9e%e4%b8%96%e7%95%8c%e5%b9%b6%e4%b8%8d%e5%ae%8c%e7%be%8e>#</a></h3><p>硬间隔 SVM 要求数据<strong>完全线性可分</strong>，即存在超平面将两类数据完美分开。但现实世界往往存在：</p><ol><li><strong>噪声 (Noise)</strong>：标注错误的样本</li><li><strong>离群点 (Outliers)</strong>：远离主流分布的样本</li><li><strong>本质不可分</strong>：两类数据分布重叠</li></ol><p>如果强行使用硬间隔 SVM：</p><ul><li>可能无解（约束不可行）</li><li>或者得到间隔极小的超平面（过拟合）</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>     完美世界 (Hard Margin)          现实世界 (Soft Margin)
</span></span><span class=line><span class=cl>         +  |  -                         +  |  -
</span></span><span class=line><span class=cl>         +  |  -                         + ×|  -    ← 噪声点
</span></span><span class=line><span class=cl>         +  |  -                         +  |  -
</span></span><span class=line><span class=cl>         +  |  -                         +  | ×-    ← 离群点
</span></span><span class=line><span class=cl>    ──────────────                  ──────────────
</span></span><span class=line><span class=cl>    所有点都满足间隔               允许少数点违反间隔</span></span></code></pre></div><p><strong>核心思想</strong>：我们需要一种机制，允许少数样本违反间隔约束，但要为这种违反付出代价。</p><hr><h3 id=42-引入松弛变量-xi>4.2 引入松弛变量 $\xi$<a class=anchor href=#42-%e5%bc%95%e5%85%a5%e6%9d%be%e5%bc%9b%e5%8f%98%e9%87%8f-xi>#</a></h3><p>对于每个样本 $(\mathbf{x}_i, y_i)$，引入<strong>松弛变量 (Slack Variable)</strong> $\xi_i \geq 0$，放宽约束：</p><p>$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i
$$
<strong>物理意义</strong>：</p><ul><li>$\xi_i = 0$：样本正确分类且满足间隔（$y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1$）</li><li>$0 &lt; \xi_i \leq 1$：样本正确分类但在间隔内（$0 &lt; y_i (\mathbf{w}^T \mathbf{x}_i + b) &lt; 1$）</li><li>$\xi_i > 1$：样本被误分类（$y_i (\mathbf{w}^T \mathbf{x}_i + b) &lt; 0$）</li></ul><p><img src=assets/soft_margin_slack.svg alt="Soft Margin and Slack Variables"></p><p>上图直观展示了松弛变量 $\xi$ 的几何含义：</p><ul><li><strong>橙色边框的点</strong>：进入间隔但未跨越决策边界（$0 &lt; \xi_1 &lt; 1$）</li><li><strong>深红色边框的点</strong>：跨越决策边界被误分类（$\xi_2 > 1$）</li><li>红色线段的长度正是松弛变量 $\xi$ 的大小，代表违规的程度</li></ul><p><strong>软间隔 SVM 的原始问题</strong>：</p><p>$$
\boxed{
\begin{aligned}
\min_{\mathbf{w}, b, \xi} \quad & \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^N \xi_i \
\text{s.t.} \quad & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, \dots, N \
& \xi_i \geq 0, \quad i = 1, \dots, N
\end{aligned}
}
$$</p><p><strong>目标函数解读</strong>：</p><p>$$
\frac{1}{2}|\mathbf{w}|^2 + C \sum_{i=1}^N \xi_i
$$</p><ul><li><strong>第一项 $\frac{1}{2}|\mathbf{w}|^2$</strong>：最大化间隔（结构风险）</li><li><strong>第二项 $C \sum_i \xi_i$</strong>：最小化违反间隔的总量（经验风险）</li><li><strong>$C > 0$</strong>：权衡参数<ul><li>$C$ 很大：严惩违反，接近硬间隔（容易过拟合）</li><li>$C$ 很小：容忍违反，间隔更大（容易欠拟合）</li></ul></li></ul><blockquote class=book-hint><p><strong>连接到第1章</strong>：这正是<strong>结构风险最小化 (SRM)</strong> 的体现！</p><p>$$
\text{结构风险} = \underbrace{\sum_i \xi_i}<em>{\text{经验风险}} + \underbrace{\frac{1}{2C} |\mathbf{w}|^2}</em>{\text{正则化项}}
$$</p></blockquote><hr><h3 id=43-软间隔的对偶问题>4.3 软间隔的对偶问题<a class=anchor href=#43-%e8%bd%af%e9%97%b4%e9%9a%94%e7%9a%84%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98>#</a></h3><p>构造 Lagrange 函数（引入乘子 $\alpha_i \geq 0$ 和 $\mu_i \geq 0$）：</p><p>$$
L(\mathbf{w}, b, \xi, \alpha, \mu) = \frac{1}{2}|\mathbf{w}|^2 + C \sum_i \xi_i - \sum_i \alpha_i [y_i(\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] - \sum_i \mu_i \xi_i
$$</p><p>对 $\mathbf{w}, b, \xi$ 求偏导并令为零：</p><p>$$
\begin{aligned}
\frac{\partial L}{\partial \mathbf{w}} &= \mathbf{w} - \sum_i \alpha_i y_i \mathbf{x}_i = 0 \quad &\Rightarrow \quad \mathbf{w} = \sum_i \alpha_i y_i \mathbf{x}_i \
\frac{\partial L}{\partial b} &= -\sum_i \alpha_i y_i = 0 \quad &\Rightarrow \quad \sum_i \alpha_i y_i = 0 \
\frac{\partial L}{\partial \xi_i} &= C - \alpha_i - \mu_i = 0 \quad &\Rightarrow \quad \alpha_i + \mu_i = C
\end{aligned}
$$</p><p><strong>关键推导</strong>：第三个等式 $\alpha_i + \mu_i = C$ 来自对松弛变量 $\xi_i$ 的求导：</p><ul><li>Lagrange 函数中，$\xi_i$ 的系数为 $C - \alpha_i - \mu_i$</li><li>令 $\frac{\partial L}{\partial \xi_i} = 0$，得到 $\alpha_i + \mu_i = C$</li><li>结合约束 $\alpha_i \geq 0$ 和 $\mu_i \geq 0$，我们得到：</li></ul><p>$$
\boxed{0 \leq \alpha_i \leq C}
$$</p><blockquote class=book-hint><p><strong>物理意义</strong>：参数 $C$ 为 Lagrange 乘子 $\alpha_i$ 设定了<strong>上界</strong>！</p><ul><li>$\alpha_i = 0$：$\mu_i = C$（非支持向量）</li><li>$0 &lt; \alpha_i &lt; C$：$\mu_i > 0$，由 $\mu_i \xi_i = 0$ 得 $\xi_i = 0$（边界上的支持向量）</li><li>$\alpha_i = C$：$\mu_i = 0$，$\xi_i$ 可以 $> 0$（间隔内或误分类的支持向量）</li></ul></blockquote><p>代入 Lagrange 函数，得到与硬间隔相同的对偶目标函数，只是约束变为<strong>箱约束 (Box Constraint)</strong>：</p><p>$$
\boxed{
\begin{aligned}
\max_{\alpha} \quad & \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}<em>j) \
\text{s.t.} \quad & \sum</em>{i=1}^N \alpha_i y_i = 0 \
& 0 \leq \alpha_i \leq C, \quad i = 1, \dots, N
\end{aligned}
}
$$</p><p><strong>KKT 条件</strong>：</p><p>$$
\begin{aligned}
\alpha_i [y_i(\mathbf{w}^T \mathbf{x}_i + b) - 1 + \xi_i] &= 0 \
\mu_i \xi_i &= 0 \
\mu_i &= C - \alpha_i
\end{aligned}
$$</p><p><strong>样本分类</strong>（根据 $\alpha_i$ 的值）：</p><ol><li><p><strong>$\alpha_i = 0$</strong>：</p><ul><li>$y_i(\mathbf{w}^T \mathbf{x}_i + b) > 1$，样本在间隔外侧，正确分类</li><li>不是支持向量</li></ul></li><li><p><strong>$0 &lt; \alpha_i &lt; C$</strong>：</p><ul><li>$\mu_i = C - \alpha_i > 0$，由 $\mu_i \xi_i = 0$ 得 $\xi_i = 0$</li><li>$y_i(\mathbf{w}^T \mathbf{x}_i + b) = 1$，样本在间隔边界上</li><li><strong>支持向量</strong>（用于计算 $b$）</li></ul></li><li><p><strong>$\alpha_i = C$</strong>：</p><ul><li>$\mu_i = 0$，$\xi_i$ 可以大于 0</li><li>$y_i(\mathbf{w}^T \mathbf{x}_i + b) = 1 - \xi_i &lt; 1$</li><li>若 $\xi_i \leq 1$：样本在间隔内但正确分类</li><li>若 $\xi_i > 1$：样本被误分类</li><li><strong>支持向量</strong>（但不用于计算 $b$）</li></ul></li></ol><hr><h3 id=44-hinge-loss打通优化视角>4.4 Hinge Loss：打通优化视角<a class=anchor href=#44-hinge-loss%e6%89%93%e9%80%9a%e4%bc%98%e5%8c%96%e8%a7%86%e8%a7%92>#</a></h3><p>现在我们从另一个角度理解 SVM——<strong>损失函数 + 正则化</strong>的框架。</p><p><strong>Hinge Loss 定义</strong>：</p><p>$$
\ell_{\text{hinge}}(y, f(x)) = \max(0, 1 - y \cdot f(x)) = [1 - y \cdot f(x)]_+
$$</p><p>其中 $f(x) = \mathbf{w}^T \mathbf{x} + b$ 是决策函数（未经 sign）。</p><p><strong>物理意义</strong>：</p><ul><li>如果 $y \cdot f(x) \geq 1$（正确分类且满足间隔）：损失为 0</li><li>如果 $y \cdot f(x) &lt; 1$（违反间隔或误分类）：损失为 $1 - y \cdot f(x)$</li></ul><p><img src=assets/hinge_loss.svg alt="Hinge Loss vs 0-1 Loss"></p><p><strong>Hinge Loss vs Logistic Loss 对比</strong>：</p><p><img src=assets/hinge_vs_logistic.svg alt="Hinge vs Logistic"></p><p><strong>关键观察</strong>：Hinge Loss 等价于松弛变量！</p><p>对于约束 $y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i$ 和 $\xi_i \geq 0$，有：</p><p>$$
\xi_i \geq \max(0, 1 - y_i(\mathbf{w}^T \mathbf{x}_i + b)) = [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+
$$</p><p>当最小化 $\sum_i \xi_i$ 时，最优解必然是：</p><p>$$
\xi_i^* = [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+
$$</p><hr><h3 id=45-svm--hinge-loss--l2-正则化>4.5 SVM = Hinge Loss + L2 正则化<a class=anchor href=#45-svm--hinge-loss--l2-%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h3><p>将 $\xi_i = [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+$ 代入软间隔 SVM 的目标函数：</p><p>$$
\min_{\mathbf{w}, b} \quad \frac{1}{2}|\mathbf{w}|^2 + C \sum_{i=1}^N [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+
$$</p><p>改写为：</p><p>$$
\boxed{
\min_{\mathbf{w}, b} \quad \underbrace{\sum_{i=1}^N [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+}<em>{\text{经验风险：Hinge Loss}} + \underbrace{\frac{\lambda}{2} |\mathbf{w}|^2}</em>{\text{正则化：L2}}
}
$$</p><p>其中 $\lambda = \frac{1}{C}$。</p><blockquote class=book-hint><p><strong>惊人的结论</strong>：</p><p><strong>SVM 的本质就是 Hinge Loss + L2 正则化！</strong></p><p>这完全符合第1章的"统计学习三要素"框架：</p><ul><li><strong>模型</strong>：线性分类器 $f(x) = \mathbf{w}^T \mathbf{x} + b$</li><li><strong>策略</strong>：Hinge Loss + 正则化</li><li><strong>算法</strong>：通过对偶问题用 SMO 求解</li></ul></blockquote><p><strong>与逻辑回归对比</strong>：</p><table><thead><tr><th>模型</th><th>损失函数</th><th>正则化</th><th>优化</th></tr></thead><tbody><tr><td><strong>逻辑回归</strong></td><td>Cross-Entropy Loss</td><td>L2 (Ridge)</td><td>梯度下降</td></tr><tr><td><strong>SVM</strong></td><td>Hinge Loss</td><td>L2</td><td>对偶 QP (SMO)</td></tr></tbody></table><p>两者都是线性分类器，区别仅在于损失函数的选择！</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>损失函数对比：
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>loss
</span></span><span class=line><span class=cl>  ^
</span></span><span class=line><span class=cl>  |   Cross-Entropy (光滑)
</span></span><span class=line><span class=cl>  |        ___---
</span></span><span class=line><span class=cl>  |   ___/
</span></span><span class=line><span class=cl>  | _/    Hinge (折线)
</span></span><span class=line><span class=cl>  |/        /
</span></span><span class=line><span class=cl>  |        /
</span></span><span class=line><span class=cl>  |───────/────→  y·f(x)
</span></span><span class=line><span class=cl> -2  -1  0  1  2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Hinge Loss 在 y·f(x) ≥ 1 时损失为 0（稀疏性）
</span></span><span class=line><span class=cl>Cross-Entropy 永远不为 0（所有样本都贡献）</span></span></code></pre></div><hr><p><strong>为什么 SVM 能产生稀疏解（支持向量）？</strong></p><p>关键在于 Hinge Loss 的<strong>零区域</strong>：</p><ul><li>当 $y_i(\mathbf{w}^T \mathbf{x}<em>i + b) \geq 1$ 时，$\ell</em>{\text{hinge}} = 0$，该样本对梯度没有贡献</li><li>对应地，对偶问题中 $\alpha_i = 0$（非支持向量）</li><li>只有在间隔边界或违反间隔的样本（$y_i(\mathbf{w}^T \mathbf{x}_i + b) \leq 1$）才是支持向量</li></ul><p>相比之下，Cross-Entropy Loss 永远 $> 0$，所有样本都有贡献，无法产生稀疏性。</p><hr><h2 id=五smo-算法高效求解对偶问题>五、SMO 算法：高效求解对偶问题<a class=anchor href=#%e4%ba%94smo-%e7%ae%97%e6%b3%95%e9%ab%98%e6%95%88%e6%b1%82%e8%a7%a3%e5%af%b9%e5%81%b6%e9%97%ae%e9%a2%98>#</a></h2><p>对偶问题是一个二次规划 (QP)，理论上可以用通用 QP Solver 求解。但对于大规模数据，这些方法效率低下。</p><p><strong>SMO (Sequential Minimal Optimization)</strong> 算法由 John Platt 于 1998 年提出，是求解 SVM 对偶问题的标准方法。</p><h3 id=51-核心思想在约束直线上跳舞>5.1 核心思想：在约束直线上跳舞<a class=anchor href=#51-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e5%9c%a8%e7%ba%a6%e6%9d%9f%e7%9b%b4%e7%ba%bf%e4%b8%8a%e8%b7%b3%e8%88%9e>#</a></h3><p>想象你要找到一个函数的最小值，但你的手脚被绳子绑住了（约束条件）。SMO 的聪明之处在于：</p><p><strong>每次只松开两只手，在绳子允许的范围内调整位置，然后再绑上。</strong></p><p>更正式地说：</p><ul><li>每次只优化<strong>两个</strong> $\alpha_i$ 和 $\alpha_j$，固定其他所有变量</li><li>由于约束 $\sum_i \alpha_i y_i = 0$，固定其他变量后，这两个变量之间存在<strong>线性关系</strong></li><li>优化路径被限制在一条直线上（见下图），问题变成了一维搜索，可以解析求解！</li></ul><p><img src=assets/coordinate_descent.svg alt="SMO 坐标下降示意图"></p><p><strong>几何直观</strong>：</p><ul><li><strong>紫色椭圆</strong>：目标函数的等高线（越靠内侧，目标函数值越优）</li><li><strong>红色直线</strong>：约束 $\alpha_1 y_1 + \alpha_2 y_2 = \zeta$（常数）</li><li><strong>优化路径</strong>：只能沿着红色直线移动（黄色→绿色点）</li><li><strong>最优解</strong>：直线与最内层等高线的切点（或在边界上）</li></ul><h3 id=52-为什么每次选两个>5.2 为什么每次选两个？<a class=anchor href=#52-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%af%8f%e6%ac%a1%e9%80%89%e4%b8%a4%e4%b8%aa>#</a></h3><p><strong>约束的限制</strong>：</p><p>约束 $\sum_{i=1}^N \alpha_i y_i = 0$ 是<strong>等式约束</strong>，这意味着：</p><ul><li>如果只改变一个 $\alpha_i$，会破坏和为零的平衡</li><li>改变两个变量是满足约束的<strong>最小可行单位</strong></li></ul><p><strong>数学推导</strong>：</p><p>固定除 $\alpha_1, \alpha_2$ 外的所有变量，约束变为：</p><p>$$
\alpha_1 y_1 + \alpha_2 y_2 = -\sum_{i=3}^N \alpha_i y_i = \zeta \quad \text{(常数)}
$$</p><p>这是一条<strong>直线</strong>！我们可以用 $\alpha_2$ 表示 $\alpha_1$：</p><p>$$
\alpha_1 = (\zeta - \alpha_2 y_2) y_1
$$</p><p>（因为 $y_1^2 = 1$）</p><p>将其代入目标函数，就得到了一个<strong>关于 $\alpha_2$ 的一元二次函数</strong>，求导即可得到解析解！</p><h3 id=53-算法框架简化版>5.3 算法框架（简化版）<a class=anchor href=#53-%e7%ae%97%e6%b3%95%e6%a1%86%e6%9e%b6%e7%ae%80%e5%8c%96%e7%89%88>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入: 数据集 D = {(x_1, y_1), ..., (x_N, y_N)}, 参数 C, 容忍度 tol
</span></span><span class=line><span class=cl>输出: α, b
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1. 初始化 α = 0, b = 0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. repeat:
</span></span><span class=line><span class=cl>     (a) 选择两个违反 KKT 条件最严重的 α_i 和 α_j
</span></span><span class=line><span class=cl>         - 外循环：遍历所有不满足 KKT 的样本
</span></span><span class=line><span class=cl>         - 内循环：选择使目标函数下降最快的配对
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     (b) 固定其他 α，解析求解最优的 (α_i, α_j)
</span></span><span class=line><span class=cl>         - 计算约束直线的端点（由箱约束 0 ≤ α ≤ C 决定）
</span></span><span class=line><span class=cl>         - 求一元二次函数的极值点
</span></span><span class=line><span class=cl>         - 截断到可行域内
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     (c) 更新 α_i, α_j
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     (d) 更新 b（使用支持向量）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   until 所有 α 都满足 KKT 条件 (在容忍度 tol 内)</span></span></code></pre></div><h3 id=54-smo-的优势>5.4 SMO 的优势<a class=anchor href=#54-smo-%e7%9a%84%e4%bc%98%e5%8a%bf>#</a></h3><ul><li>✅ <strong>无需矩阵运算</strong>：避免存储和操作 $N \times N$ 的 Gram 矩阵（内积矩阵）</li><li>✅ <strong>解析解</strong>：每步子问题有闭式解，无需数值优化</li><li>✅ <strong>内存高效</strong>：只需存储当前优化的两个变量</li><li>✅ <strong>适合大规模数据</strong>：复杂度远低于通用 QP Solver 的 $O(N^3)$</li></ul><h3 id=55-直观类比>5.5 直观类比<a class=anchor href=#55-%e7%9b%b4%e8%a7%82%e7%b1%bb%e6%af%94>#</a></h3><p><strong>传统 QP Solver</strong>：像是同时调整一个乐队中所有乐器的音量，需要复杂的协调。</p><p><strong>SMO 算法</strong>：像是一次只调整两个乐器，听听效果，再调下一对。虽然看起来慢，但每次调整都很快（解析解），而且不需要记住所有乐器的状态。</p><p><strong>关键洞见</strong>：</p><blockquote class=book-hint><p>把一个 $N$ 维的复杂优化问题，分解成一系列<strong>二维平面上的简单问题</strong>（约束直线上的一维搜索）。这是"分而治之"思想在优化算法中的精彩应用。</p></blockquote><h3 id=56-smo-vs-梯度下降>5.6 SMO vs 梯度下降<a class=anchor href=#56-smo-vs-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d>#</a></h3><table><thead><tr><th>维度</th><th>SMO</th><th>梯度下降</th></tr></thead><tbody><tr><td><strong>更新方式</strong></td><td>每次选两个变量，解析求解</td><td>每次沿负梯度方向小步移动</td></tr><tr><td><strong>约束处理</strong></td><td>自然满足约束（在约束直线上）</td><td>需要投影回可行域</td></tr><tr><td><strong>收敛速度</strong></td><td>较快（大步跳跃）</td><td>较慢（小步迭代）</td></tr><tr><td><strong>适用场景</strong></td><td>中等规模 SVM</td><td>大规模深度学习</td></tr></tbody></table><p><strong>历史趣闻</strong>：</p><p>John Platt 在微软研究院提出 SMO 时，SVM 的主要瓶颈是求解对偶问题的速度。SMO 的出现让 SVM 在工业界的大规模应用成为可能，是 SVM 从学术走向实用的关键一步。</p><hr><p><strong>延伸阅读</strong>：</p><p>完整的 SMO 算法还包括：</p><ul><li><strong>启发式选择策略</strong>：如何高效选择 $\alpha_i, \alpha_j$（Platt 的论文给出了两层循环策略）</li><li><strong>数值稳定性</strong>：处理边界情况（$\alpha = 0$ 或 $\alpha = C$）</li><li><strong>缓存优化</strong>：避免重复计算核函数值</li></ul><p>这些细节在实现 LIBSVM 等工业级库时至关重要，但核心思想就是上面的<strong>坐标下降在约束直线上的优化</strong>。</p><hr><h2 id=六本章小结>六、本章小结<a class=anchor href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>本章我们完成了从感知机到 SVM 的华丽转身。让我们回顾这个数学与几何交织的旅程：</p><p><strong>核心思想的演进</strong>：</p><pre class=mermaid>flowchart TD
    A[感知机的遗憾：解不唯一] --&gt; B[SVM 的洞见：最大间隔]
    B --&gt; C[几何间隔的数学化]

    C --&gt; D[原始问题：min 1/2‖w‖²&lt;br/&gt;约束：y_i·w·x_i+b ≥ 1]

    D --&gt; E{为何要对偶?}
    E --&gt; E1[理由1：约束优化更高效]
    E --&gt; E2[理由2：引入核函数]

    E --&gt; F[Lagrange 对偶]

    F --&gt; G[对偶问题：max Σα_i - 1/2 ΣΣα_i α_j y_i y_j x_i·x_j&lt;br/&gt;约束：Σα_i y_i = 0, α_i ≥ 0]

    G --&gt; H[KKT 条件：互补松弛性]

    H --&gt; I[支持向量的稀疏性&lt;br/&gt;α_i &gt; 0 ⟺ y_i·w·x_i+b = 1]

    I --&gt; J[现实问题：数据不完美]

    J --&gt; K[软间隔：引入松弛变量 ξ_i]

    K --&gt; L[目标函数：min 1/2‖w‖² + C Σξ_i]

    L --&gt; M[等价形式：Hinge Loss + L2]

    M --&gt; N[SVM = 损失函数 + 正则化&lt;br/&gt;统一到统计学习框架]

    N --&gt; O[求解算法：SMO&lt;br/&gt;坐标下降的智慧]

    style A fill:#ffe1e1
    style B fill:#e1f5ff
    style D fill:#fff4e1
    style G fill:#e1ffe1
    style I fill:#f0e1ff
    style M fill:#ffe1e1
    style N fill:#e1f5ff</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><hr><p><strong>从三个视角理解 SVM</strong>：</p><ol><li><p><strong>几何视角</strong>：</p><ul><li>SVM 寻找使间隔最大的分离超平面</li><li>支持向量是决定间隔的关键样本</li><li>间隔 = $\frac{2}{|\mathbf{w}|}$，最大化间隔 = 最小化 $|\mathbf{w}|^2$</li></ul></li><li><p><strong>优化视角</strong>：</p><ul><li>原始问题：带约束的凸二次规划</li><li>对偶问题：将 $N$ 个不等式约束转化为 $N$ 个变量</li><li>KKT 条件揭示了支持向量的稀疏性</li></ul></li><li><p><strong>统计学习视角</strong>：</p><ul><li>SVM = Hinge Loss + L2 正则化</li><li>目标函数 = 经验风险（$\sum \xi_i$）+ 结构风险（$\frac{1}{2}|\mathbf{w}|^2$）</li><li>参数 $C$ 控制权衡，连接到第1章的 SRM 框架</li></ul></li></ol><hr><p><strong>关键数学技巧</strong>：</p><ol><li><strong>尺度不变性</strong>：利用 $(\mathbf{w}, b) \to (kw, kb)$ 不改变超平面，固定函数间隔 $\hat{\gamma} = 1$</li><li><strong>Lagrange 对偶</strong>：将约束优化转化为无约束优化（在对偶空间）</li><li><strong>KKT 条件</strong>：互补松弛性 $\alpha_i [y_i(\mathbf{w}^T \mathbf{x}_i + b) - 1] = 0$ 揭示支持向量</li><li><strong>松弛变量消元</strong>：$\xi_i = [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+$ 连接到 Hinge Loss</li></ol><hr><p><strong>SVM 的优势</strong>：</p><ul><li>✅ 坚实的理论基础（VC 维、结构风险最小化）</li><li>✅ 凸优化问题，全局最优解</li><li>✅ 稀疏解（只依赖支持向量）</li><li>✅ 内积形式，自然支持核技巧（下一章）</li><li>✅ 对高维数据表现优异</li></ul><p><strong>SVM 的局限</strong>：</p><ul><li>❌ 对大规模数据训练慢（$O(N^2)$ 到 $O(N^3)$）</li><li>❌ 对参数（$C$、核参数）敏感</li><li>❌ 难以处理多分类（需要 OvR 或 OvO）</li><li>❌ 输出不是概率（与逻辑回归不同）</li></ul><hr><p><strong>与第1章的连接</strong>：</p><p>回顾第1章的结构风险最小化公式：</p><p>$$
\min_{f \in \mathcal{F}} \underbrace{\frac{1}{N} \sum_{i=1}^N L(y_i, f(\mathbf{x}<em>i))}</em>{\text{经验风险}} + \underbrace{\lambda J(f)}_{\text{正则化}}
$$</p><p>SVM 完美实例化了这个框架：</p><p>$$
\min_{\mathbf{w}, b} \underbrace{\sum_{i=1}^N [1 - y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+}<em>{\text{Hinge Loss（经验风险）}} + \underbrace{\frac{\lambda}{2} |\mathbf{w}|^2}</em>{\text{L2 正则化（结构风险）}}
$$</p><p>而参数 $C = \frac{1}{\lambda}$ 正是权衡这两者的旋钮。</p><hr><p><strong>哲学启示</strong>：</p><p>SVM 的成功揭示了机器学习的一个深刻原则：</p><blockquote class=book-hint><p><strong>简单性优于复杂性</strong>（Occam&rsquo;s Razor）</p><p>在满足数据的前提下，选择最"简单"的模型（最大间隔）。这种简单性不仅体现在几何上（间隔最大），也体现在代数上（$|\mathbf{w}|$ 最小），更体现在泛化能力上（支持向量的稀疏性）。</p></blockquote><p>Vapnik 的名言 &ldquo;Nothing is more practical than a good theory&rdquo; 在 SVM 中得到了完美体现：从 VC 维理论出发，推导出最大间隔原则，最终落地为高效的算法（SMO）。这是理论指导实践的典范。</p><hr><p><strong>下章预告</strong>：</p><p>本章我们推导对偶问题时，发现最优解只依赖于样本间的<strong>内积</strong> $\mathbf{x}_i^T \mathbf{x}_j$。这个看似不起眼的细节，却打开了一扇通往高维甚至无限维空间的大门。</p><p>下一章 <strong><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章：核方法 (Kernel Methods)</a></strong> 将揭示核技巧 (Kernel Trick) 的魔力：我们将看到如何在不显式计算高维特征的情况下，让 SVM 在无限维空间中工作。这不仅适用于 SVM，更是一套通用的数学工具，将线性算法拓展到非线性世界。</p><p>同时，我们还会探讨经典的核函数（多项式核、RBF 核），理解 Mercer 定理（什么样的函数可以作为核），以及核方法的几何直观（特征空间的隐式映射）。准备好迎接这场"维度的魔法"吧！</p><hr><h2 id=七推荐阅读>七、推荐阅读<a class=anchor href=#%e4%b8%83%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb>#</a></h2><p><strong>经典教材</strong>：</p><ul><li><strong>《统计学习方法》 - 第7章 支持向量机</strong> (李航)：清晰的推导，适合初学者</li><li><strong>《Pattern Recognition and Machine Learning》 - Chapter 7</strong> (Christopher Bishop)：从概率视角理解 SVM</li><li><strong>《The Elements of Statistical Learning》 - Chapter 12</strong> (Hastie, Tibshirani, Friedman)：SVM 与其他方法的对比</li></ul><p><strong>论文</strong>：</p><ul><li><strong>Cortes & Vapnik (1995). &ldquo;Support-Vector Networks&rdquo;</strong>：SVM 的开创性论文</li><li><strong>Platt (1998). &ldquo;Sequential Minimal Optimization&rdquo;</strong>：SMO 算法原文</li><li><strong>Vapnik (1999). &ldquo;The Nature of Statistical Learning Theory&rdquo;</strong>：VC 维理论的系统阐述</li></ul><p><strong>视频</strong>：</p><ul><li><strong>白板推导系列 - P7~P9 支持向量机</strong> (shuhuai008, B站)：详细的板书推导，强烈推荐</li><li><strong>Stanford CS229 - Lecture 6</strong> (Andrew Ng)：SVM 的直观讲解</li></ul><p><strong>实践工具</strong>：</p><ul><li><strong>scikit-learn SVM 文档</strong>：<code>sklearn.svm.SVC</code> 的使用与调参技巧</li><li><strong>LIBSVM</strong>：经典的 SVM 库，了解工程实现</li></ul><hr><p><strong>练习题</strong>（检验理解）：</p><ol><li><p><strong>几何理解</strong>：为什么间隔 = $\frac{2}{|\mathbf{w}|}$？画图推导。</p></li><li><p><strong>对偶推导</strong>：手动推导软间隔 SVM 的对偶问题，特别注意约束 $\alpha_i + \mu_i = C$ 的来源。</p></li><li><p><strong>KKT 条件</strong>：给定一个解 $(\mathbf{w}^<em>, b^</em>, \alpha^*)$，如何验证它满足 KKT 条件？</p></li><li><p><strong>支持向量</strong>：在软间隔 SVM 中，$\alpha_i = C$ 的样本一定被误分类吗？为什么？</p></li><li><p><strong>等价性证明</strong>：严格证明 $\min_{\mathbf{w},b} \frac{1}{2}|\mathbf{w}|^2 + C\sum_i \xi_i$ 等价于 $\min_{\mathbf{w},b} \sum_i [1-y_i(\mathbf{w}^T \mathbf{x}<em>i + b)]</em>+ + \frac{1}{2C}|\mathbf{w}|^2$。</p></li><li><p><strong>编程实现</strong>：从零实现一个简化版的 SMO 算法（只需处理线性核、小规模数据）。</p></li></ol><hr><blockquote class=book-hint><p>&ldquo;In God we trust, all others bring data.&rdquo; —— W. Edwards Deming</p><p>SVM 教会我们：当数据说话时，让数学倾听；当理论指引时，让算法践行。这种理论与实践的完美结合，正是机器学习之美。</p></blockquote></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>第06章 感知机</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/ class="flex align-center"><span>第08章 核方法</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一引言感知机的遗憾>一、引言：感知机的遗憾</a></li><li><a href=#二几何间隔-margin最宽的分界线>二、几何间隔 (Margin)：最宽的分界线</a><ul><li><a href=#21-函数间隔-vs-几何间隔>2.1 函数间隔 vs 几何间隔</a></li><li><a href=#22-最大化间隔的数学表达>2.2 最大化间隔的数学表达</a></li><li><a href=#23-为什么最大化间隔等价于最小化-frac12mathbfw2>2.3 为什么最大化间隔等价于最小化 $\frac{1}{2}|\mathbf{w}|^2$？</a></li></ul></li><li><a href=#三对偶问题-duality优雅的转换>三、对偶问题 (Duality)：优雅的转换</a><ul><li><a href=#31-为何要对偶>3.1 为何要对偶？</a></li><li><a href=#32-lagrange-函数构建>3.2 Lagrange 函数构建</a></li><li><a href=#33-对偶问题推导>3.3 对偶问题推导</a></li><li><a href=#34-kkt-条件与支持向量>3.4 KKT 条件与支持向量</a></li><li><a href=#35-对偶问题的物理意义>3.5 对偶问题的物理意义</a></li></ul></li><li><a href=#heading>$$
b^* = \frac{1}{|SV|} \sum_{s \in SV} \left[ y_s - \sum_{i=1}^N \alpha_i^* y_i (\mathbf{x}_i^T \mathbf{x}_s) \right]
$$</a></li><li><a href=#四软间隔-soft-margin拥抱不完美>四、软间隔 (Soft Margin)：拥抱不完美</a><ul><li><a href=#41-现实世界并不完美>4.1 现实世界并不完美</a></li><li><a href=#42-引入松弛变量-xi>4.2 引入松弛变量 $\xi$</a></li><li><a href=#43-软间隔的对偶问题>4.3 软间隔的对偶问题</a></li><li><a href=#44-hinge-loss打通优化视角>4.4 Hinge Loss：打通优化视角</a></li><li><a href=#45-svm--hinge-loss--l2-正则化>4.5 SVM = Hinge Loss + L2 正则化</a></li></ul></li><li><a href=#五smo-算法高效求解对偶问题>五、SMO 算法：高效求解对偶问题</a><ul><li><a href=#51-核心思想在约束直线上跳舞>5.1 核心思想：在约束直线上跳舞</a></li><li><a href=#52-为什么每次选两个>5.2 为什么每次选两个？</a></li><li><a href=#53-算法框架简化版>5.3 算法框架（简化版）</a></li><li><a href=#54-smo-的优势>5.4 SMO 的优势</a></li><li><a href=#55-直观类比>5.5 直观类比</a></li><li><a href=#56-smo-vs-梯度下降>5.6 SMO vs 梯度下降</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a></li><li><a href=#七推荐阅读>七、推荐阅读</a></li></ul></nav></div></aside></main></body></html>