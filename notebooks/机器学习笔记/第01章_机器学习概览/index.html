<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="第01章：机器学习概览# “All models are wrong, but some are useful.” —— George Box
重要提示：本章不仅是概念的堆砌，更是世界观的建立。
我们将深入探讨频率派与贝叶斯派的百年纠葛，这不仅仅是数学流派之争，更是我们认知世界的两种底层逻辑。此外，我们还将突破传统的教科书，带你领略现代深度学习中颠覆性的**“双下降” (Double Descent)** 现象，看看传统理论在过参数化时代是如何被挑战的。
目录# 一、世界观的碰撞：频率派 vs 贝叶斯派 1.1 频率派 (The Frequentist View) 1.2 贝叶斯派 (The Bayesian View) 1.3 核心案例推导：抛硬币的哲学 二、统计学习三要素：解构算法的万能公式 2.1 模型 (Model) 2.2 策略 (Strategy) 2.3 数学证明：为什么正则化等价于先验？ 2.4 算法 (Algorithm) 三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff) 四、实战演练：亲眼目睹过拟合与正则化 五、拓展深入：当传统理论失效——双下降现象 六、本章小结 一、世界观的碰撞：频率派 vs 贝叶斯派# 统计机器学习领域长期存在着两个对立统一的流派。理解这个对立，对后续理解正则化（Regularization）和概率图模型（PGM）至关重要。
1.1 频率派 (The Frequentist View)# 核心信仰：世界是确定的。参数 $\theta$ 是一个未知但固定的常量 (Unknown Constant)。虽然我们不知道它具体是多少，但它真真切切地在那里，不增不减。
方法论：极大似然估计 (MLE)。 $$ \hat{\theta}{MLE} = \arg\max{\theta} P(X|\theta) $$
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第01章 机器学习概览"><meta property="og:description" content="第01章：机器学习概览# “All models are wrong, but some are useful.” —— George Box
重要提示：本章不仅是概念的堆砌，更是世界观的建立。
我们将深入探讨频率派与贝叶斯派的百年纠葛，这不仅仅是数学流派之争，更是我们认知世界的两种底层逻辑。此外，我们还将突破传统的教科书，带你领略现代深度学习中颠覆性的**“双下降” (Double Descent)** 现象，看看传统理论在过参数化时代是如何被挑战的。
目录# 一、世界观的碰撞：频率派 vs 贝叶斯派 1.1 频率派 (The Frequentist View) 1.2 贝叶斯派 (The Bayesian View) 1.3 核心案例推导：抛硬币的哲学 二、统计学习三要素：解构算法的万能公式 2.1 模型 (Model) 2.2 策略 (Strategy) 2.3 数学证明：为什么正则化等价于先验？ 2.4 算法 (Algorithm) 三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff) 四、实战演练：亲眼目睹过拟合与正则化 五、拓展深入：当传统理论失效——双下降现象 六、本章小结 一、世界观的碰撞：频率派 vs 贝叶斯派# 统计机器学习领域长期存在着两个对立统一的流派。理解这个对立，对后续理解正则化（Regularization）和概率图模型（PGM）至关重要。
1.1 频率派 (The Frequentist View)# 核心信仰：世界是确定的。参数 $\theta$ 是一个未知但固定的常量 (Unknown Constant)。虽然我们不知道它具体是多少，但它真真切切地在那里，不增不减。
方法论：极大似然估计 (MLE)。 $$ \hat{\theta}{MLE} = \arg\max{\theta} P(X|\theta) $$"><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第01章 机器学习概览"><meta itemprop=description content="第01章：机器学习概览# “All models are wrong, but some are useful.” —— George Box
重要提示：本章不仅是概念的堆砌，更是世界观的建立。
我们将深入探讨频率派与贝叶斯派的百年纠葛，这不仅仅是数学流派之争，更是我们认知世界的两种底层逻辑。此外，我们还将突破传统的教科书，带你领略现代深度学习中颠覆性的**“双下降” (Double Descent)** 现象，看看传统理论在过参数化时代是如何被挑战的。
目录# 一、世界观的碰撞：频率派 vs 贝叶斯派 1.1 频率派 (The Frequentist View) 1.2 贝叶斯派 (The Bayesian View) 1.3 核心案例推导：抛硬币的哲学 二、统计学习三要素：解构算法的万能公式 2.1 模型 (Model) 2.2 策略 (Strategy) 2.3 数学证明：为什么正则化等价于先验？ 2.4 算法 (Algorithm) 三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff) 四、实战演练：亲眼目睹过拟合与正则化 五、拓展深入：当传统理论失效——双下降现象 六、本章小结 一、世界观的碰撞：频率派 vs 贝叶斯派# 统计机器学习领域长期存在着两个对立统一的流派。理解这个对立，对后续理解正则化（Regularization）和概率图模型（PGM）至关重要。
1.1 频率派 (The Frequentist View)# 核心信仰：世界是确定的。参数 $\theta$ 是一个未知但固定的常量 (Unknown Constant)。虽然我们不知道它具体是多少，但它真真切切地在那里，不增不减。
方法论：极大似然估计 (MLE)。 $$ \hat{\theta}{MLE} = \arg\max{\theta} P(X|\theta) $$"><meta itemprop=wordCount content="2306"><title>第01章 机器学习概览 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle checked>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/ class=active>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第01章 机器学习概览</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一世界观的碰撞频率派-vs-贝叶斯派>一、世界观的碰撞：频率派 vs 贝叶斯派</a><ul><li><a href=#11-频率派-the-frequentist-view>1.1 频率派 (The Frequentist View)</a></li><li><a href=#12-贝叶斯派-the-bayesian-view>1.2 贝叶斯派 (The Bayesian View)</a></li><li><a href=#13-核心案例推导抛硬币的哲学>1.3 核心案例推导：抛硬币的哲学</a><ul><li><a href=#131-问题设定>1.3.1 问题设定</a></li><li><a href=#132-频率派极大似然估计-mle>1.3.2 频率派：极大似然估计 (MLE)</a></li><li><a href=#133-mle-的致命缺陷>1.3.3 MLE 的致命缺陷</a></li><li><a href=#134-贝叶斯派最大后验估计-map>1.3.4 贝叶斯派：最大后验估计 (MAP)</a></li><li><a href=#135-深度直觉先验就是伪计数>1.3.5 深度直觉：先验就是"伪计数"</a></li><li><a href=#136-当数据量很大时先验被淹没>1.3.6 当数据量很大时：先验被"淹没"</a></li><li><a href=#137-可视化对比>1.3.7 可视化对比</a></li><li><a href=#138-扩展完全贝叶斯推断>1.3.8 扩展：完全贝叶斯推断</a></li></ul></li></ul></li><li><a href=#二统计学习三要素解构算法的万能公式>二、统计学习三要素：解构算法的万能公式</a><ul><li><a href=#21-模型-model>2.1 模型 (Model)</a></li><li><a href=#22-策略-strategy>2.2 策略 (Strategy)</a></li><li><a href=#23-数学证明为什么正则化等价于先验>2.3 数学证明：为什么正则化等价于先验？</a><ul><li><a href=#231-问题设定线性回归模型>2.3.1 问题设定：线性回归模型</a></li><li><a href=#232-频率派视角l2-正则化>2.3.2 频率派视角：L2 正则化</a></li><li><a href=#233-贝叶斯派视角从先验到-map>2.3.3 贝叶斯派视角：从先验到 MAP</a></li><li><a href=#234-神奇的等价性>2.3.4 神奇的等价性</a></li><li><a href=#235-深度解读>2.3.5 深度解读</a></li><li><a href=#236-扩展l1-正则化与拉普拉斯先验>2.3.6 扩展：L1 正则化与拉普拉斯先验</a></li><li><a href=#237-可视化对比>2.3.7 可视化对比</a></li></ul></li><li><a href=#24-算法-algorithm>2.4 算法 (Algorithm)</a></li></ul></li><li><a href=#三核心难题偏差-方差权衡-bias-variance-tradeoff>三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff)</a><ul><li><a href=#31-误差分解公式推导>3.1 误差分解公式推导</a></li><li><a href=#32-经典-u-型曲线>3.2 经典 U 型曲线</a></li></ul></li><li><a href=#四实战演练亲眼目睹过拟合与正则化>四、实战演练：亲眼目睹过拟合与正则化</a><ul><li><a href=#41-从跑通代码到验证理论深度解析>4.1 从"跑通代码"到"验证理论"：深度解析</a><ul><li><a href=#411-理论回顾ridge-回归的数学本质>4.1.1 理论回顾：Ridge 回归的数学本质</a></li><li><a href=#412-代码与理论的逐行对应>4.1.2 代码与理论的逐行对应</a></li><li><a href=#413-数值实验验证-lambda-的作用>4.1.3 数值实验：验证 $\lambda$ 的作用</a></li><li><a href=#414-更深层的洞见为什么正则化能杀死震荡>4.1.4 更深层的洞见：为什么正则化能"杀死"震荡？</a></li><li><a href=#415-本节小结>4.1.5 本节小结</a></li></ul></li></ul></li><li><a href=#五拓展深入当传统理论失效双下降现象>五、拓展深入：当传统理论失效——双下降现象</a><ul><li><a href=#51-经典理论的困境>5.1 经典理论的困境</a></li><li><a href=#52-深度学习的怪象>5.2 深度学习的怪象</a></li><li><a href=#53-为什么会这样>5.3 为什么会这样？</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a></li><li><a href=#推荐阅读>推荐阅读</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第01章机器学习概览>第01章：机器学习概览<a class=anchor href=#%e7%ac%ac01%e7%ab%a0%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%a7%88>#</a></h1><blockquote class=book-hint><p>&ldquo;All models are wrong, but some are useful.&rdquo; —— George Box</p><p><strong>重要提示</strong>：本章不仅是概念的堆砌，更是<strong>世界观</strong>的建立。</p><p>我们将深入探讨<strong>频率派与贝叶斯派</strong>的百年纠葛，这不仅仅是数学流派之争，更是我们认知世界的两种底层逻辑。此外，我们还将突破传统的教科书，带你领略现代深度学习中颠覆性的**&ldquo;双下降&rdquo; (Double Descent)** 现象，看看传统理论在过参数化时代是如何被挑战的。</p></blockquote><hr><h2 id=目录>目录<a class=anchor href=#%e7%9b%ae%e5%bd%95>#</a></h2><ul><li><a href=#%e4%b8%80%e4%b8%96%e7%95%8c%e8%a7%82%e7%9a%84%e7%a2%b0%e6%92%9e%e9%a2%91%e7%8e%87%e6%b4%be-vs-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be>一、世界观的碰撞：频率派 vs 贝叶斯派</a><ul><li><a href=#11-%e9%a2%91%e7%8e%87%e6%b4%be-the-frequentist-view>1.1 频率派 (The Frequentist View)</a></li><li><a href=#12-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be-the-bayesian-view>1.2 贝叶斯派 (The Bayesian View)</a></li><li><a href=#13-%e6%a0%b8%e5%bf%83%e6%a1%88%e4%be%8b%e6%8e%a8%e5%af%bc%e6%8a%9b%e7%a1%ac%e5%b8%81%e7%9a%84%e5%93%b2%e5%ad%a6>1.3 核心案例推导：抛硬币的哲学</a></li></ul></li><li><a href=#%e4%ba%8c%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e4%b8%89%e8%a6%81%e7%b4%a0%e8%a7%a3%e6%9e%84%e7%ae%97%e6%b3%95%e7%9a%84%e4%b8%87%e8%83%bd%e5%85%ac%e5%bc%8f>二、统计学习三要素：解构算法的万能公式</a><ul><li><a href=#21-%e6%a8%a1%e5%9e%8b-model>2.1 模型 (Model)</a></li><li><a href=#22-%e7%ad%96%e7%95%a5-strategy>2.2 策略 (Strategy)</a></li><li><a href=#23-%e6%95%b0%e5%ad%a6%e8%af%81%e6%98%8e%e4%b8%ba%e4%bb%80%e4%b9%88%e6%ad%a3%e5%88%99%e5%8c%96%e7%ad%89%e4%bb%b7%e4%ba%8e%e5%85%88%e9%aa%8c>2.3 数学证明：为什么正则化等价于先验？</a></li><li><a href=#24-%e7%ae%97%e6%b3%95-algorithm>2.4 算法 (Algorithm)</a></li></ul></li><li><a href=#%e4%b8%89%e6%a0%b8%e5%bf%83%e9%9a%be%e9%a2%98%e5%81%8f%e5%b7%ae-%e6%96%b9%e5%b7%ae%e6%9d%83%e8%a1%a1-bias-variance-tradeoff>三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff)</a></li><li><a href=#%e5%9b%9b%e5%ae%9e%e6%88%98%e6%bc%94%e7%bb%83%e4%ba%b2%e7%9c%bc%e7%9b%ae%e7%9d%b9%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e6%ad%a3%e5%88%99%e5%8c%96>四、实战演练：亲眼目睹过拟合与正则化</a></li><li><a href=#%e4%ba%94%e6%8b%93%e5%b1%95%e6%b7%b1%e5%85%a5%e5%bd%93%e4%bc%a0%e7%bb%9f%e7%90%86%e8%ae%ba%e5%a4%b1%e6%95%88%e5%8f%8c%e4%b8%8b%e9%99%8d%e7%8e%b0%e8%b1%a1>五、拓展深入：当传统理论失效——双下降现象</a></li><li><a href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>六、本章小结</a></li></ul><hr><h2 id=一世界观的碰撞频率派-vs-贝叶斯派>一、世界观的碰撞：频率派 vs 贝叶斯派<a class=anchor href=#%e4%b8%80%e4%b8%96%e7%95%8c%e8%a7%82%e7%9a%84%e7%a2%b0%e6%92%9e%e9%a2%91%e7%8e%87%e6%b4%be-vs-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be>#</a></h2><p>统计机器学习领域长期存在着两个对立统一的流派。理解这个对立，对后续理解正则化（Regularization）和概率图模型（PGM）至关重要。</p><h3 id=11-频率派-the-frequentist-view>1.1 频率派 (The Frequentist View)<a class=anchor href=#11-%e9%a2%91%e7%8e%87%e6%b4%be-the-frequentist-view>#</a></h3><ul><li><p><strong>核心信仰</strong>：<strong>世界是确定的</strong>。参数 $\theta$ 是一个<strong>未知但固定的常量</strong> (Unknown Constant)。虽然我们不知道它具体是多少，但它真真切切地在那里，不增不减。</p></li><li><p><strong>方法论</strong>：<strong>极大似然估计 (MLE)</strong>。
$$ \hat{\theta}<em>{MLE} = \arg\max</em>{\theta} P(X|\theta) $$</p><blockquote class=book-hint><p><strong>直觉</strong>：既然 $\theta$ 是固定的，那就找一个 $\theta$，使得"观测到当前数据 $X$&ldquo;这一事件发生的概率最大。</p></blockquote><p>但是，MLE 的这个公式在绝大多数情况下<strong>并没有解析解</strong>（除了少数简单模型如高斯分布的均值估计）。因此，我们必须借助<strong>数值优化方法</strong>（如梯度下降）来迭代求解。这就引出了频率派的核心技术路线：</p></li><li><p><strong>演进路线</strong>：频率派将机器学习视为一个<strong>优化问题 (Optimization)</strong>。
$$ \mathcal{L}(\theta) \rightarrow \nabla_\theta \mathcal{L} \rightarrow \theta_{t+1} = \theta_t - \eta g $$</p><ul><li><strong>代表算法</strong>：线性回归、逻辑回归、SVM、神经网络 (BackProp)。</li></ul></li></ul><h3 id=12-贝叶斯派-the-bayesian-view>1.2 贝叶斯派 (The Bayesian View)<a class=anchor href=#12-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be-the-bayesian-view>#</a></h3><ul><li><strong>核心信仰</strong>：<strong>世界是不确定的</strong>。参数 $\theta$ 本身是一个<strong>随机变量</strong> (Random Variable)，它服从某个分布。数据 $X$ 反而是固定的证据 (Evidence)。</li><li><strong>方法论</strong>：<strong>贝叶斯定理 (Bayes&rsquo; Theorem)</strong>。
$$ P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)} $$<ul><li>$P(\theta|X)$：<strong>后验 (Posterior)</strong> —— 看了数据修正后的信念。</li><li>$P(X|\theta)$：<strong>似然 (Likelihood)</strong> —— 数据所呈现的样子。</li><li>$P(\theta)$：<strong>先验 (Prior)</strong> —— 看数据之前的主观信念（这很重要！）。</li></ul></li><li><strong>最大后验估计 (MAP)</strong>：如果我们被迫给出一个具体的数值，而不是分布：
$$ \hat{\theta}<em>{MAP} = \arg\max</em>{\theta} P(\theta|X) = \arg\max_{\theta} \underbrace{\log P(X|\theta)}<em>{\text{Likelihood}} + \underbrace{\log P(\theta)}</em>{\text{Prior}} $$<blockquote class=book-hint><p><strong>洞见</strong>：$\hat{\theta}<em>{MLE}$ 其实就是 $P(\theta)$ 为均匀分布时的 $\hat{\theta}</em>{MAP}$！MLE 是 MAP 的特例。
<strong>正则化本质</strong>：L2 正则化其实等价于引入了 <strong>高斯先验</strong> 的 MAP；L1 正则化等价于 <strong>拉普拉斯先验</strong>。</p></blockquote></li><li><strong>演进路线</strong>：贝叶斯派将机器学习视为一个<strong>积分问题 (Integration)</strong>。
$$ P(x_{new}|X) = \int P(x_{new}|\theta) P(\theta|X) d\theta $$<ul><li><strong>代表算法</strong>：朴素贝叶斯、LDA主题模型、高斯过程、变分自动编码器 (VAE)。</li></ul></li></ul><hr><blockquote class=book-hint><p><strong>从抽象到具体：让公式活起来</strong></p><p>到这里，频率派和贝叶斯派的公式都已摆在眼前。但老实说，光看公式很难真正理解两者的差异——什么是"先验被淹没&rdquo;？什么是"正则化等价于先验"？这些话术听起来玄乎，却缺少实感。</p><p>接下来，让我们用<strong>抛硬币</strong>这个最简单的案例，亲手推导一遍 MLE 和 MAP。你会看到：MLE 如何因数据稀疏而过拟合，贝叶斯先验如何通过"伪计数"优雅地约束参数空间，以及当数据量趋于无穷时，先验如何自动退场。这个推导将成为我们理解正则化、主题模型、变分推断等高级技术的基石。</p></blockquote><h3 id=13-核心案例推导抛硬币的哲学>1.3 核心案例推导：抛硬币的哲学<a class=anchor href=#13-%e6%a0%b8%e5%bf%83%e6%a1%88%e4%be%8b%e6%8e%a8%e5%af%bc%e6%8a%9b%e7%a1%ac%e5%b8%81%e7%9a%84%e5%93%b2%e5%ad%a6>#</a></h3><blockquote class=book-hint><p><strong>为什么要推导这个？</strong></p><p>抛硬币虽然简单，但它是理解 MLE 与 MAP 差异的最佳载体。通过这个推导，你会明白：</p><ul><li>MLE 在数据少时为何容易过拟合</li><li>贝叶斯先验如何在数学上"约束"参数空间</li><li>为什么 Beta 分布是二项分布的共轭先验</li><li>当数据量 $N \to \infty$ 时，先验如何被"淹没"</li></ul></blockquote><h4 id=131-问题设定>1.3.1 问题设定<a class=anchor href=#131-%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a>#</a></h4><p>假设有一枚硬币，正面朝上的概率为 $\theta$（未知）。我们抛了 $N$ 次，观测到：</p><ul><li>$H$ 次正面 (Heads)</li><li>$T$ 次反面 (Tails)，其中 $N = H + T$</li></ul><p><strong>目标</strong>：估计参数 $\theta$。</p><hr><h4 id=132-频率派极大似然估计-mle>1.3.2 频率派：极大似然估计 (MLE)<a class=anchor href=#132-%e9%a2%91%e7%8e%87%e6%b4%be%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1-mle>#</a></h4><p><strong>Step 1：写出似然函数</strong></p><p>每次抛硬币是独立的伯努利试验，因此观测到数据 $D = {H, T}$ 的概率为：</p><p>$$
P(D|\theta) = \theta^H (1-\theta)^T
$$</p><p><strong>Step 2：对数似然函数</strong></p><p>为了求导方便，取对数：</p><p>$$
\log P(D|\theta) = H \log \theta + T \log(1-\theta)
$$</p><p><strong>Step 3：求导并令其为零</strong></p><p>$$
\frac{\partial}{\partial \theta} \log P(D|\theta) = \frac{H}{\theta} - \frac{T}{1-\theta} = 0
$$</p><p>解得：</p><p>$$
\boxed{\hat{\theta}_{MLE} = \frac{H}{H+T} = \frac{H}{N}}
$$</p><p><strong>直觉</strong>：MLE 就是频率派的"经验频率"。</p><hr><h4 id=133-mle-的致命缺陷>1.3.3 MLE 的致命缺陷<a class=anchor href=#133-mle-%e7%9a%84%e8%87%b4%e5%91%bd%e7%bc%ba%e9%99%b7>#</a></h4><blockquote class=book-hint><p><strong>思考</strong>：如果我们只抛了 5 次硬币，全是正面（$H=5, T=0$），MLE 会给出 $\hat{\theta}_{MLE} = 1$。这意味着这枚硬币 100% 会正面朝上，这合理吗？</p></blockquote><p>这就是 <strong>过拟合 (Overfitting)</strong> 的典型表现：MLE 完全依赖当前数据，没有任何"常识"约束。当数据量很少时，估计结果极其不稳定。</p><hr><h4 id=134-贝叶斯派最大后验估计-map>1.3.4 贝叶斯派：最大后验估计 (MAP)<a class=anchor href=#134-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be%e6%9c%80%e5%a4%a7%e5%90%8e%e9%aa%8c%e4%bc%b0%e8%ae%a1-map>#</a></h4><p><strong>Step 1：引入先验分布</strong></p><p>贝叶斯派认为 $\theta$ 本身是随机变量，应该有一个先验分布 $P(\theta)$。</p><p>但问题来了：先验分布有无数种选择（高斯、均匀、指数&mldr;），我们该选哪一个？答案是：<strong>选择共轭先验 (Conjugate Prior)</strong>。</p><blockquote class=book-hint><p><strong>为什么选择共轭先验？</strong></p><p>共轭先验有一个神奇的性质：<strong>先验和后验属于同一分布族</strong>。这意味着：</p><ul><li>如果先验是 Beta 分布，那么看了数据后，后验仍然是 Beta 分布（只是参数更新了）</li><li>这极大简化了计算，避免了复杂的积分运算</li></ul><p>对于抛硬币问题（二项分布似然），共轭先验恰好是 <strong>Beta 分布</strong>。这不是巧合，而是数学上的精心设计。</p></blockquote><p>因此我们选择 <strong>Beta 分布</strong> 作为先验：</p><p>$$
P(\theta) = \text{Beta}(\alpha, \beta) = \frac{\theta^{\alpha-1} (1-\theta)^{\beta-1}}{B(\alpha, \beta)}
$$</p><p>其中 $B(\alpha, \beta)$ 是归一化常数（Beta 函数）。</p><p><strong>Step 2：写出后验分布</strong></p><p>根据贝叶斯定理：</p><p>$$
\begin{aligned}
P(\theta|D) &\propto P(D|\theta) P(\theta) \
&= \theta^H (1-\theta)^T \cdot \theta^{\alpha-1} (1-\theta)^{\beta-1} \
&= \theta^{H+\alpha-1} (1-\theta)^{T+\beta-1}
\end{aligned}
$$</p><p>这正好是 $\text{Beta}(H+\alpha, T+\beta)$！验证了共轭性。</p><p><strong>Step 3：求 MAP 估计</strong></p><p>MAP 就是找后验分布的最大值点。对 $\log P(\theta|D)$ 求导：</p><p>$$
\begin{aligned}
\log P(\theta|D) &= (H+\alpha-1) \log \theta + (T+\beta-1) \log(1-\theta) + \text{const}
\end{aligned}
$$</p><p>求导并令其为零：</p><p>$$
\begin{aligned}
\frac{\partial}{\partial \theta} \log P(\theta|D) &= \frac{H+\alpha-1}{\theta} - \frac{T+\beta-1}{1-\theta} = 0
\end{aligned}
$$</p><p>解得：</p><p>$$
\boxed{\hat{\theta}_{MAP} = \frac{H + \alpha - 1}{H + T + \alpha + \beta - 2}}
$$</p><hr><h4 id=135-深度直觉先验就是伪计数>1.3.5 深度直觉：先验就是"伪计数"<a class=anchor href=#135-%e6%b7%b1%e5%ba%a6%e7%9b%b4%e8%a7%89%e5%85%88%e9%aa%8c%e5%b0%b1%e6%98%af%e4%bc%aa%e8%ae%a1%e6%95%b0>#</a></h4><p>观察 MAP 的公式，可以改写为：</p><p>$$
\hat{\theta}_{MAP} = \frac{H + (\alpha - 1)}{N + (\alpha + \beta - 2)}
$$</p><blockquote class=book-hint><p><strong>核心洞见</strong>：</p><ul><li>$(\alpha - 1)$ 相当于"伪正面"次数</li><li>$(\beta - 1)$ 相当于"伪反面"次数</li><li>总的"伪观测"次数为 $(\alpha + \beta - 2)$</li></ul></blockquote><p><strong>举例</strong>：</p><ul><li>如果我们设 $\alpha = 2, \beta = 2$（均匀先验的一种形式），相当于在看数据之前，假设已经观测到了 1 次正面和 1 次反面。</li><li>现在我们抛 5 次硬币，全是正面（$H=5, T=0$）：
$$
\hat{\theta}_{MAP} = \frac{5 + 1}{5 + 0 + 2} = \frac{6}{7} \approx 0.857
$$
比 MLE 的 1.0 更加保守合理！</li></ul><hr><h4 id=136-当数据量很大时先验被淹没>1.3.6 当数据量很大时：先验被"淹没"<a class=anchor href=#136-%e5%bd%93%e6%95%b0%e6%8d%ae%e9%87%8f%e5%be%88%e5%a4%a7%e6%97%b6%e5%85%88%e9%aa%8c%e8%a2%ab%e6%b7%b9%e6%b2%a1>#</a></h4><p>观察公式：</p><p>$$
\hat{\theta}_{MAP} = \frac{H + (\alpha - 1)}{N + (\alpha + \beta - 2)}
$$</p><p>当 $N \to \infty$ 时：</p><p>$$
\hat{\theta}<em>{MAP} \approx \frac{H}{N} = \hat{\theta}</em>{MLE}
$$</p><blockquote class=book-hint><p><strong>哲学启示</strong>：</p><ul><li>当数据很少时，先验起主导作用（防止过拟合）</li><li>当数据很多时，数据本身说话，先验的影响趋于零</li><li>这正是贝叶斯方法的优雅之处：<strong>让数据和先验在不同阶段各司其职</strong></li></ul></blockquote><hr><h4 id=137-可视化对比>1.3.7 可视化对比<a class=anchor href=#137-%e5%8f%af%e8%a7%86%e5%8c%96%e5%af%b9%e6%af%94>#</a></h4><p>假设真实 $\theta = 0.7$，我们观测 $N$ 次抛硬币，对比 MLE 和 MAP 的表现：</p><table><thead><tr><th>$N$</th><th>观测结果 ($H$)</th><th>$\hat{\theta}_{MLE}$</th><th>$\hat{\theta}_{MAP}$ (Beta(2,2))</th></tr></thead><tbody><tr><td>5</td><td>5</td><td>1.000</td><td>0.857</td></tr><tr><td>10</td><td>7</td><td>0.700</td><td>0.667</td></tr><tr><td>100</td><td>70</td><td>0.700</td><td>0.698</td></tr><tr><td>1000</td><td>700</td><td>0.700</td><td>0.699</td></tr></tbody></table><p>可以看到：</p><ul><li>当 $N=5$ 时，MLE 严重过拟合（估计为 1.0），而 MAP 更稳健（0.857）</li><li>当 $N \geq 100$ 时，两者几乎一致</li></ul><hr><h4 id=138-扩展完全贝叶斯推断>1.3.8 扩展：完全贝叶斯推断<a class=anchor href=#138-%e6%89%a9%e5%b1%95%e5%ae%8c%e5%85%a8%e8%b4%9d%e5%8f%b6%e6%96%af%e6%8e%a8%e6%96%ad>#</a></h4><p>MAP 仍然是给出一个点估计。完全的贝叶斯推断是保留整个后验分布：</p><p>$$
P(\theta|D) = \text{Beta}(H+\alpha, T+\beta)
$$</p><p>当我们预测下一次抛硬币的结果时，不是用某个固定的 $\hat{\theta}$，而是对所有可能的 $\theta$ 积分：</p><p>$$
P(\text{下次是正面}|D) = \int_0^1 \theta \cdot P(\theta|D) d\theta = \frac{H+\alpha}{N+\alpha+\beta}
$$</p><blockquote class=book-hint><p><strong>与 MAP 的区别</strong>：</p><ul><li>MAP 是取分布的最大值点（Mode）</li><li>完全贝叶斯是取分布的期望（Mean）</li><li>对于 Beta 分布，Mean = $\frac{\alpha}{\alpha+\beta}$，而 Mode = $\frac{\alpha-1}{\alpha+\beta-2}$（当 $\alpha, \beta > 1$ 时）</li></ul></blockquote><hr><p><strong>小结</strong>：通过抛硬币这个简单案例，我们完成了一次完整的数学推导之旅。我们看到了 MLE 如何因数据稀疏而过拟合，贝叶斯先验如何通过"伪计数"优雅地约束参数空间，以及当数据充足时先验如何自动退场。这个推导将成为我们理解正则化、主题模型、变分推断等高级技术的基石。</p><hr><h2 id=二统计学习三要素解构算法的万能公式>二、统计学习三要素：解构算法的万能公式<a class=anchor href=#%e4%ba%8c%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e4%b8%89%e8%a6%81%e7%b4%a0%e8%a7%a3%e6%9e%84%e7%ae%97%e6%b3%95%e7%9a%84%e4%b8%87%e8%83%bd%e5%85%ac%e5%bc%8f>#</a></h2><p>在《统计学习方法》中提出了著名的公式，这套方法论可以解构任何算法：</p><p>$$ \text{方法} = \text{模型} + \text{策略} + \text{算法} $$</p><h3 id=21-模型-model>2.1 模型 (Model)<a class=anchor href=#21-%e6%a8%a1%e5%9e%8b-model>#</a></h3><p>我们要学习的<strong>假设空间 (Hypothesis Space)</strong> $\mathcal{F}$。</p><ul><li><strong>线性模型</strong>：$f(x) = w^T x + b$</li><li><strong>树模型</strong>：非线性的 if-else 规则集合</li><li><strong>神经网络</strong>：多层非线性复合函数</li></ul><h3 id=22-策略-strategy>2.2 策略 (Strategy)<a class=anchor href=#22-%e7%ad%96%e7%95%a5-strategy>#</a></h3><p>评价模型好坏的标准，即<strong>损失函数 (Loss Function)</strong>。</p><ul><li><strong>平方损失 (L2)</strong>：$(y - f(x))^2$ —— 线性回归。</li><li><strong>交叉熵 (Cross Entropy)</strong>：$-\sum y \log p$ —— 逻辑回归/分类。</li><li><strong>Hinge Loss</strong>：$[1 - y f(x)]_+$ —— SVM 的灵魂。</li></ul><p><strong>结构风险最小化 (SRM)</strong>：
$$ \min_{f \in \mathcal{F}} \underbrace{\frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))}<em>{\text{经验风险 (拟合程度)}} + \underbrace{\lambda J(f)}</em>{\text{正则化 (模型复杂度)}} $$</p><hr><blockquote class=book-hint><p><strong>从工程技巧到数学本质</strong></p><p>我们在 2.2 节看到了正则化项 $\lambda J(f)$ 的作用——它能防止过拟合，这是工程实践中的共识。但如果你追问：&ldquo;正则化为什么有效？它背后的数学原理是什么？"，很多教材会戛然而止。</p><p>接下来，让我们从<strong>贝叶斯视角</strong>重新审视正则化。我们将通过严格的数学推导证明一个惊人的结论：<strong>频率派的 L2 正则化，在数学上完全等价于贝叶斯派的高斯先验</strong>。这不是一个模糊的类比，而是一个精确的等式。当你理解了这个等价性，你会发现频率派和贝叶斯派不过是同一枚硬币的两面——一个显式地惩罚复杂度，一个隐式地编码信念。</p></blockquote><h3 id=23-数学证明为什么正则化等价于先验>2.3 数学证明：为什么正则化等价于先验？<a class=anchor href=#23-%e6%95%b0%e5%ad%a6%e8%af%81%e6%98%8e%e4%b8%ba%e4%bb%80%e4%b9%88%e6%ad%a3%e5%88%99%e5%8c%96%e7%ad%89%e4%bb%b7%e4%ba%8e%e5%85%88%e9%aa%8c>#</a></h3><blockquote class=book-hint><p><strong>核心问题</strong>：</p><p>在 1.2 节我们提到"L2 正则化等价于高斯先验&rdquo;，但这个说法凭什么成立？这一小节将通过严格的数学推导，揭示正则化与贝叶斯先验在数学上的等价性。这是机器学习面试中的经典题，也是打通频率派与贝叶斯派的关键桥梁。</p></blockquote><h4 id=231-问题设定线性回归模型>2.3.1 问题设定：线性回归模型<a class=anchor href=#231-%e9%97%ae%e9%a2%98%e8%ae%be%e5%ae%9a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b>#</a></h4><p>假设我们有一个线性回归模型：</p><p>$$
y = w^T x + \epsilon
$$</p><p>其中：</p><ul><li>$w \in \mathbb{R}^d$ 是待估计的权重向量</li><li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$ 是高斯噪声</li><li>训练数据：$\mathcal{D} = {(x_i, y_i)}_{i=1}^N$</li></ul><hr><h4 id=232-频率派视角l2-正则化>2.3.2 频率派视角：L2 正则化<a class=anchor href=#232-%e9%a2%91%e7%8e%87%e6%b4%be%e8%a7%86%e8%a7%92l2-%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h4><p>频率派通过<strong>结构风险最小化</strong>来对抗过拟合：</p><p>$$
\hat{w}<em>{Ridge} = \arg\min</em>{w} \underbrace{\frac{1}{2N} \sum_{i=1}^N (y_i - w^T x_i)^2}<em>{\text{MSE Loss}} + \underbrace{\frac{\lambda}{2} |w|^2}</em>{\text{L2 Regularization}}
$$</p><blockquote class=book-hint><p><strong>直觉</strong>：</p><ul><li>第一项让模型拟合数据</li><li>第二项惩罚权重的大小，防止过拟合</li><li>$\lambda$ 控制正则化强度</li></ul></blockquote><hr><h4 id=233-贝叶斯派视角从先验到-map>2.3.3 贝叶斯派视角：从先验到 MAP<a class=anchor href=#233-%e8%b4%9d%e5%8f%b6%e6%96%af%e6%b4%be%e8%a7%86%e8%a7%92%e4%bb%8e%e5%85%88%e9%aa%8c%e5%88%b0-map>#</a></h4><p><strong>Step 1：写出似然函数</strong></p><p>因为噪声 $\epsilon \sim \mathcal{N}(0, \sigma^2)$，所以：</p><p>$$
y_i | x_i, w \sim \mathcal{N}(w^T x_i, \sigma^2)
$$</p><p>对于所有 $N$ 个样本，似然函数为：</p><p>$$
\begin{aligned}
P(\mathcal{D}|w) &= \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - w^T x_i)^2}{2\sigma^2}\right)
\end{aligned}
$$</p><p>取对数似然：</p><p>$$
\begin{aligned}
\log P(\mathcal{D}|w) &= -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 - \frac{N}{2} \log(2\pi\sigma^2)
\end{aligned}
$$</p><hr><p><strong>Step 2：引入高斯先验</strong></p><p>贝叶斯派认为权重 $w$ 本身应该服从某个先验分布。我们选择<strong>零均值高斯先验</strong>：</p><p>$$
P(w) = \mathcal{N}(0, \tau^2 I) = \left(\frac{1}{\sqrt{2\pi\tau^2}}\right)^d \exp\left(-\frac{|w|^2}{2\tau^2}\right)
$$</p><blockquote class=book-hint><p><strong>为什么选择零均值？</strong></p><ul><li>体现了奥卡姆剃刀原则：在没有证据时，倾向于认为权重应该接近零（简单模型）</li><li>$\tau^2$ 控制先验的"强度"：$\tau^2$ 越小，越强烈地认为 $w$ 应该接近零</li></ul></blockquote><p>取对数先验：</p><p>$$
\begin{aligned}
\log P(w) &= -\frac{|w|^2}{2\tau^2} - \frac{d}{2} \log(2\pi\tau^2)
\end{aligned}
$$</p><hr><p><strong>Step 3：写出后验分布</strong></p><p>根据贝叶斯定理：</p><p>$$
P(w|\mathcal{D}) \propto P(\mathcal{D}|w) P(w)
$$</p><p>取对数：</p><p>$$
\begin{aligned}
\log P(w|\mathcal{D}) &= \log P(\mathcal{D}|w) + \log P(w) + \text{const}
\end{aligned}
$$</p><p>代入前面的结果：</p><p>$$
\begin{aligned}
\log P(w|\mathcal{D}) &= -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 - \frac{|w|^2}{2\tau^2} + \text{const}
\end{aligned}
$$</p><hr><p><strong>Step 4：MAP 估计 = 最大化后验</strong></p><p>MAP 就是找让后验概率最大的 $w$：</p><p>$$
\begin{aligned}
\hat{w}<em>{MAP} &= \arg\max</em>{w} \log P(w|\mathcal{D})
\end{aligned}
$$</p><p>等价于：</p><p>$$
\begin{aligned}
\hat{w}<em>{MAP} &= \arg\max</em>{w} \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 - \frac{|w|^2}{2\tau^2} \right]
\end{aligned}
$$</p><p><strong>去掉负号，转为最小化</strong>：</p><p>$$
\begin{aligned}
\hat{w}<em>{MAP} &= \arg\min</em>{w} \left[ \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 + \frac{|w|^2}{2\tau^2} \right]
\end{aligned}
$$</p><hr><h4 id=234-神奇的等价性>2.3.4 神奇的等价性<a class=anchor href=#234-%e7%a5%9e%e5%a5%87%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7>#</a></h4><p>提取公共因子 $\frac{1}{2\sigma^2}$：</p><p>$$
\begin{aligned}
\hat{w}<em>{MAP} &= \arg\min</em>{w} \left[ \frac{1}{N} \sum_{i=1}^N (y_i - w^T x_i)^2 + \frac{\sigma^2}{N\tau^2} |w|^2 \right]
\end{aligned}
$$</p><p>定义正则化系数：</p><p>$$
\begin{aligned}
\lambda &= \frac{\sigma^2}{N\tau^2}
\end{aligned}
$$</p><p>最终得到：</p><p>$$
\boxed{\hat{w}<em>{MAP} = \arg\min</em>{w} \left[ \frac{1}{N} \sum_{i=1}^N (y_i - w^T x_i)^2 + \lambda |w|^2 \right] = \hat{w}_{Ridge}}
$$</p><hr><h4 id=235-深度解读>2.3.5 深度解读<a class=anchor href=#235-%e6%b7%b1%e5%ba%a6%e8%a7%a3%e8%af%bb>#</a></h4><blockquote class=book-hint><p><strong>惊人的结论</strong>：</p><p><strong>贝叶斯的 MAP 估计 在数学上完全等价于 频率派的 L2 正则化！</strong></p></blockquote><p>这个等价性揭示了：</p><ol><li><p><strong>正则化系数 $\lambda$ 的含义</strong>：
$$
\lambda = \frac{\sigma^2}{N\tau^2} = \frac{\text{噪声方差}}{\text{样本量} \times \text{先验方差}}
$$</p><ul><li>噪声 $\sigma^2$ 越大，$\lambda$ 越大（数据不可靠，更依赖先验）</li><li>样本量 $N$ 越大，$\lambda$ 越小（数据充足，先验被淹没）</li><li>先验 $\tau^2$ 越小，$\lambda$ 越大（先验越强，正则化越强）</li></ul></li><li><p><strong>频率派与贝叶斯派的统一</strong>：</p><ul><li>频率派：显式地惩罚 $|w|^2$，称为"正则化"</li><li>贝叶斯派：隐式地通过先验 $P(w) \sim \mathcal{N}(0, \tau^2)$ 约束 $w$</li><li>两者在数学上完全一致，只是哲学叙述不同</li></ul></li><li><p><strong>为什么是高斯先验？</strong></p><ul><li>L2 正则化 $|w|^2$ 对应高斯先验 $\mathcal{N}(0, \tau^2)$</li><li>L1 正则化 $|w|_1$ 对应拉普拉斯先验 $\text{Laplace}(0, b)$</li><li>正则化的形式决定了先验的类型</li></ul></li></ol><hr><h4 id=236-扩展l1-正则化与拉普拉斯先验>2.3.6 扩展：L1 正则化与拉普拉斯先验<a class=anchor href=#236-%e6%89%a9%e5%b1%95l1-%e6%ad%a3%e5%88%99%e5%8c%96%e4%b8%8e%e6%8b%89%e6%99%ae%e6%8b%89%e6%96%af%e5%85%88%e9%aa%8c>#</a></h4><p>类似地，如果我们选择<strong>拉普拉斯先验</strong>：</p><p>$$
P(w) = \prod_{j=1}^d \frac{1}{2b} \exp\left(-\frac{|w_j|}{b}\right) \propto \exp\left(-\frac{|w|_1}{b}\right)
$$</p><p>则对应的 MAP 估计为：</p><p>$$
\begin{aligned}
\hat{w}<em>{MAP} &= \arg\min</em>{w} \left[ \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 + \frac{|w|_1}{b} \right]
\end{aligned}
$$</p><p>这正是 <strong>Lasso 回归</strong>（L1 正则化）！</p><blockquote class=book-hint><p><strong>对比总结</strong>：</p><ul><li><strong>Ridge (L2)</strong> $\leftrightarrow$ 高斯先验 $\mathcal{N}(0, \tau^2)$ —— 权重倾向于小但非零</li><li><strong>Lasso (L1)</strong> $\leftrightarrow$ 拉普拉斯先验 —— 权重倾向于稀疏（很多为零）</li></ul></blockquote><hr><h4 id=237-可视化对比>2.3.7 可视化对比<a class=anchor href=#237-%e5%8f%af%e8%a7%86%e5%8c%96%e5%af%b9%e6%af%94>#</a></h4><p>让我们用一张图理解这个等价性：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>频率派的优化问题                     贝叶斯派的概率推断
</span></span><span class=line><span class=cl>┌────────────────────┐              ┌────────────────────┐
</span></span><span class=line><span class=cl>│                    │              │                    │
</span></span><span class=line><span class=cl>│  min MSE + λ‖w‖²  │              │  max P(w|D)        │
</span></span><span class=line><span class=cl>│                    │              │                    │
</span></span><span class=line><span class=cl>└─────────┬──────────┘              └─────────┬──────────┘
</span></span><span class=line><span class=cl>          │                                   │
</span></span><span class=line><span class=cl>          │                                   │
</span></span><span class=line><span class=cl>          │          等价变换                 │
</span></span><span class=line><span class=cl>          └───────────────────────────────────┘
</span></span><span class=line><span class=cl>                         │
</span></span><span class=line><span class=cl>                         ▼
</span></span><span class=line><span class=cl>          ┌──────────────────────────┐
</span></span><span class=line><span class=cl>          │  min MSE + λ‖w‖²         │
</span></span><span class=line><span class=cl>          │  = min -log P(D|w) - log P(w) │
</span></span><span class=line><span class=cl>          └──────────────────────────┘</span></span></code></pre></div><hr><p><strong>小结</strong>：通过这个推导，我们证明了"约束与信念本是同一枚硬币的两面"。频率派的正则化是显式地惩罚复杂度，贝叶斯派的先验是隐式地编码信念，但它们在数学上殊途同归。这个洞见是理解 SVM、神经网络中的 Weight Decay、以及变分推断等高级技术的基石。当你在调参 $\lambda$ 时，本质上就是在调整"先验的强度" $\tau^2$；当你在设计正则化项时，本质上就是在选择先验分布的形式。</p><h3 id=24-算法-algorithm>2.4 算法 (Algorithm)<a class=anchor href=#24-%e7%ae%97%e6%b3%95-algorithm>#</a></h3><p>求解最优解的具体计算步骤。</p><ul><li><strong>解析解</strong>：直接公式算出来（如 OLS）。</li><li><strong>数值优化</strong>：梯度下降 (SGD)、牛顿法、拟牛顿法 (BFGS)、坐标下降 (SMO)。</li></ul><hr><h2 id=三核心难题偏差-方差权衡-bias-variance-tradeoff>三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff)<a class=anchor href=#%e4%b8%89%e6%a0%b8%e5%bf%83%e9%9a%be%e9%a2%98%e5%81%8f%e5%b7%ae-%e6%96%b9%e5%b7%ae%e6%9d%83%e8%a1%a1-bias-variance-tradeoff>#</a></h2><p>为什么模型越复杂越容易过拟合？让我们从数学期望的角度把它拆解开。</p><h3 id=31-误差分解公式推导>3.1 误差分解公式推导<a class=anchor href=#31-%e8%af%af%e5%b7%ae%e5%88%86%e8%a7%a3%e5%85%ac%e5%bc%8f%e6%8e%a8%e5%af%bc>#</a></h3><p>假设真实数据生成机制为 $y = f(x) + \epsilon$，其中噪声 $E[\epsilon]=0, Var(\epsilon)=\sigma_\epsilon^2$。
我们训练出的模型为 $\hat{f}(x)$。在测试样本上的<strong>期望泛化误差</strong>可以完美分解为：
$$
\begin{aligned}
E[(y - \hat{f})^2] &= \text{Bias}^2[\hat{f}] + \text{Var}[\hat{f}] + \text{Noise}
\end{aligned}
$$</p><blockquote class=book-hint><p><strong>为什么要展开这个推导？</strong></p><p>这个分解不是显而易见的。很多教材直接给出结论，但这个展开过程蕴含了深刻的数学技巧：配方法、期望的线性性、以及独立性假设。理解这个推导，你会明白为什么"偏差"和"方差"是两个正交的概念，为什么它们可以完美分离。</p></blockquote><hr><p><strong>给定条件</strong>：</p><ul><li>真实模型：$y = f(x) + \epsilon$</li><li>噪声：$E[\epsilon] = 0, \ \text{Var}(\epsilon) = \sigma_\epsilon^2$</li><li>训练集：$\mathcal{D}$（随机抽样）</li><li>学习到的模型：$\hat{f}(x; \mathcal{D})$（依赖于训练集）</li></ul><p><strong>目标</strong>：计算在固定测试点 $x$ 处的期望泛化误差（对所有可能的训练集和噪声求期望）。</p><hr><p><strong>Step 1：写出期望泛化误差</strong></p><p>$$
\begin{aligned}
E_{\mathcal{D}, \epsilon}[(y - \hat{f})^2]
\end{aligned}
$$</p><p>为简化记号，后续我们将 $\hat{f}(x; \mathcal{D})$ 简写为 $\hat{f}$，$f(x)$ 简写为 $f$。</p><hr><p><strong>Step 2：代入真实模型 $y = f + \epsilon$</strong></p><p>$$
\begin{aligned}
E[(y - \hat{f})^2] &= E[(f + \epsilon - \hat{f})^2] \
&= E[(f - \hat{f} + \epsilon)^2]
\end{aligned}
$$</p><hr><p><strong>Step 3：展开平方项</strong></p><p>$$
\begin{aligned}
E[(f - \hat{f} + \epsilon)^2] &= E[(f - \hat{f})^2 + 2(f - \hat{f})\epsilon + \epsilon^2]
\end{aligned}
$$</p><p>利用期望的线性性：</p><p>$$
\begin{aligned}
&= E[(f - \hat{f})^2] + 2E[(f - \hat{f})\epsilon] + E[\epsilon^2]
\end{aligned}
$$</p><hr><p><strong>Step 4：处理交叉项</strong></p><p><strong>关键观察</strong>：$\hat{f}$ 只依赖于训练集 $\mathcal{D}$，而 $\epsilon$ 是测试样本的噪声，两者<strong>独立</strong>。</p><p>因此：</p><p>$$
\begin{aligned}
E[(f - \hat{f})\epsilon] &= E_{\mathcal{D}}[f - \hat{f}] \cdot E_{\epsilon}[\epsilon] \
&= E_{\mathcal{D}}[f - \hat{f}] \cdot 0 \
&= 0
\end{aligned}
$$</p><p>同时，由于 $E[\epsilon] = 0$，根据方差定义：</p><p>$$
\begin{aligned}
E[\epsilon^2] &= \text{Var}(\epsilon) + (E[\epsilon])^2 \
&= \sigma_\epsilon^2 + 0 \
&= \sigma_\epsilon^2
\end{aligned}
$$</p><p>代入得：</p><p>$$
\begin{aligned}
E[(y - \hat{f})^2] &= E[(f - \hat{f})^2] + \sigma_\epsilon^2
\end{aligned}
$$</p><hr><p><strong>Step 5：分解第一项 $E[(f - \hat{f})^2]$</strong></p><p>这是关键的一步！我们使用<strong>配方法</strong>，引入 $E[\hat{f}]$（模型的期望预测）：</p><p>$$
\begin{aligned}
E[(f - \hat{f})^2] &= E\left[(f - E[\hat{f}] + E[\hat{f}] - \hat{f})^2\right] \
&= E\left[\underbrace{(f - E[\hat{f}])}<em>{\text{偏差项}} + \underbrace{(E[\hat{f}] - \hat{f})}</em>{\text{方差项}}\right)^2]
\end{aligned}
$$</p><p>展开平方：</p><p>$$
\begin{aligned}
&= E[(f - E[\hat{f}])^2] + E[(E[\hat{f}] - \hat{f})^2] + 2E[(f - E[\hat{f}])(E[\hat{f}] - \hat{f})]
\end{aligned}
$$</p><hr><p><strong>Step 6：处理三项</strong></p><p><strong>第一项</strong>：</p><p>$$
\begin{aligned}
E[(f - E[\hat{f}])^2] &= (f - E[\hat{f}])^2 \quad \text{（$f$ 和 $E[\hat{f}]$ 都是常数）} \
&= \text{Bias}^2[\hat{f}]
\end{aligned}
$$</p><p><strong>第二项</strong>：</p><p>$$
\begin{aligned}
E[(E[\hat{f}] - \hat{f})^2] &= E[(\hat{f} - E[\hat{f}])^2] \
&= \text{Var}[\hat{f}]
\end{aligned}
$$</p><p><strong>第三项（交叉项）</strong>：</p><p>$$
\begin{aligned}
E[(f - E[\hat{f}])(E[\hat{f}] - \hat{f})] &= (f - E[\hat{f}]) \cdot E[E[\hat{f}] - \hat{f}] \
&= (f - E[\hat{f}]) \cdot (E[\hat{f}] - E[\hat{f}]) \
&= 0
\end{aligned}
$$</p><hr><p><strong>Step 7：最终结果</strong></p><p>将所有项合并：</p><p>$$
\begin{aligned}
E[(y - \hat{f})^2] &= \underbrace{(E[\hat{f}] - f)^2}<em>{\text{Bias}^2} + \underbrace{E[(\hat{f} - E[\hat{f}])^2]}</em>{\text{Variance}} + \underbrace{\sigma_\epsilon^2}_{\text{Noise}}
\end{aligned}
$$</p><p>即：</p><p>$$
\boxed{E[(y - \hat{f})^2] = \text{Bias}^2[\hat{f}] + \text{Var}[\hat{f}] + \sigma_\epsilon^2}
$$</p><hr><p><strong>核心洞见</strong>：</p><ol><li><strong>Bias</strong> 刻画"平均而言，模型离真实函数有多远"（系统性误差）</li><li><strong>Variance</strong> 刻画"不同训练集会让模型波动多大"（随机性误差）</li><li><strong>Noise</strong> 是数据本身的不可约误差</li><li>三者正交分解，互不干扰</li></ol></details><ol><li><strong>Bias (偏差)</strong>：$E[\hat{f}] - f$<ul><li>刻画模型的<strong>拟合能力</strong>。</li><li>模型越简单（如线性），Bias 越大（欠拟合）。</li></ul></li><li><strong>Variance (方差)</strong>：$E[(\hat{f} - E[\hat{f}])^2]$<ul><li>刻画模型的<strong>稳定性</strong>。</li><li>模型越复杂（如高阶多项式），Variance 越大（过拟合，对数据扰动敏感）。</li></ul></li><li><strong>Noise (噪声)</strong>：$\sigma_\epsilon^2$<ul><li>数据本身的质量。这是<strong>不可约误差 (Irreducible Error)</strong>，是泛化误差的下界。</li></ul></li></ol><h3 id=32-经典-u-型曲线>3.2 经典 U 型曲线<a class=anchor href=#32-%e7%bb%8f%e5%85%b8-u-%e5%9e%8b%e6%9b%b2%e7%ba%bf>#</a></h3><p>随着模型复杂度增加：</p><ul><li>Bias $\downarrow$ (越来越准)</li><li>Variance $\uparrow$ (越来越不稳定)</li><li><strong>总误差</strong>：先降后升，呈 U 型。最佳模型是在两者之间找到平衡点（Sweet Spot）。</li></ul><hr><h2 id=四实战演练亲眼目睹过拟合与正则化>四、实战演练：亲眼目睹过拟合与正则化<a class=anchor href=#%e5%9b%9b%e5%ae%9e%e6%88%98%e6%bc%94%e7%bb%83%e4%ba%b2%e7%9c%bc%e7%9b%ae%e7%9d%b9%e8%bf%87%e6%8b%9f%e5%90%88%e4%b8%8e%e6%ad%a3%e5%88%99%e5%8c%96>#</a></h2><p>我们通过代码直观感受：为什么加了正则化（先验），曲线就能变平滑？</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.pipeline</span> <span class=kn>import</span> <span class=n>Pipeline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>PolynomialFeatures</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>Ridge</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 真实函数：y = sin(2πx) + 噪声</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>true_fun</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 生成合成数据</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>n_samples</span> <span class=o>=</span> <span class=mi>30</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>n_samples</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>true_fun</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X_test</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 对比不同复杂度的模型</span>
</span></span><span class=line><span class=cl><span class=n>degrees</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>15</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>degree</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>degrees</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>degree</span> <span class=o>==</span> <span class=mi>15</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 特例：高阶多项式 + L2正则化</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>title_suffix</span> <span class=o>=</span> <span class=s2>&#34;(With Regularization)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>title_suffix</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;poly&#34;</span><span class=p>,</span> <span class=n>PolynomialFeatures</span><span class=p>(</span><span class=n>degree</span><span class=o>=</span><span class=n>degree</span><span class=p>,</span> <span class=n>include_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;lr&#34;</span><span class=p>,</span> <span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>pipeline</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>],</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Model&#34;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;r&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>true_fun</span><span class=p>(</span><span class=n>X_test</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;True function&#34;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s2>&#34;--&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Samples&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Degree </span><span class=si>{</span><span class=n>degree</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>title_suffix</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>关键现象</strong>：当 Degree=15 时，如果不加正则化，曲线会剧烈震荡（过拟合）；但如果我们加上 <code>Ridge(alpha=0.1)</code>，即使是 15 次多项式，曲线也变得惊人地平滑！这验证了<strong>贝叶斯先验对模型复杂度的约束作用</strong>。</p><hr><h3 id=41-从跑通代码到验证理论深度解析>4.1 从"跑通代码"到"验证理论"：深度解析<a class=anchor href=#41-%e4%bb%8e%e8%b7%91%e9%80%9a%e4%bb%a3%e7%a0%81%e5%88%b0%e9%aa%8c%e8%af%81%e7%90%86%e8%ae%ba%e6%b7%b1%e5%ba%a6%e8%a7%a3%e6%9e%90>#</a></h3><blockquote class=book-hint><p><strong>核心问题</strong>：上述代码不仅仅是"演示过拟合"，更是对 2.3 节数学推导的<strong>实证验证</strong>。让我们逐行剖析，揭示代码与理论的精确对应关系。</p></blockquote><h4 id=411-理论回顾ridge-回归的数学本质>4.1.1 理论回顾：Ridge 回归的数学本质<a class=anchor href=#411-%e7%90%86%e8%ae%ba%e5%9b%9e%e9%a1%beridge-%e5%9b%9e%e5%bd%92%e7%9a%84%e6%95%b0%e5%ad%a6%e6%9c%ac%e8%b4%a8>#</a></h4><p>在 2.3.4 节，我们证明了 Ridge 回归的目标函数：</p><p>$$
\hat{w}<em>{Ridge} = \arg\min</em>{w} \left[ \frac{1}{N} \sum_{i=1}^N (y_i - w^T x_i)^2 + \lambda |w|^2 \right]
$$</p><p>等价于贝叶斯 MAP 估计：</p><p>$$
\hat{w}<em>{MAP} = \arg\max</em>{w} \left[ \log P(y|X,w) + \log P(w) \right]
$$</p><p>其中正则化系数与先验方差的关系为：</p><p>$$
\lambda = \frac{\sigma^2}{N\tau^2}
$$</p><ul><li>$\sigma^2$：观测噪声方差（数据的不确定性）</li><li>$\tau^2$：先验方差（对权重大小的先验信念强度）</li><li>$N$：样本数量</li></ul><hr><h4 id=412-代码与理论的逐行对应>4.1.2 代码与理论的逐行对应<a class=anchor href=#412-%e4%bb%a3%e7%a0%81%e4%b8%8e%e7%90%86%e8%ae%ba%e7%9a%84%e9%80%90%e8%a1%8c%e5%af%b9%e5%ba%94>#</a></h4><p>让我们将代码与理论公式一一映射：</p><p><strong>1. 数据生成过程（对应贝叶斯模型假设）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>true_fun</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.2</span></span></span></code></pre></div><p><strong>理论对应</strong>：
$$
y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$</p><p>这里 <code>0.2</code> 正是噪声标准差 $\sigma = 0.2$，因此 $\sigma^2 = 0.04$。</p><blockquote class=book-hint><p><strong>洞见</strong>：这行代码在数学上定义了似然函数 $P(y|X,w)$ 的形式——高斯分布。</p></blockquote><hr><p><strong>2. Ridge 正则化参数（对应先验强度）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span></span></span></code></pre></div><p><strong>理论对应</strong>：
$$
\lambda = 0.1
$$</p><p>但这个 <code>alpha=0.1</code> 背后隐藏着什么？根据 $\lambda = \frac{\sigma^2}{N\tau^2}$，我们可以反推出隐式的先验方差：</p><p>$$
\tau^2 = \frac{\sigma^2}{N\lambda} = \frac{0.04}{30 \times 0.1} = \frac{0.04}{3} \approx 0.0133
$$</p><blockquote class=book-hint><p><strong>物理意义解读</strong>：</p><ul><li><strong>$\lambda$ 越大</strong> → 先验方差 $\tau^2$ 越小 → 先验信念越强 → 权重被约束得越死 → 曲线越平滑</li><li><strong>$\lambda$ 越小</strong> → 先验方差 $\tau^2$ 越大 → 先验信念越弱 → 允许权重变大 → 曲线可以更灵活（但可能过拟合）</li></ul></blockquote><p>在我们的例子中，$\tau^2 = 0.0133$ 意味着先验认为"权重的绝对值大概率不超过 $\sqrt{0.0133} \approx 0.115$"。这个强约束使得即使模型有 15 个多项式项，每个权重也不能太大，从而避免剧烈震荡。</p><hr><p><strong>3. 多项式展开（对应特征空间的维度）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>PolynomialFeatures</span><span class=p>(</span><span class=n>degree</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span> <span class=n>include_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></span></span></code></pre></div><p><strong>理论对应</strong>：将输入 $x$ 映射到高维特征空间：</p><p>$$
\phi(x) = [x, x^2, x^3, \dots, x^{15}]^T
$$</p><p>此时权重向量 $w \in \mathbb{R}^{15}$，模型变为：</p><p>$$
\hat{f}(x) = w_1 x + w_2 x^2 + \cdots + w_{15} x^{15}
$$</p><blockquote class=book-hint><p><strong>过拟合的根源</strong>：当 degree=15 时，模型有 15 个自由度，而我们只有 30 个样本点。如果没有正则化，模型会找到一组权重 $w$，使得曲线精确穿过几乎所有训练点，但在训练点之间剧烈震荡（高方差）。</p></blockquote><hr><p><strong>4. 优化过程（对应 MAP 估计的求解）</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pipeline</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>],</span> <span class=n>y</span><span class=p>)</span></span></span></code></pre></div><p><strong>理论对应</strong>：在后台，<code>Ridge</code> 正在求解：</p><p>$$
\hat{w} = \arg\min_{w} \left[ \sum_{i=1}^{30} (y_i - w^T \phi(x_i))^2 + 0.1 \times |w|^2 \right]
$$</p><p>这个优化问题有闭式解（Closed-form Solution）：</p><p>$$
\hat{w} = (X^T X + \lambda I)^{-1} X^T y
$$</p><p>其中 $X$ 是特征矩阵，$I$ 是单位矩阵。</p><blockquote class=book-hint><p><strong>关键点</strong>：注意分母中的 $\lambda I$ 项！这正是正则化（先验）的作用：</p><ul><li>当 $\lambda = 0$（无先验）时，解退化为普通最小二乘 $(X^T X)^{-1} X^T y$</li><li>当 $\lambda > 0$ 时，$\lambda I$ 使得矩阵 $(X^T X + \lambda I)$ 的条件数变小，即使 $X^T X$ 接近奇异（样本量少于特征数）时，也能稳定求解</li></ul></blockquote><hr><h4 id=413-数值实验验证-lambda-的作用>4.1.3 数值实验：验证 $\lambda$ 的作用<a class=anchor href=#413-%e6%95%b0%e5%80%bc%e5%ae%9e%e9%aa%8c%e9%aa%8c%e8%af%81-lambda-%e7%9a%84%e4%bd%9c%e7%94%a8>#</a></h4><p>让我们通过对比不同 <code>alpha</code> 值，直观感受先验强度对模型的约束：</p><table><thead><tr><th><code>alpha</code> 值</th><th>对应 $\tau^2$</th><th>先验信念强度</th><th>曲线表现</th></tr></thead><tbody><tr><td>0.001</td><td>1.33</td><td>极弱</td><td>几乎完美拟合训练点，剧烈震荡</td></tr><tr><td>0.01</td><td>0.133</td><td>弱</td><td>轻微震荡，开始平滑</td></tr><tr><td>0.1</td><td>0.0133</td><td>中等</td><td>平滑曲线，较好泛化</td></tr><tr><td>1.0</td><td>0.00133</td><td>强</td><td>过于平滑，开始欠拟合</td></tr></tbody></table><p><strong>实验建议</strong>：修改代码中的 <code>alpha</code> 值，观察曲线形状的变化，你会发现：</p><ul><li><code>alpha</code> 从 0.001 增加到 1.0 时，曲线从"狂野震荡"逐渐变为"保守平滑"</li><li>这正是贝叶斯先验从"几乎不约束"到"强力约束"的过程</li></ul><hr><h4 id=414-更深层的洞见为什么正则化能杀死震荡>4.1.4 更深层的洞见：为什么正则化能"杀死"震荡？<a class=anchor href=#414-%e6%9b%b4%e6%b7%b1%e5%b1%82%e7%9a%84%e6%b4%9e%e8%a7%81%e4%b8%ba%e4%bb%80%e4%b9%88%e6%ad%a3%e5%88%99%e5%8c%96%e8%83%bd%e6%9d%80%e6%ad%bb%e9%9c%87%e8%8d%a1>#</a></h4><p><strong>几何直觉</strong>：</p><ul><li>15 次多项式的曲线要剧烈震荡，必然需要某些高次项的系数 $w_k$ 非常大（正负交替）</li><li>L2 正则化 $|w|^2 = w_1^2 + w_2^2 + \cdots + w_{15}^2$ 会惩罚所有大的权重</li><li>在优化过程中，模型被迫在"拟合训练数据"和"保持权重较小"之间权衡</li><li>最终妥协的结果是：曲线大致跟随真实函数趋势，但不会为了穿过每个点而让高次项系数爆炸</li></ul><p><strong>代数证明</strong>（简化版）：假设某个权重 $w_k$ 很大，比如 $w_k = 10$，则：</p><ul><li>不加正则化时：只要它能让 MSE 下降，就会被接受</li><li>加正则化后：需要让 MSE 下降的收益，超过 $\lambda w_k^2 = 0.1 \times 100 = 10$ 的惩罚，才值得保留这么大的权重</li></ul><hr><h4 id=415-本节小结>4.1.5 本节小结<a class=anchor href=#415-%e6%9c%ac%e8%8a%82%e5%b0%8f%e7%bb%93>#</a></h4><p>通过这段代码，我们完成了从理论到实践的完整闭环：</p><ol><li><strong>数学推导</strong>（2.3 节）：证明了 Ridge 回归 = 高斯先验 MAP</li><li><strong>参数映射</strong>（本节）：明确了 <code>alpha=0.1</code> 对应 $\lambda = \frac{\sigma^2}{N\tau^2}$</li><li><strong>物理解释</strong>（本节）：理解了 $\lambda$ 增大 = 先验方差 $\tau^2$ 减小 = 约束变强</li><li><strong>视觉验证</strong>（代码图像）：看到了平滑曲线就是先验约束的直接体现</li></ol><blockquote class=book-hint><p><strong>关键领悟</strong>：</p><p>当你调用 <code>Ridge(alpha=0.1)</code> 时，你并不是在简单地"添加一个惩罚项"，而是在数学上等价于告诉模型：&ldquo;我先验地相信，权重应该服从 $\mathcal{N}(0, 0.0133)$ 的分布&rdquo;。这个先验信念与数据的似然函数共同决定了最终的后验分布，而 MAP 估计就是这个后验分布的峰值。</p><p><strong>下次当你在机器学习实践中调参 $\lambda$ 时，不要只是盲目网格搜索，而应该思考：</strong></p><ul><li>当前任务的噪声水平 $\sigma^2$ 大致是多少？</li><li>我有多少样本 $N$？</li><li>我愿意对权重施加多强的约束（先验方差 $\tau^2$ 应该多大）？</li><li>根据 $\lambda = \frac{\sigma^2}{N\tau^2}$ 来指导 $\lambda$ 的选择范围</li></ul><p>这就是从"炼丹"到"理性设计"的跨越。</p></blockquote><hr><h2 id=五拓展深入当传统理论失效双下降现象>五、拓展深入：当传统理论失效——双下降现象<a class=anchor href=#%e4%ba%94%e6%8b%93%e5%b1%95%e6%b7%b1%e5%85%a5%e5%bd%93%e4%bc%a0%e7%bb%9f%e7%90%86%e8%ae%ba%e5%a4%b1%e6%95%88%e5%8f%8c%e4%b8%8b%e9%99%8d%e7%8e%b0%e8%b1%a1>#</a></h2><blockquote class=book-hint><p><strong>思考</strong>：如果不加正则化，一味地增加模型参数（比如几千亿参数的大模型），模型一定会过拟合吗？</p></blockquote><h3 id=51-经典理论的困境>5.1 经典理论的困境<a class=anchor href=#51-%e7%bb%8f%e5%85%b8%e7%90%86%e8%ae%ba%e7%9a%84%e5%9b%b0%e5%a2%83>#</a></h3><p>传统的 Bias-Variance Tradeoff 告诉我们，随着参数量增加，测试误差会先降后升（U型曲线）。这解释了 SVM、随机森林等传统算法。</p><h3 id=52-深度学习的怪象>5.2 深度学习的怪象<a class=anchor href=#52-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%80%aa%e8%b1%a1>#</a></h3><p>但在深度学习时代（Deep Learning Era），我们发现了一个惊人的现象：
当参数量远远超过样本量（Over-parameterized）时，测试误差在短暂上升后，竟然<strong>再次下降</strong>！这就是著名的 <strong>&ldquo;双下降&rdquo; (Double Descent)</strong> 现象。</p><ol><li><strong>第一下降区 (Under-parameterized)</strong>：符合传统 U 型曲线。</li><li><strong>插值阈值 (Interpolation Threshold)</strong>：当模型大到刚好能记住所有训练样本时，过拟合最严重，测试误差达到峰值。</li><li><strong>第二下降区 (Over-parameterized)</strong>：当参数量继续增加，模型变得极其庞大时，虽然它依然记住了所有样本（训练误差为0），但测试误差反而继续降低，且往往低于经典的 Sweet Spot。</li></ol><h3 id=53-为什么会这样>5.3 为什么会这样？<a class=anchor href=#53-%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e8%bf%99%e6%a0%b7>#</a></h3><p>这目前是学术界的前沿热点。一种解释是：
极度过参数化的模型（如大模型）在 SGD 优化下，倾向于寻找**&ldquo;最平坦"的最小值 (Flattest Minima)<strong>。这种平坦性自带了一种</strong>隐式的正则化效果**，使得模型具有了某种奥卡姆剃刀式的简单性，从而神奇地拥有了良好的泛化能力。</p><blockquote class=book-hint><p><strong>启示</strong>：在深度学习时代，&ldquo;过拟合"的定义正在被重写。有时候，<strong>大</strong>（足够大）真的就是 <strong>美</strong>。</p></blockquote><hr><h2 id=六本章小结>六、本章小结<a class=anchor href=#%e5%85%ad%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><p>本章看似零散地讲解了流派之争、三要素、偏差方差等概念，但实际上它们构成了一个严密的认知闭环。如果我们从更高的维度俯瞰，会发现一个惊人的洞见：频率派和贝叶斯派的百年纠葛，本质上是<strong>求值 vs 求分布</strong>两种世界观的对立。</p><p>频率派认为参数 $\theta$ 是固定的未知常量，所以机器学习是一个<strong>优化问题</strong> $\arg\max_\theta P(X|\theta)$。当面对过拟合风险时，他们选择显式地惩罚模型复杂度，引入正则化项 $\lambda J(f)$。而贝叶斯派则认为 $\theta$ 本身就是随机变量，机器学习是一个<strong>积分问题</strong> $\int P(x_{new}|\theta) P(\theta|X) d\theta$，通过先验分布 $P(\theta)$ 来编码对参数的"主观信念&rdquo;。</p><p>但更深刻的是，这两种看似对立的方法论，在数学上竟然完全等价。L2 正则化等价于引入高斯先验 $\mathcal{N}(0, \sigma^2)$，L1 正则化等价于拉普拉斯先验。这揭示了一个哲学真理：<strong>约束与信念，本是同一枚硬币的两面</strong>。频率派的"惩罚复杂度"和贝叶斯派的"编码信念&rdquo;，殊途同归。</p><p>从统计学习三要素（模型、策略、算法）到偏差-方差权衡，我们看到了机器学习如何在拟合能力与泛化能力之间走钢丝。传统理论告诉我们，模型复杂度应该有一个"甜蜜点"（Sweet Spot），对应经典的 U 型曲线。但深度学习时代的双下降现象，彻底颠覆了这一认知：当模型足够大时，即使完美记住所有训练样本，测试误差反而会再次下降。这种"隐式正则化"的魔力，来自于 SGD 倾向于寻找最平坦的最小值。</p><p>回顾全章，我们建立了一套完整的认知体系，如下图所示：</p><pre class=mermaid>flowchart TD
    A[机器学习的哲学基石] --&gt; B[频率派：世界是确定的]
    A --&gt; C[贝叶斯派：世界是不确定的]

    B --&gt; D[方法论：MLE&lt;br/&gt;极大似然估计]
    C --&gt; E[方法论：MAP/MCMC&lt;br/&gt;后验分布估计]

    D --&gt; F[统计学习三要素]
    E --&gt; F

    F --&gt; F1[模型：假设空间 F]
    F --&gt; F2[策略：损失函数 + 正则化]
    F --&gt; F3[算法：优化求解器]

    F2 --&gt; G[核心难题：&lt;br/&gt;偏差-方差权衡]

    G --&gt; H1[高偏差 Low Bias&lt;br/&gt;欠拟合&lt;br/&gt;模型过于简单]
    G --&gt; H2[高方差 High Var&lt;br/&gt;过拟合&lt;br/&gt;对数据扰动敏感]

    H2 --&gt; I[解决方案分支]
    I --&gt; I1[频率派：显式正则化&lt;br/&gt;L1/L2/Dropout]
    I --&gt; I2[贝叶斯派：引入先验&lt;br/&gt;P theta ~ N 0 σ²]

    I1 -.数学等价.-&gt; I2

    I --&gt; J[现代深度学习：&lt;br/&gt;双下降现象]
    J --&gt; J1[过参数化时&lt;br/&gt;隐式正则化&lt;br/&gt;Flattest Minima]

    style A fill:#e1f5ff
    style G fill:#fff4e1
    style I1 fill:#ffe1e1
    style I2 fill:#e1ffe1
    style J fill:#f0e1ff</pre><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><p>从理论到实践，我们可以构建这样一条完整链条：世界观决定了方法论（MLE vs MAP），三要素框架解构了任何算法的本质，结构风险最小化给出了对抗过拟合的数学工具，而偏差-方差分解则提供了诊断模型问题的量化手段。传统范式通过 U 型曲线寻找 Sweet Spot，支撑着 SVM、随机森林等经典算法；而现代范式则在双下降的指引下，拥抱 Transformer、GPT 等超大规模模型。</p><p>这一章也为我们留下了几个值得深思的问题：如果贝叶斯派的先验本质上是"主观的"，我们凭什么相信它能带来客观的泛化能力？为什么 L2 正则化恰好等价于高斯先验，而不是其他分布？双下降现象告诉我们"大即是美"，但在资源受限的场景下，我们该如何平衡模型规模与计算成本？这些问题的答案，将在后续章节中逐步揭晓。</p><hr><h2 id=推荐阅读>推荐阅读<a class=anchor href=#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb>#</a></h2><ul><li><strong>白板推导系列 - P1 绪论</strong> (shuhuai008, B站)：深入讲解频率派与贝叶斯派的哲学差异，包含详细的板书推导过程。</li><li><strong>《统计学习方法》 - 第1章 概论</strong> (李航)：系统阐述统计学习三要素框架（模型、策略、算法）。</li><li><strong>Belkin et al. (2019). &ldquo;Reconciling modern machine learning practice and the classical bias-variance trade-off&rdquo;</strong>：关于传统偏差-方差权衡理论与现代机器学习实践冲突的开创性研究。</li><li><strong>Nakkiran et al. (2021). &ldquo;Deep Double Descent: Where Bigger Models and More Data Hurt&rdquo;</strong>：深入探讨双下降现象的理论解释和实验验证。</li></ul><hr><p><strong>下章预告</strong>：</p><p>理解了"为什么要学机器学习"，我们需要锻造"如何推导机器学习"的数学兵器。</p><p>下一章 <strong><a href=%e7%ac%ac02%e7%ab%a0_%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e5%9f%ba%e7%a1%80.md>第02章：线性代数基础</a></strong> 将带你掌握机器学习推导的核心工具——<strong>矩阵求导</strong>。我们不会停留在机械的公式记忆，而是从基本原理出发，理解梯度的本质和计算方法。这套工具，将帮助你理解大多数机器学习算法的推导过程。</p></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>Agent最佳设计模式</span>
</a></span><span><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/ class="flex align-center"><span>第02章 矩阵运算与微积分</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#目录>目录</a></li><li><a href=#一世界观的碰撞频率派-vs-贝叶斯派>一、世界观的碰撞：频率派 vs 贝叶斯派</a><ul><li><a href=#11-频率派-the-frequentist-view>1.1 频率派 (The Frequentist View)</a></li><li><a href=#12-贝叶斯派-the-bayesian-view>1.2 贝叶斯派 (The Bayesian View)</a></li><li><a href=#13-核心案例推导抛硬币的哲学>1.3 核心案例推导：抛硬币的哲学</a><ul><li><a href=#131-问题设定>1.3.1 问题设定</a></li><li><a href=#132-频率派极大似然估计-mle>1.3.2 频率派：极大似然估计 (MLE)</a></li><li><a href=#133-mle-的致命缺陷>1.3.3 MLE 的致命缺陷</a></li><li><a href=#134-贝叶斯派最大后验估计-map>1.3.4 贝叶斯派：最大后验估计 (MAP)</a></li><li><a href=#135-深度直觉先验就是伪计数>1.3.5 深度直觉：先验就是"伪计数"</a></li><li><a href=#136-当数据量很大时先验被淹没>1.3.6 当数据量很大时：先验被"淹没"</a></li><li><a href=#137-可视化对比>1.3.7 可视化对比</a></li><li><a href=#138-扩展完全贝叶斯推断>1.3.8 扩展：完全贝叶斯推断</a></li></ul></li></ul></li><li><a href=#二统计学习三要素解构算法的万能公式>二、统计学习三要素：解构算法的万能公式</a><ul><li><a href=#21-模型-model>2.1 模型 (Model)</a></li><li><a href=#22-策略-strategy>2.2 策略 (Strategy)</a></li><li><a href=#23-数学证明为什么正则化等价于先验>2.3 数学证明：为什么正则化等价于先验？</a><ul><li><a href=#231-问题设定线性回归模型>2.3.1 问题设定：线性回归模型</a></li><li><a href=#232-频率派视角l2-正则化>2.3.2 频率派视角：L2 正则化</a></li><li><a href=#233-贝叶斯派视角从先验到-map>2.3.3 贝叶斯派视角：从先验到 MAP</a></li><li><a href=#234-神奇的等价性>2.3.4 神奇的等价性</a></li><li><a href=#235-深度解读>2.3.5 深度解读</a></li><li><a href=#236-扩展l1-正则化与拉普拉斯先验>2.3.6 扩展：L1 正则化与拉普拉斯先验</a></li><li><a href=#237-可视化对比>2.3.7 可视化对比</a></li></ul></li><li><a href=#24-算法-algorithm>2.4 算法 (Algorithm)</a></li></ul></li><li><a href=#三核心难题偏差-方差权衡-bias-variance-tradeoff>三、核心难题：偏差-方差权衡 (Bias-Variance Tradeoff)</a><ul><li><a href=#31-误差分解公式推导>3.1 误差分解公式推导</a></li><li><a href=#32-经典-u-型曲线>3.2 经典 U 型曲线</a></li></ul></li><li><a href=#四实战演练亲眼目睹过拟合与正则化>四、实战演练：亲眼目睹过拟合与正则化</a><ul><li><a href=#41-从跑通代码到验证理论深度解析>4.1 从"跑通代码"到"验证理论"：深度解析</a><ul><li><a href=#411-理论回顾ridge-回归的数学本质>4.1.1 理论回顾：Ridge 回归的数学本质</a></li><li><a href=#412-代码与理论的逐行对应>4.1.2 代码与理论的逐行对应</a></li><li><a href=#413-数值实验验证-lambda-的作用>4.1.3 数值实验：验证 $\lambda$ 的作用</a></li><li><a href=#414-更深层的洞见为什么正则化能杀死震荡>4.1.4 更深层的洞见：为什么正则化能"杀死"震荡？</a></li><li><a href=#415-本节小结>4.1.5 本节小结</a></li></ul></li></ul></li><li><a href=#五拓展深入当传统理论失效双下降现象>五、拓展深入：当传统理论失效——双下降现象</a><ul><li><a href=#51-经典理论的困境>5.1 经典理论的困境</a></li><li><a href=#52-深度学习的怪象>5.2 深度学习的怪象</a></li><li><a href=#53-为什么会这样>5.3 为什么会这样？</a></li></ul></li><li><a href=#六本章小结>六、本章小结</a></li><li><a href=#推荐阅读>推荐阅读</a></li></ul></nav></div></aside></main></body></html>