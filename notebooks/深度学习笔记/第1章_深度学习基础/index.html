<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='第二篇:深度学习基础(快速回顾)# 目标读者:有机器学习基础,需要快速掌握深度学习和PyTorch的读者
学习重点:PyTorch实战、神经网络核心概念、CNN基础
篇章概述# 深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。
为什么选择PyTorch?# 动态计算图:更符合Python编程习惯,易于调试 学术界主流:顶级会议论文大多使用PyTorch实现 生态完善:torchvision、torchaudio等丰富的扩展库 PyTorch 2.x:引入torch.compile,性能大幅提升 章节安排# 第3章:神经网络基础# 3.1 从感知机到多层神经网络 3.2 反向传播算法详解 3.3 激活函数的选择与影响 3.4 正则化技术:BatchNorm与Dropout 实战:使用PyTorch构建第一个神经网络(MNIST手写数字识别) 核心技能:
掌握PyTorch的基本操作(Tensor、autograd、nn.Module) 理解神经网络的训练流程 学会使用GPU加速训练 第4章:卷积神经网络(CNN)# 4.1 卷积层的工作原理 4.2 池化层与降维 4.3 经典CNN架构:LeNet → AlexNet → VGG 4.4 CNN的可视化与理解 实战:CIFAR-10图像分类(从零构建CNN) 核心技能:
理解卷积操作的本质 掌握CNN的设计原则 学会使用torchvision进行图像处理 技术栈# 环境要求# # Python >= 3.10 python --version # 安装PyTorch (2025年推荐) # CPU版本 pip install torch torchvision torchaudio # GPU版本(CUDA 12.1) pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # 或使用uv(更快) uv pip install torch torchvision torchaudio核心依赖# PyTorch >= 2.0:深度学习框架 torchvision:计算机视觉工具库 matplotlib:可视化 tqdm:进度条 验证安装# import torch import torchvision print(f"PyTorch版本: {torch.__version__}") print(f"CUDA可用: {torch.cuda.is_available()}") if torch.cuda.is_available(): print(f"CUDA版本: {torch.version.cuda}") print(f"GPU设备: {torch.cuda.get_device_name(0)}") 学习建议# 1. 动手实践为主# 每个代码示例都要运行:不要只看代码 修改超参数观察变化:学习率、批次大小、网络层数等 尝试不同的数据集:Fashion-MNIST、SVHN等 2. 理解核心概念# 梯度下降:深度学习的基石 反向传播:如何高效计算梯度 正则化:防止过拟合的关键 3. 参考官方文档# PyTorch官方教程 PyTorch文档 torchvision文档 4. 循序渐进# 第3章(1-2天) → 第4章(2-3天) ↓ ↓ 理解基础 掌握CNN ↓ ↓ 为后续现代架构(ResNet、Transformer)打下坚实基础 与前后篇的关系# 第一篇:机器学习基础 ↓ (线性模型、优化算法) ↓ 第二篇:深度学习基础 ← 当前篇 ↓ (神经网络、CNN) ↓ 第三篇:现代CNN架构 ↓ (ResNet、EfficientNet等) 代码规范# 本篇所有代码遵循以下规范:
'><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://LordFoxFairy.github.io/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><meta property="og:site_name" content="LordFoxFairy的笔记本"><meta property="og:title" content="第1章 深度学习基础"><meta property="og:description" content='第二篇:深度学习基础(快速回顾)# 目标读者:有机器学习基础,需要快速掌握深度学习和PyTorch的读者
学习重点:PyTorch实战、神经网络核心概念、CNN基础
篇章概述# 深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。
为什么选择PyTorch?# 动态计算图:更符合Python编程习惯,易于调试 学术界主流:顶级会议论文大多使用PyTorch实现 生态完善:torchvision、torchaudio等丰富的扩展库 PyTorch 2.x:引入torch.compile,性能大幅提升 章节安排# 第3章:神经网络基础# 3.1 从感知机到多层神经网络 3.2 反向传播算法详解 3.3 激活函数的选择与影响 3.4 正则化技术:BatchNorm与Dropout 实战:使用PyTorch构建第一个神经网络(MNIST手写数字识别) 核心技能:
掌握PyTorch的基本操作(Tensor、autograd、nn.Module) 理解神经网络的训练流程 学会使用GPU加速训练 第4章:卷积神经网络(CNN)# 4.1 卷积层的工作原理 4.2 池化层与降维 4.3 经典CNN架构:LeNet → AlexNet → VGG 4.4 CNN的可视化与理解 实战:CIFAR-10图像分类(从零构建CNN) 核心技能:
理解卷积操作的本质 掌握CNN的设计原则 学会使用torchvision进行图像处理 技术栈# 环境要求# # Python >= 3.10 python --version # 安装PyTorch (2025年推荐) # CPU版本 pip install torch torchvision torchaudio # GPU版本(CUDA 12.1) pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # 或使用uv(更快) uv pip install torch torchvision torchaudio核心依赖# PyTorch >= 2.0:深度学习框架 torchvision:计算机视觉工具库 matplotlib:可视化 tqdm:进度条 验证安装# import torch import torchvision print(f"PyTorch版本: {torch.__version__}") print(f"CUDA可用: {torch.cuda.is_available()}") if torch.cuda.is_available(): print(f"CUDA版本: {torch.version.cuda}") print(f"GPU设备: {torch.cuda.get_device_name(0)}") 学习建议# 1. 动手实践为主# 每个代码示例都要运行:不要只看代码 修改超参数观察变化:学习率、批次大小、网络层数等 尝试不同的数据集:Fashion-MNIST、SVHN等 2. 理解核心概念# 梯度下降:深度学习的基石 反向传播:如何高效计算梯度 正则化:防止过拟合的关键 3. 参考官方文档# PyTorch官方教程 PyTorch文档 torchvision文档 4. 循序渐进# 第3章(1-2天) → 第4章(2-3天) ↓ ↓ 理解基础 掌握CNN ↓ ↓ 为后续现代架构(ResNet、Transformer)打下坚实基础 与前后篇的关系# 第一篇:机器学习基础 ↓ (线性模型、优化算法) ↓ 第二篇:深度学习基础 ← 当前篇 ↓ (神经网络、CNN) ↓ 第三篇:现代CNN架构 ↓ (ResNet、EfficientNet等) 代码规范# 本篇所有代码遵循以下规范:'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="notebooks"><meta itemprop=name content="第1章 深度学习基础"><meta itemprop=description content='第二篇:深度学习基础(快速回顾)# 目标读者:有机器学习基础,需要快速掌握深度学习和PyTorch的读者
学习重点:PyTorch实战、神经网络核心概念、CNN基础
篇章概述# 深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。
为什么选择PyTorch?# 动态计算图:更符合Python编程习惯,易于调试 学术界主流:顶级会议论文大多使用PyTorch实现 生态完善:torchvision、torchaudio等丰富的扩展库 PyTorch 2.x:引入torch.compile,性能大幅提升 章节安排# 第3章:神经网络基础# 3.1 从感知机到多层神经网络 3.2 反向传播算法详解 3.3 激活函数的选择与影响 3.4 正则化技术:BatchNorm与Dropout 实战:使用PyTorch构建第一个神经网络(MNIST手写数字识别) 核心技能:
掌握PyTorch的基本操作(Tensor、autograd、nn.Module) 理解神经网络的训练流程 学会使用GPU加速训练 第4章:卷积神经网络(CNN)# 4.1 卷积层的工作原理 4.2 池化层与降维 4.3 经典CNN架构:LeNet → AlexNet → VGG 4.4 CNN的可视化与理解 实战:CIFAR-10图像分类(从零构建CNN) 核心技能:
理解卷积操作的本质 掌握CNN的设计原则 学会使用torchvision进行图像处理 技术栈# 环境要求# # Python >= 3.10 python --version # 安装PyTorch (2025年推荐) # CPU版本 pip install torch torchvision torchaudio # GPU版本(CUDA 12.1) pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # 或使用uv(更快) uv pip install torch torchvision torchaudio核心依赖# PyTorch >= 2.0:深度学习框架 torchvision:计算机视觉工具库 matplotlib:可视化 tqdm:进度条 验证安装# import torch import torchvision print(f"PyTorch版本: {torch.__version__}") print(f"CUDA可用: {torch.cuda.is_available()}") if torch.cuda.is_available(): print(f"CUDA版本: {torch.version.cuda}") print(f"GPU设备: {torch.cuda.get_device_name(0)}") 学习建议# 1. 动手实践为主# 每个代码示例都要运行:不要只看代码 修改超参数观察变化:学习率、批次大小、网络层数等 尝试不同的数据集:Fashion-MNIST、SVHN等 2. 理解核心概念# 梯度下降:深度学习的基石 反向传播:如何高效计算梯度 正则化:防止过拟合的关键 3. 参考官方文档# PyTorch官方教程 PyTorch文档 torchvision文档 4. 循序渐进# 第3章(1-2天) → 第4章(2-3天) ↓ ↓ 理解基础 掌握CNN ↓ ↓ 为后续现代架构(ResNet、Transformer)打下坚实基础 与前后篇的关系# 第一篇:机器学习基础 ↓ (线性模型、优化算法) ↓ 第二篇:深度学习基础 ← 当前篇 ↓ (神经网络、CNN) ↓ 第三篇:现代CNN架构 ↓ (ResNet、EfficientNet等) 代码规范# 本篇所有代码遵循以下规范:'><meta itemprop=wordCount content="2304"><title>第1章 深度学习基础 | LordFoxFairy的笔记本</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://LordFoxFairy.github.io/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.a7a11a812549b6c20d4eeaf4a5a8317847527505f7a0ad3e6824fb320b3128a8.js integrity="sha256-p6EagSVJtsINTur0pagxeEdSdQX3oK0+aCT7MgsxKKg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-notebooks"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LordFoxFairy的笔记本</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-8410cdd8ef137d6cb143c08e6db6ba10 class=toggle>
<label for=section-8410cdd8ef137d6cb143c08e6db6ba10 class=flex><a role=button>LangChain笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87-%E5%9F%BA%E7%A1%80%E8%AE%A4%E7%9F%A5/>第一篇 基础认知</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%AE%9E%E6%88%98/>第二篇 快速上手实战</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87-langgraph-%E6%B7%B1%E5%85%A5/>第三篇 LangGraph 深入</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87langchain%E7%AF%87/>第四篇 RAG基础篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87-rag%E5%9F%BA%E7%A1%80%E7%AF%87llamaindex%E7%AF%87/>第四篇 RAG基础篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87langchain%E7%AF%87/>第五篇 RAG高级篇(LangChain篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87-rag%E9%AB%98%E7%BA%A7%E7%AF%87llamaindex%E7%AF%87/>第五篇 RAG高级篇(LlamaIndex篇)</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87-%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/>第六篇 文档处理与数据清洗</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87-deep-agents/>第七篇 Deep Agents</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87-middleware-%E5%B7%A5%E7%A8%8B%E5%8C%96/>第八篇 Middleware 工程化</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B9%9D%E7%AF%87-agent-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/>第九篇 Agent 架构设计</a></li><li><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%8D%81%E7%AF%87-%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%9B%91%E6%8E%A7%E8%AF%84%E4%BC%B0/>第十篇 生产实践与监控评估</a></li><li><input type=checkbox id=section-a3e9b1811e5a21dbfddb0753f565cedb class=toggle>
<label for=section-a3e9b1811e5a21dbfddb0753f565cedb class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><input type=checkbox id=section-9a64553a000534ad3c81a611c6c29ec2 class=toggle>
<label for=section-9a64553a000534ad3c81a611c6c29ec2 class=flex><a role=button>code</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-6fad2589acfdd5d6a1fb55beb392fb77 class=toggle>
<label for=section-6fad2589acfdd5d6a1fb55beb392fb77 class=flex><a href=/notebooks/langchain%E7%AC%94%E8%AE%B0/code/fasta2a/>fasta2a</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li></ul></li></ul></li><li><input type=checkbox id=section-ce33306d44f9ed4d296b9a79329bed1c class=toggle>
<label for=section-ce33306d44f9ed4d296b9a79329bed1c class=flex><a role=button>图像算法笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E7%AF%87_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第一篇 机器学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E7%AF%87_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>第二篇 深度学习基础</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AF%87_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/>第三篇 计算机视觉核心技术</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E7%AF%87_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8Eyolo%E7%B3%BB%E5%88%97/>第四篇 目标检测与YOLO系列</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E7%AF%87_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/>第五篇 图像分割</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E7%AF%87_%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>第六篇 生成模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E7%AF%87_%E8%A7%86%E8%A7%89%E5%A4%A7%E6%A8%A1%E5%9E%8B/>第七篇 视觉大模型</a></li><li><a href=/notebooks/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AB%E7%AF%87_%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5/>第八篇 生产实践</a></li></ul></li><li><input type=checkbox id=section-77f5db9e44a9af6dab4403b49a65334f class=toggle>
<label for=section-77f5db9e44a9af6dab4403b49a65334f class=flex><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/>大模型笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><input type=checkbox id=section-db59738de306cbbe384ad7fbd5cf11b5 class=toggle>
<label for=section-db59738de306cbbe384ad7fbd5cf11b5 class=flex><a role=button>第一部分：大语言模型基础</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC1%E7%AB%A0_%E5%88%9D%E8%AF%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>第1章 初识大语言模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC2%E7%AB%A0_%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第2章 与模型对话：提示工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/%E7%AC%AC3%E7%AB%A0_%E8%AF%AD%E8%A8%80%E7%9A%84%E5%9F%BA%E7%9F%B3%E5%88%86%E8%AF%8D%E4%B8%8E%E5%B5%8C%E5%85%A5/>第3章 语言的基石：分词与嵌入</a></li></ul></li><li><input type=checkbox id=section-30b17899e420f6a53aa6e578440dd132 class=toggle>
<label for=section-30b17899e420f6a53aa6e578440dd132 class=flex><a role=button>第二部分：Transformer架构揭秘</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC1%E7%AB%A0_transformer%E6%A0%B8%E5%BF%83%E6%8F%AD%E7%A7%98/>第1章 Transformer核心揭秘</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC2%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%B6%E6%97%8F%E8%B0%B1%E7%B3%BB%E4%BB%8E%E7%BC%96%E7%A0%81%E5%99%A8%E5%88%B0%E8%A7%A3%E7%A0%81%E5%99%A8/>第2章 模型家族谱系：从编码器到解码器</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86transformer%E6%9E%B6%E6%9E%84%E6%8F%AD%E7%A7%98/%E7%AC%AC3%E7%AB%A0_%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A5%A5%E7%A7%98%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%99%BA%E8%83%BD/>第3章 预训练的奥秘：从数据到智能</a></li></ul></li><li><input type=checkbox id=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=toggle>
<label for=section-da88ac5e3d01ea6f4b2996cb87ecf19f class=flex><a role=button>第三部分：数据工程与定制化</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC1%E7%AB%A0_%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/>第1章 数据工程基础</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC2%E7%AB%A0_%E5%BE%AE%E8%B0%83%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E6%A8%A1%E5%9E%8B/>第2章 微调你的专属模型</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC3%E7%AB%A0_%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96/>第3章 与人类对齐：偏好优化</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%AE%9A%E5%88%B6%E5%8C%96/%E7%AC%AC4%E7%AB%A0_%E5%88%9B%E5%BB%BA%E6%9B%B4%E4%BC%98%E7%9A%84%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/>第4章 创建更优的嵌入模型</a></li></ul></li><li><input type=checkbox id=section-81f0a7a10bc544ea0e1fd883e2a436eb class=toggle>
<label for=section-81f0a7a10bc544ea0e1fd883e2a436eb class=flex><a role=button>第四部分：大模型应用开发</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC1%E7%AB%A0_%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0/>第1章 提示工程与上下文学习</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC2%E7%AB%A0_%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag%E5%8E%9F%E7%90%86/>第2章 检索增强生成（RAG）原理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC3%E7%AB%A0_%E6%99%BA%E8%83%BD%E4%BD%93agent%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/>第3章 智能体（Agent）核心机制</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/%E7%AC%AC4%E7%AB%A0_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/>第4章 多模态大模型原理</a></li></ul></li><li><input type=checkbox id=section-0509decfe880d3d2074947902aea8022 class=toggle>
<label for=section-0509decfe880d3d2074947902aea8022 class=flex><a role=button>第五部分：工程实战工具栈</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC1%E7%AB%A0_hugging_face%E7%94%9F%E6%80%81%E5%85%A8%E6%99%AF/>第1章 Hugging Face生态全景</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC2%E7%AB%A0_llama-factory%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%8E%82/>第2章 LLaMA-Factory微调工厂</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC3%E7%AB%A0_trl%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/>第3章 TRL与强化学习实战</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC4%E7%AB%A0_deepspeed%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>第4章 DeepSpeed分布式训练</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E5%B7%A5%E5%85%B7%E6%A0%88/%E7%AC%AC5%E7%AB%A0_%E7%AB%AF%E5%88%B0%E7%AB%AFllm%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/>第5章 端到端LLM项目实战</a></li></ul></li><li><input type=checkbox id=section-dc465ed94a0f7fb78440cab8a8a2a28b class=toggle>
<label for=section-dc465ed94a0f7fb78440cab8a8a2a28b class=flex><a role=button>第六部分：生产部署与评估</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC1%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/>第1章 模型压缩与推理加速</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC2%E7%AB%A0_vllm%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86/>第2章 vLLM高性能推理</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%AF%84%E4%BC%B0/%E7%AC%AC3%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%BD%93%E7%B3%BB/>第3章 模型评估体系</a></li></ul></li><li><input type=checkbox id=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=toggle>
<label for=section-54ec04ac2cdda7238de2ed38f0bcb7b1 class=flex><a role=button>第七部分：高级技术专题</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC1%E7%AB%A0_%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8A%80%E6%9C%AF/>第1章 长上下文技术</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC2%E7%AB%A0_%E6%96%B0%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%8E%A2%E7%B4%A2/>第2章 新型架构探索</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC3%E7%AB%A0_%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%BB%91%E7%A7%91%E6%8A%80/>第3章 推理加速黑科技</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC4%E7%AB%A0_%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98/>第4章 推理模型专题</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86%E9%AB%98%E7%BA%A7%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E7%AC%AC5%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/>第5章 模型安全与可解释性</a></li></ul></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/glossary/>GLOSSARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/roadmap/>ROADMAP</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/summary/>SUMMARY</a></li><li><a href=/notebooks/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%AE%8C%E7%BB%93%E6%8A%A5%E5%91%8A/>完结报告</a></li></ul></li><li><input type=checkbox id=section-7177254393287994b491879262a62a06 class=toggle>
<label for=section-7177254393287994b491879262a62a06 class=flex><a role=button>实践笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/2.-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-fastapi/>深入理解 FastAPI</a></li><li><a href=/notebooks/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/4.-agent%E6%9C%80%E4%BD%B3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/>Agent最佳设计模式</a></li></ul></li><li><input type=checkbox id=section-0172eb8516eeb7b4e657f3949a135c25 class=toggle>
<label for=section-0172eb8516eeb7b4e657f3949a135c25 class=flex><a role=button>机器学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC01%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/>第01章 机器学习概览</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC02%E7%AB%A0_%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86/>第02章 矩阵运算与微积分</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC03%E7%AB%A0_svd%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/>第03章 SVD与矩阵分解</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC04%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83_%E6%8C%87%E6%95%B0%E6%97%8F%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/>第04章 概率分布 指数族与共轭先验</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC05%E7%AB%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>第05章 线性回归</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC06%E7%AB%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/>第06章 感知机</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC07%E7%AB%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm/>第07章 支持向量机(SVM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC08%E7%AB%A0_%E6%A0%B8%E6%96%B9%E6%B3%95/>第08章 核方法</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC09%E7%AB%A0_%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>第09章 决策树与集成学习</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC10%E7%AB%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/>第10章 逻辑回归与最大熵模型</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC11%E7%AB%A0_%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Bglm/>第11章 广义线性模型(GLM)</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC13%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E8%A1%A8%E7%A4%BA/>第13章 概率图模型 表示</a></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC14%E7%AB%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B_%E6%8E%A8%E6%96%AD/>第14章 概率图模型 推断</a></li><li><input type=checkbox id=section-b17efd79c46843a887113a063f929150 class=toggle>
<label for=section-b17efd79c46843a887113a063f929150 class=flex><a role=button>assets</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul></ul></li><li><a href=/notebooks/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/skills/>skills</a></li></ul></li><li><input type=checkbox id=section-29762f225235d2ac19b613cc28b093c8 class=toggle checked>
<label for=section-29762f225235d2ac19b613cc28b093c8 class=flex><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>深度学习笔记</a>
<img src=/icons/chevron-right.svg class=book-icon alt=Expand></label><ul><li><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AC%AC1%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ class=active>第1章 深度学习基础</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>第1章 深度学习基础</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#篇章概述>篇章概述</a><ul><li><a href=#为什么选择pytorch>为什么选择PyTorch?</a></li></ul></li><li><a href=#章节安排>章节安排</a><ul><li><a href=#第3章神经网络基础>第3章:神经网络基础</a></li><li><a href=#第4章卷积神经网络cnn>第4章:卷积神经网络(CNN)</a></li></ul></li><li><a href=#技术栈>技术栈</a><ul><li><a href=#环境要求>环境要求</a></li><li><a href=#核心依赖>核心依赖</a></li><li><a href=#验证安装>验证安装</a></li></ul></li><li><a href=#学习建议>学习建议</a><ul><li><a href=#1-动手实践为主>1. 动手实践为主</a></li><li><a href=#2-理解核心概念>2. 理解核心概念</a></li><li><a href=#3-参考官方文档>3. 参考官方文档</a></li><li><a href=#4-循序渐进>4. 循序渐进</a></li></ul></li><li><a href=#与前后篇的关系>与前后篇的关系</a></li><li><a href=#代码规范>代码规范</a><ul><li><a href=#1-目录结构>1. 目录结构</a></li><li><a href=#2-代码风格>2. 代码风格</a></li><li><a href=#3-示例模板>3. 示例模板</a></li></ul></li><li><a href=#常见问题>常见问题</a><ul><li><a href=#q1-pytorch-1x-vs-2x-主要区别>Q1: PyTorch 1.x vs 2.x 主要区别?</a></li><li><a href=#q2-需要gpu吗>Q2: 需要GPU吗?</a></li><li><a href=#q3-如何选择学习率>Q3: 如何选择学习率?</a></li><li><a href=#q4-batchnorm和dropout如何选择>Q4: BatchNorm和Dropout如何选择?</a></li></ul></li><li><a href=#拓展资源>拓展资源</a><ul><li><a href=#书籍>书籍</a></li><li><a href=#在线课程>在线课程</a></li><li><a href=#实践平台>实践平台</a></li></ul></li></ul><ul><li><a href=#31-从感知机到多层神经网络>3.1 从感知机到多层神经网络</a><ul><li><a href=#311-感知机perceptron>3.1.1 感知机(Perceptron)</a></li><li><a href=#312-多层感知机mlp>3.1.2 多层感知机(MLP)</a></li></ul></li><li><a href=#32-反向传播算法详解>3.2 反向传播算法详解</a><ul><li><a href=#321-核心思想>3.2.1 核心思想</a></li><li><a href=#322-链式法则>3.2.2 链式法则</a></li><li><a href=#323-pytorch自动微分>3.2.3 PyTorch自动微分</a></li></ul></li><li><a href=#33-激活函数的选择与影响>3.3 激活函数的选择与影响</a><ul><li><a href=#331-为什么需要激活函数>3.3.1 为什么需要激活函数?</a></li><li><a href=#332-常用激活函数>3.3.2 常用激活函数</a></li><li><a href=#333-pytorch实现>3.3.3 PyTorch实现</a></li></ul></li><li><a href=#34-正则化技术batchnorm与dropout>3.4 正则化技术:BatchNorm与Dropout</a><ul><li><a href=#341-batch-normalization>3.4.1 Batch Normalization</a></li><li><a href=#342-dropout>3.4.2 Dropout</a></li><li><a href=#343-batchnorm-vs-dropout>3.4.3 BatchNorm vs Dropout</a></li></ul></li><li><a href=#35-实战mnist手写数字识别>3.5 实战:MNIST手写数字识别</a><ul><li><a href=#351-任务描述>3.5.1 任务描述</a></li><li><a href=#352-完整代码>3.5.2 完整代码</a></li><li><a href=#353-运行与调试>3.5.3 运行与调试</a></li></ul></li><li><a href=#36-pytorch核心api总结>3.6 PyTorch核心API总结</a><ul><li><a href=#361-张量tensor操作>3.6.1 张量(Tensor)操作</a></li><li><a href=#362-nnmodule核心方法>3.6.2 nn.Module核心方法</a></li><li><a href=#363-优化器optimizer>3.6.3 优化器(Optimizer)</a></li><li><a href=#364-损失函数>3.6.4 损失函数</a></li></ul></li><li><a href=#37-本章小结>3.7 本章小结</a><ul><li><a href=#核心知识点>核心知识点</a></li><li><a href=#pytorch编程范式>PyTorch编程范式</a></li><li><a href=#检查清单>检查清单</a></li></ul></li><li><a href=#38-练习与思考>3.8 练习与思考</a><ul><li><a href=#基础练习>基础练习</a></li><li><a href=#进阶挑战>进阶挑战</a></li><li><a href=#思考题>思考题</a></li></ul></li></ul><ul><li><a href=#41-为什么需要cnn>4.1 为什么需要CNN?</a><ul><li><a href=#411-全连接网络的局限>4.1.1 全连接网络的局限</a></li><li><a href=#412-cnn的核心思想>4.1.2 CNN的核心思想</a></li></ul></li><li><a href=#42-卷积层的工作原理>4.2 卷积层的工作原理</a><ul><li><a href=#421-卷积操作>4.2.1 卷积操作</a></li><li><a href=#422-关键概念>4.2.2 关键概念</a></li><li><a href=#423-pytorch实现>4.2.3 PyTorch实现</a></li></ul></li><li><a href=#43-池化层与降维>4.3 池化层与降维</a><ul><li><a href=#431-池化操作>4.3.1 池化操作</a></li><li><a href=#432-pytorch实现>4.3.2 PyTorch实现</a></li><li><a href=#433-池化-vs-步长卷积>4.3.3 池化 vs 步长卷积</a></li></ul></li><li><a href=#44-经典cnn架构演进>4.4 经典CNN架构演进</a><ul><li><a href=#441-lenet-5-1998>4.4.1 LeNet-5 (1998)</a></li><li><a href=#442-alexnet-2012>4.4.2 AlexNet (2012)</a></li><li><a href=#443-vgg-2014>4.4.3 VGG (2014)</a></li><li><a href=#444-架构对比>4.4.4 架构对比</a></li></ul></li><li><a href=#45-实战cifar-10图像分类>4.5 实战:CIFAR-10图像分类</a><ul><li><a href=#451-任务描述>4.5.1 任务描述</a></li><li><a href=#452-网络设计>4.5.2 网络设计</a></li><li><a href=#453-完整代码>4.5.3 完整代码</a></li><li><a href=#454-数据增强>4.5.4 数据增强</a></li><li><a href=#455-运行与结果>4.5.5 运行与结果</a></li></ul></li><li><a href=#46-cnn可视化与理解>4.6 CNN可视化与理解</a><ul><li><a href=#461-为什么要可视化>4.6.1 为什么要可视化?</a></li><li><a href=#462-可视化技术>4.6.2 可视化技术</a></li></ul></li><li><a href=#47-本章小结>4.7 本章小结</a><ul><li><a href=#核心知识点-1>核心知识点</a></li><li><a href=#cnn-vs-mlp>CNN vs MLP</a></li><li><a href=#检查清单-1>检查清单</a></li></ul></li><li><a href=#48-练习与思考>4.8 练习与思考</a><ul><li><a href=#基础练习-1>基础练习</a></li><li><a href=#进阶挑战-1>进阶挑战</a></li><li><a href=#思考题-1>思考题</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=第二篇深度学习基础快速回顾>第二篇:深度学习基础(快速回顾)<a class=anchor href=#%e7%ac%ac%e4%ba%8c%e7%af%87%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e5%bf%ab%e9%80%9f%e5%9b%9e%e9%a1%be>#</a></h1><blockquote class=book-hint><p><strong>目标读者</strong>:有机器学习基础,需要快速掌握深度学习和PyTorch的读者</p><p><strong>学习重点</strong>:PyTorch实战、神经网络核心概念、CNN基础</p></blockquote><hr><h2 id=篇章概述>篇章概述<a class=anchor href=#%e7%af%87%e7%ab%a0%e6%a6%82%e8%bf%b0>#</a></h2><p>深度学习是计算机视觉的核心技术。本篇将快速回顾深度学习的关键概念,重点放在PyTorch框架和卷积神经网络(CNN)的实战应用。</p><h3 id=为什么选择pytorch>为什么选择PyTorch?<a class=anchor href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9pytorch>#</a></h3><ul><li><strong>动态计算图</strong>:更符合Python编程习惯,易于调试</li><li><strong>学术界主流</strong>:顶级会议论文大多使用PyTorch实现</li><li><strong>生态完善</strong>:torchvision、torchaudio等丰富的扩展库</li><li><strong>PyTorch 2.x</strong>:引入torch.compile,性能大幅提升</li></ul><hr><h2 id=章节安排>章节安排<a class=anchor href=#%e7%ab%a0%e8%8a%82%e5%ae%89%e6%8e%92>#</a></h2><h3 id=第3章神经网络基础><a href=chapter03/README.md>第3章:神经网络基础</a><a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80>#</a></h3><ul><li>3.1 从感知机到多层神经网络</li><li>3.2 反向传播算法详解</li><li>3.3 激活函数的选择与影响</li><li>3.4 正则化技术:BatchNorm与Dropout</li><li><strong>实战</strong>:使用PyTorch构建第一个神经网络(MNIST手写数字识别)</li></ul><p><strong>核心技能</strong>:</p><ul><li>掌握PyTorch的基本操作(Tensor、autograd、nn.Module)</li><li>理解神经网络的训练流程</li><li>学会使用GPU加速训练</li></ul><h3 id=第4章卷积神经网络cnn><a href=chapter04/README.md>第4章:卷积神经网络(CNN)</a><a class=anchor href=#%e7%ac%ac4%e7%ab%a0%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9ccnn>#</a></h3><ul><li>4.1 卷积层的工作原理</li><li>4.2 池化层与降维</li><li>4.3 经典CNN架构:LeNet → AlexNet → VGG</li><li>4.4 CNN的可视化与理解</li><li><strong>实战</strong>:CIFAR-10图像分类(从零构建CNN)</li></ul><p><strong>核心技能</strong>:</p><ul><li>理解卷积操作的本质</li><li>掌握CNN的设计原则</li><li>学会使用torchvision进行图像处理</li></ul><hr><h2 id=技术栈>技术栈<a class=anchor href=#%e6%8a%80%e6%9c%af%e6%a0%88>#</a></h2><h3 id=环境要求>环境要求<a class=anchor href=#%e7%8e%af%e5%a2%83%e8%a6%81%e6%b1%82>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Python &gt;= 3.10</span>
</span></span><span class=line><span class=cl>python --version
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 安装PyTorch (2025年推荐)</span>
</span></span><span class=line><span class=cl><span class=c1># CPU版本</span>
</span></span><span class=line><span class=cl>pip install torch torchvision torchaudio
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPU版本(CUDA 12.1)</span>
</span></span><span class=line><span class=cl>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或使用uv(更快)</span>
</span></span><span class=line><span class=cl>uv pip install torch torchvision torchaudio</span></span></code></pre></div><h3 id=核心依赖>核心依赖<a class=anchor href=#%e6%a0%b8%e5%bf%83%e4%be%9d%e8%b5%96>#</a></h3><ul><li><strong>PyTorch >= 2.0</strong>:深度学习框架</li><li><strong>torchvision</strong>:计算机视觉工具库</li><li><strong>matplotlib</strong>:可视化</li><li><strong>tqdm</strong>:进度条</li></ul><h3 id=验证安装>验证安装<a class=anchor href=#%e9%aa%8c%e8%af%81%e5%ae%89%e8%a3%85>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;PyTorch版本: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CUDA可用: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;CUDA版本: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>version</span><span class=o>.</span><span class=n>cuda</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPU设备: </span><span class=si>{</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_name</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=学习建议>学习建议<a class=anchor href=#%e5%ad%a6%e4%b9%a0%e5%bb%ba%e8%ae%ae>#</a></h2><h3 id=1-动手实践为主>1. 动手实践为主<a class=anchor href=#1-%e5%8a%a8%e6%89%8b%e5%ae%9e%e8%b7%b5%e4%b8%ba%e4%b8%bb>#</a></h3><ul><li><strong>每个代码示例都要运行</strong>:不要只看代码</li><li><strong>修改超参数观察变化</strong>:学习率、批次大小、网络层数等</li><li><strong>尝试不同的数据集</strong>:Fashion-MNIST、SVHN等</li></ul><h3 id=2-理解核心概念>2. 理解核心概念<a class=anchor href=#2-%e7%90%86%e8%a7%a3%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5>#</a></h3><ul><li><strong>梯度下降</strong>:深度学习的基石</li><li><strong>反向传播</strong>:如何高效计算梯度</li><li><strong>正则化</strong>:防止过拟合的关键</li></ul><h3 id=3-参考官方文档>3. 参考官方文档<a class=anchor href=#3-%e5%8f%82%e8%80%83%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3>#</a></h3><ul><li><a href=https://pytorch.org/tutorials/>PyTorch官方教程</a></li><li><a href=https://pytorch.org/docs/stable/index.html>PyTorch文档</a></li><li><a href=https://pytorch.org/vision/stable/index.html>torchvision文档</a></li></ul><h3 id=4-循序渐进>4. 循序渐进<a class=anchor href=#4-%e5%be%aa%e5%ba%8f%e6%b8%90%e8%bf%9b>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第3章(1-2天) → 第4章(2-3天)
</span></span><span class=line><span class=cl>   ↓                ↓
</span></span><span class=line><span class=cl>理解基础           掌握CNN
</span></span><span class=line><span class=cl>   ↓                ↓
</span></span><span class=line><span class=cl>为后续现代架构(ResNet、Transformer)打下坚实基础</span></span></code></pre></div><hr><h2 id=与前后篇的关系>与前后篇的关系<a class=anchor href=#%e4%b8%8e%e5%89%8d%e5%90%8e%e7%af%87%e7%9a%84%e5%85%b3%e7%b3%bb>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>第一篇:机器学习基础
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (线性模型、优化算法)
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>第二篇:深度学习基础 ← 当前篇
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (神经网络、CNN)
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>第三篇:现代CNN架构
</span></span><span class=line><span class=cl>       ↓
</span></span><span class=line><span class=cl>  (ResNet、EfficientNet等)</span></span></code></pre></div><hr><h2 id=代码规范>代码规范<a class=anchor href=#%e4%bb%a3%e7%a0%81%e8%a7%84%e8%8c%83>#</a></h2><p>本篇所有代码遵循以下规范:</p><h3 id=1-目录结构>1. 目录结构<a class=anchor href=#1-%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>chapter03/
</span></span><span class=line><span class=cl>├── README.md              # 理论讲解
</span></span><span class=line><span class=cl>└── code/
</span></span><span class=line><span class=cl>    ├── first_nn.py        # 完整可运行代码
</span></span><span class=line><span class=cl>    └── utils.py           # 辅助函数(如有)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>chapter04/
</span></span><span class=line><span class=cl>├── README.md
</span></span><span class=line><span class=cl>└── code/
</span></span><span class=line><span class=cl>    ├── cnn_cifar10.py
</span></span><span class=line><span class=cl>    └── visualize.py       # 可视化工具(如有)</span></span></code></pre></div><h3 id=2-代码风格>2. 代码风格<a class=anchor href=#2-%e4%bb%a3%e7%a0%81%e9%a3%8e%e6%a0%bc>#</a></h3><ul><li><strong>函数职责单一</strong>:训练、验证、测试分离</li><li><strong>类型提示</strong>:使用Python 3.10+的类型注解</li><li><strong>文档字符串</strong>:关键函数添加docstring</li><li><strong>GPU支持</strong>:自动检测并使用GPU</li></ul><h3 id=3-示例模板>3. 示例模板<a class=anchor href=#3-%e7%a4%ba%e4%be%8b%e6%a8%a1%e6%9d%bf>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>模块说明
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.data</span> <span class=kn>import</span> <span class=n>DataLoader</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_one_epoch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dataloader</span><span class=p>:</span> <span class=n>DataLoader</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>criterion</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;训练一个epoch&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>total_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dataloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataloader</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=常见问题>常见问题<a class=anchor href=#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98>#</a></h2><h3 id=q1-pytorch-1x-vs-2x-主要区别>Q1: PyTorch 1.x vs 2.x 主要区别?<a class=anchor href=#q1-pytorch-1x-vs-2x-%e4%b8%bb%e8%a6%81%e5%8c%ba%e5%88%ab>#</a></h3><p><strong>A</strong>: PyTorch 2.x引入<code>torch.compile()</code>用于图优化,可大幅提升性能。API向后兼容,但推荐使用新特性。</p><h3 id=q2-需要gpu吗>Q2: 需要GPU吗?<a class=anchor href=#q2-%e9%9c%80%e8%a6%81gpu%e5%90%97>#</a></h3><p><strong>A</strong>: 不强制,但强烈推荐。第3章代码CPU可运行,第4章建议使用GPU(训练速度差10倍以上)。</p><h3 id=q3-如何选择学习率>Q3: 如何选择学习率?<a class=anchor href=#q3-%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e5%ad%a6%e4%b9%a0%e7%8e%87>#</a></h3><p><strong>A</strong>: 初学者可从1e-3开始,根据训练曲线调整。后续章节会介绍学习率调度器。</p><h3 id=q4-batchnorm和dropout如何选择>Q4: BatchNorm和Dropout如何选择?<a class=anchor href=#q4-batchnorm%e5%92%8cdropout%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9>#</a></h3><p><strong>A</strong>: 两者可同时使用。BatchNorm主要用于加速收敛,Dropout用于防止过拟合。</p><hr><h2 id=拓展资源>拓展资源<a class=anchor href=#%e6%8b%93%e5%b1%95%e8%b5%84%e6%ba%90>#</a></h2><h3 id=书籍>书籍<a class=anchor href=#%e4%b9%a6%e7%b1%8d>#</a></h3><ul><li><strong>《Deep Learning》</strong>(Goodfellow):深度学习圣经</li><li><strong>《动手学深度学习》</strong>(李沐):PyTorch实战</li></ul><h3 id=在线课程>在线课程<a class=anchor href=#%e5%9c%a8%e7%ba%bf%e8%af%be%e7%a8%8b>#</a></h3><ul><li><strong>Fast.ai</strong>:自顶向下的深度学习课程</li><li><strong>Stanford CS231n</strong>:计算机视觉经典课程</li></ul><h3 id=实践平台>实践平台<a class=anchor href=#%e5%ae%9e%e8%b7%b5%e5%b9%b3%e5%8f%b0>#</a></h3><ul><li><strong>Kaggle</strong>:数据竞赛与学习</li><li><strong>Papers With Code</strong>:论文+代码复现</li></ul><hr><p><strong>准备好了吗?让我们从第3章开始,构建第一个神经网络!</strong></p><hr><h1 id=第3章神经网络基础-1>第3章:神经网络基础<a class=anchor href=#%e7%ac%ac3%e7%ab%a0%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80-1>#</a></h1><blockquote class=book-hint><p><strong>本章目标</strong>:理解神经网络的核心原理,掌握PyTorch构建和训练神经网络的完整流程</p></blockquote><hr><h2 id=31-从感知机到多层神经网络>3.1 从感知机到多层神经网络<a class=anchor href=#31-%e4%bb%8e%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%88%b0%e5%a4%9a%e5%b1%82%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c>#</a></h2><h3 id=311-感知机perceptron>3.1.1 感知机(Perceptron)<a class=anchor href=#311-%e6%84%9f%e7%9f%a5%e6%9c%baperceptron>#</a></h3><p>感知机是最简单的神经网络单元,由Frank Rosenblatt在1957年提出。</p><p><strong>数学表达</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>y = f(w·x + b)</span></span></code></pre></div><p>其中:</p><ul><li><code>x</code>:输入向量</li><li><code>w</code>:权重向量</li><li><code>b</code>:偏置项</li><li><code>f</code>:激活函数(通常是阶跃函数)</li></ul><p><strong>感知机的局限</strong>:</p><ul><li>只能处理线性可分问题(如无法学习XOR)</li><li>无法堆叠成深层网络</li></ul><h3 id=312-多层感知机mlp>3.1.2 多层感知机(MLP)<a class=anchor href=#312-%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%bamlp>#</a></h3><p>通过堆叠多层感知机并使用非线性激活函数,可以逼近任意复杂函数(通用逼近定理)。</p><p><strong>典型三层MLP结构</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入层 → 隐藏层1 → 隐藏层2 → 输出层
</span></span><span class=line><span class=cl>(784)  →  (256)  →  (128)  →  (10)
</span></span><span class=line><span class=cl>        ↓ ReLU    ↓ ReLU    ↓ Softmax</span></span></code></pre></div><p><strong>PyTorch实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleMLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=o>=</span><span class=mi>784</span><span class=p>,</span> <span class=n>hidden_size</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SimpleMLP</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 展平输入</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><hr><h2 id=32-反向传播算法详解>3.2 反向传播算法详解<a class=anchor href=#32-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e8%af%a6%e8%a7%a3>#</a></h2><h3 id=321-核心思想>3.2.1 核心思想<a class=anchor href=#321-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p>反向传播(Backpropagation)是训练神经网络的核心算法,通过链式法则高效计算梯度。</p><p><strong>前向传播</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入 → 计算输出 → 计算损失
</span></span><span class=line><span class=cl>x → ŷ → L(ŷ, y)</span></span></code></pre></div><p><strong>反向传播</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>损失 → 计算梯度 → 更新参数
</span></span><span class=line><span class=cl>L → ∂L/∂w → w = w - η·∂L/∂w</span></span></code></pre></div><h3 id=322-链式法则>3.2.2 链式法则<a class=anchor href=#322-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99>#</a></h3><p>对于复合函数 <code>L = f(g(h(x)))</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>∂L/∂x = (∂L/∂f) · (∂f/∂g) · (∂g/∂h) · (∂h/∂x)</span></span></code></pre></div><h3 id=323-pytorch自动微分>3.2.3 PyTorch自动微分<a class=anchor href=#323-pytorch%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86>#</a></h3><p>PyTorch的<code>autograd</code>模块自动处理反向传播:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 创建需要梯度的张量</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>2.0</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.0</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>w</span> <span class=o>*</span> <span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=mi>5</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 反向传播</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看梯度</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;∂loss/∂w = </span><span class=si>{</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># -6.0</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;∂loss/∂x = </span><span class=si>{</span><span class=n>x</span><span class=o>.</span><span class=n>grad</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># -9.0</span></span></span></code></pre></div><p><strong>重要API</strong>:</p><ul><li><code>requires_grad=True</code>:标记需要梯度的张量</li><li><code>.backward()</code>:自动计算梯度</li><li><code>.grad</code>:访问计算的梯度</li><li><code>optimizer.zero_grad()</code>:清空梯度(必须!)</li></ul><hr><h2 id=33-激活函数的选择与影响>3.3 激活函数的选择与影响<a class=anchor href=#33-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e9%80%89%e6%8b%a9%e4%b8%8e%e5%bd%b1%e5%93%8d>#</a></h2><h3 id=331-为什么需要激活函数>3.3.1 为什么需要激活函数?<a class=anchor href=#331-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>#</a></h3><p>如果没有激活函数,多层神经网络等价于单层线性模型:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>f(f(x)) = W2·(W1·x) = (W2·W1)·x = W·x</span></span></code></pre></div><h3 id=332-常用激活函数>3.3.2 常用激活函数<a class=anchor href=#332-%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>#</a></h3><table><thead><tr><th>激活函数</th><th>公式</th><th>优点</th><th>缺点</th><th>使用场景</th></tr></thead><tbody><tr><td><strong>Sigmoid</strong></td><td><code>σ(x) = 1/(1+e^-x)</code></td><td>输出范围(0,1)</td><td>梯度消失、计算慢</td><td>二分类输出层</td></tr><tr><td><strong>Tanh</strong></td><td><code>tanh(x) = (e^x-e^-x)/(e^x+e^-x)</code></td><td>输出范围(-1,1),零中心</td><td>梯度消失</td><td>RNN(历史原因)</td></tr><tr><td><strong>ReLU</strong></td><td><code>max(0, x)</code></td><td>计算快、缓解梯度消失</td><td>神经元死亡</td><td><strong>隐藏层首选</strong></td></tr><tr><td><strong>LeakyReLU</strong></td><td><code>max(0.01x, x)</code></td><td>解决神经元死亡</td><td>需调节负斜率</td><td>ReLU替代方案</td></tr><tr><td><strong>GELU</strong></td><td><code>x·Φ(x)</code></td><td>平滑、性能好</td><td>计算稍慢</td><td>Transformer标配</td></tr><tr><td><strong>Softmax</strong></td><td><code>e^xi/Σe^xj</code></td><td>输出概率分布</td><td>-</td><td><strong>多分类输出层</strong></td></tr></tbody></table><h3 id=333-pytorch实现>3.3.3 PyTorch实现<a class=anchor href=#333-pytorch%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 方式1:使用nn.Module(推荐用于网络层)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>leaky_relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=n>negative_slope</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 方式2:使用函数式API(推荐用于forward)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>leaky_relu</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>negative_slope</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span></span></span></code></pre></div><p><strong>经验法则</strong>:</p><ul><li>隐藏层:优先使用<code>ReLU</code>,性能不佳时尝试<code>GELU</code>或<code>LeakyReLU</code></li><li>输出层:二分类用<code>Sigmoid</code>,多分类用<code>Softmax</code>,回归不用激活函数</li></ul><hr><h2 id=34-正则化技术batchnorm与dropout>3.4 正则化技术:BatchNorm与Dropout<a class=anchor href=#34-%e6%ad%a3%e5%88%99%e5%8c%96%e6%8a%80%e6%9c%afbatchnorm%e4%b8%8edropout>#</a></h2><h3 id=341-batch-normalization>3.4.1 Batch Normalization<a class=anchor href=#341-batch-normalization>#</a></h3><p><strong>核心思想</strong>:对每个mini-batch的激活值进行标准化,加速收敛并缓解梯度消失。</p><p><strong>数学公式</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x_norm = (x - μ_batch) / √(σ_batch² + ε)
</span></span><span class=line><span class=cl>y = γ·x_norm + β</span></span></code></pre></div><p>其中<code>γ</code>和<code>β</code>是可学习参数。</p><p><strong>PyTorch实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLPWithBN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>256</span><span class=p>)</span>  <span class=c1># 注意:1d用于全连接层</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><p><strong>使用建议</strong>:</p><ul><li>放在激活函数<strong>之前</strong>或<strong>之后</strong>都可以(论文有争议,实践中差异不大)</li><li>通常可以使用更大的学习率</li><li>训练时<code>model.train()</code>,测试时<code>model.eval()</code>(BN行为不同)</li></ul><h3 id=342-dropout>3.4.2 Dropout<a class=anchor href=#342-dropout>#</a></h3><p><strong>核心思想</strong>:训练时随机"丢弃"部分神经元,防止过拟合。</p><p><strong>工作原理</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 训练时(p=0.5表示丢弃50%的神经元)</span>
</span></span><span class=line><span class=cl><span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mask</span> <span class=o>/</span> <span class=mf>0.5</span>  <span class=c1># 除以0.5保持期望不变</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试时:不丢弃,直接使用所有神经元</span></span></span></code></pre></div><p><strong>PyTorch实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLPWithDropout</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># 在激活后使用</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># 输出层不使用Dropout</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><p><strong>使用建议</strong>:</p><ul><li>典型dropout rate:0.2~0.5</li><li>CNN中较少使用(BatchNorm效果更好)</li><li>全连接层中常用</li></ul><h3 id=343-batchnorm-vs-dropout>3.4.3 BatchNorm vs Dropout<a class=anchor href=#343-batchnorm-vs-dropout>#</a></h3><table><thead><tr><th>特性</th><th>BatchNorm</th><th>Dropout</th></tr></thead><tbody><tr><td>主要作用</td><td>加速收敛、缓解梯度消失</td><td>防止过拟合</td></tr><tr><td>适用场景</td><td>CNN、Transformer</td><td>全连接层</td></tr><tr><td>训练/测试差异</td><td>有(统计量计算方式不同)</td><td>有(测试时关闭)</td></tr><tr><td>是否可同时使用</td><td><strong>可以</strong>(先BN后Dropout)</td><td>-</td></tr></tbody></table><hr><h2 id=35-实战mnist手写数字识别>3.5 实战:MNIST手写数字识别<a class=anchor href=#35-%e5%ae%9e%e6%88%98mnist%e6%89%8b%e5%86%99%e6%95%b0%e5%ad%97%e8%af%86%e5%88%ab>#</a></h2><h3 id=351-任务描述>3.5.1 任务描述<a class=anchor href=#351-%e4%bb%bb%e5%8a%a1%e6%8f%8f%e8%bf%b0>#</a></h3><ul><li><strong>数据集</strong>:MNIST(60,000训练+10,000测试)</li><li><strong>输入</strong>:28×28灰度图像</li><li><strong>输出</strong>:10个类别(数字0-9)</li><li><strong>目标</strong>:准确率>98%</li></ul><h3 id=352-完整代码>3.5.2 完整代码<a class=anchor href=#352-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81>#</a></h3><p>完整代码见:<code>code/chapter03_neural_network/first_nn.py</code></p><p><strong>核心组件</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 数据加载</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 模型定义</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MNISTNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 训练循环</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MNISTNet</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>train_loss</span> <span class=o>=</span> <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>val_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>val_loader</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>train_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc=</span><span class=si>{</span><span class=n>val_acc</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span></span></span></code></pre></div><h3 id=353-运行与调试>3.5.3 运行与调试<a class=anchor href=#353-%e8%bf%90%e8%a1%8c%e4%b8%8e%e8%b0%83%e8%af%95>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 进入代码目录</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> /Users/nako/PycharmProjects/Learn/LangChainDemo/ComputerVisionNotes/part2_dl_basics/chapter03/code
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 运行训练</span>
</span></span><span class=line><span class=cl>python first_nn.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 期望输出</span>
</span></span><span class=line><span class=cl>Epoch 1: <span class=nv>Loss</span><span class=o>=</span>0.2345, <span class=nv>Acc</span><span class=o>=</span>93.45%
</span></span><span class=line><span class=cl>Epoch 2: <span class=nv>Loss</span><span class=o>=</span>0.1123, <span class=nv>Acc</span><span class=o>=</span>96.78%
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>Epoch 10: <span class=nv>Loss</span><span class=o>=</span>0.0234, <span class=nv>Acc</span><span class=o>=</span>98.56%</span></span></code></pre></div><p><strong>常见问题排查</strong>:</p><ol><li><p><strong>损失不下降</strong></p><ul><li>检查学习率(太大或太小)</li><li>确认<code>optimizer.zero_grad()</code>已调用</li><li>查看数据是否正确加载</li></ul></li><li><p><strong>准确率停滞</strong></p><ul><li>尝试调整网络深度/宽度</li><li>减小dropout rate</li><li>增加训练epoch</li></ul></li><li><p><strong>GPU内存不足</strong></p><ul><li>减小batch_size</li><li>使用梯度累积</li></ul></li></ol><hr><h2 id=36-pytorch核心api总结>3.6 PyTorch核心API总结<a class=anchor href=#36-pytorch%e6%a0%b8%e5%bf%83api%e6%80%bb%e7%bb%93>#</a></h2><h3 id=361-张量tensor操作>3.6.1 张量(Tensor)操作<a class=anchor href=#361-%e5%bc%a0%e9%87%8ftensor%e6%93%8d%e4%bd%9c>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 创建张量</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># 正态分布</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 移动设备</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>  <span class=c1># 等价于to(&#39;cuda&#39;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看形状</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>  <span class=c1># torch.Size([2, 3])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>  <span class=c1># 2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 变形</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># 更灵活</span></span></span></code></pre></div><h3 id=362-nnmodule核心方法>3.6.2 nn.Module核心方法<a class=anchor href=#362-nnmodule%e6%a0%b8%e5%bf%83%e6%96%b9%e6%b3%95>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MyModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义层</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 定义前向传播</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练/评估模式切换</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># 启用Dropout、BatchNorm训练模式</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>   <span class=c1># 关闭Dropout、BatchNorm评估模式</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 参数管理</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 保存/加载</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s1>&#39;model.pth&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;model.pth&#39;</span><span class=p>))</span></span></span></code></pre></div><h3 id=363-优化器optimizer>3.6.3 优化器(Optimizer)<a class=anchor href=#363-%e4%bc%98%e5%8c%96%e5%99%a8optimizer>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 常用优化器</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练步骤</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>  <span class=c1># 1. 清空梯度</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>        <span class=c1># 2. 反向传播</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>       <span class=c1># 3. 更新参数</span></span></span></code></pre></div><h3 id=364-损失函数>3.6.4 损失函数<a class=anchor href=#364-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 分类任务</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>  <span class=c1># 多分类(自带Softmax)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCEWithLogitsLoss</span><span class=p>()</span>  <span class=c1># 二分类(自带Sigmoid)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 回归任务</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>  <span class=c1># 均方误差</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>L1Loss</span><span class=p>()</span>   <span class=c1># 平均绝对误差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span></span></span></code></pre></div><hr><h2 id=37-本章小结>3.7 本章小结<a class=anchor href=#37-%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=核心知识点>核心知识点<a class=anchor href=#%e6%a0%b8%e5%bf%83%e7%9f%a5%e8%af%86%e7%82%b9>#</a></h3><ol><li><strong>神经网络结构</strong>:输入层、隐藏层、输出层</li><li><strong>反向传播</strong>:链式法则计算梯度</li><li><strong>激活函数</strong>:ReLU(隐藏层)、Softmax(分类输出)</li><li><strong>正则化</strong>:BatchNorm(加速收敛)、Dropout(防止过拟合)</li></ol><h3 id=pytorch编程范式>PyTorch编程范式<a class=anchor href=#pytorch%e7%bc%96%e7%a8%8b%e8%8c%83%e5%bc%8f>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1. 定义模型</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Model</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span> <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 准备数据</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 设置优化器和损失函数</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 训练循环</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span></span></span></code></pre></div><h3 id=检查清单>检查清单<a class=anchor href=#%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95>#</a></h3><ul><li><input disabled type=checkbox> 理解感知机→MLP的演进</li><li><input disabled type=checkbox> 掌握PyTorch自动微分机制</li><li><input disabled type=checkbox> 会选择合适的激活函数</li><li><input disabled type=checkbox> 理解BatchNorm和Dropout的作用</li><li><input disabled type=checkbox> 能独立编写训练代码</li><li><input disabled type=checkbox> 成功运行MNIST实战代码(准确率>98%)</li></ul><hr><h2 id=38-练习与思考>3.8 练习与思考<a class=anchor href=#38-%e7%bb%83%e4%b9%a0%e4%b8%8e%e6%80%9d%e8%80%83>#</a></h2><h3 id=基础练习>基础练习<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%bb%83%e4%b9%a0>#</a></h3><ol><li><p><strong>修改网络结构</strong>:</p><ul><li>增加/减少隐藏层数量</li><li>调整每层神经元数量</li><li>观察训练速度和准确率变化</li></ul></li><li><p><strong>超参数调优</strong>:</p><ul><li>学习率:[0.0001, 0.001, 0.01]</li><li>Dropout率:[0.1, 0.3, 0.5, 0.7]</li><li>Batch size:[32, 64, 128, 256]</li></ul></li><li><p><strong>激活函数对比</strong>:</p><ul><li>分别使用ReLU、LeakyReLU、GELU</li><li>记录训练曲线差异</li></ul></li></ol><h3 id=进阶挑战>进阶挑战<a class=anchor href=#%e8%bf%9b%e9%98%b6%e6%8c%91%e6%88%98>#</a></h3><ol><li><p><strong>实现学习率调度</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span></span></span></code></pre></div></li><li><p><strong>数据增强</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomRotation</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl><span class=p>])</span></span></span></code></pre></div></li><li><p><strong>尝试其他数据集</strong>:</p><ul><li>Fashion-MNIST(服装分类)</li><li>KMNIST(日文字符)</li></ul></li></ol><h3 id=思考题>思考题<a class=anchor href=#%e6%80%9d%e8%80%83%e9%a2%98>#</a></h3><ol><li>为什么BatchNorm能加速收敛?</li><li>神经网络越深越好吗?什么时候会出现问题?</li><li>如何判断模型是过拟合还是欠拟合?</li></ol><hr><p><strong>下一章预告</strong>:我们将学习卷积神经网络(CNN),它是计算机视觉的基石。相比全连接网络,CNN能更好地处理图像数据,并大幅减少参数量。</p><p><strong>继续学习</strong> → <a href=../chapter04/README.md>第4章:卷积神经网络(CNN)</a></p><hr><h1 id=第4章卷积神经网络cnn-1>第4章:卷积神经网络(CNN)<a class=anchor href=#%e7%ac%ac4%e7%ab%a0%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9ccnn-1>#</a></h1><blockquote class=book-hint><p><strong>本章目标</strong>:理解卷积操作的本质,掌握CNN的设计原则,能独立构建CNN进行图像分类</p></blockquote><hr><h2 id=41-为什么需要cnn>4.1 为什么需要CNN?<a class=anchor href=#41-%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81cnn>#</a></h2><h3 id=411-全连接网络的局限>4.1.1 全连接网络的局限<a class=anchor href=#411-%e5%85%a8%e8%bf%9e%e6%8e%a5%e7%bd%91%e7%bb%9c%e7%9a%84%e5%b1%80%e9%99%90>#</a></h3><p>回顾第3章的MNIST分类器,我们使用全连接网络:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>28×28图像 → 展平成784维向量 → MLP → 10个类别</span></span></code></pre></div><p><strong>存在的问题</strong>:</p><ol><li><strong>参数量爆炸</strong>:对于32×32的RGB图像(3072维),第一层若有256个神经元,参数量=3072×256≈78万</li><li><strong>丢失空间信息</strong>:展平操作破坏了像素的空间位置关系</li><li><strong>无法处理不同尺寸输入</strong>:输入尺寸固定</li></ol><h3 id=412-cnn的核心思想>4.1.2 CNN的核心思想<a class=anchor href=#412-cnn%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3>#</a></h3><p>**卷积神经网络(Convolutional Neural Network)**基于三个关键假设:</p><ol><li><strong>局部连接</strong>:相邻像素比远距离像素更相关</li><li><strong>权值共享</strong>:同一个特征(如边缘)在图像不同位置都有用</li><li><strong>平移不变性</strong>:特征检测器可在任意位置工作</li></ol><p><strong>结果</strong>:大幅减少参数量,同时保留空间结构!</p><hr><h2 id=42-卷积层的工作原理>4.2 卷积层的工作原理<a class=anchor href=#42-%e5%8d%b7%e7%a7%af%e5%b1%82%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86>#</a></h2><h3 id=421-卷积操作>4.2.1 卷积操作<a class=anchor href=#421-%e5%8d%b7%e7%a7%af%e6%93%8d%e4%bd%9c>#</a></h3><p>卷积是一种<strong>滑动窗口</strong>操作:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入图像(5×5)              卷积核(3×3)           输出特征图(3×3)
</span></span><span class=line><span class=cl>┌─────────────┐            ┌─────┐              ┌─────────┐
</span></span><span class=line><span class=cl>│ 1  2  3  4  5 │          │ 1 0 -1 │            │         │
</span></span><span class=line><span class=cl>│ 6  7  8  9 10 │    *     │ 1 0 -1 │    →      │  特征   │
</span></span><span class=line><span class=cl>│11 12 13 14 15 │          │ 1 0 -1 │            │   值    │
</span></span><span class=line><span class=cl>│16 17 18 19 20 │          └─────┘              │         │
</span></span><span class=line><span class=cl>│21 22 23 24 25 │                                └─────────┘
</span></span><span class=line><span class=cl>└─────────────┘</span></span></code></pre></div><p><strong>数学定义</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输出[i,j] = Σ Σ 输入[i+m, j+n] × 卷积核[m, n] + 偏置
</span></span><span class=line><span class=cl>            m n</span></span></code></pre></div><h3 id=422-关键概念>4.2.2 关键概念<a class=anchor href=#422-%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5>#</a></h3><table><thead><tr><th>参数</th><th>说明</th><th>典型值</th></tr></thead><tbody><tr><td><strong>卷积核大小(Kernel Size)</strong></td><td>感受野大小</td><td>3×3, 5×5</td></tr><tr><td><strong>步长(Stride)</strong></td><td>滑动步长</td><td>1(不跳过), 2(降采样)</td></tr><tr><td><strong>填充(Padding)</strong></td><td>边缘补零</td><td>0(valid), (k-1)/2(same)</td></tr><tr><td><strong>通道数(Channels)</strong></td><td>输入/输出特征图数量</td><td>RGB输入=3, 隐藏层32/64/128</td></tr></tbody></table><p><strong>输出尺寸计算</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输出高度 = ⌊(输入高度 + 2×padding - kernel_size) / stride⌋ + 1
</span></span><span class=line><span class=cl>输出宽度 = ⌊(输入宽度 + 2×padding - kernel_size) / stride⌋ + 1</span></span></code></pre></div><h3 id=423-pytorch实现>4.2.3 PyTorch实现<a class=anchor href=#423-pytorch%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 单个卷积层</span>
</span></span><span class=line><span class=cl><span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>in_channels</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>      <span class=c1># 输入通道(RGB)</span>
</span></span><span class=line><span class=cl>    <span class=n>out_channels</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>    <span class=c1># 输出通道(学习32个特征)</span>
</span></span><span class=line><span class=cl>    <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>      <span class=c1># 3×3卷积核</span>
</span></span><span class=line><span class=cl>    <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>           <span class=c1># 步长1</span>
</span></span><span class=line><span class=cl>    <span class=n>padding</span><span class=o>=</span><span class=mi>1</span>           <span class=c1># 填充1(保持尺寸)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入: [batch_size, 3, 32, 32]</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: [batch_size, 32, 32, 32]</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span></span></span></code></pre></div><p><strong>常用卷积块模式</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Conv → BatchNorm → ReLU 模式</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ConvBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span></span></span></code></pre></div><hr><h2 id=43-池化层与降维>4.3 池化层与降维<a class=anchor href=#43-%e6%b1%a0%e5%8c%96%e5%b1%82%e4%b8%8e%e9%99%8d%e7%bb%b4>#</a></h2><h3 id=431-池化操作>4.3.1 池化操作<a class=anchor href=#431-%e6%b1%a0%e5%8c%96%e6%93%8d%e4%bd%9c>#</a></h3><p><strong>目的</strong>:降低特征图分辨率,减少参数量,增强不变性。</p><p><strong>最大池化(Max Pooling)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入(4×4)          2×2 Max Pooling        输出(2×2)
</span></span><span class=line><span class=cl>┌────────────┐     stride=2            ┌──────┐
</span></span><span class=line><span class=cl>│ 1  3  2  4 │                         │ 7  8 │
</span></span><span class=line><span class=cl>│ 5  7  6  8 │    ──────────&gt;          │15 16 │
</span></span><span class=line><span class=cl>│ 9 11 10 12 │                         └──────┘
</span></span><span class=line><span class=cl>│13 15 14 16 │
</span></span><span class=line><span class=cl>└────────────┘</span></span></code></pre></div><p><strong>平均池化(Average Pooling)</strong>:取区域平均值而非最大值。</p><h3 id=432-pytorch实现>4.3.2 PyTorch实现<a class=anchor href=#432-pytorch%e5%ae%9e%e7%8e%b0>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 最大池化(更常用)</span>
</span></span><span class=line><span class=cl><span class=n>maxpool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 平均池化</span>
</span></span><span class=line><span class=cl><span class=n>avgpool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AvgPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 全局平均池化(常用于分类网络末端)</span>
</span></span><span class=line><span class=cl><span class=n>global_avgpool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AdaptiveAvgPool2d</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>  <span class=c1># 输出固定为1×1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 示例</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>maxpool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># [8, 32, 16, 16]</span></span></span></code></pre></div><h3 id=433-池化-vs-步长卷积>4.3.3 池化 vs 步长卷积<a class=anchor href=#433-%e6%b1%a0%e5%8c%96-vs-%e6%ad%a5%e9%95%bf%e5%8d%b7%e7%a7%af>#</a></h3><table><thead><tr><th>特性</th><th>池化</th><th>步长卷积(stride>1)</th></tr></thead><tbody><tr><td>参数量</td><td>0(无参数)</td><td>有参数</td></tr><tr><td>可学习性</td><td>否</td><td>是</td></tr><tr><td>使用趋势</td><td>减少</td><td>增加</td></tr></tbody></table><p><strong>现代趋势</strong>:很多新架构(如ResNet)倾向于使用stride=2的卷积代替池化。</p><hr><h2 id=44-经典cnn架构演进>4.4 经典CNN架构演进<a class=anchor href=#44-%e7%bb%8f%e5%85%b8cnn%e6%9e%b6%e6%9e%84%e6%bc%94%e8%bf%9b>#</a></h2><h3 id=441-lenet-5-1998>4.4.1 LeNet-5 (1998)<a class=anchor href=#441-lenet-5-1998>#</a></h3><p><strong>历史地位</strong>:第一个成功的CNN,用于手写数字识别。</p><p><strong>架构</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入(32×32) → Conv5×5(6) → AvgPool → Conv5×5(16) → AvgPool → FC(120) → FC(84) → FC(10)</span></span></code></pre></div><p><strong>PyTorch实现</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LeNet5</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>features</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>AvgPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>AvgPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>16</span> <span class=o>*</span> <span class=mi>5</span> <span class=o>*</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>120</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>120</span><span class=p>,</span> <span class=mi>84</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>84</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><h3 id=442-alexnet-2012>4.4.2 AlexNet (2012)<a class=anchor href=#442-alexnet-2012>#</a></h3><p><strong>历史地位</strong>:ImageNet竞赛冠军,引爆深度学习热潮。</p><p><strong>关键创新</strong>:</p><ul><li>使用<strong>ReLU</strong>替代Sigmoid/Tanh</li><li>引入<strong>Dropout</strong>防止过拟合</li><li>使用<strong>数据增强</strong>(随机裁剪、翻转)</li><li><strong>GPU并行训练</strong></li></ul><p><strong>架构特点</strong>:</p><ul><li>5个卷积层 + 3个全连接层</li><li>约6000万参数</li></ul><p><strong>使用torchvision预训练模型</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.models</span> <span class=kn>import</span> <span class=n>alexnet</span><span class=p>,</span> <span class=n>AlexNet_Weights</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载预训练权重</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>AlexNet_Weights</span><span class=o>.</span><span class=n>DEFAULT</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>alexnet</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 修改最后一层用于自定义分类任务</span>
</span></span><span class=line><span class=cl><span class=n>num_classes</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>classifier</span><span class=p>[</span><span class=mi>6</span><span class=p>]</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span></span></span></code></pre></div><h3 id=443-vgg-2014>4.4.3 VGG (2014)<a class=anchor href=#443-vgg-2014>#</a></h3><p><strong>核心思想</strong>:更深的网络(16-19层),统一使用3×3卷积核。</p><p><strong>设计原则</strong>:</p><ul><li><strong>所有卷积核都是3×3</strong>:两个3×3卷积感受野=一个5×5,但参数更少</li><li><strong>每次池化后通道数翻倍</strong>:64→128→256→512→512</li><li><strong>结构规整</strong>:易于理解和实现</li></ul><p><strong>VGG16架构</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>块1: Conv3×3(64)×2 → MaxPool
</span></span><span class=line><span class=cl>块2: Conv3×3(128)×2 → MaxPool
</span></span><span class=line><span class=cl>块3: Conv3×3(256)×3 → MaxPool
</span></span><span class=line><span class=cl>块4: Conv3×3(512)×3 → MaxPool
</span></span><span class=line><span class=cl>块5: Conv3×3(512)×3 → MaxPool
</span></span><span class=line><span class=cl>全连接: FC(4096) → FC(4096) → FC(1000)</span></span></code></pre></div><p><strong>PyTorch实现(简化版)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>VGG16</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 特征提取层</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>features</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=c1># 块1</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 块2</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 块3</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 块4</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 块5</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 分类层</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>4096</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=mi>4096</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4096</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><p><strong>使用torchvision</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.models</span> <span class=kn>import</span> <span class=n>vgg16</span><span class=p>,</span> <span class=n>VGG16_Weights</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>VGG16_Weights</span><span class=o>.</span><span class=n>DEFAULT</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>vgg16</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=n>weights</span><span class=p>)</span></span></span></code></pre></div><h3 id=444-架构对比>4.4.4 架构对比<a class=anchor href=#444-%e6%9e%b6%e6%9e%84%e5%af%b9%e6%af%94>#</a></h3><table><thead><tr><th>模型</th><th>年份</th><th>层数</th><th>参数量</th><th>Top-5错误率</th><th>关键创新</th></tr></thead><tbody><tr><td>LeNet-5</td><td>1998</td><td>7</td><td>60K</td><td>-</td><td>首个成功CNN</td></tr><tr><td>AlexNet</td><td>2012</td><td>8</td><td>60M</td><td>15.3%</td><td>ReLU+Dropout+GPU</td></tr><tr><td>VGG16</td><td>2014</td><td>16</td><td>138M</td><td>7.3%</td><td>小卷积核+深网络</td></tr></tbody></table><hr><h2 id=45-实战cifar-10图像分类>4.5 实战:CIFAR-10图像分类<a class=anchor href=#45-%e5%ae%9e%e6%88%98cifar-10%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb>#</a></h2><h3 id=451-任务描述>4.5.1 任务描述<a class=anchor href=#451-%e4%bb%bb%e5%8a%a1%e6%8f%8f%e8%bf%b0>#</a></h3><p><strong>CIFAR-10数据集</strong>:</p><ul><li><strong>图像数量</strong>:60,000张(50,000训练+10,000测试)</li><li><strong>图像尺寸</strong>:32×32 RGB彩色图像</li><li><strong>类别数</strong>:10类(飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车)</li><li><strong>难度</strong>:比MNIST高(彩色、类内差异大、背景复杂)</li></ul><p><strong>目标</strong>:构建CNN达到>75%准确率(随机猜测=10%)</p><h3 id=452-网络设计>4.5.2 网络设计<a class=anchor href=#452-%e7%bd%91%e7%bb%9c%e8%ae%be%e8%ae%a1>#</a></h3><p>我们将构建一个现代化的小型CNN:</p><p><strong>架构</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>输入(3, 32, 32)
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>Conv Block1: Conv3×3(64)×2 + MaxPool → (64, 16, 16)
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>Conv Block2: Conv3×3(128)×2 + MaxPool → (128, 8, 8)
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>Conv Block3: Conv3×3(256)×2 + MaxPool → (256, 4, 4)
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>Global AvgPool → (256, 1, 1)
</span></span><span class=line><span class=cl>  ↓
</span></span><span class=line><span class=cl>Flatten + Dropout + FC(10)</span></span></code></pre></div><h3 id=453-完整代码>4.5.3 完整代码<a class=anchor href=#453-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81>#</a></h3><p>详见:<code>code/chapter04_cnn/cnn_cifar10.py</code></p><p><strong>核心模型定义</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CIFAR10CNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 卷积块1: 32×32 → 16×16</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 卷积块2: 16×16 → 8×8</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 卷积块3: 8×8 → 4×4</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 全局平均池化</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>global_pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AdaptiveAvgPool2d</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 分类器</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>global_pool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></div><h3 id=454-数据增强>4.5.4 数据增强<a class=anchor href=#454-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba>#</a></h3><p>提升模型泛化能力的关键技术:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision</span> <span class=kn>import</span> <span class=n>transforms</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练集增强</span>
</span></span><span class=line><span class=cl><span class=n>train_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomCrop</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>4</span><span class=p>),</span>      <span class=c1># 随机裁剪</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(),</span>         <span class=c1># 随机水平翻转</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ColorJitter</span><span class=p>(</span>                    <span class=c1># 颜色抖动</span>
</span></span><span class=line><span class=cl>        <span class=n>brightness</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>contrast</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>saturation</span><span class=o>=</span><span class=mf>0.2</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span>                      <span class=c1># 标准化</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.4914</span><span class=p>,</span> <span class=mf>0.4822</span><span class=p>,</span> <span class=mf>0.4465</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.2470</span><span class=p>,</span> <span class=mf>0.2435</span><span class=p>,</span> <span class=mf>0.2616</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试集不增强</span>
</span></span><span class=line><span class=cl><span class=n>test_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.4914</span><span class=p>,</span> <span class=mf>0.4822</span><span class=p>,</span> <span class=mf>0.4465</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.2470</span><span class=p>,</span> <span class=mf>0.2435</span><span class=p>,</span> <span class=mf>0.2616</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span></span></span></code></pre></div><h3 id=455-运行与结果>4.5.5 运行与结果<a class=anchor href=#455-%e8%bf%90%e8%a1%8c%e4%b8%8e%e7%bb%93%e6%9e%9c>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 运行训练</span>
</span></span><span class=line><span class=cl>python cnn_cifar10.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 期望结果(30 epochs)</span>
</span></span><span class=line><span class=cl>Epoch 1:  Train <span class=nv>Loss</span><span class=o>=</span>1.6234, Val <span class=nv>Loss</span><span class=o>=</span>1.3456, Val <span class=nv>Acc</span><span class=o>=</span>52.34%
</span></span><span class=line><span class=cl>Epoch 10: Train <span class=nv>Loss</span><span class=o>=</span>0.8123, Val <span class=nv>Loss</span><span class=o>=</span>0.9234, Val <span class=nv>Acc</span><span class=o>=</span>68.45%
</span></span><span class=line><span class=cl>Epoch 20: Train <span class=nv>Loss</span><span class=o>=</span>0.5234, Val <span class=nv>Loss</span><span class=o>=</span>0.7123, Val <span class=nv>Acc</span><span class=o>=</span>75.67%
</span></span><span class=line><span class=cl>Epoch 30: Train <span class=nv>Loss</span><span class=o>=</span>0.3456, Val <span class=nv>Loss</span><span class=o>=</span>0.6789, Val <span class=nv>Acc</span><span class=o>=</span>78.23%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>测试集准确率: 77.56%</span></span></code></pre></div><hr><h2 id=46-cnn可视化与理解>4.6 CNN可视化与理解<a class=anchor href=#46-cnn%e5%8f%af%e8%a7%86%e5%8c%96%e4%b8%8e%e7%90%86%e8%a7%a3>#</a></h2><h3 id=461-为什么要可视化>4.6.1 为什么要可视化?<a class=anchor href=#461-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e5%8f%af%e8%a7%86%e5%8c%96>#</a></h3><ul><li><strong>调试模型</strong>:检查是否学到有意义的特征</li><li><strong>解释预测</strong>:理解模型为什么做出某个判断</li><li><strong>发现问题</strong>:如数据偏差、过拟合等</li></ul><h3 id=462-可视化技术>4.6.2 可视化技术<a class=anchor href=#462-%e5%8f%af%e8%a7%86%e5%8c%96%e6%8a%80%e6%9c%af>#</a></h3><p><strong>1. 卷积核可视化</strong></p><p>查看第一层卷积核学到的模式:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>visualize_filters</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>layer_name</span><span class=o>=</span><span class=s1>&#39;conv1&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;可视化第一层卷积核&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>layer</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>())[</span><span class=n>layer_name</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>  <span class=c1># [out_channels, in_channels, H, W]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 绘制前16个卷积核</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>ax</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>axes</span><span class=o>.</span><span class=n>flat</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>weights</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># 取第i个卷积核的第一个通道</span>
</span></span><span class=line><span class=cl>            <span class=n>kernel</span> <span class=o>=</span> <span class=n>weights</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>kernel</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>ax</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div><p><strong>2. 特征图可视化</strong></p><p>查看中间层的激活值:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>visualize_feature_maps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>image</span><span class=p>,</span> <span class=n>layer_name</span><span class=o>=</span><span class=s1>&#39;conv1&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;可视化某层的特征图&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>activation</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>hook</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=nb>input</span><span class=p>,</span> <span class=n>output</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>activation</span><span class=p>[</span><span class=s1>&#39;features&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 注册钩子</span>
</span></span><span class=line><span class=cl>    <span class=n>layer</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>())[</span><span class=n>layer_name</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>handle</span> <span class=o>=</span> <span class=n>layer</span><span class=o>.</span><span class=n>register_forward_hook</span><span class=p>(</span><span class=n>hook</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 可视化</span>
</span></span><span class=line><span class=cl>    <span class=n>features</span> <span class=o>=</span> <span class=n>activation</span><span class=p>[</span><span class=s1>&#39;features&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># [C, H, W]</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>ax</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>axes</span><span class=o>.</span><span class=n>flat</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>features</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>features</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>cpu</span><span class=p>(),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>ax</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>handle</span><span class=o>.</span><span class=n>remove</span><span class=p>()</span></span></span></code></pre></div><p><strong>3. 类激活图(CAM)</strong></p><p>显示模型关注图像的哪些区域:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 使用Grad-CAM等技术(后续章节详细介绍)</span></span></span></code></pre></div><hr><h2 id=47-本章小结>4.7 本章小结<a class=anchor href=#47-%e6%9c%ac%e7%ab%a0%e5%b0%8f%e7%bb%93>#</a></h2><h3 id=核心知识点-1>核心知识点<a class=anchor href=#%e6%a0%b8%e5%bf%83%e7%9f%a5%e8%af%86%e7%82%b9-1>#</a></h3><ol><li><strong>卷积层</strong>:局部连接+权值共享,大幅减少参数</li><li><strong>池化层</strong>:降低分辨率,增强不变性</li><li><strong>经典架构</strong>:LeNet(开创) → AlexNet(引爆) → VGG(深化)</li><li><strong>设计原则</strong>:<ul><li>通道数逐渐增加(64→128→256→512)</li><li>特征图尺寸逐渐减小(卷积+池化)</li><li>使用BatchNorm加速训练</li><li>数据增强提升泛化</li></ul></li></ol><h3 id=cnn-vs-mlp>CNN vs MLP<a class=anchor href=#cnn-vs-mlp>#</a></h3><table><thead><tr><th>特性</th><th>CNN</th><th>MLP</th></tr></thead><tbody><tr><td>参数量</td><td>少(权值共享)</td><td>多</td></tr><tr><td>空间信息</td><td>保留</td><td>丢失</td></tr><tr><td>平移不变性</td><td>有</td><td>无</td></tr><tr><td>适用场景</td><td>图像、视频</td><td>表格数据</td></tr></tbody></table><h3 id=检查清单-1>检查清单<a class=anchor href=#%e6%a3%80%e6%9f%a5%e6%b8%85%e5%8d%95-1>#</a></h3><ul><li><input disabled type=checkbox> 理解卷积操作的数学定义</li><li><input disabled type=checkbox> 掌握卷积核、步长、填充的作用</li><li><input disabled type=checkbox> 能计算卷积层输出尺寸</li><li><input disabled type=checkbox> 理解VGG的设计原则</li><li><input disabled type=checkbox> 成功运行CIFAR-10代码(准确率>75%)</li><li><input disabled type=checkbox> 会使用torchvision加载预训练模型</li></ul><hr><h2 id=48-练习与思考>4.8 练习与思考<a class=anchor href=#48-%e7%bb%83%e4%b9%a0%e4%b8%8e%e6%80%9d%e8%80%83>#</a></h2><h3 id=基础练习-1>基础练习<a class=anchor href=#%e5%9f%ba%e7%a1%80%e7%bb%83%e4%b9%a0-1>#</a></h3><ol><li><p><strong>修改CIFAR-10网络</strong>:</p><ul><li>增加/减少卷积块数量</li><li>调整每层通道数</li><li>观察参数量和性能变化</li></ul></li><li><p><strong>数据增强实验</strong>:</p><ul><li>对比有/无数据增强的效果</li><li>尝试不同增强策略(旋转、缩放等)</li></ul></li><li><p><strong>使用预训练模型</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.models</span> <span class=kn>import</span> <span class=n>resnet18</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>resnet18</span><span class=p>(</span><span class=n>weights</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>  <span class=c1># 从头训练</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>   <span class=c1># 修改最后一层</span></span></span></code></pre></div></li></ol><h3 id=进阶挑战-1>进阶挑战<a class=anchor href=#%e8%bf%9b%e9%98%b6%e6%8c%91%e6%88%98-1>#</a></h3><ol><li><p><strong>实现ResNet基本块</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ResidualBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>residual</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>out</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>+=</span> <span class=n>residual</span>  <span class=c1># 残差连接</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span></span></span></code></pre></div></li><li><p><strong>可视化卷积核和特征图</strong></p></li><li><p><strong>尝试其他数据集</strong>:</p><ul><li>CIFAR-100(100个类别)</li><li>Tiny ImageNet</li></ul></li></ol><h3 id=思考题-1>思考题<a class=anchor href=#%e6%80%9d%e8%80%83%e9%a2%98-1>#</a></h3><ol><li>为什么VGG使用3×3卷积而不是更大的卷积核?</li><li>什么时候应该使用池化?什么时候使用步长卷积?</li><li>如何判断网络容量是否合适?(过拟合vs欠拟合)</li></ol><hr><p><strong>恭喜!你已经掌握了CNN的核心知识。</strong></p><p>第二篇(深度学习基础)到此结束。接下来的第三篇,我们将学习现代CNN架构,包括:</p><ul><li><strong>ResNet</strong>:残差连接解决深度网络退化问题</li><li><strong>Inception</strong>:多尺度特征融合</li><li><strong>EfficientNet</strong>:神经架构搜索(NAS)</li></ul><p>这些现代架构是当前计算机视觉应用的基石,是从学术研究走向工业应用的桥梁。</p><p><strong>继续学习</strong> → <a href=../../part3_modern_cnns/README.md>第三篇:现代CNN架构</a></p><hr></article><div style="margin-top:2rem;border-top:1px solid #e5e7eb;padding-top:1rem;font-size:.85rem;color:#6b7280;text-align:center">[统计组件仅在生产环境显示]</div><div class=giscus style=margin-top:2rem></div><script src=https://giscus.app/client.js data-repo=LordFoxFairy/LordFoxFairy.github.io data-repo-id=R_kgDOQ-JRGA data-category=Announcements data-category-id=DIC_kwDOQ-JRGM4C1PDC data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN data-loading=lazy crossorigin=anonymous async></script><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/notebooks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>深度学习笔记</span>
</a></span><span></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#篇章概述>篇章概述</a><ul><li><a href=#为什么选择pytorch>为什么选择PyTorch?</a></li></ul></li><li><a href=#章节安排>章节安排</a><ul><li><a href=#第3章神经网络基础>第3章:神经网络基础</a></li><li><a href=#第4章卷积神经网络cnn>第4章:卷积神经网络(CNN)</a></li></ul></li><li><a href=#技术栈>技术栈</a><ul><li><a href=#环境要求>环境要求</a></li><li><a href=#核心依赖>核心依赖</a></li><li><a href=#验证安装>验证安装</a></li></ul></li><li><a href=#学习建议>学习建议</a><ul><li><a href=#1-动手实践为主>1. 动手实践为主</a></li><li><a href=#2-理解核心概念>2. 理解核心概念</a></li><li><a href=#3-参考官方文档>3. 参考官方文档</a></li><li><a href=#4-循序渐进>4. 循序渐进</a></li></ul></li><li><a href=#与前后篇的关系>与前后篇的关系</a></li><li><a href=#代码规范>代码规范</a><ul><li><a href=#1-目录结构>1. 目录结构</a></li><li><a href=#2-代码风格>2. 代码风格</a></li><li><a href=#3-示例模板>3. 示例模板</a></li></ul></li><li><a href=#常见问题>常见问题</a><ul><li><a href=#q1-pytorch-1x-vs-2x-主要区别>Q1: PyTorch 1.x vs 2.x 主要区别?</a></li><li><a href=#q2-需要gpu吗>Q2: 需要GPU吗?</a></li><li><a href=#q3-如何选择学习率>Q3: 如何选择学习率?</a></li><li><a href=#q4-batchnorm和dropout如何选择>Q4: BatchNorm和Dropout如何选择?</a></li></ul></li><li><a href=#拓展资源>拓展资源</a><ul><li><a href=#书籍>书籍</a></li><li><a href=#在线课程>在线课程</a></li><li><a href=#实践平台>实践平台</a></li></ul></li></ul><ul><li><a href=#31-从感知机到多层神经网络>3.1 从感知机到多层神经网络</a><ul><li><a href=#311-感知机perceptron>3.1.1 感知机(Perceptron)</a></li><li><a href=#312-多层感知机mlp>3.1.2 多层感知机(MLP)</a></li></ul></li><li><a href=#32-反向传播算法详解>3.2 反向传播算法详解</a><ul><li><a href=#321-核心思想>3.2.1 核心思想</a></li><li><a href=#322-链式法则>3.2.2 链式法则</a></li><li><a href=#323-pytorch自动微分>3.2.3 PyTorch自动微分</a></li></ul></li><li><a href=#33-激活函数的选择与影响>3.3 激活函数的选择与影响</a><ul><li><a href=#331-为什么需要激活函数>3.3.1 为什么需要激活函数?</a></li><li><a href=#332-常用激活函数>3.3.2 常用激活函数</a></li><li><a href=#333-pytorch实现>3.3.3 PyTorch实现</a></li></ul></li><li><a href=#34-正则化技术batchnorm与dropout>3.4 正则化技术:BatchNorm与Dropout</a><ul><li><a href=#341-batch-normalization>3.4.1 Batch Normalization</a></li><li><a href=#342-dropout>3.4.2 Dropout</a></li><li><a href=#343-batchnorm-vs-dropout>3.4.3 BatchNorm vs Dropout</a></li></ul></li><li><a href=#35-实战mnist手写数字识别>3.5 实战:MNIST手写数字识别</a><ul><li><a href=#351-任务描述>3.5.1 任务描述</a></li><li><a href=#352-完整代码>3.5.2 完整代码</a></li><li><a href=#353-运行与调试>3.5.3 运行与调试</a></li></ul></li><li><a href=#36-pytorch核心api总结>3.6 PyTorch核心API总结</a><ul><li><a href=#361-张量tensor操作>3.6.1 张量(Tensor)操作</a></li><li><a href=#362-nnmodule核心方法>3.6.2 nn.Module核心方法</a></li><li><a href=#363-优化器optimizer>3.6.3 优化器(Optimizer)</a></li><li><a href=#364-损失函数>3.6.4 损失函数</a></li></ul></li><li><a href=#37-本章小结>3.7 本章小结</a><ul><li><a href=#核心知识点>核心知识点</a></li><li><a href=#pytorch编程范式>PyTorch编程范式</a></li><li><a href=#检查清单>检查清单</a></li></ul></li><li><a href=#38-练习与思考>3.8 练习与思考</a><ul><li><a href=#基础练习>基础练习</a></li><li><a href=#进阶挑战>进阶挑战</a></li><li><a href=#思考题>思考题</a></li></ul></li></ul><ul><li><a href=#41-为什么需要cnn>4.1 为什么需要CNN?</a><ul><li><a href=#411-全连接网络的局限>4.1.1 全连接网络的局限</a></li><li><a href=#412-cnn的核心思想>4.1.2 CNN的核心思想</a></li></ul></li><li><a href=#42-卷积层的工作原理>4.2 卷积层的工作原理</a><ul><li><a href=#421-卷积操作>4.2.1 卷积操作</a></li><li><a href=#422-关键概念>4.2.2 关键概念</a></li><li><a href=#423-pytorch实现>4.2.3 PyTorch实现</a></li></ul></li><li><a href=#43-池化层与降维>4.3 池化层与降维</a><ul><li><a href=#431-池化操作>4.3.1 池化操作</a></li><li><a href=#432-pytorch实现>4.3.2 PyTorch实现</a></li><li><a href=#433-池化-vs-步长卷积>4.3.3 池化 vs 步长卷积</a></li></ul></li><li><a href=#44-经典cnn架构演进>4.4 经典CNN架构演进</a><ul><li><a href=#441-lenet-5-1998>4.4.1 LeNet-5 (1998)</a></li><li><a href=#442-alexnet-2012>4.4.2 AlexNet (2012)</a></li><li><a href=#443-vgg-2014>4.4.3 VGG (2014)</a></li><li><a href=#444-架构对比>4.4.4 架构对比</a></li></ul></li><li><a href=#45-实战cifar-10图像分类>4.5 实战:CIFAR-10图像分类</a><ul><li><a href=#451-任务描述>4.5.1 任务描述</a></li><li><a href=#452-网络设计>4.5.2 网络设计</a></li><li><a href=#453-完整代码>4.5.3 完整代码</a></li><li><a href=#454-数据增强>4.5.4 数据增强</a></li><li><a href=#455-运行与结果>4.5.5 运行与结果</a></li></ul></li><li><a href=#46-cnn可视化与理解>4.6 CNN可视化与理解</a><ul><li><a href=#461-为什么要可视化>4.6.1 为什么要可视化?</a></li><li><a href=#462-可视化技术>4.6.2 可视化技术</a></li></ul></li><li><a href=#47-本章小结>4.7 本章小结</a><ul><li><a href=#核心知识点-1>核心知识点</a></li><li><a href=#cnn-vs-mlp>CNN vs MLP</a></li><li><a href=#检查清单-1>检查清单</a></li></ul></li><li><a href=#48-练习与思考>4.8 练习与思考</a><ul><li><a href=#基础练习-1>基础练习</a></li><li><a href=#进阶挑战-1>进阶挑战</a></li><li><a href=#思考题-1>思考题</a></li></ul></li></ul></nav></div></aside></main></body></html>